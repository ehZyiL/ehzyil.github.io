{"meta":{"title":"ehzyil's blog","subtitle":"","description":"test description","author":"John Doe","url":"https://blog.ehzyil.xyz","root":"/"},"pages":[{"title":"","date":"2024-06-17T01:04:54.019Z","updated":"2024-06-17T01:04:54.019Z","comments":true,"path":"about/index.html","permalink":"https://blog.ehzyil.xyz/about/index.html","excerpt":"","text":"个人简介昵称：ehzyil性别：男爱好：跑步"},{"title":"所有分类","date":"2024-06-17T01:04:54.019Z","updated":"2024-06-17T01:04:54.019Z","comments":true,"path":"categories/index.html","permalink":"https://blog.ehzyil.xyz/categories/index.html","excerpt":"","text":""},{"title":"","date":"2021-09-19T04:33:48.000Z","updated":"2024-06-17T01:04:54.019Z","comments":true,"path":"friends/index.html","permalink":"https://blog.ehzyil.xyz/friends/index.html","excerpt":"友链格式。title: xxxurl: xxxavatar: xxx","text":"友链格式。title: xxxurl: xxxavatar: xxx 收藏的链接 volantis_developervolantis_developerxaoxuuhttps://github.com/xaoxuuMHuiGhttps://github.com/MHuiGinksshttps://github.com/inkssColsrchhttps://github.com/ColsrchDrew233https://github.com/Drew233Linhk1606https://github.com/Linhk1606W4J1ehttps://github.com/W4J1ecommunity_builderxaoxuuhttps://github.com/xaoxuuMHuiGhttps://github.com/MHuiGColsrchhttps://github.com/Colsrchpennduhttps://github.com/penndu有用的网站或博客收藏夹太乱,就放这里了Hexo+Qexo+Vercel建站https://hoyue.fun/hexo_qexo.html福音戰士標題生成器https://lab.magiconch.com/eva-title/MikuTools - 工具集合https://tools.miku.ac/一个小型常用shell工具箱https://www.nodeseek.com/post-21153-1从零开始搭建个人书签导航应用：Flarehttps://soulteary.com/2022/02/23/building-a-personal-bookmark-navigation-app-from-scratch-flare.htmMS365 E5 Renew Xhttps://ms-e5.hm0420.cc/User/Home在Linux上部署kuku的tg机器人https://www.kuku.me/archives/40/在线工具箱https://tools.laoda.de/一键DD系统脚本 支持国内小鸡，长期更新https://blockxyz.notion.site/DD-fb837703b3ac4011bb2362e3a56ac148萌导航网https://www.rrnav.com/xuexiMac软件收录https://www.macat.vip/SHSSEDU-开源知识社区https://shssedu.github.io/易波叶平|https://zhaouncle.com/posts/xloghttps://xlog.app/网盘直链解析API演示https://lz.qaiu.top/ 友链 volantis_developervolantis_developerxaoxuuMHuiGxaoxuuMHuiGColsrch"},{"title":"","date":"2024-06-17T01:04:54.087Z","updated":"2024-06-17T01:04:54.087Z","comments":true,"path":"shuoshuo/index.html","permalink":"https://blog.ehzyil.xyz/shuoshuo/index.html","excerpt":"","text":"已废弃"},{"title":"我的动态","date":"2024-06-17T01:04:54.087Z","updated":"2024-06-17T01:04:54.087Z","comments":true,"path":"memos/index.html","permalink":"https://blog.ehzyil.xyz/memos/index.html","excerpt":"","text":"","author":"ehzyil"},{"title":"所有标签","date":"2024-06-17T01:04:54.087Z","updated":"2024-06-17T01:04:54.087Z","comments":true,"path":"tags/index.html","permalink":"https://blog.ehzyil.xyz/tags/index.html","excerpt":"","text":""},{"title":"这是分页标题","date":"2024-06-17T01:04:54.087Z","updated":"2024-06-17T01:04:54.087Z","comments":true,"path":"wiki/stellar/index.html","permalink":"https://blog.ehzyil.xyz/wiki/stellar/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"2024/Java 业务开发常见错误 100 例/20、Spring框架：框架帮我们做了很多工作也带来了复杂度","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/20、Spring框架：框架帮我们做了很多工作也带来了复杂度/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/20%E3%80%81Spring%E6%A1%86%E6%9E%B6%EF%BC%9A%E6%A1%86%E6%9E%B6%E5%B8%AE%E6%88%91%E4%BB%AC%E5%81%9A%E4%BA%86%E5%BE%88%E5%A4%9A%E5%B7%A5%E4%BD%9C%E4%B9%9F%E5%B8%A6%E6%9D%A5%E4%BA%86%E5%A4%8D%E6%9D%82%E5%BA%A6/","excerpt":"","text":"20 | Spring框架：框架帮我们做了很多工作也带来了复杂度作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我们聊聊Spring框架给业务代码带来的复杂度，以及与之相关的坑。 在上一讲，通过AOP实现统一的监控组件的案例，我们看到了IoC和AOP配合使用的威力：当对象由Spring容器管理成为Bean之后，我们不但可以通过容器管理配置Bean的属性，还可以方便地对感兴趣的方法做AOP。 不过，前提是对象必须是Bean。你可能会觉得这个结论很明显，也很容易理解啊。但就和上一讲提到的Bean默认是单例一样，理解起来简单，实践的时候却非常容易踩坑。其中原因，一方面是，理解Spring的体系结构和使用方式有一定曲线；另一方面是，Spring多年发展堆积起来的内部结构非常复杂，这也是更重要的原因。 在我看来，Spring框架内部的复杂度主要表现为三点： 第一，Spring框架借助IoC和AOP的功能，实现了修改、拦截Bean的定义和实例的灵活性，因此真正执行的代码流程并不是串行的。 第二，Spring Boot根据当前依赖情况实现了自动配置，虽然省去了手动配置的麻烦，但也因此多了一些黑盒、提升了复杂度。 第三，Spring Cloud模块多版本也多，Spring Boot 1.x和2.x的区别也很大。如果要对Spring Cloud或Spring Boot进行二次开发的话，考虑兼容性的成本会很高。 今天，我们就通过配置AOP切入Spring Cloud Feign组件失败、Spring Boot程序的文件配置被覆盖这两个案例，感受一下Spring的复杂度。我希望这一讲的内容，能帮助你面对Spring这个复杂框架出现的问题时，可以非常自信地找到解决方案。 Feign AOP切不到的诡异案例我曾遇到过这么一个案例：使用Spring Cloud做微服务调用，为方便统一处理Feign，想到了用AOP实现，即使用within指示器匹配feign.Client接口的实现进行AOP切入。 代码如下，通过@Before注解在执行方法前打印日志，并在代码中定义了一个标记了@FeignClient注解的Client类，让其成为一个Feign接口： &#x2F;&#x2F;测试Feign @FeignClient(name &#x3D; &quot;client&quot;) public interface Client &#123; @GetMapping(&quot;&#x2F;feignaop&#x2F;server&quot;) String api(); &#125; &#x2F;&#x2F;AOP切入feign.Client的实现 @Aspect @Slf4j @Component public class WrongAspect &#123; @Before(&quot;within(feign.Client+)&quot;) public void before(JoinPoint pjp) &#123; log.info(&quot;within(feign.Client+) pjp &#123;&#125;, args:&#123;&#125;&quot;, pjp, pjp.getArgs()); &#125; &#125; &#x2F;&#x2F;配置扫描Feign @Configuration @EnableFeignClients(basePackages &#x3D; &quot;org.geekbang.time.commonmistakes.spring.demo4.feign&quot;) public class Config &#123; &#125; 通过Feign调用服务后可以看到日志中有输出，的确实现了feign.Client的切入，切入的是execute方法： [15:48:32.850] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo4.WrongAspect :20 ] - within(feign.Client+) pjp execution(Response feign.Client.execute(Request,Options)), args:[GET http:&#x2F;&#x2F;client&#x2F;feignaop&#x2F;server HTTP&#x2F;1.1 Binary data, feign.Request$Options@5c16561a] 一开始这个项目使用的是客户端的负载均衡，也就是让Ribbon来做负载均衡，代码没啥问题。后来因为后端服务通过Nginx实现服务端负载均衡，所以开发同学把@FeignClient的配置设置了URL属性，直接通过一个固定URL调用后端服务： @FeignClient(name &#x3D; &quot;anotherClient&quot;,url &#x3D; &quot;http:&#x2F;&#x2F;localhost:45678&quot;) public interface ClientWithUrl &#123; @GetMapping(&quot;&#x2F;feignaop&#x2F;server&quot;) String api(); &#125; 但这样配置后，之前的AOP切面竟然失效了，也就是within(feign.Client+)无法切入ClientWithUrl的调用了。 为了还原这个场景，我写了一段代码，定义两个方法分别通过Client和ClientWithUrl这两个Feign进行接口调用： @Autowired private Client client; @Autowired private ClientWithUrl clientWithUrl; @GetMapping(&quot;client&quot;) public String client() &#123; return client.api(); &#125; @GetMapping(&quot;clientWithUrl&quot;) public String clientWithUrl() &#123; return clientWithUrl.api(); &#125; 可以看到，调用Client后AOP有日志输出，调用ClientWithUrl后却没有： [15:50:32.850] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo4.WrongAspect :20 ] - within(feign.Client+) pjp execution(Response feign.Client.execute(Request,Options)), args:[GET http:&#x2F;&#x2F;client&#x2F;feignaop&#x2F;server HTTP&#x2F;1.1 Binary data, feign.Request$Options@5c16561 这就很费解了。难道为Feign指定了URL，其实现就不是feign.Clinet了吗？ 要明白原因，我们需要分析一下FeignClient的创建过程，也就是分析FeignClientFactoryBean类的getTarget方法。源码第4行有一个if判断，当URL没有内容也就是为空或者不配置时调用loadBalance方法，在其内部通过FeignContext从容器获取feign.Client的实例： &lt;T&gt; T getTarget() &#123; FeignContext context &#x3D; this.applicationContext.getBean(FeignContext.class); Feign.Builder builder &#x3D; feign(context); if (!StringUtils.hasText(this.url)) &#123; ... return (T) loadBalance(builder, context, new HardCodedTarget&lt;&gt;(this.type, this.name, this.url)); &#125; ... String url &#x3D; this.url + cleanPath(); Client client &#x3D; getOptional(context, Client.class); if (client !&#x3D; null) &#123; if (client instanceof LoadBalancerFeignClient) &#123; &#x2F;&#x2F; not load balancing because we have a url, &#x2F;&#x2F; but ribbon is on the classpath, so unwrap client &#x3D; ((LoadBalancerFeignClient) client).getDelegate(); &#125; builder.client(client); &#125; ... &#125; protected &lt;T&gt; T loadBalance(Feign.Builder builder, FeignContext context, HardCodedTarget&lt;T&gt; target) &#123; Client client &#x3D; getOptional(context, Client.class); if (client !&#x3D; null) &#123; builder.client(client); Targeter targeter &#x3D; get(context, Targeter.class); return targeter.target(this, builder, context, target); &#125; ... &#125; protected &lt;T&gt; T getOptional(FeignContext context, Class&lt;T&gt; type) &#123; return context.getInstance(this.contextId, type); &#125; 调试一下可以看到，client是LoadBalanceFeignClient，已经是经过代理增强的，明显是一个Bean： 所以，没有指定URL的@FeignClient对应的LoadBalanceFeignClient，是可以通过feign.Client切入的。 在我们上面贴出来的源码的16行可以看到，当URL不为空的时候，client设置为了LoadBalanceFeignClient的delegate属性。其原因注释中有提到，因为有了URL就不需要客户端负载均衡了，但因为Ribbon在classpath中，所以需要从LoadBalanceFeignClient提取出真正的Client。断点调试下可以看到，这时client是一个ApacheHttpClient： 那么，这个ApacheHttpClient是从哪里来的呢？这里，我教你一个小技巧：如果你希望知道一个类是怎样调用栈初始化的，可以在构造方法中设置一个断点进行调试。这样，你就可以在IDE的栈窗口看到整个方法调用栈，然后点击每一个栈帧看到整个过程。 用这种方式，我们可以看到，是HttpClientFeignLoadBalancedConfiguration类实例化的ApacheHttpClient： 进一步查看HttpClientFeignLoadBalancedConfiguration的源码可以发现，LoadBalancerFeignClient这个Bean在实例化的时候，new出来一个ApacheHttpClient作为delegate放到了LoadBalancerFeignClient中： @Bean @ConditionalOnMissingBean(Client.class) public Client feignClient(CachingSpringLoadBalancerFactory cachingFactory, SpringClientFactory clientFactory, HttpClient httpClient) &#123; ApacheHttpClient delegate &#x3D; new ApacheHttpClient(httpClient); return new LoadBalancerFeignClient(delegate, cachingFactory, clientFactory); &#125; public LoadBalancerFeignClient(Client delegate, CachingSpringLoadBalancerFactory lbClientFactory, SpringClientFactory clientFactory) &#123; this.delegate &#x3D; delegate; this.lbClientFactory &#x3D; lbClientFactory; this.clientFactory &#x3D; clientFactory; &#125; 显然，ApacheHttpClient是new出来的，并不是Bean，而LoadBalancerFeignClient是一个Bean。 有了这个信息，我们再来捋一下，为什么within(feign.Client+)无法切入设置过URL的@FeignClient ClientWithUrl： 表达式声明的是切入feign.Client的实现类。 Spring只能切入由自己管理的Bean。 虽然LoadBalancerFeignClient和ApacheHttpClient都是feign.Client接口的实现，但是HttpClientFeignLoadBalancedConfiguration的自动配置只是把前者定义为Bean，后者是new出来的、作为了LoadBalancerFeignClient的delegate，不是Bean。 在定义了FeignClient的URL属性后，我们获取的是LoadBalancerFeignClient的delegate，它不是Bean。 因此，定义了URL的FeignClient采用within(feign.Client+)无法切入。 那，如何解决这个问题呢？有一位同学提出，修改一下切点表达式，通过@FeignClient注解来切： @Before(&quot;@within(org.springframework.cloud.openfeign.FeignClient)&quot;) public void before(JoinPoint pjp)&#123; log.info(&quot;@within(org.springframework.cloud.openfeign.FeignClient) pjp &#123;&#125;, args:&#123;&#125;&quot;, pjp, pjp.getArgs()); &#125; 修改后通过日志看到，AOP的确切成功了： [15:53:39.093] [http-nio-45678-exec-3] [INFO ] [o.g.t.c.spring.demo4.Wrong2Aspect :17 ] - @within(org.springframework.cloud.openfeign.FeignClient) pjp execution(String org.geekbang.time.commonmistakes.spring.demo4.feign.ClientWithUrl.api()), args:[] 但仔细一看就会发现，这次切入的是ClientWithUrl接口的API方法，并不是client.Feign接口的execute方法，显然不符合预期。 这位同学犯的错误是，没有弄清楚真正希望切的是什么对象。@FeignClient注解标记在Feign Client接口上，所以切的是Feign定义的接口，也就是每一个实际的API接口。而通过feign.Client接口切的是客户端实现类，切到的是通用的、执行所有Feign调用的execute方法。 那么问题来了，ApacheHttpClient不是Bean无法切入，切Feign接口本身又不符合要求。怎么办呢？ 经过一番研究发现，ApacheHttpClient其实有机会独立成为Bean。查看HttpClientFeignConfiguration的源码可以发现，当没有ILoadBalancer类型的时候，自动装配会把ApacheHttpClient设置为Bean。 这么做的原因很明确，如果我们不希望做客户端负载均衡的话，应该不会引用Ribbon组件的依赖，自然没有LoadBalancerFeignClient，只有ApacheHttpClient： @Configuration @ConditionalOnClass(ApacheHttpClient.class) @ConditionalOnMissingClass(&quot;com.netflix.loadbalancer.ILoadBalancer&quot;) @ConditionalOnMissingBean(CloseableHttpClient.class) @ConditionalOnProperty(value &#x3D; &quot;feign.httpclient.enabled&quot;, matchIfMissing &#x3D; true) protected static class HttpClientFeignConfiguration &#123; @Bean @ConditionalOnMissingBean(Client.class) public Client feignClient(HttpClient httpClient) &#123; return new ApacheHttpClient(httpClient); &#125; &#125; 那，把pom.xml中的ribbon模块注释之后，是不是可以解决问题呢？ &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 但，问题并没解决，启动出错误了： Caused by: java.lang.IllegalArgumentException: Cannot subclass final class feign.httpclient.ApacheHttpClient at org.springframework.cglib.proxy.Enhancer.generateClass(Enhancer.java:657) at org.springframework.cglib.core.DefaultGeneratorStrategy.generate(DefaultGeneratorStrategy.java:25) 这里，又涉及了Spring实现动态代理的两种方式： JDK动态代理，通过反射实现，只支持对实现接口的类进行代理； CGLIB动态字节码注入方式，通过继承实现代理，没有这个限制。 Spring Boot 2.x默认使用CGLIB的方式，但通过继承实现代理有个问题是，无法继承final的类。因为，ApacheHttpClient类就是定义为了final： public final class ApacheHttpClient implements Client &#123; 为解决这个问题，我们把配置参数proxy-target-class的值修改为false，以切换到使用JDK动态代理的方式： spring.aop.proxy-target-class&#x3D;false 修改后执行clientWithUrl接口可以看到，通过within(feign.Client+)方式可以切入feign.Client子类了。以下日志显示了@within和within的两次切入： [16:29:55.303] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo4.Wrong2Aspect :16 ] - @within(org.springframework.cloud.openfeign.FeignClient) pjp execution(String org.geekbang.time.commonmistakes.spring.demo4.feign.ClientWithUrl.api()), args:[] [16:29:55.310] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo4.WrongAspect :15 ] - within(feign.Client+) pjp execution(Response feign.Client.execute(Request,Options)), args:[GET http:&#x2F;&#x2F;localhost:45678&#x2F;feignaop&#x2F;server HTTP&#x2F;1.1 Binary data, feign.Request$Options@387550b0] 这下我们就明白了，Spring Cloud使用了自动装配来根据依赖装配组件，组件是否成为Bean决定了AOP是否可以切入，在尝试通过AOP切入Spring Bean的时候要注意。 加上上一讲的两个案例，我就把IoC和AOP相关的坑点和你说清楚了。除此之外，我们在业务开发时，还有一个绕不开的点是，Spring程序的配置问题。接下来，我们就具体看看吧。 Spring程序配置的优先级问题我们知道，通过配置文件application.properties，可以实现Spring Boot应用程序的参数配置。但我们可能不知道的是，Spring程序配置是有优先级的，即当两个不同的配置源包含相同的配置项时，其中一个配置项很可能会被覆盖掉。这，也是为什么我们会遇到些看似诡异的配置失效问题。 我们来通过一个实际案例，研究下配置源以及配置源的优先级问题。 对于Spring Boot应用程序，一般我们会通过设置management.server.port参数，来暴露独立的actuator管理端口。这样做更安全，也更方便监控系统统一监控程序是否健康。 management.server.port&#x3D;45679 有一天程序重新发布后，监控系统显示程序离线。但排查下来发现，程序是正常工作的，只是actuator管理端口的端口号被改了，不是配置文件中定义的45679了。 后来发现，运维同学在服务器上定义了两个环境变量MANAGEMENT_SERVER_IP和MANAGEMENT_SERVER_PORT，目的是方便监控Agent把监控数据上报到统一的管理服务上： MANAGEMENT_SERVER_IP&#x3D;192.168.0.2 MANAGEMENT_SERVER_PORT&#x3D;12345 问题就是出在这里。MANAGEMENT_SERVER_PORT覆盖了配置文件中的management.server.port，修改了应用程序本身的端口。当然，监控系统也就无法通过老的管理端口访问到应用的health端口了。如下图所示，actuator的端口号变成了12345： 到这里坑还没完，为了方便用户登录，需要在页面上显示默认的管理员用户名，于是开发同学在配置文件中定义了一个user.name属性，并设置为defaultadminname： user.name&#x3D;defaultadminname 后来发现，程序读取出来的用户名根本就不是配置文件中定义的。这，又是咋回事？ 带着这个问题，以及之前环境变量覆盖配置文件配置的问题，我们写段代码看看，从Spring中到底能读取到几个management.server.port和user.name配置项。 要想查询Spring中所有的配置，我们需要以环境Environment接口为入口。接下来，我就与你说说Spring通过环境Environment抽象出的Property和Profile： 针对Property，又抽象出各种PropertySource类代表配置源。一个环境下可能有多个配置源，每个配置源中有诸多配置项。在查询配置信息时，需要按照配置源优先级进行查询。 Profile定义了场景的概念。通常，我们会定义类似dev、test、stage和prod等环境作为不同的Profile，用于按照场景对Bean进行逻辑归属。同时，Profile和配置文件也有关系，每个环境都有独立的配置文件，但我们只会激活某一个环境来生效特定环境的配置文件。 接下来，我们重点看看Property的查询过程。 对于非Web应用，Spring对于Environment接口的实现是StandardEnvironment类。我们通过Spring注入StandardEnvironment后循环getPropertySources获得的PropertySource，来查询所有的PropertySource中key是user.name或management.server.port的属性值；然后遍历getPropertySources方法，获得所有配置源并打印出来： @Autowired private StandardEnvironment env; @PostConstruct public void init()&#123; Arrays.asList(&quot;user.name&quot;, &quot;management.server.port&quot;).forEach(key -&gt; &#123; env.getPropertySources().forEach(propertySource -&gt; &#123; if (propertySource.containsProperty(key)) &#123; log.info(&quot;&#123;&#125; -&gt; &#123;&#125; 实际取值：&#123;&#125;&quot;, propertySource, propertySource.getProperty(key), env.getProperty(key)); &#125; &#125;); &#125;); System.out.println(&quot;配置优先级：&quot;); env.getPropertySources().stream().forEach(System.out::println); &#125; 我们研究下输出的日志： 2020-01-15 16:08:34.054 INFO 40123 --- [ main] o.g.t.c.s.d.CommonMistakesApplication : ConfigurationPropertySourcesPropertySource &#123;name&#x3D;&#39;configurationProperties&#39;&#125; -&gt; zhuye 实际取值：zhuye 2020-01-15 16:08:34.054 INFO 40123 --- [ main] o.g.t.c.s.d.CommonMistakesApplication : PropertiesPropertySource &#123;name&#x3D;&#39;systemProperties&#39;&#125; -&gt; zhuye 实际取值：zhuye 2020-01-15 16:08:34.054 INFO 40123 --- [ main] o.g.t.c.s.d.CommonMistakesApplication : OriginTrackedMapPropertySource &#123;name&#x3D;&#39;applicationConfig: [classpath:&#x2F;application.properties]&#39;&#125; -&gt; defaultadminname 实际取值：zhuye 2020-01-15 16:08:34.054 INFO 40123 --- [ main] o.g.t.c.s.d.CommonMistakesApplication : ConfigurationPropertySourcesPropertySource &#123;name&#x3D;&#39;configurationProperties&#39;&#125; -&gt; 12345 实际取值：12345 2020-01-15 16:08:34.054 INFO 40123 --- [ main] o.g.t.c.s.d.CommonMistakesApplication : OriginAwareSystemEnvironmentPropertySource &#123;name&#x3D;&#39;&#39;&#125; -&gt; 12345 实际取值：12345 2020-01-15 16:08:34.054 INFO 40123 --- [ main] o.g.t.c.s.d.CommonMistakesApplication : OriginTrackedMapPropertySource &#123;name&#x3D;&#39;applicationConfig: [classpath:&#x2F;application.properties]&#39;&#125; -&gt; 45679 实际取值：12345 配置优先级： ConfigurationPropertySourcesPropertySource &#123;name&#x3D;&#39;configurationProperties&#39;&#125; StubPropertySource &#123;name&#x3D;&#39;servletConfigInitParams&#39;&#125; ServletContextPropertySource &#123;name&#x3D;&#39;servletContextInitParams&#39;&#125; PropertiesPropertySource &#123;name&#x3D;&#39;systemProperties&#39;&#125; OriginAwareSystemEnvironmentPropertySource &#123;name&#x3D;&#39;systemEnvironment&#39;&#125; RandomValuePropertySource &#123;name&#x3D;&#39;random&#39;&#125; OriginTrackedMapPropertySource &#123;name&#x3D;&#39;applicationConfig: [classpath:&#x2F;application.properties]&#39;&#125; MapPropertySource &#123;name&#x3D;&#39;springCloudClientHostInfo&#39;&#125; MapPropertySource &#123;name&#x3D;&#39;defaultProperties&#39;&#125; 有三处定义了user.name：第一个是configurationProperties，值是zhuye；第二个是systemProperties，代表系统配置，值是zhuye；第三个是applicationConfig，也就是我们的配置文件，值是配置文件中定义的defaultadminname。 同样地，也有三处定义了management.server.port：第一个是configurationProperties，值是12345；第二个是systemEnvironment代表系统环境，值是12345；第三个是applicationConfig，也就是我们的配置文件，值是配置文件中定义的45679。 第7到16行的输出显示，Spring中有9个配置源，值得关注是ConfigurationPropertySourcesPropertySource、PropertiesPropertySource、OriginAwareSystemEnvironmentPropertySource和我们的配置文件。 那么，Spring真的是按这个顺序查询配置吗？最前面的configurationProperties，又是什么？为了回答这2个问题，我们需要分析下源码。我先说明下，下面源码分析的逻辑有些复杂，你可以结合着下面的整体流程图来理解： Demo中注入的StandardEnvironment，继承的是AbstractEnvironment（图中紫色类）。AbstractEnvironment的源码如下： public abstract class AbstractEnvironment implements ConfigurableEnvironment &#123; private final MutablePropertySources propertySources &#x3D; new MutablePropertySources(); private final ConfigurablePropertyResolver propertyResolver &#x3D; new PropertySourcesPropertyResolver(this.propertySources); public String getProperty(String key) &#123; return this.propertyResolver.getProperty(key); &#125; &#125; 可以看到： MutablePropertySources类型的字段propertySources，看起来代表了所有配置源； getProperty方法，通过PropertySourcesPropertyResolver类进行查询配置； 实例化PropertySourcesPropertyResolver的时候，传入了当前的MutablePropertySources。 接下来，我们继续分析MutablePropertySources和PropertySourcesPropertyResolver。先看看MutablePropertySources的源码（图中蓝色类）： public class MutablePropertySources implements PropertySources &#123; private final List&lt;PropertySource&lt;?&gt;&gt; propertySourceList &#x3D; new CopyOnWriteArrayList&lt;&gt;(); public void addFirst(PropertySource&lt;?&gt; propertySource) &#123; removeIfPresent(propertySource); this.propertySourceList.add(0, propertySource); &#125; public void addLast(PropertySource&lt;?&gt; propertySource) &#123; removeIfPresent(propertySource); this.propertySourceList.add(propertySource); &#125; public void addBefore(String relativePropertySourceName, PropertySource&lt;?&gt; propertySource) &#123; ... int index &#x3D; assertPresentAndGetIndex(relativePropertySourceName); addAtIndex(index, propertySource); &#125; public void addAfter(String relativePropertySourceName, PropertySource&lt;?&gt; propertySource) &#123; ... int index &#x3D; assertPresentAndGetIndex(relativePropertySourceName); addAtIndex(index + 1, propertySource); &#125; private void addAtIndex(int index, PropertySource&lt;?&gt; propertySource) &#123; removeIfPresent(propertySource); this.propertySourceList.add(index, propertySource); &#125; &#125; 可以发现： propertySourceList字段用来真正保存PropertySource的List，且这个List是一个CopyOnWriteArrayList。 类中定义了addFirst、addLast、addBefore、addAfter等方法，来精确控制PropertySource加入propertySourceList的顺序。这也说明了顺序的重要性。 继续看下PropertySourcesPropertyResolver（图中绿色类）的源码，找到真正查询配置的方法getProperty。 这里，我们重点看一下第9行代码：遍历的propertySources是PropertySourcesPropertyResolver构造方法传入的，再结合AbstractEnvironment的源码可以发现，这个propertySources正是AbstractEnvironment中的MutablePropertySources对象。遍历时，如果发现配置源中有对应的Key值，则使用这个值。因此，MutablePropertySources中配置源的次序尤为重要。 public class PropertySourcesPropertyResolver extends AbstractPropertyResolver &#123; private final PropertySources propertySources; public PropertySourcesPropertyResolver(@Nullable PropertySources propertySources) &#123; this.propertySources &#x3D; propertySources; &#125; protected &lt;T&gt; T getProperty(String key, Class&lt;T&gt; targetValueType, boolean resolveNestedPlaceholders) &#123; if (this.propertySources !&#x3D; null) &#123; for (PropertySource&lt;?&gt; propertySource : this.propertySources) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Searching for key &#39;&quot; + key + &quot;&#39; in PropertySource &#39;&quot; + propertySource.getName() + &quot;&#39;&quot;); &#125; Object value &#x3D; propertySource.getProperty(key); if (value !&#x3D; null) &#123; if (resolveNestedPlaceholders &amp;&amp; value instanceof String) &#123; value &#x3D; resolveNestedPlaceholders((String) value); &#125; logKeyFound(key, propertySource, value); return convertValueIfNecessary(value, targetValueType); &#125; &#125; &#125; ... &#125; &#125; 回到之前的问题，在查询所有配置源的时候，我们注意到处在第一位的是ConfigurationPropertySourcesPropertySource，这是什么呢？ 其实，它不是一个实际存在的配置源，扮演的是一个代理的角色。但通过调试你会发现，我们获取的值竟然是由它提供并且返回的，且没有循环遍历后面的PropertySource： 继续查看ConfigurationPropertySourcesPropertySource（图中红色类）的源码可以发现，getProperty方法其实是通过findConfigurationProperty方法查询配置的。如第25行代码所示，这其实还是在遍历所有的配置源： class ConfigurationPropertySourcesPropertySource extends PropertySource&lt;Iterable&lt;ConfigurationPropertySource&gt;&gt; implements OriginLookup&lt;String&gt; &#123; ConfigurationPropertySourcesPropertySource(String name, Iterable&lt;ConfigurationPropertySource&gt; source) &#123; super(name, source); &#125; @Override public Object getProperty(String name) &#123; ConfigurationProperty configurationProperty &#x3D; findConfigurationProperty(name); return (configurationProperty !&#x3D; null) ? configurationProperty.getValue() : null; &#125; private ConfigurationProperty findConfigurationProperty(String name) &#123; try &#123; return findConfigurationProperty(ConfigurationPropertyName.of(name, true)); &#125; catch (Exception ex) &#123; return null; &#125; &#125; private ConfigurationProperty findConfigurationProperty(ConfigurationPropertyName name) &#123; if (name &#x3D;&#x3D; null) &#123; return null; &#125; for (ConfigurationPropertySource configurationPropertySource : getSource()) &#123; ConfigurationProperty configurationProperty &#x3D; configurationPropertySource.getConfigurationProperty(name); if (configurationProperty !&#x3D; null) &#123; return configurationProperty; &#125; &#125; return null; &#125; &#125; 调试可以发现，这个循环遍历（getSource()的结果）的配置源，其实是SpringConfigurationPropertySources（图中黄色类），其中包含的配置源列表就是之前看到的9个配置源，而第一个就是ConfigurationPropertySourcesPropertySource。看到这里，我们的第一感觉是会不会产生死循环，它在遍历的时候怎么排除自己呢？ 同时观察configurationProperty可以看到，这个ConfigurationProperty其实类似代理的角色，实际配置是从系统属性中获得的： 继续查看SpringConfigurationPropertySources可以发现，它返回的迭代器是内部类SourcesIterator，在fetchNext方法获取下一个项时，通过isIgnored方法排除了ConfigurationPropertySourcesPropertySource（源码第38行）： class SpringConfigurationPropertySources implements Iterable&lt;ConfigurationPropertySource&gt; &#123; private final Iterable&lt;PropertySource&lt;?&gt;&gt; sources; private final Map&lt;PropertySource&lt;?&gt;, ConfigurationPropertySource&gt; cache &#x3D; new ConcurrentReferenceHashMap&lt;&gt;(16, ReferenceType.SOFT); SpringConfigurationPropertySources(Iterable&lt;PropertySource&lt;?&gt;&gt; sources) &#123; Assert.notNull(sources, &quot;Sources must not be null&quot;); this.sources &#x3D; sources; &#125; @Override public Iterator&lt;ConfigurationPropertySource&gt; iterator() &#123; return new SourcesIterator(this.sources.iterator(), this::adapt); &#125; private static class SourcesIterator implements Iterator&lt;ConfigurationPropertySource&gt; &#123; @Override public boolean hasNext() &#123; return fetchNext() !&#x3D; null; &#125; private ConfigurationPropertySource fetchNext() &#123; if (this.next &#x3D;&#x3D; null) &#123; if (this.iterators.isEmpty()) &#123; return null; &#125; if (!this.iterators.peek().hasNext()) &#123; this.iterators.pop(); return fetchNext(); &#125; PropertySource&lt;?&gt; candidate &#x3D; this.iterators.peek().next(); if (candidate.getSource() instanceof ConfigurableEnvironment) &#123; push((ConfigurableEnvironment) candidate.getSource()); return fetchNext(); &#125; if (isIgnored(candidate)) &#123; return fetchNext(); &#125; this.next &#x3D; this.adapter.apply(candidate); &#125; return this.next; &#125; private void push(ConfigurableEnvironment environment) &#123; this.iterators.push(environment.getPropertySources().iterator()); &#125; private boolean isIgnored(PropertySource&lt;?&gt; candidate) &#123; return (candidate instanceof StubPropertySource || candidate instanceof ConfigurationPropertySourcesPropertySource); &#125; &#125; &#125; 我们已经了解了ConfigurationPropertySourcesPropertySource是所有配置源中的第一个，实现了对PropertySourcesPropertyResolver中遍历逻辑的“劫持”，并且知道了其遍历逻辑。最后一个问题是，它如何让自己成为第一个配置源呢？ 再次运用之前我们学到的那个小技巧，来查看实例化ConfigurationPropertySourcesPropertySource的地方： 可以看到，ConfigurationPropertySourcesPropertySource类是由ConfigurationPropertySources的attach方法实例化的。查阅源码可以发现，这个方法的确从环境中获得了原始的MutablePropertySources，把自己加入成为一个元素： public final class ConfigurationPropertySources &#123; public static void attach(Environment environment) &#123; MutablePropertySources sources &#x3D; ((ConfigurableEnvironment) environment).getPropertySources(); PropertySource&lt;?&gt; attached &#x3D; sources.get(ATTACHED_PROPERTY_SOURCE_NAME); if (attached &#x3D;&#x3D; null) &#123; sources.addFirst(new ConfigurationPropertySourcesPropertySource(ATTACHED_PROPERTY_SOURCE_NAME, new SpringConfigurationPropertySources(sources))); &#125; &#125; &#125; 而这个attach方法，是Spring应用程序启动时准备环境的时候调用的。在SpringApplication的run方法中调用了prepareEnvironment方法，然后又调用了ConfigurationPropertySources.attach方法： public class SpringApplication &#123; public ConfigurableApplicationContext run(String... args) &#123; ... try &#123; ApplicationArguments applicationArguments &#x3D; new DefaultApplicationArguments(args); ConfigurableEnvironment environment &#x3D; prepareEnvironment(listeners, applicationArguments); ... &#125; private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; ... ConfigurationPropertySources.attach(environment); ... &#125; &#125; 看到这里你是否彻底理清楚Spring劫持PropertySourcesPropertyResolver的实现方式，以及配置源有优先级的原因了呢？如果你想知道Spring各种预定义的配置源的优先级，可以参考官方文档。 重点回顾今天，我用两个业务开发中的实际案例，带你进一步学习了Spring的AOP和配置优先级这两大知识点。现在，你应该也感受到Spring实现的复杂度了。 对于AOP切Feign的案例，我们在实现功能时走了一些弯路。Spring Cloud会使用Spring Boot的特性，根据当前引入包的情况做各种自动装配。如果我们要扩展Spring的组件，那么只有清晰了解Spring自动装配的运作方式，才能鉴别运行时对象在Spring容器中的情况，不能想当然认为代码中能看到的所有Spring的类都是Bean。 对于配置优先级的案例，分析配置源优先级时，如果我们以为看到PropertySourcesPropertyResolver就看到了真相，后续进行扩展开发时就可能会踩坑。我们一定要注意，分析Spring源码时，你看到的表象不一定是实际运行时的情况，还需要借助日志或调试工具来理清整个过程。如果没有调试工具，你可以借助第11讲用到的Arthas，来分析代码调用路径。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 除了我们这两讲用到execution、within、@within、@annotation四个指示器外，Spring AOP还支持this、target、args、@target、@args。你能说说后面五种指示器的作用吗？ Spring的Environment中的PropertySources属性可以包含多个PropertySource，越往前优先级越高。那，我们能否利用这个特点实现配置文件中属性值的自动赋值呢？比如，我们可以定义%%MYSQL.URL%%、%%MYSQL.USERNAME%%和%%MYSQL.PASSWORD%%，分别代表数据库连接字符串、用户名和密码。在配置数据源时，我们只要设置其值为占位符，框架就可以自动根据当前应用程序名application.name，统一把占位符替换为真实的数据库信息。这样，生产的数据库信息就不需要放在配置文件中了，会更安全。 关于Spring Core、Spring Boot和Spring Cloud，你还遇到过其他坑吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/21、 代码重复：搞定代码重复的三个绝招","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/21、 代码重复：搞定代码重复的三个绝招/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/21%E3%80%81%20%E4%BB%A3%E7%A0%81%E9%87%8D%E5%A4%8D%EF%BC%9A%E6%90%9E%E5%AE%9A%E4%BB%A3%E7%A0%81%E9%87%8D%E5%A4%8D%E7%9A%84%E4%B8%89%E4%B8%AA%E7%BB%9D%E6%8B%9B/","excerpt":"","text":"21 | 代码重复：搞定代码重复的三个绝招作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来和你聊聊搞定代码重复的三个绝招。 业务同学抱怨业务开发没有技术含量，用不到设计模式、Java高级特性、OOP，平时写代码都在堆CRUD，个人成长无从谈起。每次面试官问到“请说说平时常用的设计模式”，都只能答单例模式，因为其他设计模式的确是听过但没用过；对于反射、注解之类的高级特性，也只是知道它们在写框架的时候非常常用，但自己又不写框架代码，没有用武之地。 其实，我认为不是这样的。设计模式、OOP是前辈们在大型项目中积累下来的经验，通过这些方法论来改善大型项目的可维护性。反射、注解、泛型等高级特性在框架中大量使用的原因是，框架往往需要以同一套算法来应对不同的数据结构，而这些特性可以帮助减少重复代码，提升项目可维护性。 在我看来，可维护性是大型项目成熟度的一个重要指标，而提升可维护性非常重要的一个手段就是减少代码重复。那为什么这样说呢？ 如果多处重复代码实现完全相同的功能，很容易修改一处忘记修改另一处，造成Bug； 有一些代码并不是完全重复，而是相似度很高，修改这些类似的代码容易改（复制粘贴）错，把原本有区别的地方改为了一样。 今天，我就从业务代码中最常见的三个需求展开，和你聊聊如何使用Java中的一些高级特性、设计模式，以及一些工具消除重复代码，才能既优雅又高端。通过今天的学习，也希望改变你对业务代码没有技术含量的看法。 利用工厂模式+模板方法模式，消除if…else和重复代码假设要开发一个购物车下单的功能，针对不同用户进行不同处理： 普通用户需要收取运费，运费是商品价格的10%，无商品折扣； VIP用户同样需要收取商品价格10%的快递费，但购买两件以上相同商品时，第三件开始享受一定折扣； 内部用户可以免运费，无商品折扣。 我们的目标是实现三种类型的购物车业务逻辑，把入参Map对象（Key是商品ID，Value是商品数量），转换为出参购物车类型Cart。 先实现针对普通用户的购物车处理逻辑： &#x2F;&#x2F;购物车 @Data public class Cart &#123; &#x2F;&#x2F;商品清单 private List&lt;Item&gt; items &#x3D; new ArrayList&lt;&gt;(); &#x2F;&#x2F;总优惠 private BigDecimal totalDiscount; &#x2F;&#x2F;商品总价 private BigDecimal totalItemPrice; &#x2F;&#x2F;总运费 private BigDecimal totalDeliveryPrice; &#x2F;&#x2F;应付总价 private BigDecimal payPrice; &#125; &#x2F;&#x2F;购物车中的商品 @Data public class Item &#123; &#x2F;&#x2F;商品ID private long id; &#x2F;&#x2F;商品数量 private int quantity; &#x2F;&#x2F;商品单价 private BigDecimal price; &#x2F;&#x2F;商品优惠 private BigDecimal couponPrice; &#x2F;&#x2F;商品运费 private BigDecimal deliveryPrice; &#125; &#x2F;&#x2F;普通用户购物车处理 public class NormalUserCart &#123; public Cart process(long userId, Map&lt;Long, Integer&gt; items) &#123; Cart cart &#x3D; new Cart(); &#x2F;&#x2F;把Map的购物车转换为Item列表 List&lt;Item&gt; itemList &#x3D; new ArrayList&lt;&gt;(); items.entrySet().stream().forEach(entry -&gt; &#123; Item item &#x3D; new Item(); item.setId(entry.getKey()); item.setPrice(Db.getItemPrice(entry.getKey())); item.setQuantity(entry.getValue()); itemList.add(item); &#125;); cart.setItems(itemList); &#x2F;&#x2F;处理运费和商品优惠 itemList.stream().forEach(item -&gt; &#123; &#x2F;&#x2F;运费为商品总价的10% item.setDeliveryPrice(item.getPrice().multiply(BigDecimal.valueOf(item.getQuantity())).multiply(new BigDecimal(&quot;0.1&quot;))); &#x2F;&#x2F;无优惠 item.setCouponPrice(BigDecimal.ZERO); &#125;); &#x2F;&#x2F;计算商品总价 cart.setTotalItemPrice(cart.getItems().stream().map(item -&gt; item.getPrice().multiply(BigDecimal.valueOf(item.getQuantity()))).reduce(BigDecimal.ZERO, BigDecimal::add)); &#x2F;&#x2F;计算运费总价 cart.setTotalDeliveryPrice(cart.getItems().stream().map(Item::getDeliveryPrice).reduce(BigDecimal.ZERO, BigDecimal::add)); &#x2F;&#x2F;计算总优惠 cart.setTotalDiscount(cart.getItems().stream().map(Item::getCouponPrice).reduce(BigDecimal.ZERO, BigDecimal::add)); &#x2F;&#x2F;应付总价&#x3D;商品总价+运费总价-总优惠 cart.setPayPrice(cart.getTotalItemPrice().add(cart.getTotalDeliveryPrice()).subtract(cart.getTotalDiscount())); return cart; &#125; &#125; 然后实现针对VIP用户的购物车逻辑。与普通用户购物车逻辑的不同在于，VIP用户能享受同类商品多买的折扣。所以，这部分代码只需要额外处理多买折扣部分： public class VipUserCart &#123; public Cart process(long userId, Map&lt;Long, Integer&gt; items) &#123; ... itemList.stream().forEach(item -&gt; &#123; &#x2F;&#x2F;运费为商品总价的10% item.setDeliveryPrice(item.getPrice().multiply(BigDecimal.valueOf(item.getQuantity())).multiply(new BigDecimal(&quot;0.1&quot;))); &#x2F;&#x2F;购买两件以上相同商品，第三件开始享受一定折扣 if (item.getQuantity() &gt; 2) &#123; item.setCouponPrice(item.getPrice() .multiply(BigDecimal.valueOf(100 - Db.getUserCouponPercent(userId)).divide(new BigDecimal(&quot;100&quot;))) .multiply(BigDecimal.valueOf(item.getQuantity() - 2))); &#125; else &#123; item.setCouponPrice(BigDecimal.ZERO); &#125; &#125;); ... return cart; &#125; &#125; 最后是免运费、无折扣的内部用户，同样只是处理商品折扣和运费时的逻辑差异： public class InternalUserCart &#123; public Cart process(long userId, Map&lt;Long, Integer&gt; items) &#123; ... itemList.stream().forEach(item -&gt; &#123; &#x2F;&#x2F;免运费 item.setDeliveryPrice(BigDecimal.ZERO); &#x2F;&#x2F;无优惠 item.setCouponPrice(BigDecimal.ZERO); &#125;); ... return cart; &#125; &#125; 对比一下代码量可以发现，三种购物车70%的代码是重复的。原因很简单，虽然不同类型用户计算运费和优惠的方式不同，但整个购物车的初始化、统计总价、总运费、总优惠和支付价格的逻辑都是一样的。 正如我们开始时提到的，代码重复本身不可怕，可怕的是漏改或改错。比如，写VIP用户购物车的同学发现商品总价计算有Bug，不应该是把所有Item的price加在一起，而是应该把所有Item的price*quantity加在一起。这时，他可能会只修改VIP用户购物车的代码，而忽略了普通用户、内部用户的购物车中，重复的逻辑实现也有相同的Bug。 有了三个购物车后，我们就需要根据不同的用户类型使用不同的购物车了。如下代码所示，使用三个if实现不同类型用户调用不同购物车的process方法： @GetMapping(&quot;wrong&quot;) public Cart wrong(@RequestParam(&quot;userId&quot;) int userId) &#123; &#x2F;&#x2F;根据用户ID获得用户类型 String userCategory &#x3D; Db.getUserCategory(userId); &#x2F;&#x2F;普通用户处理逻辑 if (userCategory.equals(&quot;Normal&quot;)) &#123; NormalUserCart normalUserCart &#x3D; new NormalUserCart(); return normalUserCart.process(userId, items); &#125; &#x2F;&#x2F;VIP用户处理逻辑 if (userCategory.equals(&quot;Vip&quot;)) &#123; VipUserCart vipUserCart &#x3D; new VipUserCart(); return vipUserCart.process(userId, items); &#125; &#x2F;&#x2F;内部用户处理逻辑 if (userCategory.equals(&quot;Internal&quot;)) &#123; InternalUserCart internalUserCart &#x3D; new InternalUserCart(); return internalUserCart.process(userId, items); &#125; return null; &#125; 电商的营销玩法是多样的，以后势必还会有更多用户类型，需要更多的购物车。我们就只能不断增加更多的购物车类，一遍一遍地写重复的购物车逻辑、写更多的if逻辑吗？ 当然不是，相同的代码应该只在一处出现！ 如果我们熟记抽象类和抽象方法的定义的话，这时或许就会想到，是否可以把重复的逻辑定义在抽象类中，三个购物车只要分别实现不同的那份逻辑呢？ 其实，这个模式就是模板方法模式。我们在父类中实现了购物车处理的流程模板，然后把需要特殊处理的地方留空白也就是留抽象方法定义，让子类去实现其中的逻辑。由于父类的逻辑不完整无法单独工作，因此需要定义为抽象类。 如下代码所示，AbstractCart抽象类实现了购物车通用的逻辑，额外定义了两个抽象方法让子类去实现。其中，processCouponPrice方法用于计算商品折扣，processDeliveryPrice方法用于计算运费。 public abstract class AbstractCart &#123; &#x2F;&#x2F;处理购物车的大量重复逻辑在父类实现 public Cart process(long userId, Map&lt;Long, Integer&gt; items) &#123; Cart cart &#x3D; new Cart(); List&lt;Item&gt; itemList &#x3D; new ArrayList&lt;&gt;(); items.entrySet().stream().forEach(entry -&gt; &#123; Item item &#x3D; new Item(); item.setId(entry.getKey()); item.setPrice(Db.getItemPrice(entry.getKey())); item.setQuantity(entry.getValue()); itemList.add(item); &#125;); cart.setItems(itemList); &#x2F;&#x2F;让子类处理每一个商品的优惠 itemList.stream().forEach(item -&gt; &#123; processCouponPrice(userId, item); processDeliveryPrice(userId, item); &#125;); &#x2F;&#x2F;计算商品总价 cart.setTotalItemPrice(cart.getItems().stream().map(item -&gt; item.getPrice().multiply(BigDecimal.valueOf(item.getQuantity()))).reduce(BigDecimal.ZERO, BigDecimal::add)); &#x2F;&#x2F;计算总运费 cart.setTotalDeliveryPrice(cart.getItems().stream().map(Item::getDeliveryPrice).reduce(BigDecimal.ZERO, BigDecimal::add)); &#x2F;&#x2F;计算总折扣 cart.setTotalDiscount(cart.getItems().stream().map(Item::getCouponPrice).reduce(BigDecimal.ZERO, BigDecimal::add)); &#x2F;&#x2F;计算应付价格 cart.setPayPrice(cart.getTotalItemPrice().add(cart.getTotalDeliveryPrice()).subtract(cart.getTotalDiscount())); return cart; &#125; &#x2F;&#x2F;处理商品优惠的逻辑留给子类实现 protected abstract void processCouponPrice(long userId, Item item); &#x2F;&#x2F;处理配送费的逻辑留给子类实现 protected abstract void processDeliveryPrice(long userId, Item item); &#125; 有了这个抽象类，三个子类的实现就非常简单了。普通用户的购物车NormalUserCart，实现的是0优惠和10%运费的逻辑： @Service(value &#x3D; &quot;NormalUserCart&quot;) public class NormalUserCart extends AbstractCart &#123; @Override protected void processCouponPrice(long userId, Item item) &#123; item.setCouponPrice(BigDecimal.ZERO); &#125; @Override protected void processDeliveryPrice(long userId, Item item) &#123; item.setDeliveryPrice(item.getPrice() .multiply(BigDecimal.valueOf(item.getQuantity())) .multiply(new BigDecimal(&quot;0.1&quot;))); &#125; &#125; VIP用户的购物车VipUserCart，直接继承了NormalUserCart，只需要修改多买优惠策略： @Service(value &#x3D; &quot;VipUserCart&quot;) public class VipUserCart extends NormalUserCart &#123; @Override protected void processCouponPrice(long userId, Item item) &#123; if (item.getQuantity() &gt; 2) &#123; item.setCouponPrice(item.getPrice() .multiply(BigDecimal.valueOf(100 - Db.getUserCouponPercent(userId)).divide(new BigDecimal(&quot;100&quot;))) .multiply(BigDecimal.valueOf(item.getQuantity() - 2))); &#125; else &#123; item.setCouponPrice(BigDecimal.ZERO); &#125; &#125; &#125; 内部用户购物车InternalUserCart是最简单的，直接设置0运费和0折扣即可： @Service(value &#x3D; &quot;InternalUserCart&quot;) public class InternalUserCart extends AbstractCart &#123; @Override protected void processCouponPrice(long userId, Item item) &#123; item.setCouponPrice(BigDecimal.ZERO); &#125; @Override protected void processDeliveryPrice(long userId, Item item) &#123; item.setDeliveryPrice(BigDecimal.ZERO); &#125; &#125; 抽象类和三个子类的实现关系图，如下所示： 是不是比三个独立的购物车程序简单了很多呢？接下来，我们再看看如何能避免三个if逻辑。 或许你已经注意到了，定义三个购物车子类时，我们在@Service注解中对Bean进行了命名。既然三个购物车都叫XXXUserCart，那我们就可以把用户类型字符串拼接UserCart构成购物车Bean的名称，然后利用Spring的IoC容器，通过Bean的名称直接获取到AbstractCart，调用其process方法即可实现通用。 其实，这就是工厂模式，只不过是借助Spring容器实现罢了： @GetMapping(&quot;right&quot;) public Cart right(@RequestParam(&quot;userId&quot;) int userId) &#123; String userCategory &#x3D; Db.getUserCategory(userId); AbstractCart cart &#x3D; (AbstractCart) applicationContext.getBean(userCategory + &quot;UserCart&quot;); return cart.process(userId, items); &#125; 试想， 之后如果有了新的用户类型、新的用户逻辑，是不是完全不用对代码做任何修改，只要新增一个XXXUserCart类继承AbstractCart，实现特殊的优惠和运费处理逻辑就可以了？ 这样一来，我们就利用工厂模式+模板方法模式，不仅消除了重复代码，还避免了修改既有代码的风险。这就是设计模式中的开闭原则：对修改关闭，对扩展开放。 利用注解+反射消除重复代码是不是有点兴奋了，业务代码居然也能OOP了。我们再看一个三方接口的调用案例，同样也是一个普通的业务逻辑。 假设银行提供了一些API接口，对参数的序列化有点特殊，不使用JSON，而是需要我们把参数依次拼在一起构成一个大字符串。 按照银行提供的API文档的顺序，把所有参数构成定长的数据，然后拼接在一起作为整个字符串。 因为每一种参数都有固定长度，未达到长度时需要做填充处理： - 字符串类型的参数不满长度部分需要以下划线右填充，也就是字符串内容靠左； 数字类型的参数不满长度部分以0左填充，也就是实际数字靠右； 货币类型的表示需要把金额向下舍入2位到分，以分为单位，作为数字类型同样进行左填充。 对所有参数做MD5操作作为签名（为了方便理解，Demo中不涉及加盐处理）。 比如，创建用户方法和支付方法的定义是这样的： 代码很容易实现，直接根据接口定义实现填充操作、加签名、请求调用操作即可： public class BankService &#123; &#x2F;&#x2F;创建用户方法 public static String createUser(String name, String identity, String mobile, int age) throws IOException &#123; StringBuilder stringBuilder &#x3D; new StringBuilder(); &#x2F;&#x2F;字符串靠左，多余的地方填充_ stringBuilder.append(String.format(&quot;%-10s&quot;, name).replace(&#39; &#39;, &#39;_&#39;)); &#x2F;&#x2F;字符串靠左，多余的地方填充_ stringBuilder.append(String.format(&quot;%-18s&quot;, identity).replace(&#39; &#39;, &#39;_&#39;)); &#x2F;&#x2F;数字靠右，多余的地方用0填充 stringBuilder.append(String.format(&quot;%05d&quot;, age)); &#x2F;&#x2F;字符串靠左，多余的地方用_填充 stringBuilder.append(String.format(&quot;%-11s&quot;, mobile).replace(&#39; &#39;, &#39;_&#39;)); &#x2F;&#x2F;最后加上MD5作为签名 stringBuilder.append(DigestUtils.md2Hex(stringBuilder.toString())); return Request.Post(&quot;http:&#x2F;&#x2F;localhost:45678&#x2F;reflection&#x2F;bank&#x2F;createUser&quot;) .bodyString(stringBuilder.toString(), ContentType.APPLICATION_JSON) .execute().returnContent().asString(); &#125; &#x2F;&#x2F;支付方法 public static String pay(long userId, BigDecimal amount) throws IOException &#123; StringBuilder stringBuilder &#x3D; new StringBuilder(); &#x2F;&#x2F;数字靠右，多余的地方用0填充 stringBuilder.append(String.format(&quot;%020d&quot;, userId)); &#x2F;&#x2F;金额向下舍入2位到分，以分为单位，作为数字靠右，多余的地方用0填充 stringBuilder.append(String.format(&quot;%010d&quot;, amount.setScale(2, RoundingMode.DOWN).multiply(new BigDecimal(&quot;100&quot;)).longValue())); &#x2F;&#x2F;最后加上MD5作为签名 stringBuilder.append(DigestUtils.md2Hex(stringBuilder.toString())); return Request.Post(&quot;http:&#x2F;&#x2F;localhost:45678&#x2F;reflection&#x2F;bank&#x2F;pay&quot;) .bodyString(stringBuilder.toString(), ContentType.APPLICATION_JSON) .execute().returnContent().asString(); &#125; &#125; 可以看到，这段代码的重复粒度更细： 三种标准数据类型的处理逻辑有重复，稍有不慎就会出现Bug； 处理流程中字符串拼接、加签和发请求的逻辑，在所有方法重复； 实际方法的入参的参数类型和顺序，不一定和接口要求一致，容易出错； 代码层面针对每一个参数硬编码，无法清晰地进行核对，如果参数达到几十个、上百个，出错的概率极大。 那应该如何改造这段代码呢？没错，就是要用注解和反射！ 使用注解和反射这两个武器，就可以针对银行请求的所有逻辑均使用一套代码实现，不会出现任何重复。 要实现接口逻辑和逻辑实现的剥离，首先需要以POJO类（只有属性没有任何业务逻辑的数据类）的方式定义所有的接口参数。比如，下面这个创建用户API的参数： @Data public class CreateUserAPI &#123; private String name; private String identity; private String mobile; private int age; &#125; 有了接口参数定义，我们就能通过自定义注解为接口和所有参数增加一些元数据。如下所示，我们定义一个接口API的注解BankAPI，包含接口URL地址和接口说明： @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Documented @Inherited public @interface BankAPI &#123; String desc() default &quot;&quot;; String url() default &quot;&quot;; &#125; 然后，我们再定义一个自定义注解@BankAPIField，用于描述接口的每一个字段规范，包含参数的次序、类型和长度三个属性： @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.FIELD) @Documented @Inherited public @interface BankAPIField &#123; int order() default -1; int length() default -1; String type() default &quot;&quot;; &#125; 接下来，注解就可以发挥威力了。 如下所示，我们定义了CreateUserAPI类描述创建用户接口的信息，通过为接口增加@BankAPI注解，来补充接口的URL和描述等元数据；通过为每一个字段增加@BankAPIField注解，来补充参数的顺序、类型和长度等元数据： @BankAPI(url &#x3D; &quot;&#x2F;bank&#x2F;createUser&quot;, desc &#x3D; &quot;创建用户接口&quot;) @Data public class CreateUserAPI extends AbstractAPI &#123; @BankAPIField(order &#x3D; 1, type &#x3D; &quot;S&quot;, length &#x3D; 10) private String name; @BankAPIField(order &#x3D; 2, type &#x3D; &quot;S&quot;, length &#x3D; 18) private String identity; @BankAPIField(order &#x3D; 4, type &#x3D; &quot;S&quot;, length &#x3D; 11) &#x2F;&#x2F;注意这里的order需要按照API表格中的顺序 private String mobile; @BankAPIField(order &#x3D; 3, type &#x3D; &quot;N&quot;, length &#x3D; 5) private int age; &#125; 另一个PayAPI类也是类似的实现： @BankAPI(url &#x3D; &quot;&#x2F;bank&#x2F;pay&quot;, desc &#x3D; &quot;支付接口&quot;) @Data public class PayAPI extends AbstractAPI &#123; @BankAPIField(order &#x3D; 1, type &#x3D; &quot;N&quot;, length &#x3D; 20) private long userId; @BankAPIField(order &#x3D; 2, type &#x3D; &quot;M&quot;, length &#x3D; 10) private BigDecimal amount; &#125; 这2个类继承的AbstractAPI类是一个空实现，因为这个案例中的接口并没有公共数据可以抽象放到基类。 通过这2个类，我们可以在几秒钟内完成和API清单表格的核对。理论上，如果我们的核心翻译过程（也就是把注解和接口API序列化为请求需要的字符串的过程）没问题，只要注解和表格一致，API请求的翻译就不会有任何问题。 以上，我们通过注解实现了对API参数的描述。接下来，我们再看看反射如何配合注解实现动态的接口参数组装： 第3行代码中，我们从类上获得了BankAPI注解，然后拿到其URL属性，后续进行远程调用。 第6~9行代码，使用stream快速实现了获取类中所有带BankAPIField注解的字段，并把字段按order属性排序，然后设置私有字段反射可访问。 第12~38行代码，实现了反射获取注解的值，然后根据BankAPIField拿到的参数类型，按照三种标准进行格式化，将所有参数的格式化逻辑集中在了这一处。 第41~48行代码，实现了参数加签和请求调用。 private static String remoteCall(AbstractAPI api) throws IOException &#123; &#x2F;&#x2F;从BankAPI注解获取请求地址 BankAPI bankAPI &#x3D; api.getClass().getAnnotation(BankAPI.class); bankAPI.url(); StringBuilder stringBuilder &#x3D; new StringBuilder(); Arrays.stream(api.getClass().getDeclaredFields()) &#x2F;&#x2F;获得所有字段 .filter(field -&gt; field.isAnnotationPresent(BankAPIField.class)) &#x2F;&#x2F;查找标记了注解的字段 .sorted(Comparator.comparingInt(a -&gt; a.getAnnotation(BankAPIField.class).order())) &#x2F;&#x2F;根据注解中的order对字段排序 .peek(field -&gt; field.setAccessible(true)) &#x2F;&#x2F;设置可以访问私有字段 .forEach(field -&gt; &#123; &#x2F;&#x2F;获得注解 BankAPIField bankAPIField &#x3D; field.getAnnotation(BankAPIField.class); Object value &#x3D; &quot;&quot;; try &#123; &#x2F;&#x2F;反射获取字段值 value &#x3D; field.get(api); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#x2F;&#x2F;根据字段类型以正确的填充方式格式化字符串 switch (bankAPIField.type()) &#123; case &quot;S&quot;: &#123; stringBuilder.append(String.format(&quot;%-&quot; + bankAPIField.length() + &quot;s&quot;, value.toString()).replace(&#39; &#39;, &#39;_&#39;)); break; &#125; case &quot;N&quot;: &#123; stringBuilder.append(String.format(&quot;%&quot; + bankAPIField.length() + &quot;s&quot;, value.toString()).replace(&#39; &#39;, &#39;0&#39;)); break; &#125; case &quot;M&quot;: &#123; if (!(value instanceof BigDecimal)) throw new RuntimeException(String.format(&quot;&#123;&#125; 的 &#123;&#125; 必须是BigDecimal&quot;, api, field)); stringBuilder.append(String.format(&quot;%0&quot; + bankAPIField.length() + &quot;d&quot;, ((BigDecimal) value).setScale(2, RoundingMode.DOWN).multiply(new BigDecimal(&quot;100&quot;)).longValue())); break; &#125; default: break; &#125; &#125;); &#x2F;&#x2F;签名逻辑 stringBuilder.append(DigestUtils.md2Hex(stringBuilder.toString())); String param &#x3D; stringBuilder.toString(); long begin &#x3D; System.currentTimeMillis(); &#x2F;&#x2F;发请求 String result &#x3D; Request.Post(&quot;http:&#x2F;&#x2F;localhost:45678&#x2F;reflection&quot; + bankAPI.url()) .bodyString(param, ContentType.APPLICATION_JSON) .execute().returnContent().asString(); log.info(&quot;调用银行API &#123;&#125; url:&#123;&#125; 参数:&#123;&#125; 耗时:&#123;&#125;ms&quot;, bankAPI.desc(), bankAPI.url(), param, System.currentTimeMillis() - begin); return result; &#125; 可以看到，所有处理参数排序、填充、加签、请求调用的核心逻辑，都汇聚在了remoteCall方法中。有了这个核心方法，BankService中每一个接口的实现就非常简单了，只是参数的组装，然后调用remoteCall即可。 &#x2F;&#x2F;创建用户方法 public static String createUser(String name, String identity, String mobile, int age) throws IOException &#123; CreateUserAPI createUserAPI &#x3D; new CreateUserAPI(); createUserAPI.setName(name); createUserAPI.setIdentity(identity); createUserAPI.setAge(age); createUserAPI.setMobile(mobile); return remoteCall(createUserAPI); &#125; &#x2F;&#x2F;支付方法 public static String pay(long userId, BigDecimal amount) throws IOException &#123; PayAPI payAPI &#x3D; new PayAPI(); payAPI.setUserId(userId); payAPI.setAmount(amount); return remoteCall(payAPI); &#125; 其实，许多涉及类结构性的通用处理，都可以按照这个模式来减少重复代码。反射给予了我们在不知晓类结构的时候，按照固定的逻辑处理类的成员；而注解给了我们为这些成员补充元数据的能力，使得我们利用反射实现通用逻辑的时候，可以从外部获得更多我们关心的数据。 利用属性拷贝工具消除重复代码最后，我们再来看一种业务代码中经常出现的代码逻辑，实体之间的转换复制。 对于三层架构的系统，考虑到层之间的解耦隔离以及每一层对数据的不同需求，通常每一层都会有自己的POJO作为数据实体。比如，数据访问层的实体一般叫作DataObject或DO，业务逻辑层的实体一般叫作Domain，表现层的实体一般叫作Data Transfer Object或DTO。 这里我们需要注意的是，如果手动写这些实体之间的赋值代码，同样容易出错。 对于复杂的业务系统，实体有几十甚至几百个属性也很正常。就比如ComplicatedOrderDTO这个数据传输对象，描述的是一个订单中的几十个属性。如果我们要把这个DTO转换为一个类似的DO，复制其中大部分的字段，然后把数据入库，势必需要进行很多属性映射赋值操作。就像这样，密密麻麻的代码是不是已经让你头晕了？ ComplicatedOrderDTO orderDTO &#x3D; new ComplicatedOrderDTO(); ComplicatedOrderDO orderDO &#x3D; new ComplicatedOrderDO(); orderDO.setAcceptDate(orderDTO.getAcceptDate()); orderDO.setAddress(orderDTO.getAddress()); orderDO.setAddressId(orderDTO.getAddressId()); orderDO.setCancelable(orderDTO.isCancelable()); orderDO.setCommentable(orderDTO.isComplainable()); &#x2F;&#x2F;属性错误 orderDO.setComplainable(orderDTO.isCommentable()); &#x2F;&#x2F;属性错误 orderDO.setCancelable(orderDTO.isCancelable()); orderDO.setCouponAmount(orderDTO.getCouponAmount()); orderDO.setCouponId(orderDTO.getCouponId()); orderDO.setCreateDate(orderDTO.getCreateDate()); orderDO.setDirectCancelable(orderDTO.isDirectCancelable()); orderDO.setDeliverDate(orderDTO.getDeliverDate()); orderDO.setDeliverGroup(orderDTO.getDeliverGroup()); orderDO.setDeliverGroupOrderStatus(orderDTO.getDeliverGroupOrderStatus()); orderDO.setDeliverMethod(orderDTO.getDeliverMethod()); orderDO.setDeliverPrice(orderDTO.getDeliverPrice()); orderDO.setDeliveryManId(orderDTO.getDeliveryManId()); orderDO.setDeliveryManMobile(orderDO.getDeliveryManMobile()); &#x2F;&#x2F;对象错误 orderDO.setDeliveryManName(orderDTO.getDeliveryManName()); orderDO.setDistance(orderDTO.getDistance()); orderDO.setExpectDate(orderDTO.getExpectDate()); orderDO.setFirstDeal(orderDTO.isFirstDeal()); orderDO.setHasPaid(orderDTO.isHasPaid()); orderDO.setHeadPic(orderDTO.getHeadPic()); orderDO.setLongitude(orderDTO.getLongitude()); orderDO.setLatitude(orderDTO.getLongitude()); &#x2F;&#x2F;属性赋值错误 orderDO.setMerchantAddress(orderDTO.getMerchantAddress()); orderDO.setMerchantHeadPic(orderDTO.getMerchantHeadPic()); orderDO.setMerchantId(orderDTO.getMerchantId()); orderDO.setMerchantAddress(orderDTO.getMerchantAddress()); orderDO.setMerchantName(orderDTO.getMerchantName()); orderDO.setMerchantPhone(orderDTO.getMerchantPhone()); orderDO.setOrderNo(orderDTO.getOrderNo()); orderDO.setOutDate(orderDTO.getOutDate()); orderDO.setPayable(orderDTO.isPayable()); orderDO.setPaymentAmount(orderDTO.getPaymentAmount()); orderDO.setPaymentDate(orderDTO.getPaymentDate()); orderDO.setPaymentMethod(orderDTO.getPaymentMethod()); orderDO.setPaymentTimeLimit(orderDTO.getPaymentTimeLimit()); orderDO.setPhone(orderDTO.getPhone()); orderDO.setRefundable(orderDTO.isRefundable()); orderDO.setRemark(orderDTO.getRemark()); orderDO.setStatus(orderDTO.getStatus()); orderDO.setTotalQuantity(orderDTO.getTotalQuantity()); orderDO.setUpdateTime(orderDTO.getUpdateTime()); orderDO.setName(orderDTO.getName()); orderDO.setUid(orderDTO.getUid()); 如果不是代码中有注释，你能看出其中的诸多问题吗？ 如果原始的DTO有100个字段，我们需要复制90个字段到DO中，保留10个不赋值，最后应该如何校验正确性呢？数数吗？即使数出有90行代码，也不一定正确，因为属性可能重复赋值。 有的时候字段命名相近，比如complainable和commentable，容易搞反（第7和第8行），或者对两个目标字段重复赋值相同的来源字段（比如第28行） 明明要把DTO的值赋值到DO中，却在set的时候从DO自己取值（比如第20行），导致赋值无效。 这段代码并不是我随手写出来的，而是一个真实案例。有位同学就像代码中那样把经纬度赋值反了，因为落库的字段实在太多了。这个Bug很久都没发现，直到真正用到数据库中的经纬度做计算时，才发现一直以来都存错了。 修改方法很简单，可以使用类似BeanUtils这种Mapping工具来做Bean的转换，copyProperties方法还允许我们提供需要忽略的属性： ComplicatedOrderDTO orderDTO &#x3D; new ComplicatedOrderDTO(); ComplicatedOrderDO orderDO &#x3D; new ComplicatedOrderDO(); BeanUtils.copyProperties(orderDTO, orderDO, &quot;id&quot;); return orderDO; 重点回顾正所谓“常在河边走哪有不湿鞋”，重复代码多了总有一天会出错。今天，我从几个最常见的维度，和你分享了几个实际业务场景中可能出现的重复问题，以及消除重复的方式。 第一种代码重复是，有多个并行的类实现相似的代码逻辑。我们可以考虑提取相同逻辑在父类中实现，差异逻辑通过抽象方法留给子类实现。使用类似的模板方法把相同的流程和逻辑固定成模板，保留差异的同时尽可能避免代码重复。同时，可以使用Spring的IoC特性注入相应的子类，来避免实例化子类时的大量if…else代码。 第二种代码重复是，使用硬编码的方式重复实现相同的数据处理算法。我们可以考虑把规则转换为自定义注解，作为元数据对类或对字段、方法进行描述，然后通过反射动态读取这些元数据、字段或调用方法，实现规则参数和规则定义的分离。也就是说，把变化的部分也就是规则的参数放入注解，规则的定义统一处理。 第三种代码重复是，业务代码中常见的DO、DTO、VO转换时大量字段的手动赋值，遇到有上百个属性的复杂类型，非常非常容易出错。我的建议是，不要手动进行赋值，考虑使用Bean映射工具进行。此外，还可以考虑采用单元测试对所有字段进行赋值正确性校验。 最后，我想说的是，我会把代码重复度作为评估一个项目质量的重要指标，如果一个项目几乎没有任何重复代码，那么它内部的抽象一定是非常好的。在做项目重构的时候，你也可以以消除重复为第一目标去考虑实现。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 除了模板方法设计模式是减少重复代码的一把好手，观察者模式也常用于减少代码重复（并且是松耦合方式）。Spring也提供了类似工具（点击这里查看），你能想到有哪些应用场景吗？ 关于Bean属性复制工具，除了最简单的Spring的BeanUtils工具类的使用，你还知道哪些对象映射类库吗？它们又有什么功能呢？ 你还有哪些消除重复代码的心得和方法吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/22、接口设计：系统间对话的语言，一定要统一","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/22、接口设计：系统间对话的语言，一定要统一/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/22%E3%80%81%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1%EF%BC%9A%E7%B3%BB%E7%BB%9F%E9%97%B4%E5%AF%B9%E8%AF%9D%E7%9A%84%E8%AF%AD%E8%A8%80%EF%BC%8C%E4%B8%80%E5%AE%9A%E8%A6%81%E7%BB%9F%E4%B8%80/","excerpt":"","text":"22 | 接口设计：系统间对话的语言，一定要统一作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我要和你分享的主题是，在做接口设计时一定要确保系统之间对话的语言是统一的。 我们知道，开发一个服务的第一步就是设计接口。接口的设计需要考虑的点非常多，比如接口的命名、参数列表、包装结构体、接口粒度、版本策略、幂等性实现、同步异步处理方式等。 这其中，和接口设计相关比较重要的点有三个，分别是包装结构体、版本策略、同步异步处理方式。今天，我就通过我遇到的实际案例，和你一起看看因为接口设计思路和调用方理解不一致所导致的问题，以及相关的实践经验。 接口的响应要明确表示接口的处理结果我曾遇到过一个处理收单的收单中心项目，下单接口返回的响应体中，包含了success、code、info、message等属性，以及二级嵌套对象data结构体。在对项目进行重构的时候，我们发现真的是无从入手，接口缺少文档，代码一有改动就出错。 有时候，下单操作的响应结果是这样的：success是true、message是OK，貌似代表下单成功了；但info里却提示订单存在风险，code是一个5001的错误码，data中能看到订单状态是Cancelled，订单ID是-1，好像又说明没有下单成功。 &#123; &quot;success&quot;: true, &quot;code&quot;: 5001, &quot;info&quot;: &quot;Risk order detected&quot;, &quot;message&quot;: &quot;OK&quot;, &quot;data&quot;: &#123; &quot;orderStatus&quot;: &quot;Cancelled&quot;, &quot;orderId&quot;: -1 &#125; &#125; 有些时候，这个下单接口又会返回这样的结果：success是false，message提示非法用户ID，看上去下单失败；但data里的orderStatus是Created、info是空、code是0。那么，这次下单到底是成功还是失败呢？ &#123; &quot;success&quot;: false, &quot;code&quot;: 0, &quot;info&quot;: &quot;&quot;, &quot;message&quot;: &quot;Illegal userId&quot;, &quot;data&quot;: &#123; &quot;orderStatus&quot;: &quot;Created&quot;, &quot;orderId&quot;: 0 &#125; &#125; 这样的结果，让我们非常疑惑： 结构体的code和HTTP响应状态码，是什么关系？ success到底代表下单成功还是失败？ info和message的区别是什么？ data中永远都有数据吗？什么时候应该去查询data？ 造成如此混乱的原因是：这个收单服务本身并不真正处理下单操作，只是做一些预校验和预处理；真正的下单操作，需要在收单服务内部调用另一个订单服务来处理；订单服务处理完成后，会返回订单状态和ID。 在一切正常的情况下，下单后的订单状态就是已创建Created，订单ID是一个大于0的数字。而结构体中的message和success，其实是收单服务的处理异常信息和处理成功与否的结果，code、info是调用订单服务的结果。 对于第一次调用，收单服务自己没问题，success是true，message是OK，但调用订单服务时却因为订单风险问题被拒绝，所以code是5001，info是Risk order detected，data中的信息是订单服务返回的，所以最终订单状态是Cancelled。 对于第二次调用，因为用户ID非法，所以收单服务在校验了参数后直接就返回了success是false，message是Illegal userId。因为请求没有到订单服务，所以info、code、data都是默认值，订单状态的默认值是Created。因此，第二次下单肯定失败了，但订单状态却是已创建。 可以看到，如此混乱的接口定义和实现方式，是无法让调用者分清到底应该怎么处理的。为了将接口设计得更合理，我们需要考虑如下两个原则： 对外隐藏内部实现。虽然说收单服务调用订单服务进行真正的下单操作，但是直接接口其实是收单服务提供的，收单服务不应该“直接”暴露其背后订单服务的状态码、错误描述。 设计接口结构时，明确每个字段的含义，以及客户端的处理方式。 基于这两个原则，我们调整一下返回结构体，去掉外层的info，即不再把订单服务的调用结果告知客户端： @Data public class APIResponse&lt;T&gt; &#123; private boolean success; private T data; private int code; private String message; &#125; 并明确接口的设计逻辑： 如果出现非200的HTTP响应状态码，就代表请求没有到收单服务，可能是网络出问题、网络超时，或者网络配置的问题。这时，肯定无法拿到服务端的响应体，客户端可以给予友好提示，比如让用户重试，不需要继续解析响应结构体。 如果HTTP响应码是200，解析响应体查看success，为false代表下单请求处理失败，可能是因为收单服务参数验证错误，也可能是因为订单服务下单操作失败。这时，根据收单服务定义的错误码表和code，做不同处理。比如友好提示，或是让用户重新填写相关信息，其中友好提示的文字内容可以从message中获取。 success为true的情况下，才需要继续解析响应体中的data结构体。data结构体代表了业务数据，通常会有下面两种情况。 - 通常情况下，success为true时订单状态是Created，获取orderId属性可以拿到订单号。 特殊情况下，比如收单服务内部处理不当，或是订单服务出现了额外的状态，虽然success为true，但订单实际状态不是Created，这时可以给予友好的错误提示。 明确了接口的设计逻辑，我们就是可以实现收单服务的服务端和客户端来模拟这些情况了。 首先，实现服务端的逻辑： @GetMapping(&quot;server&quot;) public APIResponse&lt;OrderInfo&gt; server(@RequestParam(&quot;userId&quot;) Long userId) &#123; APIResponse&lt;OrderInfo&gt; response &#x3D; new APIResponse&lt;&gt;(); if (userId &#x3D;&#x3D; null) &#123; &#x2F;&#x2F;对于userId为空的情况，收单服务直接处理失败，给予相应的错误码和错误提示 response.setSuccess(false); response.setCode(3001); response.setMessage(&quot;Illegal userId&quot;); &#125; else if (userId &#x3D;&#x3D; 1) &#123; &#x2F;&#x2F;对于userId&#x3D;1的用户，模拟订单服务对于风险用户的情况 response.setSuccess(false); &#x2F;&#x2F;把订单服务返回的错误码转换为收单服务错误码 response.setCode(3002); response.setMessage(&quot;Internal Error, order is cancelled&quot;); &#x2F;&#x2F;同时日志记录内部错误 log.warn(&quot;用户 &#123;&#125; 调用订单服务失败，原因是 Risk order detected&quot;, userId); &#125; else &#123; &#x2F;&#x2F;其他用户，下单成功 response.setSuccess(true); response.setCode(2000); response.setMessage(&quot;OK&quot;); response.setData(new OrderInfo(&quot;Created&quot;, 2L)); &#125; return response; &#125; 客户端代码，则可以按照流程图上的逻辑来实现，同样模拟三种出错情况和正常下单的情况： error&#x3D;&#x3D;1的用例模拟一个不存在的URL，请求无法到收单服务，会得到404的HTTP状态码，直接进行友好提示，这是第一层处理。 error&#x3D;&#x3D;2的用例模拟userId参数为空的情况，收单服务会因为缺少userId参数提示非法用户。这时，可以把响应体中的message展示给用户，这是第二层处理。 error&#x3D;&#x3D;3的用例模拟userId为1的情况，因为用户有风险，收单服务调用订单服务出错。处理方式和之前没有任何区别，因为收单服务会屏蔽订单服务的内部错误。 但在服务端可以看到如下错误信息： [14:13:13.951] [http-nio-45678-exec-8] [WARN ] [.c.a.d.APIThreeLevelStatusController:36 ] - 用户 1 调用订单服务失败，原因是 Risk order detected error&#x3D;&#x3D;0的用例模拟正常用户，下单成功。这时可以解析data结构体提取业务结果，作为兜底，需要判断订单状态，如果不是Created则给予友好提示，否则查询orderId获得下单的订单号，这是第三层处理。 客户端的实现代码如下： @GetMapping(&quot;client&quot;) public String client(@RequestParam(value &#x3D; &quot;error&quot;, defaultValue &#x3D; &quot;0&quot;) int error) &#123; String url &#x3D; Arrays.asList(&quot;http:&#x2F;&#x2F;localhost:45678&#x2F;apiresposne&#x2F;server?userId&#x3D;2&quot;, &quot;http:&#x2F;&#x2F;localhost:45678&#x2F;apiresposne&#x2F;server2&quot;, &quot;http:&#x2F;&#x2F;localhost:45678&#x2F;apiresposne&#x2F;server?userId&#x3D;&quot;, &quot;http:&#x2F;&#x2F;localhost:45678&#x2F;apiresposne&#x2F;server?userId&#x3D;1&quot;).get(error); &#x2F;&#x2F;第一层，先看状态码，如果状态码不是200，不处理响应体 String response &#x3D; &quot;&quot;; try &#123; response &#x3D; Request.Get(url).execute().returnContent().asString(); &#125; catch (HttpResponseException e) &#123; log.warn(&quot;请求服务端出现返回非200&quot;, e); return &quot;服务器忙，请稍后再试！&quot;; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#x2F;&#x2F;状态码为200的情况下处理响应体 if (!response.equals(&quot;&quot;)) &#123; try &#123; APIResponse&lt;OrderInfo&gt; apiResponse &#x3D; objectMapper.readValue(response, new TypeReference&lt;APIResponse&lt;OrderInfo&gt;&gt;() &#123; &#125;); &#x2F;&#x2F;第二层，success是false直接提示用户 if (!apiResponse.isSuccess()) &#123; return String.format(&quot;创建订单失败，请稍后再试，错误代码： %s 错误原因：%s&quot;, apiResponse.getCode(), apiResponse.getMessage()); &#125; else &#123; &#x2F;&#x2F;第三层，往下解析OrderInfo OrderInfo orderInfo &#x3D; apiResponse.getData(); if (&quot;Created&quot;.equals(orderInfo.getStatus())) return String.format(&quot;创建订单成功，订单号是：%s，状态是：%s&quot;, orderInfo.getOrderId(), orderInfo.getStatus()); else return String.format(&quot;创建订单失败，请联系客服处理&quot;); &#125; &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; &#125; return &quot;&quot;; &#125; 相比原来混乱的接口定义和处理逻辑，改造后的代码，明确了接口每一个字段的含义，以及对于各种情况服务端的输出和客户端的处理步骤，对齐了客户端和服务端的处理逻辑。那么现在，你能回答前面那4个让人疑惑的问题了吗？ 最后分享一个小技巧。为了简化服务端代码，我们可以把包装API响应体APIResponse的工作交由框架自动完成，这样直接返回DTO OrderInfo即可。对于业务逻辑错误，可以抛出一个自定义异常： @GetMapping(&quot;server&quot;) public OrderInfo server(@RequestParam(&quot;userId&quot;) Long userId) &#123; if (userId &#x3D;&#x3D; null) &#123; throw new APIException(3001, &quot;Illegal userId&quot;); &#125; if (userId &#x3D;&#x3D; 1) &#123; ... &#x2F;&#x2F;直接抛出异常 throw new APIException(3002, &quot;Internal Error, order is cancelled&quot;); &#125; &#x2F;&#x2F;直接返回DTO return new OrderInfo(&quot;Created&quot;, 2L); &#125; 在APIException中包含错误码和错误消息： public class APIException extends RuntimeException &#123; @Getter private int errorCode; @Getter private String errorMessage; public APIException(int errorCode, String errorMessage) &#123; super(errorMessage); this.errorCode &#x3D; errorCode; this.errorMessage &#x3D; errorMessage; &#125; public APIException(Throwable cause, int errorCode, String errorMessage) &#123; super(errorMessage, cause); this.errorCode &#x3D; errorCode; this.errorMessage &#x3D; errorMessage; &#125; &#125; 然后，定义一个@RestControllerAdvice来完成自动包装响应体的工作： 通过实现ResponseBodyAdvice接口的beforeBodyWrite方法，来处理成功请求的响应体转换。 实现一个@ExceptionHandler来处理业务异常时，APIException到APIResponse的转换。 &#x2F;&#x2F;此段代码只是Demo，生产级应用还需要扩展很多细节 @RestControllerAdvice @Slf4j public class APIResponseAdvice implements ResponseBodyAdvice&lt;Object&gt; &#123; &#x2F;&#x2F;自动处理APIException，包装为APIResponse @ExceptionHandler(APIException.class) public APIResponse handleApiException(HttpServletRequest request, APIException ex) &#123; log.error(&quot;process url &#123;&#125; failed&quot;, request.getRequestURL().toString(), ex); APIResponse apiResponse &#x3D; new APIResponse(); apiResponse.setSuccess(false); apiResponse.setCode(ex.getErrorCode()); apiResponse.setMessage(ex.getErrorMessage()); return apiResponse; &#125; &#x2F;&#x2F;仅当方法或类没有标记@NoAPIResponse才自动包装 @Override public boolean supports(MethodParameter returnType, Class converterType) &#123; return returnType.getParameterType() !&#x3D; APIResponse.class &amp;&amp; AnnotationUtils.findAnnotation(returnType.getMethod(), NoAPIResponse.class) &#x3D;&#x3D; null &amp;&amp; AnnotationUtils.findAnnotation(returnType.getDeclaringClass(), NoAPIResponse.class) &#x3D;&#x3D; null; &#125; &#x2F;&#x2F;自动包装外层APIResposne响应 @Override public Object beforeBodyWrite(Object body, MethodParameter returnType, MediaType selectedContentType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; selectedConverterType, ServerHttpRequest request, ServerHttpResponse response) &#123; APIResponse apiResponse &#x3D; new APIResponse(); apiResponse.setSuccess(true); apiResponse.setMessage(&quot;OK&quot;); apiResponse.setCode(2000); apiResponse.setData(body); return apiResponse; &#125; &#125; 在这里，我们实现了一个@NoAPIResponse自定义注解。如果某些@RestController的接口不希望实现自动包装的话，可以标记这个注解： @Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;) @Retention(RetentionPolicy.RUNTIME) public @interface NoAPIResponse &#123; &#125; 在ResponseBodyAdvice的support方法中，我们排除了标记有这个注解的方法或类的自动响应体包装。比如，对于刚才我们实现的测试客户端client方法不需要包装为APIResponse，就可以标记上这个注解： @GetMapping(&quot;client&quot;) @NoAPIResponse public String client(@RequestParam(value &#x3D; &quot;error&quot;, defaultValue &#x3D; &quot;0&quot;) int error) 这样我们的业务逻辑中就不需要考虑响应体的包装，代码会更简洁。 要考虑接口变迁的版本控制策略接口不可能一成不变，需要根据业务需求不断增加内部逻辑。如果做大的功能调整或重构，涉及参数定义的变化或是参数废弃，导致接口无法向前兼容，这时接口就需要有版本的概念。在考虑接口版本策略设计时，我们需要注意的是，最好一开始就明确版本策略，并考虑在整个服务端统一版本策略。 第一，版本策略最好一开始就考虑。 既然接口总是要变迁的，那么最好一开始就确定版本策略。比如，确定是通过URL Path实现，是通过QueryString实现，还是通过HTTP头实现。这三种实现方式的代码如下： &#x2F;&#x2F;通过URL Path实现版本控制 @GetMapping(&quot;&#x2F;v1&#x2F;api&#x2F;user&quot;) public int right1()&#123; return 1; &#125; &#x2F;&#x2F;通过QueryString中的version参数实现版本控制 @GetMapping(value &#x3D; &quot;&#x2F;api&#x2F;user&quot;, params &#x3D; &quot;version&#x3D;2&quot;) public int right2(@RequestParam(&quot;version&quot;) int version) &#123; return 2; &#125; &#x2F;&#x2F;通过请求头中的X-API-VERSION参数实现版本控制 @GetMapping(value &#x3D; &quot;&#x2F;api&#x2F;user&quot;, headers &#x3D; &quot;X-API-VERSION&#x3D;3&quot;) public int right3(@RequestHeader(&quot;X-API-VERSION&quot;) int version) &#123; return 3; &#125; 这样，客户端就可以在配置中处理相关版本控制的参数，有可能实现版本的动态切换。 这三种方式中，URL Path的方式最直观也最不容易出错；QueryString不易携带，不太推荐作为公开API的版本策略；HTTP头的方式比较没有侵入性，如果仅仅是部分接口需要进行版本控制，可以考虑这种方式。 第二，版本实现方式要统一。 之前，我就遇到过一个O2O项目，需要针对商品、商店和用户实现REST接口。虽然大家约定通过URL Path方式实现API版本控制，但实现方式不统一，有的是&#x2F;api&#x2F;item&#x2F;v1，有的是&#x2F;api&#x2F;v1&#x2F;shop，还有的是&#x2F;v1&#x2F;api&#x2F;merchant： @GetMapping(&quot;&#x2F;api&#x2F;item&#x2F;v1&quot;) public void wrong1()&#123; &#125; @GetMapping(&quot;&#x2F;api&#x2F;v1&#x2F;shop&quot;) public void wrong2()&#123; &#125; @GetMapping(&quot;&#x2F;v1&#x2F;api&#x2F;merchant&quot;) public void wrong3()&#123; &#125; 显然，商品、商店和商户的接口开发同学，没有按照一致的URL格式来实现接口的版本控制。更要命的是，我们可能开发出两个URL类似接口，比如一个是&#x2F;api&#x2F;v1&#x2F;user，另一个是&#x2F;api&#x2F;user&#x2F;v1，这到底是一个接口还是两个接口呢？ 相比于在每一个接口的URL Path中设置版本号，更理想的方式是在框架层面实现统一。如果你使用Spring框架的话，可以按照下面的方式自定义RequestMappingHandlerMapping来实现。 首先，创建一个注解来定义接口的版本。@APIVersion自定义注解可以应用于方法或Controller上： @Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;) @Retention(RetentionPolicy.RUNTIME) public @interface APIVersion &#123; String[] value(); &#125; 然后，定义一个APIVersionHandlerMapping类继承RequestMappingHandlerMapping。 RequestMappingHandlerMapping的作用，是根据类或方法上的@RequestMapping来生成RequestMappingInfo的实例。我们覆盖registerHandlerMethod方法的实现，从@APIVersion自定义注解中读取版本信息，拼接上原有的、不带版本号的URL Pattern，构成新的RequestMappingInfo，来通过注解的方式为接口增加基于URL的版本号： public class APIVersionHandlerMapping extends RequestMappingHandlerMapping &#123; @Override protected boolean isHandler(Class&lt;?&gt; beanType) &#123; return AnnotatedElementUtils.hasAnnotation(beanType, Controller.class); &#125; @Override protected void registerHandlerMethod(Object handler, Method method, RequestMappingInfo mapping) &#123; Class&lt;?&gt; controllerClass &#x3D; method.getDeclaringClass(); &#x2F;&#x2F;类上的APIVersion注解 APIVersion apiVersion &#x3D; AnnotationUtils.findAnnotation(controllerClass, APIVersion.class); &#x2F;&#x2F;方法上的APIVersion注解 APIVersion methodAnnotation &#x3D; AnnotationUtils.findAnnotation(method, APIVersion.class); &#x2F;&#x2F;以方法上的注解优先 if (methodAnnotation !&#x3D; null) &#123; apiVersion &#x3D; methodAnnotation; &#125; String[] urlPatterns &#x3D; apiVersion &#x3D;&#x3D; null ? new String[0] : apiVersion.value(); PatternsRequestCondition apiPattern &#x3D; new PatternsRequestCondition(urlPatterns); PatternsRequestCondition oldPattern &#x3D; mapping.getPatternsCondition(); PatternsRequestCondition updatedFinalPattern &#x3D; apiPattern.combine(oldPattern); &#x2F;&#x2F;重新构建RequestMappingInfo mapping &#x3D; new RequestMappingInfo(mapping.getName(), updatedFinalPattern, mapping.getMethodsCondition(), mapping.getParamsCondition(), mapping.getHeadersCondition(), mapping.getConsumesCondition(), mapping.getProducesCondition(), mapping.getCustomCondition()); super.registerHandlerMethod(handler, method, mapping); &#125; &#125; 最后，也是特别容易忽略的一点，要通过实现WebMvcRegistrations接口，来生效自定义的APIVersionHandlerMapping： @SpringBootApplication public class CommonMistakesApplication implements WebMvcRegistrations &#123; ... @Override public RequestMappingHandlerMapping getRequestMappingHandlerMapping() &#123; return new APIVersionHandlerMapping(); &#125; &#125; 这样，就实现了在Controller上或接口方法上通过注解，来实现以统一的Pattern进行版本号控制： @GetMapping(value &#x3D; &quot;&#x2F;api&#x2F;user&quot;) @APIVersion(&quot;v4&quot;) public int right4() &#123; return 4; &#125; 加上注解后，访问浏览器查看效果： 使用框架来明确API版本的指定策略，不仅实现了标准化，更实现了强制的API版本控制。对上面代码略做修改，我们就可以实现不设置@APIVersion接口就给予报错提示。 接口处理方式要明确同步还是异步看到这个标题，你可能感觉不太好理解，我们直接看一个实际案例吧。 有一个文件上传服务FileService，其中一个upload文件上传接口特别慢，原因是这个上传接口在内部需要进行两步操作，首先上传原图，然后压缩后上传缩略图。如果每一步都耗时5秒的话，那么这个接口返回至少需要10秒的时间。 于是，开发同学把接口改为了异步处理，每一步操作都限定了超时时间，也就是分别把上传原文件和上传缩略图的操作提交到线程池，然后等待一定的时间： private ExecutorService threadPool &#x3D; Executors.newFixedThreadPool(2); &#x2F;&#x2F;我没有贴出两个文件上传方法uploadFile和uploadThumbnailFile的实现，它们在内部只是随机进行休眠然后返回文件名，对于本例来说不是很重要 public UploadResponse upload(UploadRequest request) &#123; UploadResponse response &#x3D; new UploadResponse(); &#x2F;&#x2F;上传原始文件任务提交到线程池处理 Future&lt;String&gt; uploadFile &#x3D; threadPool.submit(() -&gt; uploadFile(request.getFile())); &#x2F;&#x2F;上传缩略图任务提交到线程池处理 Future&lt;String&gt; uploadThumbnailFile &#x3D; threadPool.submit(() -&gt; uploadThumbnailFile(request.getFile())); &#x2F;&#x2F;等待上传原始文件任务完成，最多等待1秒 try &#123; response.setDownloadUrl(uploadFile.get(1, TimeUnit.SECONDS)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#x2F;&#x2F;等待上传缩略图任务完成，最多等待1秒 try &#123; response.setThumbnailDownloadUrl(uploadThumbnailFile.get(1, TimeUnit.SECONDS)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return response; &#125; 上传接口的请求和响应比较简单，传入二进制文件，传出原文件和缩略图下载地址： @Data public class UploadRequest &#123; private byte[] file; &#125; @Data public class UploadResponse &#123; private String downloadUrl; private String thumbnailDownloadUrl; &#125; 到这里，你能看出这种实现方式的问题是什么吗？ 从接口命名上看虽然是同步上传操作，但其内部通过线程池进行异步上传，并因为设置了较短超时所以接口整体响应挺快。但是，一旦遇到超时，接口就不能返回完整的数据，不是无法拿到原文件下载地址，就是无法拿到缩略图下载地址，接口的行为变得不可预测： 所以，这种优化接口响应速度的方式并不可取，更合理的方式是，让上传接口要么是彻底的同步处理，要么是彻底的异步处理： 所谓同步处理，接口一定是同步上传原文件和缩略图的，调用方可以自己选择调用超时，如果来得及可以一直等到上传完成，如果等不及可以结束等待，下一次再重试； 所谓异步处理，接口是两段式的，上传接口本身只是返回一个任务ID，然后异步做上传操作，上传接口响应很快，客户端需要之后再拿着任务ID调用任务查询接口查询上传的文件URL。 同步上传接口的实现代码如下，把超时的选择留给客户端： public SyncUploadResponse syncUpload(SyncUploadRequest request) &#123; SyncUploadResponse response &#x3D; new SyncUploadResponse(); response.setDownloadUrl(uploadFile(request.getFile())); response.setThumbnailDownloadUrl(uploadThumbnailFile(request.getFile())); return response; &#125; 这里的SyncUploadRequest和SyncUploadResponse类，与之前定义的UploadRequest和UploadResponse是一致的。对于接口的入参和出参DTO的命名，我比较建议的方式是，使用接口名+Request和Response后缀。 接下来，我们看看异步的上传文件接口如何实现。异步上传接口在出参上有点区别，不再返回文件URL，而是返回一个任务ID： @Data public class AsyncUploadRequest &#123; private byte[] file; &#125; @Data public class AsyncUploadResponse &#123; private String taskId; &#125; 在接口实现上，我们同样把上传任务提交到线程池处理，但是并不会同步等待任务完成，而是完成后把结果写入一个HashMap，任务查询接口通过查询这个HashMap来获得文件的URL： &#x2F;&#x2F;计数器，作为上传任务的ID private AtomicInteger atomicInteger &#x3D; new AtomicInteger(0); &#x2F;&#x2F;暂存上传操作的结果，生产代码需要考虑数据持久化 private ConcurrentHashMap&lt;String, SyncQueryUploadTaskResponse&gt; downloadUrl &#x3D; new ConcurrentHashMap&lt;&gt;(); &#x2F;&#x2F;异步上传操作 public AsyncUploadResponse asyncUpload(AsyncUploadRequest request) &#123; AsyncUploadResponse response &#x3D; new AsyncUploadResponse(); &#x2F;&#x2F;生成唯一的上传任务ID String taskId &#x3D; &quot;upload&quot; + atomicInteger.incrementAndGet(); &#x2F;&#x2F;异步上传操作只返回任务ID response.setTaskId(taskId); &#x2F;&#x2F;提交上传原始文件操作到线程池异步处理 threadPool.execute(() -&gt; &#123; String url &#x3D; uploadFile(request.getFile()); &#x2F;&#x2F;如果ConcurrentHashMap不包含Key，则初始化一个SyncQueryUploadTaskResponse，然后设置DownloadUrl downloadUrl.computeIfAbsent(taskId, id -&gt; new SyncQueryUploadTaskResponse(id)).setDownloadUrl(url); &#125;); &#x2F;&#x2F;提交上传缩略图操作到线程池异步处理 threadPool.execute(() -&gt; &#123; String url &#x3D; uploadThumbnailFile(request.getFile()); downloadUrl.computeIfAbsent(taskId, id -&gt; new SyncQueryUploadTaskResponse(id)).setThumbnailDownloadUrl(url); &#125;); return response; &#125; 文件上传查询接口则以任务ID作为入参，返回两个文件的下载地址，因为文件上传查询接口是同步的，所以直接命名为syncQueryUploadTask： &#x2F;&#x2F;syncQueryUploadTask接口入参 @Data @RequiredArgsConstructor public class SyncQueryUploadTaskRequest &#123; private final String taskId;&#x2F;&#x2F;使用上传文件任务ID查询上传结果 &#125; &#x2F;&#x2F;syncQueryUploadTask接口出参 @Data @RequiredArgsConstructor public class SyncQueryUploadTaskResponse &#123; private final String taskId; &#x2F;&#x2F;任务ID private String downloadUrl; &#x2F;&#x2F;原始文件下载URL private String thumbnailDownloadUrl; &#x2F;&#x2F;缩略图下载URL &#125; public SyncQueryUploadTaskResponse syncQueryUploadTask(SyncQueryUploadTaskRequest request) &#123; SyncQueryUploadTaskResponse response &#x3D; new SyncQueryUploadTaskResponse(request.getTaskId()); &#x2F;&#x2F;从之前定义的downloadUrl ConcurrentHashMap查询结果 response.setDownloadUrl(downloadUrl.getOrDefault(request.getTaskId(), response).getDownloadUrl()); response.setThumbnailDownloadUrl(downloadUrl.getOrDefault(request.getTaskId(), response).getThumbnailDownloadUrl()); return response; &#125; 经过改造的FileService不再提供一个看起来是同步上传，内部却是异步上传的upload方法，改为提供很明确的： 同步上传接口syncUpload； 异步上传接口asyncUpload，搭配syncQueryUploadTask查询上传结果。 使用方可以根据业务性质选择合适的方法：如果是后端批处理使用，那么可以使用同步上传，多等待一些时间问题不大；如果是面向用户的接口，那么接口响应时间不宜过长，可以调用异步上传接口，然后定时轮询上传结果，拿到结果再显示。 重点回顾今天，我针对接口设计，和你深入探讨了三个方面的问题。 第一，针对响应体的设计混乱、响应结果的不明确问题，服务端需要明确响应体每一个字段的意义，以一致的方式进行处理，并确保不透传下游服务的错误。 第二，针对接口版本控制问题，主要就是在开发接口之前明确版本控制策略，以及尽量使用统一的版本控制策略两方面。 第三，针对接口的处理方式，我认为需要明确要么是同步要么是异步。如果API列表中既有同步接口也有异步接口，那么最好直接在接口名中明确。 一个良好的接口文档不仅仅需要说明如何调用接口，更需要补充接口使用的最佳实践以及接口的SLA标准。我看到的大部分接口文档只给出了参数定义，但诸如幂等性、同步异步、缓存策略等看似内部实现相关的一些设计，其实也会影响调用方对接口的使用策略，最好也可以体现在接口文档中。 最后，我再额外提一下，对于服务端出错的时候是否返回200响应码的问题，其实一直有争论。从RESTful设计原则来看，我们应该尽量利用HTTP状态码来表达错误，但也不是这么绝对。 如果我们认为HTTP 状态码是协议层面的履约，那么当这个错误已经不涉及HTTP协议时（换句话说，服务端已经收到请求进入服务端业务处理后产生的错误），不一定需要硬套协议本身的错误码。但涉及非法URL、非法参数、没有权限等无法处理请求的情况，还是应该使用正确的响应码来应对。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在第一节的例子中，接口响应结构体中的code字段代表执行结果的错误码，对于业务特别复杂的接口，可能会有很多错误情况，code可能会有几十甚至几百个。客户端开发人员需要根据每一种错误情况逐一写if-else进行不同交互处理，会非常麻烦，你觉得有什么办法来改进吗？作为服务端，是否有必要告知客户端接口执行的错误码呢？ 在第二节的例子中，我们在类或方法上标记@APIVersion自定义注解，实现了URL方式统一的接口版本定义。你可以用类似的方式（也就是自定义RequestMappingHandlerMapping），来实现一套统一的基于请求头方式的版本控制吗？ 关于接口设计，你还遇到过其他问题吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/23、缓存设计：缓存可以锦上添花也可以落井下石","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/23、缓存设计：缓存可以锦上添花也可以落井下石/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/23%E3%80%81%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1%EF%BC%9A%E7%BC%93%E5%AD%98%E5%8F%AF%E4%BB%A5%E9%94%A6%E4%B8%8A%E6%B7%BB%E8%8A%B1%E4%B9%9F%E5%8F%AF%E4%BB%A5%E8%90%BD%E4%BA%95%E4%B8%8B%E7%9F%B3/","excerpt":"","text":"23 | 缓存设计：缓存可以锦上添花也可以落井下石作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我从设计的角度，与你聊聊缓存。 通常我们会使用更快的介质（比如内存）作为缓存，来解决较慢介质（比如磁盘）读取数据慢的问题，缓存是用空间换时间，来解决性能问题的一种架构设计模式。更重要的是，磁盘上存储的往往是原始数据，而缓存中保存的可以是面向呈现的数据。这样一来，缓存不仅仅是加快了IO，还可以减少原始数据的计算工作。 此外，缓存系统一般设计简单，功能相对单一，所以诸如Redis这种缓存系统的整体吞吐量，能达到关系型数据库的几倍甚至几十倍，因此缓存特别适用于互联网应用的高并发场景。 使用Redis做缓存虽然简单好用，但使用和设计缓存并不是set一下这么简单，需要注意缓存的同步、雪崩、并发、穿透等问题。今天，我们就来详细聊聊。 不要把Redis当作数据库通常，我们会使用Redis等分布式缓存数据库来缓存数据，但是千万别把Redis当做数据库来使用。我就见过许多案例，因为Redis中数据消失导致业务逻辑错误，并且因为没有保留原始数据，业务都无法恢复。 Redis的确具有数据持久化功能，可以实现服务重启后数据不丢失。这一点，很容易让我们误认为Redis可以作为高性能的KV数据库。 其实，从本质上来看，Redis（免费版）是一个内存数据库，所有数据保存在内存中，并且直接从内存读写数据响应操作，只不过具有数据持久化能力。所以，Redis的特点是，处理请求很快，但无法保存超过内存大小的数据。 备注：VM模式虽然可以保存超过内存大小的数据，但是因为性能原因从2.6开始已经被废弃。此外，Redis企业版提供了Redis on Flash可以实现Key+字典+热数据保存在内存中，冷数据保存在SSD中。 因此，把Redis用作缓存，我们需要注意两点。 第一，从客户端的角度来说，缓存数据的特点一定是有原始数据来源，且允许丢失，即使设置的缓存时间是1分钟，在30秒时缓存数据因为某种原因消失了，我们也要能接受。当数据丢失后，我们需要从原始数据重新加载数据，不能认为缓存系统是绝对可靠的，更不能认为缓存系统不会删除没有过期的数据。 第二，从Redis服务端的角度来说，缓存系统可以保存的数据量一定是小于原始数据的。首先，我们应该限制Redis对内存的使用量，也就是设置maxmemory参数；其次，我们应该根据数据特点，明确Redis应该以怎样的算法来驱逐数据。 从Redis的文档可以看到，常用的数据淘汰策略有： allkeys-lru，针对所有Key，优先删除最近最少使用的Key； volatile-lru，针对带有过期时间的Key，优先删除最近最少使用的Key； volatile-ttl，针对带有过期时间的Key，优先删除即将过期的Key（根据TTL的值）； allkeys-lfu（Redis 4.0以上），针对所有Key，优先删除最少使用的Key； volatile-lfu（Redis 4.0以上），针对带有过期时间的Key，优先删除最少使用的Key。 其实，这些算法是Key范围+Key选择算法的搭配组合，其中范围有allkeys和volatile两种，算法有LRU、TTL和LFU三种。接下来，我就从Key范围和算法角度，和你说说如何选择合适的驱逐算法。 首先，从算法角度来说，Redis 4.0以后推出的LFU比LRU更“实用”。试想一下，如果一个Key访问频率是1天一次，但正好在1秒前刚访问过，那么LRU可能不会选择优先淘汰这个Key，反而可能会淘汰一个5秒访问一次但最近2秒没有访问过的Key，而LFU算法不会有这个问题。而TTL会比较“头脑简单”一点，优先删除即将过期的Key，但有可能这个Key正在被大量访问。 然后，从Key范围角度来说，allkeys可以确保即使Key没有TTL也能回收，如果使用的时候客户端总是“忘记”设置缓存的过期时间，那么可以考虑使用这个系列的算法。而volatile会更稳妥一些，万一客户端把Redis当做了长效缓存使用，只是启动时候初始化一次缓存，那么一旦删除了此类没有TTL的数据，可能就会导致客户端出错。 所以，不管是使用者还是管理者都要考虑Redis的使用方式，使用者需要考虑应该以缓存的姿势来使用Redis，管理者应该为Redis设置内存限制和合适的驱逐策略，避免出现OOM。 注意缓存雪崩问题由于缓存系统的IOPS比数据库高很多，因此要特别小心短时间内大量缓存失效的情况。这种情况一旦发生，可能就会在瞬间有大量的数据需要回源到数据库查询，对数据库造成极大的压力，极限情况下甚至导致后端数据库直接崩溃。这就是我们常说的缓存失效，也叫作缓存雪崩。 从广义上说，产生缓存雪崩的原因有两种： 第一种是，缓存系统本身不可用，导致大量请求直接回源到数据库； 第二种是，应用设计层面大量的Key在同一时间过期，导致大量的数据回源。 第一种原因，主要涉及缓存系统本身高可用的配置，不属于缓存设计层面的问题，所以今天我主要和你说说如何确保大量Key不在同一时间被动过期。 程序初始化的时候放入1000条城市数据到Redis缓存中，过期时间是30秒；数据过期后从数据库获取数据然后写入缓存，每次从数据库获取数据后计数器+1；在程序启动的同时，启动一个定时任务线程每隔一秒输出计数器的值，并把计数器归零。 压测一个随机查询某城市信息的接口，观察一下数据库的QPS： @Autowired private StringRedisTemplate stringRedisTemplate; private AtomicInteger atomicInteger &#x3D; new AtomicInteger(); @PostConstruct public void wrongInit() &#123; &#x2F;&#x2F;初始化1000个城市数据到Redis，所有缓存数据有效期30秒 IntStream.rangeClosed(1, 1000).forEach(i -&gt; stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, getCityFromDb(i), 30, TimeUnit.SECONDS)); log.info(&quot;Cache init finished&quot;); &#x2F;&#x2F;每秒一次，输出数据库访问的QPS Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; &#123; log.info(&quot;DB QPS : &#123;&#125;&quot;, atomicInteger.getAndSet(0)); &#125;, 0, 1, TimeUnit.SECONDS); &#125; @GetMapping(&quot;city&quot;) public String city() &#123; &#x2F;&#x2F;随机查询一个城市 int id &#x3D; ThreadLocalRandom.current().nextInt(1000) + 1; String key &#x3D; &quot;city&quot; + id; String data &#x3D; stringRedisTemplate.opsForValue().get(key); if (data &#x3D;&#x3D; null) &#123; &#x2F;&#x2F;回源到数据库查询 data &#x3D; getCityFromDb(id); if (!StringUtils.isEmpty(data)) &#x2F;&#x2F;缓存30秒过期 stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS); &#125; return data; &#125; private String getCityFromDb(int cityId) &#123; &#x2F;&#x2F;模拟查询数据库，查一次增加计数器加一 atomicInteger.incrementAndGet(); return &quot;citydata&quot; + System.currentTimeMillis(); &#125; 使用wrk工具，设置10线程10连接压测city接口： wrk -c10 -t10 -d 100s http:&#x2F;&#x2F;localhost:45678&#x2F;cacheinvalid&#x2F;city 启动程序30秒后缓存过期，回源的数据库QPS最高达到了700多： 解决缓存Key同时大规模失效需要回源，导致数据库压力激增问题的方式有两种。 方案一，差异化缓存过期时间，不要让大量的Key在同一时间过期。比如，在初始化缓存的时候，设置缓存的过期时间是30秒+10秒以内的随机延迟（扰动值）。这样，这些Key不会集中在30秒这个时刻过期，而是会分散在30~40秒之间过期： @PostConstruct public void rightInit1() &#123; &#x2F;&#x2F;这次缓存的过期时间是30秒+10秒内的随机延迟 IntStream.rangeClosed(1, 1000).forEach(i -&gt; stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, getCityFromDb(i), 30 + ThreadLocalRandom.current().nextInt(10), TimeUnit.SECONDS)); log.info(&quot;Cache init finished&quot;); &#x2F;&#x2F;同样1秒一次输出数据库QPS： Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; &#123; log.info(&quot;DB QPS : &#123;&#125;&quot;, atomicInteger.getAndSet(0)); &#125;, 0, 1, TimeUnit.SECONDS); &#125; 修改后，缓存过期时的回源不会集中在同一秒，数据库的QPS从700多降到了最高100左右： 方案二，让缓存不主动过期。初始化缓存数据的时候设置缓存永不过期，然后启动一个后台线程30秒一次定时把所有数据更新到缓存，而且通过适当的休眠，控制从数据库更新数据的频率，降低数据库压力： @PostConstruct public void rightInit2() throws InterruptedException &#123; CountDownLatch countDownLatch &#x3D; new CountDownLatch(1); &#x2F;&#x2F;每隔30秒全量更新一次缓存 Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; &#123; IntStream.rangeClosed(1, 1000).forEach(i -&gt; &#123; String data &#x3D; getCityFromDb(i); &#x2F;&#x2F;模拟更新缓存需要一定的时间 try &#123; TimeUnit.MILLISECONDS.sleep(20); &#125; catch (InterruptedException e) &#123; &#125; if (!StringUtils.isEmpty(data)) &#123; &#x2F;&#x2F;缓存永不过期，被动更新 stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, data); &#125; &#125;); log.info(&quot;Cache update finished&quot;); &#x2F;&#x2F;启动程序的时候需要等待首次更新缓存完成 countDownLatch.countDown(); &#125;, 0, 30, TimeUnit.SECONDS); Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; &#123; log.info(&quot;DB QPS : &#123;&#125;&quot;, atomicInteger.getAndSet(0)); &#125;, 0, 1, TimeUnit.SECONDS); countDownLatch.await(); &#125; 这样修改后，虽然缓存整体更新的耗时在21秒左右，但数据库的压力会比较稳定： 关于这两种解决方案，我们需要特别注意以下三点： 方案一和方案二是截然不同的两种缓存方式，如果无法全量缓存所有数据，那么只能使用方案一； 即使使用了方案二，缓存永不过期，同样需要在查询的时候，确保有回源的逻辑。正如之前所说，我们无法确保缓存系统中的数据永不丢失。 不管是方案一还是方案二，在把数据从数据库加入缓存的时候，都需要判断来自数据库的数据是否合法，比如进行最基本的判空检查。 之前我就遇到过这样一个重大事故，某系统会在缓存中对基础数据进行长达半年的缓存，在某个时间点DBA把数据库中的原始数据进行了归档（可以认为是删除）操作。因为缓存中的数据一直在所以一开始没什么问题，但半年后的一天缓存中数据过期了，就从数据库中查询到了空数据加入缓存，爆发了大面积的事故。 这个案例说明，缓存会让我们更不容易发现原始数据的问题，所以在把数据加入缓存之前一定要校验数据，如果发现有明显异常要及时报警。 说到这里，我们再仔细看一下回源QPS超过700的截图，可以看到在并发情况下，总共1000条数据回源达到了1002次，说明有一些条目出现了并发回源。这，就是我后面要讲到的缓存并发问题。 注意缓存击穿问题在某些Key属于极端热点数据，且并发量很大的情况下，如果这个Key过期，可能会在某个瞬间出现大量的并发请求同时回源，相当于大量的并发请求直接打到了数据库。这种情况，就是我们常说的缓存击穿或缓存并发问题。 我们来重现下这个问题。在程序启动的时候，初始化一个热点数据到Redis中，过期时间设置为5秒，每隔1秒输出一下回源的QPS： @PostConstruct public void init() &#123; &#x2F;&#x2F;初始化一个热点数据到Redis中，过期时间设置为5秒 stringRedisTemplate.opsForValue().set(&quot;hotsopt&quot;, getExpensiveData(), 5, TimeUnit.SECONDS); &#x2F;&#x2F;每隔1秒输出一下回源的QPS Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; &#123; log.info(&quot;DB QPS : &#123;&#125;&quot;, atomicInteger.getAndSet(0)); &#125;, 0, 1, TimeUnit.SECONDS); &#125; @GetMapping(&quot;wrong&quot;) public String wrong() &#123; String data &#x3D; stringRedisTemplate.opsForValue().get(&quot;hotsopt&quot;); if (StringUtils.isEmpty(data)) &#123; data &#x3D; getExpensiveData(); &#x2F;&#x2F;重新加入缓存，过期时间还是5秒 stringRedisTemplate.opsForValue().set(&quot;hotsopt&quot;, data, 5, TimeUnit.SECONDS); &#125; return data; &#125; 可以看到，每隔5秒数据库都有20左右的QPS： 如果回源操作特别昂贵，那么这种并发就不能忽略不计。这时，我们可以考虑使用锁机制来限制回源的并发。比如如下代码示例，使用Redisson来获取一个基于Redis的分布式锁，在查询数据库之前先尝试获取锁： @Autowired private RedissonClient redissonClient; @GetMapping(&quot;right&quot;) public String right() &#123; String data &#x3D; stringRedisTemplate.opsForValue().get(&quot;hotsopt&quot;); if (StringUtils.isEmpty(data)) &#123; RLock locker &#x3D; redissonClient.getLock(&quot;locker&quot;); &#x2F;&#x2F;获取分布式锁 if (locker.tryLock()) &#123; try &#123; data &#x3D; stringRedisTemplate.opsForValue().get(&quot;hotsopt&quot;); &#x2F;&#x2F;双重检查，因为可能已经有一个B线程过了第一次判断，在等锁，然后A线程已经把数据写入了Redis中 if (StringUtils.isEmpty(data)) &#123; &#x2F;&#x2F;回源到数据库查询 data &#x3D; getExpensiveData(); stringRedisTemplate.opsForValue().set(&quot;hotsopt&quot;, data, 5, TimeUnit.SECONDS); &#125; &#125; finally &#123; &#x2F;&#x2F;别忘记释放，另外注意写法，获取锁后整段代码try+finally，确保unlock万无一失 locker.unlock(); &#125; &#125; &#125; return data; &#125; 这样，可以把回源到数据库的并发限制在1： 在真实的业务场景下，不一定要这么严格地使用双重检查分布式锁进行全局的并发限制，因为这样虽然可以把数据库回源并发降到最低，但也限制了缓存失效时的并发。可以考虑的方式是： 方案一，使用进程内的锁进行限制，这样每一个节点都可以以一个并发回源数据库； 方案二，不使用锁进行限制，而是使用类似Semaphore的工具限制并发数，比如限制为10，这样既限制了回源并发数不至于太大，又能使得一定量的线程可以同时回源。 注意缓存穿透问题在之前的例子中，缓存回源的逻辑都是当缓存中查不到需要的数据时，回源到数据库查询。这里容易出现的一个漏洞是，缓存中没有数据不一定代表数据没有缓存，还有一种可能是原始数据压根就不存在。 比如下面的例子。数据库中只保存有ID介于0（不含）和10000（包含）之间的用户，如果从数据库查询ID不在这个区间的用户，会得到空字符串，所以缓存中缓存的也是空字符串。如果使用ID&#x3D;0去压接口的话，从缓存中查出了空字符串，认为是缓存中没有数据回源查询，其实相当于每次都回源： @GetMapping(&quot;wrong&quot;) public String wrong(@RequestParam(&quot;id&quot;) int id) &#123; String key &#x3D; &quot;user&quot; + id; String data &#x3D; stringRedisTemplate.opsForValue().get(key); &#x2F;&#x2F;无法区分是无效用户还是缓存失效 if (StringUtils.isEmpty(data)) &#123; data &#x3D; getCityFromDb(id); stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS); &#125; return data; &#125; private String getCityFromDb(int id) &#123; atomicInteger.incrementAndGet(); &#x2F;&#x2F;注意，只有ID介于0（不含）和10000（包含）之间的用户才是有效用户，可以查询到用户信息 if (id &gt; 0 &amp;&amp; id &lt;&#x3D; 10000) return &quot;userdata&quot;; &#x2F;&#x2F;否则返回空字符串 return &quot;&quot;; &#125; 压测后数据库的QPS达到了几千： 如果这种漏洞被恶意利用的话，就会对数据库造成很大的性能压力。这就是缓存穿透。 这里需要注意，缓存穿透和缓存击穿的区别： 缓存穿透是指，缓存没有起到压力缓冲的作用； 而缓存击穿是指，缓存失效时瞬时的并发打到数据库。 解决缓存穿透有以下两种方案。 方案一，对于不存在的数据，同样设置一个特殊的Value到缓存中，比如当数据库中查出的用户信息为空的时候，设置NODATA这样具有特殊含义的字符串到缓存中。这样下次请求缓存的时候还是可以命中缓存，即直接从缓存返回结果，不查询数据库： @GetMapping(&quot;right&quot;) public String right(@RequestParam(&quot;id&quot;) int id) &#123; String key &#x3D; &quot;user&quot; + id; String data &#x3D; stringRedisTemplate.opsForValue().get(key); if (StringUtils.isEmpty(data)) &#123; data &#x3D; getCityFromDb(id); &#x2F;&#x2F;校验从数据库返回的数据是否有效 if (!StringUtils.isEmpty(data)) &#123; stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS); &#125; else &#123; &#x2F;&#x2F;如果无效，直接在缓存中设置一个NODATA，这样下次查询时即使是无效用户还是可以命中缓存 stringRedisTemplate.opsForValue().set(key, &quot;NODATA&quot;, 30, TimeUnit.SECONDS); &#125; &#125; return data; &#125; 但，这种方式可能会把大量无效的数据加入缓存中，如果担心大量无效数据占满缓存的话还可以考虑方案二，即使用布隆过滤器做前置过滤。 布隆过滤器是一种概率型数据库结构，由一个很长的二进制向量和一系列随机映射函数组成。它的原理是，当一个元素被加入集合时，通过k个散列函数将这个元素映射成一个m位bit数组中的k个点，并置为1。 检索时，我们只要看看这些点是不是都是1就（大概）知道集合中有没有它了。如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。 原理如下图所示： 布隆过滤器不保存原始值，空间效率很高，平均每一个元素占用2.4字节就可以达到万分之一的误判率。这里的误判率是指，过滤器判断值存在而实际并不存在的概率。我们可以设置布隆过滤器使用更大的存储空间，来得到更小的误判率。 你可以把所有可能的值保存在布隆过滤器中，从缓存读取数据前先过滤一次： 如果布隆过滤器认为值不存在，那么值一定是不存在的，无需查询缓存也无需查询数据库； 对于极小概率的误判请求，才会最终让非法Key的请求走到缓存或数据库。 要用上布隆过滤器，我们可以使用Google的Guava工具包提供的BloomFilter类改造一下程序：启动时，初始化一个具有所有有效用户ID的、10000个元素的BloomFilter，在从缓存查询数据之前调用其mightContain方法，来检测用户ID是否可能存在；如果布隆过滤器说值不存在，那么一定是不存在的，直接返回： private BloomFilter&lt;Integer&gt; bloomFilter; @PostConstruct public void init() &#123; &#x2F;&#x2F;创建布隆过滤器，元素数量10000，期望误判率1% bloomFilter &#x3D; BloomFilter.create(Funnels.integerFunnel(), 10000, 0.01); &#x2F;&#x2F;填充布隆过滤器 IntStream.rangeClosed(1, 10000).forEach(bloomFilter::put); &#125; @GetMapping(&quot;right2&quot;) public String right2(@RequestParam(&quot;id&quot;) int id) &#123; String data &#x3D; &quot;&quot;; &#x2F;&#x2F;通过布隆过滤器先判断 if (bloomFilter.mightContain(id)) &#123; String key &#x3D; &quot;user&quot; + id; &#x2F;&#x2F;走缓存查询 data &#x3D; stringRedisTemplate.opsForValue().get(key); if (StringUtils.isEmpty(data)) &#123; &#x2F;&#x2F;走数据库查询 data &#x3D; getCityFromDb(id); stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS); &#125; &#125; return data; &#125; 对于方案二，我们需要同步所有可能存在的值并加入布隆过滤器，这是比较麻烦的地方。如果业务规则明确的话，你也可以考虑直接根据业务规则判断值是否存在。 其实，方案二可以和方案一同时使用，即将布隆过滤器前置，对于误判的情况再保存特殊值到缓存，双重保险避免无效数据查询请求打到数据库。 注意缓存数据同步策略前面提到的3个案例，其实都属于缓存数据过期后的被动删除。在实际情况下，修改了原始数据后，考虑到缓存数据更新的及时性，我们可能会采用主动更新缓存的策略。这些策略可能是： 先更新缓存，再更新数据库； 先更新数据库，再更新缓存； 先删除缓存，再更新数据库，访问的时候按需加载数据到缓存； 先更新数据库，再删除缓存，访问的时候按需加载数据到缓存。 那么，我们应该选择哪种更新策略呢？我来和你逐一分析下这4种策略： “先更新缓存再更新数据库”策略不可行。数据库设计复杂，压力集中，数据库因为超时等原因更新操作失败的可能性较大，此外还会涉及事务，很可能因为数据库更新失败，导致缓存和数据库的数据不一致。 “先更新数据库再更新缓存”策略不可行。一是，如果线程A和B先后完成数据库更新，但更新缓存时却是B和A的顺序，那很可能会把旧数据更新到缓存中引起数据不一致；二是，我们不确定缓存中的数据是否会被访问，不一定要把所有数据都更新到缓存中去。 “先删除缓存再更新数据库，访问的时候按需加载数据到缓存”策略也不可行。在并发的情况下，很可能删除缓存后还没来得及更新数据库，就有另一个线程先读取了旧值到缓存中，如果并发量很大的话这个概率也会很大。 “先更新数据库再删除缓存，访问的时候按需加载数据到缓存”策略是最好的。虽然在极端情况下，这种策略也可能出现数据不一致的问题，但概率非常低，基本可以忽略。举一个“极端情况”的例子，比如更新数据的时间节点恰好是缓存失效的瞬间，这时A先读取到了旧值，随后在B操作数据库完成更新并且删除了缓存之后，A再把旧值加入缓存。 需要注意的是，更新数据库后删除缓存的操作可能失败，如果失败则考虑把任务加入延迟队列进行延迟重试，确保数据可以删除，缓存可以及时更新。因为删除操作是幂等的，所以即使重复删问题也不是太大，这又是删除比更新好的一个原因。 因此，针对缓存更新更推荐的方式是，缓存中的数据不由数据更新操作主动触发，统一在需要使用的时候按需加载，数据更新后及时删除缓存中的数据即可。 重点回顾今天，我主要是从设计的角度，和你分享了数据缓存的三大问题。 第一，我们不能把诸如Redis的缓存数据库完全当作数据库来使用。我们不能假设缓存始终可靠，也不能假设没有过期的数据必然可以被读取到，需要处理好缓存的回源逻辑；而且要显式设置Redis的最大内存使用和数据淘汰策略，避免出现OOM的问题。 第二，缓存的性能比数据库好很多，我们需要考虑大量请求绕过缓存直击数据库造成数据库瘫痪的各种情况。对于缓存瞬时大面积失效的缓存雪崩问题，可以通过差异化缓存过期时间解决；对于高并发的缓存Key回源问题，可以使用锁来限制回源并发数；对于不存在的数据穿透缓存的问题，可以通过布隆过滤器进行数据存在性的预判，或在缓存中也设置一个值来解决。 第三，当数据库中的数据有更新的时候，需要考虑如何确保缓存中数据的一致性。我们看到，“先更新数据库再删除缓存，访问的时候按需加载数据到缓存”的策略是最为妥当的，并且要尽量设置合适的缓存过期时间，这样即便真的发生不一致，也可以在缓存过期后数据得到及时同步。 最后，我要提醒你的是，在使用缓存系统的时候，要监控缓存系统的内存使用量、命中率、对象平均过期时间等重要指标，以便评估系统的有效性，并及时发现问题。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在聊到缓存并发问题时，我们说到热点Key回源会对数据库产生的压力问题，如果Key特别热的话，可能缓存系统也无法承受，毕竟所有的访问都集中打到了一台缓存服务器。如果我们使用Redis来做缓存，那可以把一个热点Key的缓存查询压力，分散到多个Redis节点上吗？ 大Key也是数据缓存容易出现的一个问题。如果一个Key的Value特别大，那么可能会对Redis产生巨大的性能影响，因为Redis是单线程模型，对大Key进行查询或删除等操作，可能会引起Redis阻塞甚至是高可用切换。你知道怎么查询Redis中的大Key，以及如何在设计上实现大Key的拆分吗？ 关于缓存设计，你还遇到过哪些坑呢？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/25、异步处理好用，但非常容易用错","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/25、异步处理好用，但非常容易用错/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/25%E3%80%81%E5%BC%82%E6%AD%A5%E5%A4%84%E7%90%86%E5%A5%BD%E7%94%A8%EF%BC%8C%E4%BD%86%E9%9D%9E%E5%B8%B8%E5%AE%B9%E6%98%93%E7%94%A8%E9%94%99/","excerpt":"","text":"25 | 异步处理好用，但非常容易用错作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来和你聊聊好用但容易出错的异步处理。 异步处理是互联网应用不可或缺的一种架构模式，大多数业务项目都是由同步处理、异步处理和定时任务处理三种模式相辅相成实现的。 区别于同步处理，异步处理无需同步等待流程处理完毕，因此适用场景主要包括： 服务于主流程的分支流程。比如，在注册流程中，把数据写入数据库的操作是主流程，但注册后给用户发优惠券或欢迎短信的操作是分支流程，时效性不那么强，可以进行异步处理。 用户不需要实时看到结果的流程。比如，下单后的配货、送货流程完全可以进行异步处理，每个阶段处理完成后，再给用户发推送或短信让用户知晓即可。 同时，异步处理因为可以有MQ中间件的介入用于任务的缓冲的分发，所以相比于同步处理，在应对流量洪峰、实现模块解耦和消息广播方面有功能优势。 不过，异步处理虽然好用，但在实现的时候却有三个最容易犯的错，分别是异步处理流程的可靠性问题、消息发送模式的区分问题，以及大量死信消息堵塞队列的问题。今天，我就用三个代码案例结合目前常用的MQ系统RabbitMQ，来和你具体聊聊。 今天这一讲的演示，我都会使用Spring AMQP来操作RabbitMQ，所以你需要先引入amqp依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 异步处理需要消息补偿闭环使用类似RabbitMQ、RocketMQ等MQ系统来做消息队列实现异步处理，虽然说消息可以落地到磁盘保存，即使MQ出现问题消息数据也不会丢失，但是异步流程在消息发送、传输、处理等环节，都可能发生消息丢失。此外，任何MQ中间件都无法确保100%可用，需要考虑不可用时异步流程如何继续进行。 因此，对于异步处理流程，必须考虑补偿或者说建立主备双活流程。 我们来看一个用户注册后异步发送欢迎消息的场景。用户注册落数据库的流程为同步流程，会员服务收到消息后发送欢迎消息的流程为异步流程。 我们来分析一下： 蓝色的线，使用MQ进行的异步处理，我们称作主线，可能存在消息丢失的情况（虚线代表异步调用）； 绿色的线，使用补偿Job定期进行消息补偿，我们称作备线，用来补偿主线丢失的消息； 考虑到极端的MQ中间件失效的情况，我们要求备线的处理吞吐能力达到主线的能力水平。 我们来看一下相关的实现代码。 首先，定义UserController用于注册+发送异步消息。对于注册方法，我们一次性注册10个用户，用户注册消息不能发送出去的概率为50%。 @RestController @Slf4j @RequestMapping(&quot;user&quot;) public class UserController &#123; @Autowired private UserService userService; @Autowired private RabbitTemplate rabbitTemplate; @GetMapping(&quot;register&quot;) public void register() &#123; &#x2F;&#x2F;模拟10个用户注册 IntStream.rangeClosed(1, 10).forEach(i -&gt; &#123; &#x2F;&#x2F;落库 User user &#x3D; userService.register(); &#x2F;&#x2F;模拟50%的消息可能发送失败 if (ThreadLocalRandom.current().nextInt(10) % 2 &#x3D;&#x3D; 0) &#123; &#x2F;&#x2F;通过RabbitMQ发送消息 rabbitTemplate.convertAndSend(RabbitConfiguration.EXCHANGE, RabbitConfiguration.ROUTING_KEY, user); log.info(&quot;sent mq user &#123;&#125;&quot;, user.getId()); &#125; &#125;); &#125; &#125; 然后，定义MemberService类用于模拟会员服务。会员服务监听用户注册成功的消息，并发送欢迎短信。我们使用ConcurrentHashMap来存放那些发过短信的用户ID实现幂等，避免相同的用户进行补偿时重复发送短信： @Component @Slf4j public class MemberService &#123; &#x2F;&#x2F;发送欢迎消息的状态 private Map&lt;Long, Boolean&gt; welcomeStatus &#x3D; new ConcurrentHashMap&lt;&gt;(); &#x2F;&#x2F;监听用户注册成功的消息，发送欢迎消息 @RabbitListener(queues &#x3D; RabbitConfiguration.QUEUE) public void listen(User user) &#123; log.info(&quot;receive mq user &#123;&#125;&quot;, user.getId()); welcome(user); &#125; &#x2F;&#x2F;发送欢迎消息 public void welcome(User user) &#123; &#x2F;&#x2F;去重操作 if (welcomeStatus.putIfAbsent(user.getId(), true) &#x3D;&#x3D; null) &#123; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; &#125; log.info(&quot;memberService: welcome new user &#123;&#125;&quot;, user.getId()); &#125; &#125; &#125; 对于MQ消费程序，处理逻辑务必考虑去重（支持幂等），原因有几个： MQ消息可能会因为中间件本身配置错误、稳定性等原因出现重复。 自动补偿重复，比如本例，同一条消息可能既走MQ也走补偿，肯定会出现重复，而且考虑到高内聚，补偿Job本身不会做去重处理。 人工补偿重复。出现消息堆积时，异步处理流程必然会延迟。如果我们提供了通过后台进行补偿的功能，那么在处理遇到延迟的时候，很可能会先进行人工补偿，过了一段时间后处理程序又收到消息了，重复处理。我之前就遇到过一次由MQ故障引发的事故，MQ中堆积了几十万条发放资金的消息，导致业务无法及时处理，运营以为程序出错了就先通过后台进行了人工处理，结果MQ系统恢复后消息又被重复处理了一次，造成大量资金重复发放。 接下来，定义补偿Job也就是备线操作。 我们在CompensationJob中定义一个@Scheduled定时任务，5秒做一次补偿操作，因为Job并不知道哪些用户注册的消息可能丢失，所以是全量补偿，补偿逻辑是：每5秒补偿一次，按顺序一次补偿5个用户，下一次补偿操作从上一次补偿的最后一个用户ID开始；对于补偿任务我们提交到线程池进行“异步”处理，提高处理能力。 @Component @Slf4j public class CompensationJob &#123; &#x2F;&#x2F;补偿Job异步处理线程池 private static ThreadPoolExecutor compensationThreadPool &#x3D; new ThreadPoolExecutor( 10, 10, 1, TimeUnit.HOURS, new ArrayBlockingQueue&lt;&gt;(1000), new ThreadFactoryBuilder().setNameFormat(&quot;compensation-threadpool-%d&quot;).get()); @Autowired private UserService userService; @Autowired private MemberService memberService; &#x2F;&#x2F;目前补偿到哪个用户ID private long offset &#x3D; 0; &#x2F;&#x2F;10秒后开始补偿，5秒补偿一次 @Scheduled(initialDelay &#x3D; 10_000, fixedRate &#x3D; 5_000) public void compensationJob() &#123; log.info(&quot;开始从用户ID &#123;&#125; 补偿&quot;, offset); &#x2F;&#x2F;获取从offset开始的用户 userService.getUsersAfterIdWithLimit(offset, 5).forEach(user -&gt; &#123; compensationThreadPool.execute(() -&gt; memberService.welcome(user)); offset &#x3D; user.getId(); &#125;); &#125; &#125; 为了实现高内聚，主线和备线处理消息，最好使用同一个方法。比如，本例中MemberService监听到MQ消息和CompensationJob补偿，调用的都是welcome方法。 此外值得一说的是，Demo中的补偿逻辑比较简单，生产级的代码应该在以下几个方面进行加强： 考虑配置补偿的频次、每次处理数量，以及补偿线程池大小等参数为合适的值，以满足补偿的吞吐量。 考虑备线补偿数据进行适当延迟。比如，对注册时间在30秒之前的用户再进行补偿，以方便和主线MQ实时流程错开，避免冲突。 诸如当前补偿到哪个用户的offset数据，需要落地数据库。 补偿Job本身需要高可用，可以使用类似XXLJob或ElasticJob等任务系统。 运行程序，执行注册方法注册10个用户，输出如下： [17:01:16.570] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.a.compensation.UserController:28 ] - sent mq user 1 [17:01:16.571] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.a.compensation.UserController:28 ] - sent mq user 5 [17:01:16.572] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.a.compensation.UserController:28 ] - sent mq user 7 [17:01:16.573] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.a.compensation.UserController:28 ] - sent mq user 8 [17:01:16.594] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.compensation.MemberService:18 ] - receive mq user 1 [17:01:18.597] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 1 [17:01:18.601] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.compensation.MemberService:18 ] - receive mq user 5 [17:01:20.603] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 5 [17:01:20.604] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.compensation.MemberService:18 ] - receive mq user 7 [17:01:22.605] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 7 [17:01:22.606] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.compensation.MemberService:18 ] - receive mq user 8 [17:01:24.611] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 8 [17:01:25.498] [scheduling-1] [INFO ] [o.g.t.c.a.compensation.CompensationJob:29 ] - 开始从用户ID 0 补偿 [17:01:27.510] [compensation-threadpool-1] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 2 [17:01:27.510] [compensation-threadpool-3] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 4 [17:01:27.511] [compensation-threadpool-2] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 3 [17:01:30.496] [scheduling-1] [INFO ] [o.g.t.c.a.compensation.CompensationJob:29 ] - 开始从用户ID 5 补偿 [17:01:32.500] [compensation-threadpool-6] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 6 [17:01:32.500] [compensation-threadpool-9] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 9 [17:01:35.496] [scheduling-1] [INFO ] [o.g.t.c.a.compensation.CompensationJob:29 ] - 开始从用户ID 9 补偿 [17:01:37.501] [compensation-threadpool-0] [INFO ] [o.g.t.c.a.compensation.MemberService:28 ] - memberService: welcome new user 10 [17:01:40.495] [scheduling-1] [INFO ] [o.g.t.c.a.compensation.CompensationJob:29 ] - 开始从用户ID 10 补偿 可以看到： 总共10个用户，MQ发送成功的用户有四个，分别是用户1、5、7、8。 补偿任务第一次运行，补偿了用户2、3、4，第二次运行补偿了用户6、9，第三次运行补充了用户10。 最后提一下，针对消息的补偿闭环处理的最高标准是，能够达到补偿全量数据的吞吐量。也就是说，如果补偿备线足够完善，即使直接把MQ停机，虽然会略微影响处理的及时性，但至少确保流程都能正常执行。 注意消息模式是广播还是工作队列在今天这一讲的一开始，我们提到异步处理的一个重要优势，是实现消息广播。 消息广播，和我们平时说的“广播”意思差不多，就是希望同一条消息，不同消费者都能分别消费；而队列模式，就是不同消费者共享消费同一个队列的数据，相同消息只能被某一个消费者消费一次。 比如，同一个用户的注册消息，会员服务需要监听以发送欢迎短信，营销服务同样需要监听以发送新用户小礼物。但是，会员服务、营销服务都可能有多个实例，我们期望的是同一个用户的消息，可以同时广播给不同的服务（广播模式），但对于同一个服务的不同实例（比如会员服务1和会员服务2），不管哪个实例来处理，处理一次即可（工作队列模式）： 在实现代码的时候，我们务必确认MQ系统的机制，确保消息的路由按照我们的期望。 对于类似RocketMQ这样的MQ来说，实现类似功能比较简单直白：如果消费者属于一个组，那么消息只会由同一个组的一个消费者来消费；如果消费者属于不同组，那么每个组都能消费一遍消息。 而对于RabbitMQ来说，消息路由的模式采用的是队列+交换器，队列是消息的载体，交换器决定了消息路由到队列的方式，配置比较复杂，容易出错。所以，接下来我重点和你讲讲RabbitMQ的相关代码实现。 我们还是以上面的架构图为例，来演示使用RabbitMQ实现广播模式和工作队列模式的坑。 第一步，实现会员服务监听用户服务发出的新用户注册消息的那部分逻辑。 如果我们启动两个会员服务，那么同一个用户的注册消息应该只能被其中一个实例消费。 我们分别实现RabbitMQ队列、交换器、绑定三件套。其中，队列用的是匿名队列，交换器用的是直接交换器DirectExchange，交换器绑定到匿名队列的路由Key是空字符串。在收到消息之后，我们会打印所在实例使用的端口： &#x2F;&#x2F;为了代码简洁直观，我们把消息发布者、消费者、以及MQ的配置代码都放在了一起 @Slf4j @Configuration @RestController @RequestMapping(&quot;workqueuewrong&quot;) public class WorkQueueWrong &#123; private static final String EXCHANGE &#x3D; &quot;newuserExchange&quot;; @Autowired private RabbitTemplate rabbitTemplate; @GetMapping public void sendMessage() &#123; rabbitTemplate.convertAndSend(EXCHANGE, &quot;&quot;, UUID.randomUUID().toString()); &#125; &#x2F;&#x2F;使用匿名队列作为消息队列 @Bean public Queue queue() &#123; return new AnonymousQueue(); &#125; &#x2F;&#x2F;声明DirectExchange交换器，绑定队列到交换器 @Bean public Declarables declarables() &#123; DirectExchange exchange &#x3D; new DirectExchange(EXCHANGE); return new Declarables(queue(), exchange, BindingBuilder.bind(queue()).to(exchange).with(&quot;&quot;)); &#125; &#x2F;&#x2F;监听队列，队列名称直接通过SpEL表达式引用Bean @RabbitListener(queues &#x3D; &quot;#&#123;queue.name&#125;&quot;) public void memberService(String userName) &#123; log.info(&quot;memberService: welcome message sent to new user &#123;&#125; from &#123;&#125;&quot;, userName, System.getProperty(&quot;server.port&quot;)); &#125; &#125; 使用12345和45678两个端口启动两个程序实例后，调用sendMessage接口发送一条消息，输出的日志，显示同一个会员服务两个实例都收到了消息： 出现这个问题的原因是，我们没有理清楚RabbitMQ直接交换器和队列的绑定关系。 如下图所示，RabbitMQ的直接交换器根据routingKey对消息进行路由。由于我们的程序每次启动都会创建匿名（随机命名）的队列，所以相当于每一个会员服务实例都对应独立的队列，以空routingKey绑定到直接交换器。用户服务发出消息的时候也设置了routingKey为空，所以直接交换器收到消息之后，发现有两条队列匹配，于是都转发了消息： 要修复这个问题其实很简单，对于会员服务不要使用匿名队列，而是使用同一个队列即可。把上面代码中的匿名队列替换为一个普通队列： private static final String QUEUE &#x3D; &quot;newuserQueue&quot;; @Bean public Queue queue() &#123; return new Queue(QUEUE); &#125; 测试发现，对于同一条消息来说，两个实例中只有一个实例可以收到，不同的消息按照轮询分发给不同的实例。现在，交换器和队列的关系是这样的： 第二步，进一步完整实现用户服务需要广播消息给会员服务和营销服务的逻辑。 我们希望会员服务和营销服务都可以收到广播消息，但会员服务或营销服务中的每个实例只需要收到一次消息。 代码如下，我们声明了一个队列和一个广播交换器FanoutExchange，然后模拟两个用户服务和两个营销服务： @Slf4j @Configuration @RestController @RequestMapping(&quot;fanoutwrong&quot;) public class FanoutQueueWrong &#123; private static final String QUEUE &#x3D; &quot;newuser&quot;; private static final String EXCHANGE &#x3D; &quot;newuser&quot;; @Autowired private RabbitTemplate rabbitTemplate; @GetMapping public void sendMessage() &#123; rabbitTemplate.convertAndSend(EXCHANGE, &quot;&quot;, UUID.randomUUID().toString()); &#125; &#x2F;&#x2F;声明FanoutExchange，然后绑定到队列，FanoutExchange绑定队列的时候不需要routingKey @Bean public Declarables declarables() &#123; Queue queue &#x3D; new Queue(QUEUE); FanoutExchange exchange &#x3D; new FanoutExchange(EXCHANGE); return new Declarables(queue, exchange, BindingBuilder.bind(queue).to(exchange)); &#125; &#x2F;&#x2F;会员服务实例1 @RabbitListener(queues &#x3D; QUEUE) public void memberService1(String userName) &#123; log.info(&quot;memberService1: welcome message sent to new user &#123;&#125;&quot;, userName); &#125; &#x2F;&#x2F;会员服务实例2 @RabbitListener(queues &#x3D; QUEUE) public void memberService2(String userName) &#123; log.info(&quot;memberService2: welcome message sent to new user &#123;&#125;&quot;, userName); &#125; &#x2F;&#x2F;营销服务实例1 @RabbitListener(queues &#x3D; QUEUE) public void promotionService1(String userName) &#123; log.info(&quot;promotionService1: gift sent to new user &#123;&#125;&quot;, userName); &#125; &#x2F;&#x2F;营销服务实例2 @RabbitListener(queues &#x3D; QUEUE) public void promotionService2(String userName) &#123; log.info(&quot;promotionService2: gift sent to new user &#123;&#125;&quot;, userName); &#125; &#125; 我们请求四次sendMessage接口，注册四个用户。通过日志可以发现，一条用户注册的消息，要么被会员服务收到，要么被营销服务收到，显然这不是广播。那，我们使用的FanoutExchange，看名字就应该是实现广播的交换器，为什么根本没有起作用呢？ 其实，广播交换器非常简单，它会忽略routingKey，广播消息到所有绑定的队列。在这个案例中，两个会员服务和两个营销服务都绑定了同一个队列，所以这四个服务只能收到一次消息： 修改方式很简单，我们把队列进行拆分，会员和营销两组服务分别使用一条独立队列绑定到广播交换器即可： @Slf4j @Configuration @RestController @RequestMapping(&quot;fanoutright&quot;) public class FanoutQueueRight &#123; private static final String MEMBER_QUEUE &#x3D; &quot;newusermember&quot;; private static final String PROMOTION_QUEUE &#x3D; &quot;newuserpromotion&quot;; private static final String EXCHANGE &#x3D; &quot;newuser&quot;; @Autowired private RabbitTemplate rabbitTemplate; @GetMapping public void sendMessage() &#123; rabbitTemplate.convertAndSend(EXCHANGE, &quot;&quot;, UUID.randomUUID().toString()); &#125; @Bean public Declarables declarables() &#123; &#x2F;&#x2F;会员服务队列 Queue memberQueue &#x3D; new Queue(MEMBER_QUEUE); &#x2F;&#x2F;营销服务队列 Queue promotionQueue &#x3D; new Queue(PROMOTION_QUEUE); &#x2F;&#x2F;广播交换器 FanoutExchange exchange &#x3D; new FanoutExchange(EXCHANGE); &#x2F;&#x2F;两个队列绑定到同一个交换器 return new Declarables(memberQueue, promotionQueue, exchange, BindingBuilder.bind(memberQueue).to(exchange), BindingBuilder.bind(promotionQueue).to(exchange)); &#125; @RabbitListener(queues &#x3D; MEMBER_QUEUE) public void memberService1(String userName) &#123; log.info(&quot;memberService1: welcome message sent to new user &#123;&#125;&quot;, userName); &#125; @RabbitListener(queues &#x3D; MEMBER_QUEUE) public void memberService2(String userName) &#123; log.info(&quot;memberService2: welcome message sent to new user &#123;&#125;&quot;, userName); &#125; @RabbitListener(queues &#x3D; PROMOTION_QUEUE) public void promotionService1(String userName) &#123; log.info(&quot;promotionService1: gift sent to new user &#123;&#125;&quot;, userName); &#125; @RabbitListener(queues &#x3D; PROMOTION_QUEUE) public void promotionService2(String userName) &#123; log.info(&quot;promotionService2: gift sent to new user &#123;&#125;&quot;, userName); &#125; &#125; 现在，交换器和队列的结构是这样的： 从日志输出可以验证，对于每一条MQ消息，会员服务和营销服务分别都会收到一次，一条消息广播到两个服务的同时，在每一个服务的两个实例中通过轮询接收： 所以说，理解了RabbitMQ直接交换器、广播交换器的工作方式之后，我们对消息的路由方式了解得很清晰了，实现代码就不会出错。 对于异步流程来说，消息路由模式一旦配置出错，轻则可能导致消息的重复处理，重则可能导致重要的服务无法接收到消息，最终造成业务逻辑错误。 每个MQ中间件对消息的路由处理的配置各不相同，我们一定要先了解原理再着手编码。 别让死信堵塞了消息队列我们在介绍线程池的时候提到，如果线程池的任务队列没有上限，那么最终可能会导致OOM。使用消息队列处理异步流程的时候，我们也同样要注意消息队列的任务堆积问题。对于突发流量引起的消息队列堆积，问题并不大，适当调整消费者的消费能力应该就可以解决。但在很多时候，消息队列的堆积堵塞，是因为有大量始终无法处理的消息。 比如，用户服务在用户注册后发出一条消息，会员服务监听到消息后给用户派发优惠券，但因为用户并没有保存成功，会员服务处理消息始终失败，消息重新进入队列，然后还是处理失败。这种在MQ中像幽灵一样回荡的同一条消息，就是死信。 随着MQ被越来越多的死信填满，消费者需要花费大量时间反复处理死信，导致正常消息的消费受阻，最终MQ可能因为数据量过大而崩溃。 我们来测试一下这个场景。首先，定义一个队列、一个直接交换器，然后把队列绑定到交换器： @Bean public Declarables declarables() &#123; &#x2F;&#x2F;队列 Queue queue &#x3D; new Queue(Consts.QUEUE); &#x2F;&#x2F;交换器 DirectExchange directExchange &#x3D; new DirectExchange(Consts.EXCHANGE); &#x2F;&#x2F;快速声明一组对象，包含队列、交换器，以及队列到交换器的绑定 return new Declarables(queue, directExchange, BindingBuilder.bind(queue).to(directExchange).with(Consts.ROUTING_KEY)); &#125; 然后，实现一个sendMessage方法来发送消息到MQ，访问一次提交一条消息，使用自增标识作为消息内容： &#x2F;&#x2F;自增消息标识 AtomicLong atomicLong &#x3D; new AtomicLong(); @Autowired private RabbitTemplate rabbitTemplate; @GetMapping(&quot;sendMessage&quot;) public void sendMessage() &#123; String msg &#x3D; &quot;msg&quot; + atomicLong.incrementAndGet(); log.info(&quot;send message &#123;&#125;&quot;, msg); &#x2F;&#x2F;发送消息 rabbitTemplate.convertAndSend(Consts.EXCHANGE, msg); &#125; 收到消息后，直接抛出空指针异常，模拟处理出错的情况： @RabbitListener(queues &#x3D; Consts.QUEUE) public void handler(String data) &#123; log.info(&quot;got message &#123;&#125;&quot;, data); throw new NullPointerException(&quot;error&quot;); &#125; 调用sendMessage接口发送两条消息，然后来到RabbitMQ管理台，可以看到这两条消息始终在队列中，不断被重新投递，导致重新投递QPS达到了1063。 同时，在日志中可以看到大量异常信息： [20:02:31.533] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [WARN ] [o.s.a.r.l.ConditionalRejectingErrorHandler:129 ] - Execution of Rabbit message listener failed. org.springframework.amqp.rabbit.support.ListenerExecutionFailedException: Listener method &#39;public void org.geekbang.time.commonmistakes.asyncprocess.deadletter.MQListener.handler(java.lang.String)&#39; threw exception at org.springframework.amqp.rabbit.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:219) at org.springframework.amqp.rabbit.listener.adapter.MessagingMessageListenerAdapter.invokeHandlerAndProcessResult(MessagingMessageListenerAdapter.java:143) at org.springframework.amqp.rabbit.listener.adapter.MessagingMessageListenerAdapter.onMessage(MessagingMessageListenerAdapter.java:132) at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:1569) at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.actualInvokeListener(AbstractMessageListenerContainer.java:1488) at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:1476) at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:1467) at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.executeListener(AbstractMessageListenerContainer.java:1411) at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doReceiveAndExecute(SimpleMessageListenerContainer.java:958) at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.receiveAndExecute(SimpleMessageListenerContainer.java:908) at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.access$1600(SimpleMessageListenerContainer.java:81) at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.mainLoop(SimpleMessageListenerContainer.java:1279) at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:1185) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.NullPointerException: error at org.geekbang.time.commonmistakes.asyncprocess.deadletter.MQListener.handler(MQListener.java:14) at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:171) at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:120) at org.springframework.amqp.rabbit.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:50) at org.springframework.amqp.rabbit.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:211) ... 13 common frames omitted 解决死信无限重复进入队列最简单的方式是，在程序处理出错的时候，直接抛出AmqpRejectAndDontRequeueException异常，避免消息重新进入队列： throw new AmqpRejectAndDontRequeueException(&quot;error&quot;); 但，我们更希望的逻辑是，对于同一条消息，能够先进行几次重试，解决因为网络问题导致的偶发消息处理失败，如果还是不行的话，再把消息投递到专门的一个死信队列。对于来自死信队列的数据，我们可能只是记录日志发送报警，即使出现异常也不会再重复投递。整个逻辑如下图所示： 针对这个问题，Spring AMQP提供了非常方便的解决方案： 首先，定义死信交换器和死信队列。其实，这些都是普通的交换器和队列，只不过被我们专门用于处理死信消息。 然后，通过RetryInterceptorBuilder构建一个RetryOperationsInterceptor，用于处理失败时候的重试。这里的策略是，最多尝试5次（重试4次）；并且采取指数退避重试，首次重试延迟1秒，第二次2秒，以此类推，最大延迟是10秒；如果第4次重试还是失败，则使用RepublishMessageRecoverer把消息重新投入一个“死信交换器”中。 最后，定义死信队列的处理程序。这个案例中，我们只是简单记录日志。 对应的实现代码如下： &#x2F;&#x2F;定义死信交换器和队列，并且进行绑定 @Bean public Declarables declarablesForDead() &#123; Queue queue &#x3D; new Queue(Consts.DEAD_QUEUE); DirectExchange directExchange &#x3D; new DirectExchange(Consts.DEAD_EXCHANGE); return new Declarables(queue, directExchange, BindingBuilder.bind(queue).to(directExchange).with(Consts.DEAD_ROUTING_KEY)); &#125; &#x2F;&#x2F;定义重试操作拦截器 @Bean public RetryOperationsInterceptor interceptor() &#123; return RetryInterceptorBuilder.stateless() .maxAttempts(5) &#x2F;&#x2F;最多尝试（不是重试）5次 .backOffOptions(1000, 2.0, 10000) &#x2F;&#x2F;指数退避重试 .recoverer(new RepublishMessageRecoverer(rabbitTemplate, Consts.DEAD_EXCHANGE, Consts.DEAD_ROUTING_KEY)) &#x2F;&#x2F;重新投递重试达到上限的消息 .build(); &#125; &#x2F;&#x2F;通过定义SimpleRabbitListenerContainerFactory，设置其adviceChain属性为之前定义的RetryOperationsInterceptor来启用重试拦截器 @Bean public SimpleRabbitListenerContainerFactory rabbitListenerContainerFactory(ConnectionFactory connectionFactory) &#123; SimpleRabbitListenerContainerFactory factory &#x3D; new SimpleRabbitListenerContainerFactory(); factory.setConnectionFactory(connectionFactory); factory.setAdviceChain(interceptor()); return factory; &#125; &#x2F;&#x2F;死信队列处理程序 @RabbitListener(queues &#x3D; Consts.DEAD_QUEUE) public void deadHandler(String data) &#123; log.error(&quot;got dead message &#123;&#125;&quot;, data); &#125; 执行程序，发送两条消息，日志如下： [11:22:02.193] [http-nio-45688-exec-1] [INFO ] [o.g.t.c.a.d.DeadLetterController:24 ] - send message msg1 [11:22:02.219] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg1 [11:22:02.614] [http-nio-45688-exec-2] [INFO ] [o.g.t.c.a.d.DeadLetterController:24 ] - send message msg2 [11:22:03.220] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg1 [11:22:05.221] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg1 [11:22:09.223] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg1 [11:22:17.224] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg1 [11:22:17.226] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [WARN ] [o.s.a.r.retry.RepublishMessageRecoverer:172 ] - Republishing failed message to exchange &#39;deadtest&#39; with routing key deadtest [11:22:17.227] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg2 [11:22:17.229] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#1-1] [ERROR] [o.g.t.c.a.deadletter.MQListener:20 ] - got dead message msg1 [11:22:18.232] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg2 [11:22:20.237] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg2 [11:22:24.241] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg2 [11:22:32.245] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.deadletter.MQListener:13 ] - got message msg2 [11:22:32.246] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [WARN ] [o.s.a.r.retry.RepublishMessageRecoverer:172 ] - Republishing failed message to exchange &#39;deadtest&#39; with routing key deadtest [11:22:32.250] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#1-1] [ERROR] [o.g.t.c.a.deadletter.MQListener:20 ] - got dead message msg2 可以看到： msg1的4次重试间隔分别是1秒、2秒、4秒、8秒，再加上首次的失败，所以最大尝试次数是5。 4次重试后，RepublishMessageRecoverer把消息发往了死信交换器。 死信处理程序输出了got dead message日志。 这里需要尤其注意的一点是，虽然我们几乎同时发送了两条消息，但是msg2是在msg1的四次重试全部结束后才开始处理。原因是，默认情况下SimpleMessageListenerContainer只有一个消费线程。可以通过增加消费线程来避免性能问题，如下我们直接设置concurrentConsumers参数为10，来增加到10个工作线程： @Bean public SimpleRabbitListenerContainerFactory rabbitListenerContainerFactory(ConnectionFactory connectionFactory) &#123; SimpleRabbitListenerContainerFactory factory &#x3D; new SimpleRabbitListenerContainerFactory(); factory.setConnectionFactory(connectionFactory); factory.setAdviceChain(interceptor()); factory.setConcurrentConsumers(10); return factory; &#125; 当然，我们也可以设置maxConcurrentConsumers参数，来让SimpleMessageListenerContainer自己动态地调整消费者线程数。不过，我们需要特别注意它的动态开启新线程的策略。你可以通过官方文档，来了解这个策略。 重点回顾在使用异步处理这种架构模式的时候，我们一般都会使用MQ中间件配合实现异步流程，需要重点考虑四个方面的问题。 第一，要考虑异步流程丢消息或处理中断的情况，异步流程需要有备线进行补偿。比如，我们今天介绍的全量补偿方式，即便异步流程彻底失效，通过补偿也能让业务继续进行。 第二，异步处理的时候需要考虑消息重复的可能性，处理逻辑需要实现幂等，防止重复处理。 第三，微服务场景下不同服务多个实例监听消息的情况，一般不同服务需要同时收到相同的消息，而相同服务的多个实例只需要轮询接收消息。我们需要确认MQ的消息路由配置是否满足需求，以避免消息重复或漏发问题。 第四，要注意始终无法处理的死信消息，可能会引发堵塞MQ的问题。一般在遇到消息处理失败的时候，我们可以设置一定的重试策略。如果重试还是不行，那可以把这个消息扔到专有的死信队列特别处理，不要让死信影响到正常消息的处理。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在用户注册后发送消息到MQ，然后会员服务监听消息进行异步处理的场景下，有些时候我们会发现，虽然用户服务先保存数据再发送MQ，但会员服务收到消息后去查询数据库，却发现数据库中还没有新用户的信息。你觉得，这可能是什么问题呢，又该如何解决呢？ 除了使用Spring AMQP实现死信消息的重投递外，RabbitMQ 2.8.0 后支持的死信交换器DLX也可以实现类似功能。你能尝试用DLX实现吗，并比较下这两种处理机制？ 关于使用MQ进行异步处理流程，你还遇到过其他问题吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/27 _ 数据源头：任何客户端的东西都不可信任","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/27 _ 数据源头：任何客户端的东西都不可信任/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/27%20_%20%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A4%B4%EF%BC%9A%E4%BB%BB%E4%BD%95%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E4%B8%9C%E8%A5%BF%E9%83%BD%E4%B8%8D%E5%8F%AF%E4%BF%A1%E4%BB%BB/","excerpt":"","text":"27 | 数据源头：任何客户端的东西都不可信任作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 从今天开始，我要和你讨论几个有关安全的话题。首先声明，我不是安全专家，但我发现有这么一个问题，那就是许多做业务开发的同学往往一点点安全意识都没有。如果有些公司没有安全部门或专家的话，安全问题就会非常严重。 如果只是用一些所谓的渗透服务浅层次地做一下扫描和渗透，而不在代码和逻辑层面做进一步分析的话，能够发现的安全问题非常有限。要做好安全，还是要靠一线程序员和产品经理点点滴滴的意识。 所以接下来的几篇文章，我会从业务开发的角度，和你说说我们应该最应该具备的安全意识。 对于HTTP请求，我们要在脑子里有一个根深蒂固的概念，那就是任何客户端传过来的数据都是不能直接信任的。客户端传给服务端的数据只是信息收集，数据需要经过有效性验证、权限验证等后才能使用，并且这些数据只能认为是用户操作的意图，不能直接代表数据当前的状态。 举一个简单的例子，我们打游戏的时候，客户端发给服务端的只是用户的操作，比如移动了多少位置，由服务端根据用户当前的状态来设置新的位置再返回给客户端。为了防止作弊，不可能由客户端直接告诉服务端用户当前的位置。 因此，客户端发给服务端的指令，代表的只是操作指令，并不能直接决定用户的状态，对于状态改变的计算在服务端。而网络不好时，我们往往会遇到走了10步又被服务端拉回来的现象，就是因为有指令丢失，客户端使用服务端计算的实际位置修正了客户端玩家的位置。 今天，我通过四个案例来和你说说，为什么“任何客户端的东西都不可信任”。 客户端的计算不可信我们先看一个电商下单操作的案例。 在这个场景下，可能会暴露这么一个&#x2F;order的POST接口给客户端，让客户端直接把组装后的订单信息Order传给服务端： @PostMapping(&quot;&#x2F;order&quot;) public void wrong(@RequestBody Order order) &#123; this.createOrder(order); &#125; 订单信息Order可能包括商品ID、商品价格、数量、商品总价： @Data public class Order &#123; private long itemId; &#x2F;&#x2F;商品ID private BigDecimal itemPrice; &#x2F;&#x2F;商品价格 private int quantity; &#x2F;&#x2F;商品数量 private BigDecimal itemTotalPrice; &#x2F;&#x2F;商品总价 &#125; 虽然用户下单时客户端肯定有商品的价格等信息，也会计算出订单的总价给用户确认，但是这些信息只能用于呈现和核对。即使客户端传给服务端的POJO中包含了这些信息，服务端也一定要重新从数据库来初始化商品的价格，重新计算最终的订单价格。如果不这么做的话，很可能会被黑客利用，商品总价被恶意修改为比较低的价格。 因此，我们真正直接使用的、可信赖的只是客户端传过来的商品ID和数量，服务端会根据这些信息重新计算最终的总价。如果服务端计算出来的商品价格和客户端传过来的价格不匹配的话，可以给客户端友好提示，让用户重新下单。修改后的代码如下： @PostMapping(&quot;&#x2F;orderRight&quot;) public void right(@RequestBody Order order) &#123; &#x2F;&#x2F;根据ID重新查询商品 Item item &#x3D; Db.getItem(order.getItemId()); &#x2F;&#x2F;客户端传入的和服务端查询到的商品单价不匹配的时候，给予友好提示 if (!order.getItemPrice().equals(item.getItemPrice())) &#123; throw new RuntimeException(&quot;您选购的商品价格有变化，请重新下单&quot;); &#125; &#x2F;&#x2F;重新设置商品单价 order.setItemPrice(item.getItemPrice()); &#x2F;&#x2F;重新计算商品总价 BigDecimal totalPrice &#x3D; item.getItemPrice().multiply(BigDecimal.valueOf(order.getQuantity())); &#x2F;&#x2F;客户端传入的和服务端查询到的商品总价不匹配的时候，给予友好提示 if (order.getItemTotalPrice().compareTo(totalPrice)!&#x3D;0) &#123; throw new RuntimeException(&quot;您选购的商品总价有变化，请重新下单&quot;); &#125; &#x2F;&#x2F;重新设置商品总价 order.setItemTotalPrice(totalPrice); createOrder(order); &#125; 还有一种可行的做法是，让客户端仅传入需要的数据给服务端，像这样重新定义一个POJO CreateOrderRequest作为接口入参，比直接使用领域模型Order更合理。在设计接口时，我们会思考哪些数据需要客户端提供，而不是把一个大而全的对象作为参数提供给服务端，以避免因为忘记在服务端重置客户端数据而导致的安全问题。 下单成功后，服务端处理完成后会返回诸如商品单价、总价等信息给客户端。此时，客户端可以进行一次判断，如果和之前客户端的数据不一致的话，给予用户提示，用户确认没问题后再进入支付阶段： @Data public class CreateOrderRequest &#123; private long itemId; &#x2F;&#x2F;商品ID private int quantity; &#x2F;&#x2F;商品数量 &#125; @PostMapping(&quot;orderRight2&quot;) public Order right2(@RequestBody CreateOrderRequest createOrderRequest) &#123; &#x2F;&#x2F;商品ID和商品数量是可信的没问题，其他数据需要由服务端计算 Item item &#x3D; Db.getItem(createOrderRequest.getItemId()); Order order &#x3D; new Order(); order.setItemPrice(item.getItemPrice()); order.setItemTotalPrice(item.getItemPrice().multiply(BigDecimal.valueOf(order.getQuantity()))); createOrder(order); return order; &#125; 通过这个案例我们可以看到，在处理客户端提交过来的数据时，服务端需要明确区分，哪些数据是需要客户端提供的，哪些数据是客户端从服务端获取后在客户端计算的。其中，前者可以信任；而后者不可信任，服务端需要重新计算，如果客户端和服务端计算结果不一致的话，可以给予友好提示。 客户端提交的参数需要校验对于客户端的数据，我们还容易忽略的一点是，误以为客户端的数据来源是服务端，客户端就不可能提交异常数据。我们看一个案例。 有一个用户注册页面要让用户选择所在国家，我们会把服务端支持的国家列表返回给页面，供用户选择。如下代码所示，我们的注册只支持中国、美国和英国三个国家，并不对其他国家开放，因此从数据库中筛选了id&lt;4的国家返回给页面进行填充： @Slf4j @RequestMapping(&quot;trustclientdata&quot;) @Controller public class TrustClientDataController &#123; &#x2F;&#x2F;所有支持的国家 private HashMap&lt;Integer, Country&gt; allCountries &#x3D; new HashMap&lt;&gt;(); public TrustClientDataController() &#123; allCountries.put(1, new Country(1, &quot;China&quot;)); allCountries.put(2, new Country(2, &quot;US&quot;)); allCountries.put(3, new Country(3, &quot;UK&quot;)); allCountries.put(4, new Country(4, &quot;Japan&quot;)); &#125; @GetMapping(&quot;&#x2F;&quot;) public String index(ModelMap modelMap) &#123; List&lt;Country&gt; countries &#x3D; new ArrayList&lt;&gt;(); &#x2F;&#x2F;从数据库查出ID&lt;4的三个国家作为白名单在页面显示 countries.addAll(allCountries.values().stream().filter(country -&gt; country.getId()&lt;4).collect(Collectors.toList())); modelMap.addAttribute(&quot;countries&quot;, countries); return &quot;index&quot;; &#125; &#125; 我们通过服务端返回的数据来渲染模板： ... &lt;form id&#x3D;&quot;myForm&quot; method&#x3D;&quot;post&quot; th:action&#x3D;&quot;@&#123;&#x2F;trustclientdata&#x2F;wrong&#125;&quot;&gt; &lt;select id&#x3D;&quot;countryId&quot; name&#x3D;&quot;countryId&quot;&gt; &lt;option value&#x3D;&quot;0&quot;&gt;Select country&lt;&#x2F;option&gt; &lt;option th:each&#x3D;&quot;country : $&#123;countries&#125;&quot; th:text&#x3D;&quot;$&#123;country.name&#125;&quot; th:value&#x3D;&quot;$&#123;country.id&#125;&quot;&gt;&lt;&#x2F;option&gt; &lt;&#x2F;select&gt; &lt;button th:text&#x3D;&quot;Register&quot; type&#x3D;&quot;submit&quot;&#x2F;&gt; &lt;&#x2F;form&gt; ... 在页面上，的确也只有这三个国家的可选项： 但我们要知道的是，页面是给普通用户使用的，而黑客不会在乎页面显示什么，完全有可能尝试给服务端返回页面上没显示的其他国家ID。如果像这样直接信任客户端传来的国家ID的话，很可能会把用户注册功能开放给其他国家的人： @PostMapping(&quot;&#x2F;wrong&quot;) @ResponseBody public String wrong(@RequestParam(&quot;countryId&quot;) int countryId) &#123; return allCountries.get(countryId).getName(); &#125; 即使我们知道参数的范围来自下拉框，而下拉框的内容也来自服务端，也需要对参数进行校验。因为接口不一定要通过浏览器请求，只要知道接口定义完全可以通过其他工具提交： curl http:&#x2F;&#x2F;localhost:45678&#x2F;trustclientdata&#x2F;wrong\\?countryId&#x3D;4 -X POST 修改方式是，在使用客户端传过来的参数之前，对参数进行有效性校验： @PostMapping(&quot;&#x2F;right&quot;) @ResponseBody public String right(@RequestParam(&quot;countryId&quot;) int countryId) &#123; if (countryId &lt; 1 || countryId &gt; 3) throw new RuntimeException(&quot;非法参数&quot;); return allCountries.get(countryId).getName(); &#125; 或者是，使用Spring Validation采用注解的方式进行参数校验，更优雅： @Validated public class TrustClientParameterController &#123; @PostMapping(&quot;&#x2F;better&quot;) @ResponseBody public String better( @RequestParam(&quot;countryId&quot;) @Min(value &#x3D; 1, message &#x3D; &quot;非法参数&quot;) @Max(value &#x3D; 3, message &#x3D; &quot;非法参数&quot;) int countryId) &#123; return allCountries.get(countryId).getName(); &#125; &#125; 客户端提交的参数需要校验的问题，可以引申出一个更容易忽略的点是，我们可能会把一些服务端的数据暂存在网页的隐藏域中，这样下次页面提交的时候可以把相关数据再传给服务端。虽然用户通过网页界面的操作无法修改这些数据，但这些数据对于HTTP请求来说就是普通数据，完全可以随时修改为任意值。所以，服务端在使用这些数据的时候，也同样要特别小心。 不能信任请求头里的任何内容刚才我们介绍了，不能直接信任客户端的传参，也就是通过GET或POST方法传过来的数据，此外请求头的内容也不能信任。 一个比较常见的需求是，为了防刷，我们需要判断用户的唯一性。比如，针对未注册的新用户发送一些小奖品，我们不希望相同用户多次获得奖品。考虑到未注册的用户因为没有登录过所以没有用户标识，我们可能会想到根据请求的IP地址，来判断用户是否已经领过奖品。 比如，下面的这段测试代码。我们通过一个HashSet模拟已发放过奖品的IP名单，每次领取奖品后把IP地址加入这个名单中。IP地址的获取方式是：优先通过X-Forwarded-For请求头来获取，如果没有的话再通过HttpServletRequest的getRemoteAddr方法来获取。 @Slf4j @RequestMapping(&quot;trustclientip&quot;) @RestController public class TrustClientIpController &#123; HashSet&lt;String&gt; activityLimit &#x3D; new HashSet&lt;&gt;(); @GetMapping(&quot;test&quot;) public String test(HttpServletRequest request) &#123; String ip &#x3D; getClientIp(request); if (activityLimit.contains(ip)) &#123; return &quot;您已经领取过奖品&quot;; &#125; else &#123; activityLimit.add(ip); return &quot;奖品领取成功&quot;; &#125; &#125; private String getClientIp(HttpServletRequest request) &#123; String xff &#x3D; request.getHeader(&quot;X-Forwarded-For&quot;); if (xff &#x3D;&#x3D; null) &#123; return request.getRemoteAddr(); &#125; else &#123; return xff.contains(&quot;,&quot;) ? xff.split(&quot;,&quot;)[0] : xff; &#125; &#125; &#125; 之所以这么做是因为，通常我们的应用之前都部署了反向代理或负载均衡器，remoteAddr获得的只能是代理的IP地址，而不是访问用户实际的IP。这不符合我们的需求，因为反向代理在转发请求时，通常会把用户真实IP放入X-Forwarded-For这个请求头中。 这种过于依赖X-Forwarded-For请求头来判断用户唯一性的实现方式，是有问题的： 完全可以通过cURL类似的工具来模拟请求，随意篡改头的内容： curl http:&#x2F;&#x2F;localhost:45678&#x2F;trustclientip&#x2F;test -H &quot;X-Forwarded-For:183.84.18.71, 10.253.15.1&quot; 网吧、学校等机构的出口IP往往是同一个，在这个场景下，可能只有最先打开这个页面的用户才能领取到奖品，而其他用户会被阻拦。 因此，IP地址或者说请求头里的任何信息，包括Cookie中的信息、Referer，只能用作参考，不能用作重要逻辑判断的依据。而对于类似这个案例唯一性的判断需求，更好的做法是，让用户进行登录或三方授权登录（比如微信），拿到用户标识来做唯一性判断。 用户标识不能从客户端获取聊到用户登录，业务代码非常容易犯错的一个地方是，使用了客户端传给服务端的用户ID，类似这样： @GetMapping(&quot;wrong&quot;) public String wrong(@RequestParam(&quot;userId&quot;) Long userId) &#123; return &quot;当前用户Id：&quot; + userId; &#125; 你可能觉得没人会这么干，但我就真实遇到过：一个大项目因为服务端直接使用了客户端传过来的用户标识，导致了安全问题。 犯类似低级错误的原因，有三个： 开发同学没有正确认识接口或服务面向的用户。如果接口面向内部服务，由服务调用方传入用户ID没什么不合理，但是这样的接口不能直接开放给客户端或H5使用。 在测试阶段为了方便测试调试，我们通常会实现一些无需登录即可使用的接口，直接使用客户端传过来的用户标识，却在上线之前忘记删除类似的超级接口。 一个大型网站前端可能由不同的模块构成，不一定是一个系统，而用户登录状态可能也没有打通。有些时候，我们图简单可能会在URL中直接传用户ID，以实现通过前端传值来打通用户登录状态。 如果你的接口直面用户（比如给客户端或H5页面调用），那么一定需要用户先登录才能使用。登录后用户标识保存在服务端，接口需要从服务端（比如Session中）获取。这里有段代码演示了一个最简单的登录操作，登录后在Session中设置了当前用户的标识： @GetMapping(&quot;login&quot;) public long login(@RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password, HttpSession session) &#123; if (username.equals(&quot;admin&quot;) &amp;&amp; password.equals(&quot;admin&quot;)) &#123; session.setAttribute(&quot;currentUser&quot;, 1L); return 1L; &#125; return 0L; &#125; 这里，我再分享一个Spring Web的小技巧。 如果希望每一个需要登录的方法，都从Session中获得当前用户标识，并进行一些后续处理的话，我们没有必要在每一个方法内都复制粘贴相同的获取用户身份的逻辑，可以定义一个自定义注解@LoginRequired到userId参数上，然后通过HandlerMethodArgumentResolver自动实现参数的组装： @GetMapping(&quot;right&quot;) public String right(@LoginRequired Long userId) &#123; return &quot;当前用户Id：&quot; + userId; &#125; @LoginRequired本身并无特殊，只是一个自定义注解： @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.PARAMETER) @Documented public @interface LoginRequired &#123; String sessionKey() default &quot;currentUser&quot;; &#125; 魔法来自HandlerMethodArgumentResolver。我们自定义了一个实现类LoginRequiredArgumentResolver，实现了HandlerMethodArgumentResolver接口的2个方法： supportsParameter方法判断当参数上有@LoginRequired注解时，再做自定义参数解析的处理； resolveArgument方法用来实现解析逻辑本身。在这里，我们尝试从Session中获取当前用户的标识，如果无法获取到的话提示非法调用的错误，如果获取到则返回userId。这样一来，Controller中的userId参数就可以自动赋值了。 @Slf4j public class LoginRequiredArgumentResolver implements HandlerMethodArgumentResolver &#123; &#x2F;&#x2F;解析哪些参数 @Override public boolean supportsParameter(MethodParameter methodParameter) &#123; &#x2F;&#x2F;匹配参数上具有@LoginRequired注解的参数 return methodParameter.hasParameterAnnotation(LoginRequired.class); &#125; @Override public Object resolveArgument(MethodParameter methodParameter, ModelAndViewContainer modelAndViewContainer, NativeWebRequest nativeWebRequest, WebDataBinderFactory webDataBinderFactory) throws Exception &#123; &#x2F;&#x2F;从参数上获得注解 LoginRequired loginRequired &#x3D; methodParameter.getParameterAnnotation(LoginRequired.class); &#x2F;&#x2F;根据注解中的Session Key，从Session中查询用户信息 Object object &#x3D; nativeWebRequest.getAttribute(loginRequired.sessionKey(), NativeWebRequest.SCOPE_SESSION); if (object &#x3D;&#x3D; null) &#123; log.error(&quot;接口 &#123;&#125; 非法调用！&quot;, methodParameter.getMethod().toString()); throw new RuntimeException(&quot;请先登录！&quot;); &#125; return object; &#125; &#125; 当然，我们要实现WebMvcConfigurer接口的addArgumentResolvers方法，来增加这个自定义的处理器LoginRequiredArgumentResolver： SpringBootApplication public class CommonMistakesApplication implements WebMvcConfigurer &#123; ... @Override public void addArgumentResolvers(List&lt;HandlerMethodArgumentResolver&gt; resolvers) &#123; resolvers.add(new LoginRequiredArgumentResolver()); &#125; &#125; 测试发现，经过这样的实现，登录后所有需要登录的方法都可以一键通过加@LoginRequired注解来拿到用户标识，方便且安全： 重点回顾今天，我就“任何客户端的东西都不可信任”这个结论，和你讲解了一些有代表性的错误。 第一，客户端的计算不可信。虽然目前很多项目的前端都是富前端，会做大量的逻辑计算，无需访问服务端接口就可以顺畅完成各种功能，但来自客户端的计算结果不能直接信任。最终在进行业务操作时，客户端只能扮演信息收集的角色，虽然可以将诸如价格等信息传给服务端，但只能用于校对比较，最终要以服务端的计算结果为准。 第二，所有来自客户端的参数都需要校验判断合法性。即使我们知道用户是在一个下拉列表选择数据，即使我们知道用户通过网页正常操作不可能提交不合法的值，服务端也应该进行参数校验，防止非法用户绕过浏览器UI页面通过工具直接向服务端提交参数。 第三，除了请求Body中的信息，请求头里的任何信息同样不能信任。我们要知道，来自请求头的IP、Referer和Cookie都有被篡改的可能性，相关数据只能用来参考和记录，不能用作重要业务逻辑。 第四，如果接口面向外部用户，那么一定不能出现用户标识这样的参数，当前用户的标识一定来自服务端，只有经过身份认证后的用户才会在服务端留下标识。如果你的接口现在面向内部其他服务，那么也要千万小心这样的接口只能内部使用，还可能需要进一步考虑服务端调用方的授权问题。 安全问题是木桶效应，整个系统的安全等级取决于安全性最薄弱的那个模块。在写业务代码的时候，要从我做起，建立最基本的安全意识，从源头杜绝低级安全问题。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在讲述用户标识不能从客户端获取这个要点的时候，我提到开发同学可能会因为用户信息未打通而通过前端来传用户ID。那我们有什么好办法，来打通不同的系统甚至不同网站的用户标识吗？ 还有一类和客户端数据相关的漏洞非常重要，那就是URL地址中的数据。在把匿名用户重定向到登录页面的时候，我们一般会带上redirectUrl，这样用户登录后可以快速返回之前的页面。黑客可能会伪造一个活动链接，由真实的网站+钓鱼的redirectUrl构成，发邮件诱导用户进行登录。用户登录时访问的其实是真的网站，所以不容易察觉到redirectUrl是钓鱼网站，登录后却来到了钓鱼网站，用户可能会不知不觉就把重要信息泄露了。这种安全问题，我们叫做开放重定向问题。你觉得，从代码层面应该怎么预防开放重定向问题呢？ 你还遇到过因为信任HTTP请求中客户端传给服务端的信息导致的安全问题吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/28 _ 安全兜底：涉及钱时，必须考虑防刷、限量和防重","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/28 _ 安全兜底：涉及钱时，必须考虑防刷、限量和防重/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/28%20_%20%E5%AE%89%E5%85%A8%E5%85%9C%E5%BA%95%EF%BC%9A%E6%B6%89%E5%8F%8A%E9%92%B1%E6%97%B6%EF%BC%8C%E5%BF%85%E9%A1%BB%E8%80%83%E8%99%91%E9%98%B2%E5%88%B7%E3%80%81%E9%99%90%E9%87%8F%E5%92%8C%E9%98%B2%E9%87%8D/","excerpt":"","text":"28 | 安全兜底：涉及钱时，必须考虑防刷、限量和防重作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我要和你分享的主题是，任何涉及钱的代码必须要考虑防刷、限量和防重，要做好安全兜底。 涉及钱的代码，主要有以下三类。 第一，代码本身涉及有偿使用的三方服务。如果因为代码本身缺少授权、用量控制而被利用导致大量调用，势必会消耗大量的钱，给公司造成损失。有些三方服务可能采用后付款方式的结算，出现问题后如果没及时发现，下个月结算时就会收到一笔数额巨大的账单。 第二，代码涉及虚拟资产的发放，比如积分、优惠券等。虽然说虚拟资产不直接对应货币，但一般可以在平台兑换具有真实价值的资产。比如，优惠券可以在下单时使用，积分可以兑换积分商城的商品。所以从某种意义上说，虚拟资产就是具有一定价值的钱，但因为不直接涉及钱和外部资金通道，所以容易产生随意性发放而导致漏洞。 第三，代码涉及真实钱的进出。比如，对用户扣款，如果出现非正常的多次重复扣款，小则用户投诉、用户流失，大则被相关管理机构要求停业整改，影响业务。又比如，给用户发放返现的付款功能，如果出现漏洞造成重复付款，涉及B端的可能还好，但涉及C端用户的重复付款可能永远无法追回。 前段时间拼多多一夜之间被刷了大量100元无门槛优惠券的事情，就是限量和防刷出了问题。 今天，我们就通过三个例子，和你说明如何在代码层面做好安全兜底。 开放平台资源的使用需要考虑防刷我以真实遇到的短信服务被刷案例，和你说说防刷。 有次短信账单月结时发现，之前每个月是几千元的短信费用，这个月突然变为了几万元。查数据库记录发现，之前是每天发送几千条短信验证码，从某天开始突然变为了每天几万条，但注册用户数并没有激增。显然，这是短信接口被刷了。 我们知道，短信验证码服务属于开放性服务，由用户侧触发，且因为是注册验证码所以不需要登录就可以使用。如果我们的发短信接口像这样没有任何防刷的防护，直接调用三方短信通道，就相当于“裸奔”，很容易被短信轰炸平台利用： @GetMapping(&quot;wrong&quot;) public void wrong() &#123; sendSMSCaptcha(&quot;13600000000&quot;); &#125; private void sendSMSCaptcha(String mobile) &#123; &#x2F;&#x2F;调用短信通道 &#125; 对于短信验证码这种开放接口，程序逻辑内需要有防刷逻辑。好的防刷逻辑是，对正常使用的用户毫无影响，只有疑似异常使用的用户才会感受到。对于短信验证码，有如下4种可行的方式来防刷。 第一种方式，只有固定的请求头才能发送验证码。 也就是说，我们通过请求头中网页或App客户端传给服务端的一些额外参数，来判断请求是不是App发起的。其实，这种方式“防君子不防小人”。 比如，判断是否存在浏览器或手机型号、设备分辨率请求头。对于那些使用爬虫来抓取短信接口地址的程序来说，往往只能抓取到URL，而难以分析出请求发送短信还需要的额外请求头，可以看作第一道基本防御。 第二种方式，只有先到过注册页面才能发送验证码。 对于普通用户来说，不管是通过App注册还是H5页面注册，一定是先进入注册页面才能看到发送验证码按钮，再点击发送。我们可以在页面或界面打开时请求固定的前置接口，为这个设备开启允许发送验证码的窗口，之后的请求发送验证码才是有效请求。 这种方式可以防御直接绕开固定流程，通过接口直接调用的发送验证码请求，并不会干扰普通用户。 第三种方式，控制相同手机号的发送次数和发送频次。 除非是短信无法收到，否则用户不太会请求了验证码后不完成注册流程，再重新请求。因此，我们可以限制同一手机号每天的最大请求次数。验证码的到达需要时间，太短的发送间隔没有意义，所以我们还可以控制发送的最短间隔。比如，我们可以控制相同手机号一天只能发送10次验证码，最短发送间隔1分钟。 第四种方式，增加前置图形验证码。 短信轰炸平台一般会收集很多免费短信接口，一个接口只会给一个用户发一次短信，所以控制相同手机号发送次数和间隔的方式不够有效。这时，我们可以考虑对用户体验稍微有影响，但也是最有效的方式作为保底，即将弹出图形验证码作为前置。 除了图形验证码，我们还可以使用其他更友好的人机验证手段（比如滑动、点击验证码等），甚至是引入比较新潮的无感知验证码方案（比如，通过判断用户输入手机号的打字节奏，来判断是用户还是机器），来改善用户体验。 此外，我们也可以考虑在监测到异常的情况下再弹出人机检测。比如，短时间内大量相同远端IP发送验证码的时候，才会触发人机检测。 总之，我们要确保，只有正常用户经过正常的流程才能使用开放平台资源，并且资源的用量在业务需求合理范围内。此外，还需要考虑做好短信发送量的实时监控，遇到发送量激增要及时报警。 接下来，我们一起看看限量的问题。 虚拟资产并不能凭空产生无限使用虚拟资产虽然是平台方自己生产和控制，但如果生产出来可以立即使用就有立即变现的可能性。比如，因为平台Bug有大量用户领取高额优惠券，并立即下单使用。 在商家看来，这很可能只是一个用户支付的订单，并不会感知到用户使用平台方优惠券的情况；同时，因为平台和商家是事后结算的，所以会马上安排发货。而发货后基本就不可逆了，一夜之间造成了大量资金损失。 我们从代码层面模拟一个优惠券被刷的例子。 假设有一个CouponCenter类负责优惠券的产生和发放。如下是错误做法，只要调用方需要，就可以凭空产生无限的优惠券： @Slf4j public class CouponCenter &#123; &#x2F;&#x2F;用于统计发了多少优惠券 AtomicInteger totalSent &#x3D; new AtomicInteger(0); public void sendCoupon(Coupon coupon) &#123; if (coupon !&#x3D; null) totalSent.incrementAndGet(); &#125; public int getTotalSentCoupon() &#123; return totalSent.get(); &#125; &#x2F;&#x2F;没有任何限制，来多少请求生成多少优惠券 public Coupon generateCouponWrong(long userId, BigDecimal amount) &#123; return new Coupon(userId, amount); &#125; &#125; 这样一来，使用CouponCenter的generateCouponWrong方法，想发多少优惠券就可以发多少： @GetMapping(&quot;wrong&quot;) public int wrong() &#123; CouponCenter couponCenter &#x3D; new CouponCenter(); &#x2F;&#x2F;发送10000个优惠券 IntStream.rangeClosed(1, 10000).forEach(i -&gt; &#123; Coupon coupon &#x3D; couponCenter.generateCouponWrong(1L, new BigDecimal(&quot;100&quot;)); couponCenter.sendCoupon(coupon); &#125;); return couponCenter.getTotalSentCoupon(); &#125; 更合适的做法是，把优惠券看作一种资源，其生产不是凭空的，而是需要事先申请，理由是： 虚拟资产如果最终可以对应到真实金钱上的优惠，那么，能发多少取决于运营和财务的核算，应该是有计划、有上限的。引言提到的无门槛优惠券，需要特别小心。有门槛优惠券的大量使用至少会带来大量真实的消费，而使用无门槛优惠券下的订单，可能用户一分钱都没有支付。 即使虚拟资产不值钱，大量不合常规的虚拟资产流入市场，也会冲垮虚拟资产的经济体系，造成虚拟货币的极速贬值。有量的控制才有价值。 资产的申请需要理由，甚至需要走流程，这样才可以追溯是什么活动需要、谁提出的申请，程序依据申请批次来发放。 接下来，我们按照这个思路改进一下程序。 首先，定义一个CouponBatch类，要产生优惠券必须先向运营申请优惠券批次，批次中包含了固定张数的优惠券、申请原因等信息： &#x2F;&#x2F;优惠券批次 @Data public class CouponBatch &#123; private long id; private AtomicInteger totalCount; private AtomicInteger remainCount; private BigDecimal amount; private String reason; &#125; 在业务需要发放优惠券的时候，先申请批次，然后再通过批次发放优惠券： @GetMapping(&quot;right&quot;) public int right() &#123; CouponCenter couponCenter &#x3D; new CouponCenter(); &#x2F;&#x2F;申请批次 CouponBatch couponBatch &#x3D; couponCenter.generateCouponBatch(); IntStream.rangeClosed(1, 10000).forEach(i -&gt; &#123; Coupon coupon &#x3D; couponCenter.generateCouponRight(1L, couponBatch); &#x2F;&#x2F;发放优惠券 couponCenter.sendCoupon(coupon); &#125;); return couponCenter.getTotalSentCoupon(); &#125; 可以看到，generateCouponBatch方法申请批次时，设定了这个批次包含100张优惠券。在通过generateCouponRight方法发放优惠券时，每发一次都会从批次中扣除一张优惠券，发完了就没有了： public Coupon generateCouponRight(long userId, CouponBatch couponBatch) &#123; if (couponBatch.getRemainCount().decrementAndGet() &gt;&#x3D; 0) &#123; return new Coupon(userId, couponBatch.getAmount()); &#125; else &#123; log.info(&quot;优惠券批次 &#123;&#125; 剩余优惠券不足&quot;, couponBatch.getId()); return null; &#125; &#125; public CouponBatch generateCouponBatch() &#123; CouponBatch couponBatch &#x3D; new CouponBatch(); couponBatch.setAmount(new BigDecimal(&quot;100&quot;)); couponBatch.setId(1L); couponBatch.setTotalCount(new AtomicInteger(100)); couponBatch.setRemainCount(couponBatch.getTotalCount()); couponBatch.setReason(&quot;XXX活动&quot;); return couponBatch; &#125; 这样改进后的程序，一个批次最多只能发放100张优惠券： 因为是Demo，所以我们只是凭空new出来一个Coupon。在真实的生产级代码中，一定是根据CouponBatch在数据库中插入一定量的Coupon记录，每一个优惠券都有唯一的ID，可跟踪、可注销。 最后，我们再看看防重。 钱的进出一定要和订单挂钩并且实现幂等涉及钱的进出，需要做好以下两点。 第一，任何资金操作都需要在平台侧生成业务属性的订单，可以是优惠券发放订单，可以是返现订单，也可以是借款订单，一定是先有订单再去做资金操作。同时，订单的产生需要有业务属性。业务属性是指，订单不是凭空产生的，否则就没有控制的意义。比如，返现发放订单必须关联到原先的商品订单产生；再比如，借款订单必须关联到同一个借款合同产生。 第二，一定要做好防重，也就是实现幂等处理，并且幂等处理必须是全链路的。这里的全链路是指，从前到后都需要有相同的业务订单号来贯穿，实现最终的支付防重。 关于这两点，你可以参考下面的代码示例： &#x2F;&#x2F;错误：每次使用UUID作为订单号 @GetMapping(&quot;wrong&quot;) public void wrong(@RequestParam(&quot;orderId&quot;) String orderId) &#123; PayChannel.pay(UUID.randomUUID().toString(), &quot;123&quot;, new BigDecimal(&quot;100&quot;)); &#125; &#x2F;&#x2F;正确：使用相同的业务订单号 @GetMapping(&quot;right&quot;) public void right(@RequestParam(&quot;orderId&quot;) String orderId) &#123; PayChannel.pay(orderId, &quot;123&quot;, new BigDecimal(&quot;100&quot;)); &#125; &#x2F;&#x2F;三方支付通道 public class PayChannel &#123; public static void pay(String orderId, String account, BigDecimal amount) &#123; ... &#125; &#125; 对于支付操作，我们一定是调用三方支付公司的接口或银行接口进行处理的。一般而言，这些接口都会有商户订单号的概念，对于相同的商户订单号，无法进行重复的资金处理，所以三方公司的接口可以实现唯一订单号的幂等处理。 但是，业务系统在实现资金操作时容易犯的错是，没有自始至终地使用一个订单号作为商户订单号，透传给三方支付接口。出现这个问题的原因是，比较大的互联网公司一般会把支付独立一个部门。支付部门可能会针对支付做聚合操作，内部会维护一个支付订单号，然后使用支付订单号和三方支付接口交互。最终虽然商品订单是一个，但支付订单是多个，相同的商品订单因为产生多个支付订单导致多次支付。 如果说，支付出现了重复扣款，我们可以给用户进行退款操作，但给用户付款的操作一旦出现重复付款，就很难把钱追回来了，所以更要小心。 这，就是全链路的意义，从一开始就需要先有业务订单产生，然后使用相同的业务订单号一直贯穿到最后的资金通路，才能真正避免重复资金操作。 重点回顾今天，我从安全兜底聊起，和你分享了涉及钱的业务最需要做的三方面工作，防刷、限量和防重。 第一，使用开放的、面向用户的平台资源要考虑防刷，主要包括正常使用流程识别、人机识别、单人限量和全局限量等手段。 第二，虚拟资产不能凭空产生，一定是先有发放计划、申请批次，然后通过批次来生产资产。这样才能达到限量、有审计、能追溯的目的。 第三，真实钱的进出操作要额外小心，做好防重处理。不能凭空去操作用户的账户，每次操作以真实的订单作为依据，通过业务订单号实现全链路的幂等控制。 如果程序逻辑涉及有价值的资源或是真实的钱，我们必须有敬畏之心。程序上线后，人是有休息时间的，但程序是一直运行着的，如果产生安全漏洞，就很可能在一夜之间爆发，被大量人利用导致大量的金钱损失。 除了在流程上做好防刷、限量和防重控制之外，我们还需要做好三方平台调用量、虚拟资产使用量、交易量、交易金额等重要数据的监控报警，这样即使出现问题也能第一时间发现。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 防重、防刷都是事前手段，如果我们的系统正在被攻击或利用，你有什么办法及时发现问题吗？ 任何三方资源的使用一般都会定期对账，如果在对账中发现我们系统记录的调用量低于对方系统记录的使用量，你觉得一般是什么问题引起的呢？ 有关安全兜底，你还有什么心得吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/29 _ 数据和代码：数据就是数据，代码就是代码","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/29 _ 数据和代码：数据就是数据，代码就是代码/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/29%20_%20%E6%95%B0%E6%8D%AE%E5%92%8C%E4%BB%A3%E7%A0%81%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%B0%B1%E6%98%AF%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%BB%A3%E7%A0%81%E5%B0%B1%E6%98%AF%E4%BB%A3%E7%A0%81/","excerpt":"","text":"29 | 数据和代码：数据就是数据，代码就是代码作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来和你聊聊数据和代码的问题。 正如这一讲标题“数据就是数据，代码就是代码”所说，Web安全方面的很多漏洞，都是源自把数据当成了代码来执行，也就是注入类问题，比如： 客户端提供给服务端的查询值，是一个数据，会成为SQL查询的一部分。黑客通过修改这个值注入一些SQL，来达到在服务端运行SQL的目的，相当于把查询条件的数据变为了查询代码。这种攻击方式，叫做SQL注入。 对于规则引擎，我们可能会用动态语言做一些计算，和SQL注入一样外部传入的数据只能当做数据使用，如果被黑客利用传入了代码，那么代码可能就会被动态执行。这种攻击方式，叫做代码注入。 对于用户注册、留言评论等功能，服务端会从客户端收集一些信息，本来用户名、邮箱这类信息是纯文本信息，但是黑客把信息替换为了JavaScript代码。那么，这些信息在页面呈现时，可能就相当于执行了JavaScript代码。甚至是，服务端可能把这样的代码，当作普通信息保存到了数据库。黑客通过构建JavaScript代码来实现修改页面呈现、盗取信息，甚至蠕虫攻击的方式，叫做XSS（跨站脚本）攻击。 今天，我们就通过案例来看一下这三个问题，并了解下应对方式。 SQL注入能干的事情比你想象的更多我们应该都听说过SQL注入，也可能知道最经典的SQL注入的例子，是通过构造’or’1’&#x3D;’1作为密码实现登录。这种简单的攻击方式，在十几年前可以突破很多后台的登录，但现在很难奏效了。 最近几年，我们的安全意识增强了，都知道使用参数化查询来避免SQL注入问题。其中的原理是，使用参数化查询的话，参数只能作为普通数据，不可能作为SQL的一部分，以此有效避免SQL注入问题。 虽然我们已经开始关注SQL注入的问题，但还是有一些认知上的误区，主要表现在以下三个方面： 第一，认为SQL注入问题只可能发生于Http Get请求，也就是通过URL传入的参数才可能产生注入点。这是很危险的想法。从注入的难易度上来说，修改URL上的QueryString和修改Post请求体中的数据，没有任何区别，因为黑客是通过工具来注入的，而不是通过修改浏览器上的URL来注入的。甚至Cookie都可以用来SQL注入，任何提供数据的地方都可能成为注入点。 第二，认为不返回数据的接口，不可能存在注入问题。其实，黑客完全可以利用SQL语句构造出一些不正确的SQL，导致执行出错。如果服务端直接显示了错误信息，那黑客需要的数据就有可能被带出来，从而达到查询数据的目的。甚至是，即使没有详细的出错信息，黑客也可以通过所谓盲注的方式进行攻击。我后面再具体解释。 第三，认为SQL注入的影响范围，只是通过短路实现突破登录，只需要登录操作加强防范即可。首先，SQL注入完全可以实现拖库，也就是下载整个数据库的内容（之后我们会演示），SQL注入的危害不仅仅是突破后台登录。其次，根据木桶原理，整个站点的安全性受限于安全级别最低的那块短板。因此，对于安全问题，站点的所有模块必须一视同仁，并不是只加强防范所谓的重点模块。 在日常开发中，虽然我们是使用框架来进行数据访问的，但还可能会因为疏漏而导致注入问题。接下来，我就用一个实际的例子配合专业的SQL注入工具sqlmap，来测试下SQL注入。 首先，在程序启动的时候使用JdbcTemplate创建一个userdata表（表中只有ID、用户名、密码三列），并初始化两条用户信息。然后，创建一个不返回任何数据的Http Post接口。在实现上，我们通过SQL拼接的方式，把传入的用户名入参拼接到LIKE子句中实现模糊查询。 &#x2F;&#x2F;程序启动时进行表结构和数据初始化 @PostConstruct public void init() &#123; &#x2F;&#x2F;删除表 jdbcTemplate.execute(&quot;drop table IF EXISTS &#96;userdata&#96;;&quot;); &#x2F;&#x2F;创建表，不包含自增ID、用户名、密码三列 jdbcTemplate.execute(&quot;create TABLE &#96;userdata&#96; (\\n&quot; + &quot; &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT,\\n&quot; + &quot; &#96;name&#96; varchar(255) NOT NULL,\\n&quot; + &quot; &#96;password&#96; varchar(255) NOT NULL,\\n&quot; + &quot; PRIMARY KEY (&#96;id&#96;)\\n&quot; + &quot;) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4;&quot;); &#x2F;&#x2F;插入两条测试数据 jdbcTemplate.execute(&quot;INSERT INTO &#96;userdata&#96; (name,password) VALUES (&#39;test1&#39;,&#39;haha1&#39;),(&#39;test2&#39;,&#39;haha2&#39;)&quot;); &#125; @Autowired private JdbcTemplate jdbcTemplate; &#x2F;&#x2F;用户模糊搜索接口 @PostMapping(&quot;jdbcwrong&quot;) public void jdbcwrong(@RequestParam(&quot;name&quot;) String name) &#123; &#x2F;&#x2F;采用拼接SQL的方式把姓名参数拼到LIKE子句中 log.info(&quot;&#123;&#125;&quot;, jdbcTemplate.queryForList(&quot;SELECT id,name FROM userdata WHERE name LIKE &#39;%&quot; + name + &quot;%&#39;&quot;)); &#125; 使用sqlmap来探索这个接口： python sqlmap.py -u http:&#x2F;&#x2F;localhost:45678&#x2F;sqlinject&#x2F;jdbcwrong --data name&#x3D;test 一段时间后，sqlmap给出了如下结果： 可以看到，这个接口的name参数有两种可能的注入方式：一种是报错注入，一种是基于时间的盲注。 接下来，仅需简单的三步，就可以直接导出整个用户表的内容了。 第一步，查询当前数据库： python sqlmap.py -u http:&#x2F;&#x2F;localhost:45678&#x2F;sqlinject&#x2F;jdbcwrong --data name&#x3D;test --current-db 可以得到当前数据库是common_mistakes： current database: &#39;common_mistakes&#39; 第二步，查询数据库下的表： python sqlmap.py -u http:&#x2F;&#x2F;localhost:45678&#x2F;sqlinject&#x2F;jdbcwrong --data name&#x3D;test --tables -D &quot;common_mistakes&quot; 可以看到其中有一个敏感表userdata： Database: common_mistakes [7 tables] +--------------------+ | user | | common_store | | hibernate_sequence | | m | | news | | r | | userdata | +--------------------+ 第三步，查询userdata的数据： python sqlmap.py -u http:&#x2F;&#x2F;localhost:45678&#x2F;sqlinject&#x2F;jdbcwrong --data name&#x3D;test -D &quot;common_mistakes&quot; -T &quot;userdata&quot; --dump 你看，用户密码信息一览无遗。当然，你也可以继续查看其他表的数据： Database: common_mistakes Table: userdata [2 entries] +----+-------+----------+ | id | name | password | +----+-------+----------+ | 1 | test1 | haha1 | | 2 | test2 | haha2 | +----+-------+----------+ 在日志中可以看到，sqlmap实现拖库的方式是，让SQL执行后的出错信息包含字段内容。注意看下错误日志的第二行，错误信息中包含ID为2的用户的密码字段的值“haha2”。这，就是报错注入的基本原理： [13:22:27.375] [http-nio-45678-exec-10] [ERROR] [o.a.c.c.C.[.[.[&#x2F;].[dispatcherServlet]:175 ] - Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is org.springframework.dao.DuplicateKeyException: StatementCallback; SQL [SELECT id,name FROM userdata WHERE name LIKE &#39;%test&#39;||(SELECT 0x694a6e64 WHERE 3941&#x3D;3941 AND (SELECT 9927 FROM(SELECT COUNT(*),CONCAT(0x71626a7a71,(SELECT MID((IFNULL(CAST(password AS NCHAR),0x20)),1,54) FROM common_mistakes.userdata ORDER BY id LIMIT 1,1),0x7170706271,FLOOR(RAND(0)*2))x FROM INFORMATION_SCHEMA.PLUGINS GROUP BY x)a))||&#39;%&#39;]; Duplicate entry &#39;qbjzqhaha2qppbq1&#39; for key &#39;&lt;group_key&gt;&#39;; nested exception is java.sql.SQLIntegrityConstraintViolationException: Duplicate entry &#39;qbjzqhaha2qppbq1&#39; for key &#39;&lt;group_key&gt;&#39;] with root cause java.sql.SQLIntegrityConstraintViolationException: Duplicate entry &#39;qbjzqhaha2qppbq1&#39; for key &#39;&lt;group_key&gt;&#39; 既然是这样，我们就实现一个ExceptionHandler来屏蔽异常，看看能否解决注入问题： @ExceptionHandler public void handle(HttpServletRequest req, HandlerMethod method, Exception ex) &#123; log.warn(String.format(&quot;访问 %s -&gt; %s 出现异常！&quot;, req.getRequestURI(), method.toString()), ex); &#125; 重启程序后重新运行刚才的sqlmap命令，可以看到报错注入是没戏了，但使用时间盲注还是可以查询整个表的数据： 所谓盲注，指的是注入后并不能从服务器得到任何执行结果（甚至是错误信息），只能寄希望服务器对于SQL中的真假条件表现出不同的状态。比如，对于布尔盲注来说，可能是“真”可以得到200状态码，“假”可以得到500错误状态码；或者，“真”可以得到内容输出，“假”得不到任何输出。总之，对于不同的SQL注入可以得到不同的输出即可。 在这个案例中，因为接口没有输出，也彻底屏蔽了错误，布尔盲注这招儿行不通了。那么退而求其次的方式，就是时间盲注。也就是说，通过在真假条件中加入SLEEP，来实现通过判断接口的响应时间，知道条件的结果是真还是假。 不管是什么盲注，都是通过真假两种状态来完成的。你可能会好奇，通过真假两种状态如何实现数据导出？ 其实你可以想一下，我们虽然不能直接查询出password字段的值，但可以按字符逐一来查，判断第一个字符是否是a、是否是b……，查询到h时发现响应变慢了，自然知道这就是真的，得出第一位就是h。以此类推，可以查询出整个值。 所以，sqlmap在返回数据的时候，也是一个字符一个字符跳出结果的，并且时间盲注的整个过程会比报错注入慢许多。 你可以引入p6spy工具打印出所有执行的SQL，观察sqlmap构造的一些SQL，来分析其中原理： &lt;dependency&gt; &lt;groupId&gt;com.github.gavlyukovskiy&lt;&#x2F;groupId&gt; &lt;artifactId&gt;p6spy-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.6.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; 所以说，即使屏蔽错误信息错误码，也不能彻底防止SQL注入。真正的解决方式，还是使用参数化查询，让任何外部输入值只可能作为数据来处理。 比如，对于之前那个接口，在SQL语句中使用“?”作为参数占位符，然后提供参数值。这样修改后，sqlmap也就无能为力了： @PostMapping(&quot;jdbcright&quot;) public void jdbcright(@RequestParam(&quot;name&quot;) String name) &#123; log.info(&quot;&#123;&#125;&quot;, jdbcTemplate.queryForList(&quot;SELECT id,name FROM userdata WHERE name LIKE ?&quot;, &quot;%&quot; + name + &quot;%&quot;)); &#125; 对于MyBatis来说，同样需要使用参数化的方式来写SQL语句。在MyBatis中，“#{}”是参数化的方式，“${}”只是占位符替换。 比如LIKE语句。因为使用“#{}”会为参数带上单引号，导致LIKE语法错误，所以一些同学会退而求其次，选择“${}”的方式，比如： @Select(&quot;SELECT id,name FROM &#96;userdata&#96; WHERE name LIKE &#39;%$&#123;name&#125;%&#39;&quot;) List&lt;UserData&gt; findByNameWrong(@Param(&quot;name&quot;) String name); 你可以尝试一下，使用sqlmap同样可以实现注入。正确的做法是，使用“#{}”来参数化name参数，对于LIKE操作可以使用CONCAT函数来拼接%符号： @Select(&quot;SELECT id,name FROM &#96;userdata&#96; WHERE name LIKE CONCAT(&#39;%&#39;,#&#123;name&#125;,&#39;%&#39;)&quot;) List&lt;UserData&gt; findByNameRight(@Param(&quot;name&quot;) String name); 又比如IN子句。因为涉及多个元素的拼接，一些同学不知道如何处理，也可能会选择使用“${}”。因为使用“#{}”会把输入当做一个字符串来对待： &lt;select id&#x3D;&quot;findByNamesWrong&quot; resultType&#x3D;&quot;org.geekbang.time.commonmistakes.codeanddata.sqlinject.UserData&quot;&gt; SELECT id,name FROM &#96;userdata&#96; WHERE name in ($&#123;names&#125;) &lt;&#x2F;select&gt; 但是，这样直接把外部传入的内容替换到IN内部，同样会有注入漏洞： @PostMapping(&quot;mybatiswrong2&quot;) public List mybatiswrong2(@RequestParam(&quot;names&quot;) String names) &#123; return userDataMapper.findByNamesWrong(names); &#125; 你可以使用下面这条命令测试下： python sqlmap.py -u http:&#x2F;&#x2F;localhost:45678&#x2F;sqlinject&#x2F;mybatiswrong2 --data names&#x3D;&quot;&#39;test1&#39;,&#39;test2&#39;&quot; 最后可以发现，有4种可行的注入方式，分别是布尔盲注、报错注入、时间盲注和联合查询注入： 修改方式是，给MyBatis传入一个List，然后使用其foreach标签来拼接出IN中的内容，并确保IN中的每一项都是使用“#{}”来注入参数： @PostMapping(&quot;mybatisright2&quot;) public List mybatisright2(@RequestParam(&quot;names&quot;) List&lt;String&gt; names) &#123; return userDataMapper.findByNamesRight(names); &#125; &lt;select id&#x3D;&quot;findByNamesRight&quot; resultType&#x3D;&quot;org.geekbang.time.commonmistakes.codeanddata.sqlinject.UserData&quot;&gt; SELECT id,name FROM &#96;userdata&#96; WHERE name in &lt;foreach collection&#x3D;&quot;names&quot; item&#x3D;&quot;item&quot; open&#x3D;&quot;(&quot; separator&#x3D;&quot;,&quot; close&#x3D;&quot;)&quot;&gt; #&#123;item&#125; &lt;&#x2F;foreach&gt; &lt;&#x2F;select&gt; 修改后这个接口就不会被注入了，你可以自行测试一下。 小心动态执行代码时代码注入漏洞总结下，我们刚刚看到的SQL注入漏洞的原因是，黑客把SQL攻击代码通过传参混入SQL语句中执行。同样，对于任何解释执行的其他语言代码，也可以产生类似的注入漏洞。我们看一个动态执行JavaScript代码导致注入漏洞的案例。 现在，我们要对用户名实现动态的规则判断：通过ScriptEngineManager获得一个JavaScript脚本引擎，使用Java代码来动态执行JavaScript代码，实现当外部传入的用户名为admin的时候返回1，否则返回0： private ScriptEngineManager scriptEngineManager &#x3D; new ScriptEngineManager(); &#x2F;&#x2F;获得JavaScript脚本引擎 private ScriptEngine jsEngine &#x3D; scriptEngineManager.getEngineByName(&quot;js&quot;); @GetMapping(&quot;wrong&quot;) public Object wrong(@RequestParam(&quot;name&quot;) String name) &#123; try &#123; &#x2F;&#x2F;通过eval动态执行JavaScript脚本，这里name参数通过字符串拼接方式混入JavaScript代码 return jsEngine.eval(String.format(&quot;var name&#x3D;&#39;%s&#39;; name&#x3D;&#x3D;&#39;admin&#39;?1:0;&quot;, name)); &#125; catch (ScriptException e) &#123; e.printStackTrace(); &#125; return null; &#125; 这个功能本身没什么问题： 但是，如果我们把传入的用户名修改为这样： haha&#39;;java.lang.System.exit(0);&#39; 就可以达到关闭整个程序的目的。原因是，我们直接把代码和数据拼接在了一起。外部如果构造了一个特殊的用户名先闭合字符串的单引号，再执行一条System.exit命令的话，就可以满足脚本不出错，命令被执行。 解决这个问题有两种方式。 第一种方式和解决SQL注入一样，需要把外部传入的条件数据仅仅当做数据来对待。我们可以通过SimpleBindings来绑定参数初始化name变量，而不是直接拼接代码： @GetMapping(&quot;right&quot;) public Object right(@RequestParam(&quot;name&quot;) String name) &#123; try &#123; &#x2F;&#x2F;外部传入的参数 Map&lt;String, Object&gt; parm &#x3D; new HashMap&lt;&gt;(); parm.put(&quot;name&quot;, name); &#x2F;&#x2F;name参数作为绑定传给eval方法，而不是拼接JavaScript代码 return jsEngine.eval(&quot;name&#x3D;&#x3D;&#39;admin&#39;?1:0;&quot;, new SimpleBindings(parm)); &#125; catch (ScriptException e) &#123; e.printStackTrace(); &#125; return null; &#125; 这样就避免了注入问题： 第二种解决方法是，使用SecurityManager配合AccessControlContext，来构建一个脚本运行的沙箱环境。脚本能执行的所有操作权限，是通过setPermissions方法精细化设置的： @Slf4j public class ScriptingSandbox &#123; private ScriptEngine scriptEngine; private AccessControlContext accessControlContext; private SecurityManager securityManager; private static ThreadLocal&lt;Boolean&gt; needCheck &#x3D; ThreadLocal.withInitial(() -&gt; false); public ScriptingSandbox(ScriptEngine scriptEngine) throws InstantiationException &#123; this.scriptEngine &#x3D; scriptEngine; securityManager &#x3D; new SecurityManager()&#123; &#x2F;&#x2F;仅在需要的时候检查权限 @Override public void checkPermission(Permission perm) &#123; if (needCheck.get() &amp;&amp; accessControlContext !&#x3D; null) &#123; super.checkPermission(perm, accessControlContext); &#125; &#125; &#125;; &#x2F;&#x2F;设置执行脚本需要的权限 setPermissions(Arrays.asList( new RuntimePermission(&quot;getProtectionDomain&quot;), new PropertyPermission(&quot;jdk.internal.lambda.dumpProxyClasses&quot;,&quot;read&quot;), new FilePermission(Shell.class.getProtectionDomain().getPermissions().elements().nextElement().getName(),&quot;read&quot;), new RuntimePermission(&quot;createClassLoader&quot;), new RuntimePermission(&quot;accessClassInPackage.jdk.internal.org.objectweb.*&quot;), new RuntimePermission(&quot;accessClassInPackage.jdk.nashorn.internal.*&quot;), new RuntimePermission(&quot;accessDeclaredMembers&quot;), new ReflectPermission(&quot;suppressAccessChecks&quot;) )); &#125; &#x2F;&#x2F;设置执行上下文的权限 public void setPermissions(List&lt;Permission&gt; permissionCollection) &#123; Permissions perms &#x3D; new Permissions(); if (permissionCollection !&#x3D; null) &#123; for (Permission p : permissionCollection) &#123; perms.add(p); &#125; &#125; ProtectionDomain domain &#x3D; new ProtectionDomain(new CodeSource(null, (CodeSigner[]) null), perms); accessControlContext &#x3D; new AccessControlContext(new ProtectionDomain[]&#123;domain&#125;); &#125; public Object eval(final String code) &#123; SecurityManager oldSecurityManager &#x3D; System.getSecurityManager(); System.setSecurityManager(securityManager); needCheck.set(true); try &#123; &#x2F;&#x2F;在AccessController的保护下执行脚本 return AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; try &#123; return scriptEngine.eval(code); &#125; catch (ScriptException e) &#123; e.printStackTrace(); &#125; return null; &#125;, accessControlContext); &#125; catch (Exception ex) &#123; log.error(&quot;抱歉，无法执行脚本 &#123;&#125;&quot;, code, ex); &#125; finally &#123; needCheck.set(false); System.setSecurityManager(oldSecurityManager); &#125; return null; &#125; 写一段测试代码，使用刚才定义的ScriptingSandbox沙箱工具类来执行脚本： @GetMapping(&quot;right2&quot;) public Object right2(@RequestParam(&quot;name&quot;) String name) throws InstantiationException &#123; &#x2F;&#x2F;使用沙箱执行脚本 ScriptingSandbox scriptingSandbox &#x3D; new ScriptingSandbox(jsEngine); return scriptingSandbox.eval(String.format(&quot;var name&#x3D;&#39;%s&#39;; name&#x3D;&#x3D;&#39;admin&#39;?1:0;&quot;, name)); &#125; 这次，我们再使用之前的注入脚本调用这个接口： http:&#x2F;&#x2F;localhost:45678&#x2F;codeinject&#x2F;right2?name&#x3D;haha%27;java.lang.System.exit(0);%27 可以看到，结果中抛出了AccessControlException异常，注入攻击失效了： [13:09:36.080] [http-nio-45678-exec-1] [ERROR] [o.g.t.c.c.codeinject.ScriptingSandbox:77 ] - 抱歉，无法执行脚本 var name&#x3D;&#39;haha&#39;;java.lang.System.exit(0);&#39;&#39;; name&#x3D;&#x3D;&#39;admin&#39;?1:0; java.security.AccessControlException: access denied (&quot;java.lang.RuntimePermission&quot; &quot;exitVM.0&quot;) at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) at java.lang.SecurityManager.checkPermission(SecurityManager.java:585) at org.geekbang.time.commonmistakes.codeanddata.codeinject.ScriptingSandbox$1.checkPermission(ScriptingSandbox.java:30) at java.lang.SecurityManager.checkExit(SecurityManager.java:761) at java.lang.Runtime.exit(Runtime.java:107) 在实际应用中，我们可以考虑同时使用这两种方法，确保代码执行的安全性。 XSS必须全方位严防死堵对于业务开发来说，XSS的问题同样要引起关注。 XSS问题的根源在于，原本是让用户传入或输入正常数据的地方，被黑客替换为了JavaScript脚本，页面没有经过转义直接显示了这个数据，然后脚本就被执行了。更严重的是，脚本没有经过转义就保存到了数据库中，随后页面加载数据的时候，数据中混入的脚本又当做代码执行了。黑客可以利用这个漏洞来盗取敏感数据，诱骗用户访问钓鱼网站等。 我们写一段代码测试下。首先，服务端定义两个接口，其中index接口查询用户名信息返回给xss页面，save接口使用@RequestParam注解接收用户名，并创建用户保存到数据库；然后，重定向浏览器到index接口： @RequestMapping(&quot;xss&quot;) @Slf4j @Controller public class XssController &#123; @Autowired private UserRepository userRepository; &#x2F;&#x2F;显示xss页面 @GetMapping public String index(ModelMap modelMap) &#123; &#x2F;&#x2F;查数据库 User user &#x3D; userRepository.findById(1L).orElse(new User()); &#x2F;&#x2F;给View提供Model modelMap.addAttribute(&quot;username&quot;, user.getName()); return &quot;xss&quot;; &#125; &#x2F;&#x2F;保存用户信息 @PostMapping public String save(@RequestParam(&quot;username&quot;) String username, HttpServletRequest request) &#123; User user &#x3D; new User(); user.setId(1L); user.setName(username); userRepository.save(user); &#x2F;&#x2F;保存完成后重定向到首页 return &quot;redirect:&#x2F;xss&#x2F;&quot;; &#125; &#125; &#x2F;&#x2F;用户类，同时作为DTO和Entity @Entity @Data public class User &#123; @Id private Long id; private String name; &#125; 我们使用Thymeleaf模板引擎来渲染页面。模板代码比较简单，页面加载的时候会在标签显示用户名，用户输入用户名提交后调用save接口创建用户： &lt;div style&#x3D;&quot;font-size: 14px&quot;&gt; &lt;form id&#x3D;&quot;myForm&quot; method&#x3D;&quot;post&quot; th:action&#x3D;&quot;@&#123;&#x2F;xss&#x2F;&#125;&quot;&gt; &lt;label th:utext&#x3D;&quot;$&#123;username&#125;&quot;&#x2F;&gt; &lt;input id&#x3D;&quot;username&quot; name&#x3D;&quot;username&quot; size&#x3D;&quot;100&quot; type&#x3D;&quot;text&quot;&#x2F;&gt; &lt;button th:text&#x3D;&quot;Register&quot; type&#x3D;&quot;submit&quot;&#x2F;&gt; &lt;&#x2F;form&gt; &lt;&#x2F;div&gt; 打开xss页面后，在文本框中输入alert(‘test’)点击Register按钮提交，页面会弹出alert对话框： 并且，脚本被保存到了数据库： 你可能想到了，解决方式就是HTML转码。既然是通过@RequestParam来获取请求参数，那我们定义一个@InitBinder实现数据绑定的时候，对字符串进行转码即可： @ControllerAdvice public class SecurityAdvice &#123; @InitBinder protected void initBinder(WebDataBinder binder) &#123; &#x2F;&#x2F;注册自定义的绑定器 binder.registerCustomEditor(String.class, new PropertyEditorSupport() &#123; @Override public String getAsText() &#123; Object value &#x3D; getValue(); return value !&#x3D; null ? value.toString() : &quot;&quot;; &#125; @Override public void setAsText(String text) &#123; &#x2F;&#x2F;赋值时进行HTML转义 setValue(text &#x3D;&#x3D; null ? null : HtmlUtils.htmlEscape(text)); &#125; &#125;); &#125; &#125; 的确，针对这个场景，这种做法是可行的。数据库中保存了转义后的数据，因此数据会被当做HTML显示在页面上，而不是当做脚本执行： 但是，这种处理方式犯了一个严重的错误，那就是没有从根儿上来处理安全问题。因为@InitBinder是Spring Web层面的处理逻辑，如果有代码不通过@RequestParam来获取数据，而是直接从HTTP请求获取数据的话，这种方式就不会奏效。比如这样： user.setName(request.getParameter(&quot;username&quot;)); 更合理的解决方式是，定义一个servlet Filter，通过HttpServletRequestWrapper实现servlet层面的统一参数替换： &#x2F;&#x2F;自定义过滤器 @Component @Order(Ordered.HIGHEST_PRECEDENCE) public class XssFilter implements Filter &#123; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; chain.doFilter(new XssRequestWrapper((HttpServletRequest) request), response); &#125; &#125; public class XssRequestWrapper extends HttpServletRequestWrapper &#123; public XssRequestWrapper(HttpServletRequest request) &#123; super(request); &#125; @Override public String[] getParameterValues(String parameter) &#123; &#x2F;&#x2F;获取多个参数值的时候对所有参数值应用clean方法逐一清洁 return Arrays.stream(super.getParameterValues(parameter)).map(this::clean).toArray(String[]::new); &#125; @Override public String getHeader(String name) &#123; &#x2F;&#x2F;同样清洁请求头 return clean(super.getHeader(name)); &#125; @Override public String getParameter(String parameter) &#123; &#x2F;&#x2F;获取参数单一值也要处理 return clean(super.getParameter(parameter)); &#125; &#x2F;&#x2F;clean方法就是对值进行HTML转义 private String clean(String value) &#123; return StringUtils.isEmpty(value)? &quot;&quot; : HtmlUtils.htmlEscape(value); &#125; &#125; 这样，我们就可以实现所有请求参数的HTML转义了。不过，这种方式还是不够彻底，原因是无法处理通过@RequestBody注解提交的JSON数据。比如，有这样一个PUT接口，直接保存了客户端传入的JSON User对象： @PutMapping public void put(@RequestBody User user) &#123; userRepository.save(user); &#125; 通过Postman请求这个接口，保存到数据库中的数据还是没有转义： 我们需要自定义一个Jackson反列化器，来实现反序列化时的字符串的HTML转义： &#x2F;&#x2F;注册自定义的Jackson反序列器 @Bean public Module xssModule() &#123; SimpleModule module &#x3D; new SimpleModule(); module.module.addDeserializer(String.class, new XssJsonDeserializer()); return module; &#125; public class XssJsonDeserializer extends JsonDeserializer&lt;String&gt; &#123; @Override public String deserialize(JsonParser jsonParser, DeserializationContext ctxt) throws IOException, JsonProcessingException &#123; String value &#x3D; jsonParser.getValueAsString(); if (value !&#x3D; null) &#123; &#x2F;&#x2F;对于值进行HTML转义 return HtmlUtils.htmlEscape(value); &#125; return value; &#125; @Override public Class&lt;String&gt; handledType() &#123; return String.class; &#125; &#125; 这样就实现了既能转义Get&#x2F;Post通过请求参数提交的数据，又能转义请求体中直接提交的JSON数据。 你可能觉得做到这里，我们的防范已经很全面了，但其实不是。这种只能堵新漏，确保新数据进入数据库之前转义。如果因为之前的漏洞，数据库中已经保存了一些JavaScript代码，那么读取的时候同样可能出问题。因此，我们还要实现数据读取的时候也转义。 接下来，我们看一下具体的实现方式。 首先，之前我们处理了JSON反序列化问题，那么就需要同样处理序列化，实现数据从数据库中读取的时候转义，否则读出来的JSON可能包含JavaScript代码。 比如，我们定义这样一个GET接口以JSON来返回用户信息： @GetMapping(&quot;user&quot;) @ResponseBody public User query() &#123; return userRepository.findById(1L).orElse(new User()); &#125; 修改之前的SimpleModule加入自定义序列化器，并且实现序列化时处理字符串转义： &#x2F;&#x2F;注册自定义的Jackson序列器 @Bean public Module xssModule() &#123; SimpleModule module &#x3D; new SimpleModule(); module.addDeserializer(String.class, new XssJsonDeserializer()); module.addSerializer(String.class, new XssJsonSerializer()); return module; &#125; public class XssJsonSerializer extends JsonSerializer&lt;String&gt; &#123; @Override public Class&lt;String&gt; handledType() &#123; return String.class; &#125; @Override public void serialize(String value, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException &#123; if (value !&#x3D; null) &#123; &#x2F;&#x2F;对字符串进行HTML转义 jsonGenerator.writeString(HtmlUtils.htmlEscape(value)); &#125; &#125; &#125; 可以看到，这次读到的JSON也转义了： 其次，我们还需要处理HTML模板。对于Thymeleaf模板引擎，需要注意的是，使用th:utext来显示数据是不会进行转义的，需要使用th:text： &lt;label th:text&#x3D;&quot;$&#123;username&#125;&quot;&#x2F;&gt; 经过修改后，即使数据库中已经保存了JavaScript代码，呈现的时候也只能作为HTML显示了。现在，对于进和出两个方向，我们都实现了补漏。 但，所谓百密总有一疏。为了避免疏漏，进一步控制XSS可能带来的危害，我们还要考虑一种情况：如果需要在Cookie中写入敏感信息的话，我们可以开启HttpOnly属性。这样JavaScript代码就无法读取Cookie了，即便页面被XSS注入了攻击代码，也无法获得我们的Cookie。 写段代码测试一下。定义两个接口，其中readCookie接口读取Key为test的Cookie，writeCookie接口写入Cookie，根据参数HttpOnly确定Cookie是否开启HttpOnly： &#x2F;&#x2F;服务端读取Cookie @GetMapping(&quot;readCookie&quot;) @ResponseBody public String readCookie(@CookieValue(&quot;test&quot;) String cookieValue) &#123; return cookieValue; &#125; &#x2F;&#x2F;服务端写入Cookie @GetMapping(&quot;writeCookie&quot;) @ResponseBody public void writeCookie(@RequestParam(&quot;httpOnly&quot;) boolean httpOnly, HttpServletResponse response) &#123; Cookie cookie &#x3D; new Cookie(&quot;test&quot;, &quot;zhuye&quot;); &#x2F;&#x2F;根据httpOnly入参决定是否开启HttpOnly属性 cookie.setHttpOnly(httpOnly); response.addCookie(cookie); &#125; 可以看到，由于test和_ga这两个Cookie不是HttpOnly的。通过document.cookie可以输出这两个Cookie的内容： 为test这个Cookie启用了HttpOnly属性后，就不能被document.cookie读取到了，输出中只有_ga一项： 但是服务端可以读取到这个cookie： 重点回顾今天，我通过案例，和你具体分析了SQL注入和XSS攻击这两类注入类安全问题。 在学习SQL注入的时候，我们通过sqlmap工具看到了几种常用注入方式，这可能改变了我们对SQL注入威力的认知：对于POST请求、请求没有任何返回数据、请求不会出错的情况下，仍然可以完成注入，并可以导出数据库的所有数据。 对于SQL注入来说，使用参数化的查询是最好的堵漏方式；对于JdbcTemplate来说，我们可以使用“?”作为参数占位符；对于MyBatis来说，我们需要使用“#{}”进行参数化处理。 和SQL注入类似的是，脚本引擎动态执行代码，需要确保外部传入的数据只能作为数据来处理，不能和代码拼接在一起，只能作为参数来处理。代码和数据之间需要划出清晰的界限，否则可能产生代码注入问题。同时，我们可以通过设置一个代码的执行沙箱来细化代码的权限，这样即便产生了注入问题，因为权限受限注入攻击也很难发挥威力。 随后通过学习XSS案例，我们认识到处理安全问题需要确保三点。 第一，要从根本上、从最底层进行堵漏，尽量不要在高层框架层面做，否则堵漏可能不彻底。 第二，堵漏要同时考虑进和出，不仅要确保数据存入数据库的时候进行了转义或过滤，还要在取出数据呈现的时候再次转义，确保万无一失。 第三，除了直接堵漏外，我们还可以通过一些额外的手段限制漏洞的威力。比如，为Cookie设置HttpOnly属性，来防止数据被脚本读取；又比如，尽可能限制字段的最大保存长度，即使出现漏洞，也会因为长度问题限制黑客构造复杂攻击脚本的能力。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在讨论SQL注入案例时，最后那次测试我们看到sqlmap返回了4种注入方式。其中，布尔盲注、时间盲注和报错注入，我都介绍过了。你知道联合查询注入，是什么吗？ 在讨论XSS的时候，对于Thymeleaf模板引擎，我们知道如何让文本进行HTML转义显示。FreeMarker也是Java中很常用的模板引擎，你知道如何处理转义吗？ 你还遇到过其他类型的注入问题吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/30 _ 如何正确保存和传输敏感数据？","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/30 _ 如何正确保存和传输敏感数据？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/30%20_%20%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E4%BF%9D%E5%AD%98%E5%92%8C%E4%BC%A0%E8%BE%93%E6%95%8F%E6%84%9F%E6%95%B0%E6%8D%AE%EF%BC%9F/","excerpt":"","text":"30 | 如何正确保存和传输敏感数据？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 今天，我们从安全角度来聊聊用户名、密码、身份证等敏感信息，应该怎么保存和传输。同时，你还可以进一步复习加密算法中的散列、对称加密和非对称加密算法，以及HTTPS等相关知识。 应该怎样保存用户密码？最敏感的数据恐怕就是用户的密码了。黑客一旦窃取了用户密码，或许就可以登录进用户的账号，消耗其资产、发布不良信息等；更可怕的是，有些用户至始至终都是使用一套密码，密码一旦泄露，就可以被黑客用来登录全网。 为了防止密码泄露，最重要的原则是不要保存用户密码。你可能会觉得很好笑，不保存用户密码，之后用户登录的时候怎么验证？其实，我指的是不保存原始密码，这样即使拖库也不会泄露用户密码。 我经常会听到大家说，不要明文保存用户密码，应该把密码通过MD5加密后保存。这的确是一个正确的方向，但这个说法并不准确。 首先，MD5其实不是真正的加密算法。所谓加密算法，是可以使用密钥把明文加密为密文，随后还可以使用密钥解密出明文，是双向的。 而MD5是散列、哈希算法或者摘要算法。不管多长的数据，使用MD5运算后得到的都是固定长度的摘要信息或指纹信息，无法再解密为原始数据。所以，MD5是单向的。最重要的是，仅仅使用MD5对密码进行摘要，并不安全。 比如，使用如下代码在保持用户信息时，对密码进行了MD5计算： UserData userData &#x3D; new UserData(); userData.setId(1L); userData.setName(name); &#x2F;&#x2F;密码字段使用MD5哈希后保存 userData.setPassword(DigestUtils.md5Hex(password)); return userRepository.save(userData); 通过输出，可以看到密码是32位的MD5： &quot;password&quot;: &quot;325a2cc052914ceeb8c19016c091d2ac&quot; 到某MD5破解网站上输入这个MD5，不到1秒就得到了原始密码： 其实你可以想一下，虽然MD5不可解密，但是我们可以构建一个超大的数据库，把所有20位以内的数字和字母组合的密码全部计算一遍MD5存进去，需要解密的时候搜索一下MD5就可以得到原始值了。这就是字典表。 目前，有些MD5解密网站使用的是彩虹表，是一种使用时间空间平衡的技术，即可以使用更大的空间来降低破解时间，也可以使用更长的破解时间来换取更小的空间。 此外，你可能会觉得多次MD5比较安全，其实并不是这样。比如，如下代码使用两次MD5进行摘要： userData.setPassword(DigestUtils.md5Hex(DigestUtils.md5Hex( password))); 得到下面的MD5： &quot;password&quot;: &quot;ebbca84993fe002bac3a54e90d677d09&quot; 也可以破解出密码，并且破解网站还告知我们这是两次MD5算法： 所以直接保存MD5后的密码是不安全的。一些同学可能会说，还需要加盐。是的，但是加盐如果不当，还是非常不安全，比较重要的有两点。 第一，不能在代码中写死盐，且盐需要有一定的长度，比如这样： userData.setPassword(DigestUtils.md5Hex(&quot;salt&quot; + password)); 得到了如下MD5： &quot;password&quot;: &quot;58b1d63ed8492f609993895d6ba6b93a&quot; 对于这样一串MD5，虽然破解网站上找不到原始密码，但是黑客可以自己注册一个账号，使用一个简单的密码，比如1： &quot;password&quot;: &quot;55f312f84e7785aa1efa552acbf251db&quot; 然后，再去破解网站试一下这个MD5，就可以得到原始密码是salt，也就知道了盐值是salt： 其实，知道盐是什么没什么关系，关键的是我们是在代码里写死了盐，并且盐很短、所有用户都是这个盐。这么做有三个问题： 因为盐太短、太简单了，如果用户原始密码也很简单，那么整个拼起来的密码也很短，这样一般的MD5破解网站都可以直接解密这个MD5，除去盐就知道原始密码了。 相同的盐，意味着使用相同密码的用户MD5值是一样的，知道了一个用户的密码就可能知道了多个。 我们也可以使用这个盐来构建一张彩虹表，虽然会花不少代价，但是一旦构建完成，所有人的密码都可以被破解。 所以，最好是每一个密码都有独立的盐，并且盐要长一点，比如超过20位。 第二，虽然说每个人的盐最好不同，但我也不建议将一部分用户数据作为盐。比如，使用用户名作为盐： userData.setPassword(DigestUtils.md5Hex(name + password)); 如果世界上所有的系统都是按照这个方案来保存密码，那么root、admin这样的用户使用再复杂的密码也总有一天会被破解，因为黑客们完全可以针对这些常用用户名来做彩虹表。所以，盐最好是随机的值，并且是全球唯一的，意味着全球不可能有现成的彩虹表给你用。 正确的做法是，使用全球唯一的、和用户无关的、足够长的随机值作为盐。比如，可以使用UUID作为盐，把盐一起保存到数据库中： userData.setSalt(UUID.randomUUID().toString()); userData.setPassword(DigestUtils.md5Hex(userData.getSalt() + password)); 并且每次用户修改密码的时候都重新计算盐，重新保存新的密码。你可能会问，盐保存在数据库中，那被拖库了不是就可以看到了吗？难道不应该加密保存吗？ 在我看来，盐没有必要加密保存。盐的作用是，防止通过彩虹表快速实现密码“解密”，如果用户的盐都是唯一的，那么生成一次彩虹表只可能拿到一个用户的密码，这样黑客的动力会小很多。 更好的做法是，不要使用像MD5这样快速的摘要算法，而是使用慢一点的算法。比如Spring Security已经废弃了MessageDigestPasswordEncoder，推荐使用BCryptPasswordEncoder，也就是BCrypt来进行密码哈希。BCrypt是为保存密码设计的算法，相比MD5要慢很多。 写段代码来测试一下MD5，以及使用不同代价因子的BCrypt，看看哈希一次密码的耗时。 private static BCryptPasswordEncoder passwordEncoder &#x3D; new BCryptPasswordEncoder(); @GetMapping(&quot;performance&quot;) public void performance() &#123; StopWatch stopWatch &#x3D; new StopWatch(); String password &#x3D; &quot;Abcd1234&quot;; stopWatch.start(&quot;MD5&quot;); &#x2F;&#x2F;MD5 DigestUtils.md5Hex(password); stopWatch.stop(); stopWatch.start(&quot;BCrypt(10)&quot;); &#x2F;&#x2F;代价因子为10的BCrypt String hash1 &#x3D; BCrypt.gensalt(10); BCrypt.hashpw(password, hash1); System.out.println(hash1); stopWatch.stop(); stopWatch.start(&quot;BCrypt(12)&quot;); &#x2F;&#x2F;代价因子为12的BCrypt String hash2 &#x3D; BCrypt.gensalt(12); BCrypt.hashpw(password, hash2); System.out.println(hash2); stopWatch.stop(); stopWatch.start(&quot;BCrypt(14)&quot;); &#x2F;&#x2F;代价因子为14的BCrypt String hash3 &#x3D; BCrypt.gensalt(14); BCrypt.hashpw(password, hash3); System.out.println(hash3); stopWatch.stop(); log.info(&quot;&#123;&#125;&quot;, stopWatch.prettyPrint()); &#125; 可以看到，MD5只需要0.8毫秒，而三次BCrypt哈希（代价因子分别设置为10、12和14）耗时分别是82毫秒、312毫秒和1.2秒： 也就是说，如果制作8位密码长度的MD5彩虹表需要5个月，那么对于BCrypt来说，可能就需要几十年，大部分黑客应该都没有这个耐心。 我们写一段代码观察下，BCryptPasswordEncoder生成的密码哈希的规律： @GetMapping(&quot;better&quot;) public UserData better(@RequestParam(value &#x3D; &quot;name&quot;, defaultValue &#x3D; &quot;zhuye&quot;) String name, @RequestParam(value &#x3D; &quot;password&quot;, defaultValue &#x3D; &quot;Abcd1234&quot;) String password) &#123; UserData userData &#x3D; new UserData(); userData.setId(1L); userData.setName(name); &#x2F;&#x2F;保存哈希后的密码 userData.setPassword(passwordEncoder.encode(password)); userRepository.save(userData); &#x2F;&#x2F;判断密码是否匹配 log.info(&quot;match ? &#123;&#125;&quot;, passwordEncoder.matches(password, userData.getPassword())); return userData; &#125; 我们可以发现三点规律。 第一，我们调用encode、matches方法进行哈希、做密码比对的时候，不需要传入盐。BCrypt把盐作为了算法的一部分，强制我们遵循安全保存密码的最佳实践。 第二，生成的盐和哈希后的密码拼在了一起：$是字段分隔符，其中第一个$后的2a代表算法版本，第二个$后的10是代价因子（默认是10，代表2的10次方次哈希），第三个$后的22个字符是盐，再后面是摘要。所以说，我们不需要使用单独的数据库字段来保存盐。 &quot;password&quot;: &quot;$2a$10$wPWdQwfQO2lMxqSIb6iCROXv7lKnQq5XdMO96iCYCj7boK9pk6QPC&quot; &#x2F;&#x2F;格式为：$&lt;ver&gt;$&lt;cost&gt;$&lt;salt&gt;&lt;digest&gt; 第三，代价因子的值越大，BCrypt哈希的耗时越久。因此，对于代价因子的值，更建议的实践是，根据用户的忍耐程度和硬件，设置一个尽可能大的值。 最后，我们需要注意的是，虽然黑客已经很难通过彩虹表来破解密码了，但是仍然有可能暴力破解密码，也就是对于同一个用户名使用常见的密码逐一尝试登录。因此，除了做好密码哈希保存的工作外，我们还要建设一套完善的安全防御机制，在感知到暴力破解危害的时候，开启短信验证、图形验证码、账号暂时锁定等防御机制来抵御暴力破解。 应该怎么保存姓名和身份证？我们把姓名和身份证，叫做二要素。 现在互联网非常发达，很多服务都可以在网上办理，很多网站仅仅依靠二要素来确认你是谁。所以，二要素是比较敏感的数据，如果在数据库中明文保存，那么数据库被攻破后，黑客就可能拿到大量的二要素信息。如果这些二要素被用来申请贷款等，后果不堪设想。 之前我们提到的单向散列算法，显然不适合用来加密保存二要素，因为数据无法解密。这个时候，我们需要选择真正的加密算法。可供选择的算法，包括对称加密和非对称加密算法两类。 对称加密算法，是使用相同的密钥进行加密和解密。使用对称加密算法来加密双方的通信的话，双方需要先约定一个密钥，加密方才能加密，接收方才能解密。如果密钥在发送的时候被窃取，那么加密就是白忙一场。因此，这种加密方式的特点是，加密速度比较快，但是密钥传输分发有泄露风险。 非对称加密算法，或者叫公钥密码算法。公钥密码是由一对密钥对构成的，使用公钥或者说加密密钥来加密，使用私钥或者说解密密钥来解密，公钥可以任意公开，私钥不能公开。使用非对称加密的话，通信双方可以仅分享公钥用于加密，加密后的数据没有私钥无法解密。因此，这种加密方式的特点是，加密速度比较慢，但是解决了密钥的配送分发安全问题。 但是，对于保存敏感信息的场景来说，加密和解密都是我们的服务端程序，不太需要考虑密钥的分发安全性，也就是说使用非对称加密算法没有太大的意义。在这里，我们使用对称加密算法来加密数据。 接下来，我就重点与你说说对称加密算法。对称加密常用的加密算法，有DES、3DES和AES。 虽然，现在仍有许多老项目使用了DES算法，但我不推荐使用。在1999年的DES挑战赛3中，DES密码破解耗时不到一天，而现在DES密码破解更快，使用DES来加密数据非常不安全。因此，在业务代码中要避免使用DES加密。 而3DES算法，是使用不同的密钥进行三次DES串联调用，虽然解决了DES不够安全的问题，但是比AES慢，也不太推荐。 AES是当前公认的比较安全，兼顾性能的对称加密算法。不过严格来说，AES并不是实际的算法名称，而是算法标准。2000年，NIST选拔出Rijndael算法作为AES的标准。 AES有一个重要的特点就是分组加密体制，一次只能处理128位的明文，然后生成128位的密文。如果要加密很长的明文，那么就需要迭代处理，而迭代方式就叫做模式。网上很多使用AES来加密的代码，使用的是最简单的ECB模式（也叫电子密码本模式），其基本结构如下： 可以看到，这种结构有两个风险：明文和密文是一一对应的，如果明文中有重复的分组，那么密文中可以观察到重复，掌握密文的规律；因为每一个分组是独立加密和解密的 ，如果密文分组的顺序，也可以反过来操纵明文，那么就可以实现不解密密文的情况下，来修改明文。 我们写一段代码来测试下。在下面的代码中，我们使用ECB模式测试： 加密一段包含16个字符的字符串，得到密文A；然后把这段字符串复制一份成为一个32个字符的字符串，再进行加密得到密文B。我们验证下密文B是不是重复了一遍的密文A。 模拟银行转账的场景，假设整个数据由发送方账号、接收方账号、金额三个字段构成。我们尝试改变密文中数据的顺序来操纵明文。 private static final String KEY &#x3D; &quot;secretkey1234567&quot;; &#x2F;&#x2F;密钥 &#x2F;&#x2F;测试ECB模式 @GetMapping(&quot;ecb&quot;) public void ecb() throws Exception &#123; Cipher cipher &#x3D; Cipher.getInstance(&quot;AES&#x2F;ECB&#x2F;NoPadding&quot;); test(cipher, null); &#125; &#x2F;&#x2F;获取加密秘钥帮助方法 private static SecretKeySpec setKey(String secret) &#123; return new SecretKeySpec(secret.getBytes(), &quot;AES&quot;); &#125; &#x2F;&#x2F;测试逻辑 private static void test(Cipher cipher, AlgorithmParameterSpec parameterSpec) throws Exception &#123; &#x2F;&#x2F;初始化Cipher cipher.init(Cipher.ENCRYPT_MODE, setKey(KEY), parameterSpec); &#x2F;&#x2F;加密测试文本 System.out.println(&quot;一次：&quot; + Hex.encodeHexString(cipher.doFinal(&quot;abcdefghijklmnop&quot;.getBytes()))); &#x2F;&#x2F;加密重复一次的测试文本 System.out.println(&quot;两次：&quot; + Hex.encodeHexString(cipher.doFinal(&quot;abcdefghijklmnopabcdefghijklmnop&quot;.getBytes()))); &#x2F;&#x2F;下面测试是否可以通过操纵密文来操纵明文 &#x2F;&#x2F;发送方账号 byte[] sender &#x3D; &quot;1000000000012345&quot;.getBytes(); &#x2F;&#x2F;接收方账号 byte[] receiver &#x3D; &quot;1000000000034567&quot;.getBytes(); &#x2F;&#x2F;转账金额 byte[] money &#x3D; &quot;0000000010000000&quot;.getBytes(); &#x2F;&#x2F;加密发送方账号 System.out.println(&quot;发送方账号：&quot; + Hex.encodeHexString(cipher.doFinal(sender))); &#x2F;&#x2F;加密接收方账号 System.out.println(&quot;接收方账号：&quot; + Hex.encodeHexString(cipher.doFinal(receiver))); &#x2F;&#x2F;加密金额 System.out.println(&quot;金额：&quot; + Hex.encodeHexString(cipher.doFinal(money))); &#x2F;&#x2F;加密完整的转账信息 byte[] result &#x3D; cipher.doFinal(ByteUtils.concatAll(sender, receiver, money)); System.out.println(&quot;完整数据：&quot; + Hex.encodeHexString(result)); &#x2F;&#x2F;用于操纵密文的临时字节数组 byte[] hack &#x3D; new byte[result.length]; &#x2F;&#x2F;把密文前两段交换 System.arraycopy(result, 16, hack, 0, 16); System.arraycopy(result, 0, hack, 16, 16); System.arraycopy(result, 32, hack, 32, 16); cipher.init(Cipher.DECRYPT_MODE, setKey(KEY), parameterSpec); &#x2F;&#x2F;尝试解密 System.out.println(&quot;原始明文：&quot; + new String(ByteUtils.concatAll(sender, receiver, money))); System.out.println(&quot;操纵密文：&quot; + new String(cipher.doFinal(hack))); &#125; 输出如下： 可以看到： 两个相同明文分组产生的密文，就是两个相同的密文分组叠在一起。 在不知道密钥的情况下，我们操纵密文实现了对明文数据的修改，对调了发送方账号和接收方账号。 所以说，ECB模式虽然简单，但是不安全，不推荐使用。我们再看一下另一种常用的加密模式，CBC模式。 CBC模式，在解密或解密之前引入了XOR运算，第一个分组使用外部提供的初始化向量IV，从第二个分组开始使用前一个分组的数据，这样即使明文是一样的，加密后的密文也是不同的，并且分组的顺序不能任意调换。这就解决了ECB模式的缺陷： 我们把之前的代码修改为CBC模式，再次进行测试： private static final String initVector &#x3D; &quot;abcdefghijklmnop&quot;; &#x2F;&#x2F;初始化向量 @GetMapping(&quot;cbc&quot;) public void cbc() throws Exception &#123; Cipher cipher &#x3D; Cipher.getInstance(&quot;AES&#x2F;CBC&#x2F;NoPadding&quot;); IvParameterSpec iv &#x3D; new IvParameterSpec(initVector.getBytes(&quot;UTF-8&quot;)); test(cipher, iv); &#125; 可以看到，相同的明文字符串复制一遍得到的密文并不是重复两个密文分组，并且调换密文分组的顺序无法操纵明文： 其实，除了ECB模式和CBC模式外，AES算法还有CFB、OFB、CTR模式，你可以参考这里了解它们的区别。《实用密码学》一书比较推荐的是CBC和CTR模式。还需要注意的是，ECB和CBC模式还需要设置合适的填充模式，才能处理超过一个分组的数据。 对于敏感数据保存，除了选择AES+合适模式进行加密外，我还推荐以下几个实践： 不要在代码中写死一个固定的密钥和初始化向量，最好和之前提到的盐一样，是唯一、独立并且每次都变化的。 推荐使用独立的加密服务来管控密钥、做加密操作，千万不要把密钥和密文存在一个数据库，加密服务需要设置非常高的管控标准。 数据库中不能保存明文的敏感信息，但可以保存脱敏的信息。普通查询的时候，直接查脱敏信息即可。 接下来，我们按照这个策略完成相关代码实现。 第一步，对于用户姓名和身份证，我们分别保存三个信息，脱敏后的明文、密文和加密ID。加密服务加密后返回密文和加密ID，随后使用加密ID来请求加密服务进行解密： @Data @Entity public class UserData &#123; @Id private Long id; private String idcard;&#x2F;&#x2F;脱敏的身份证 private Long idcardCipherId;&#x2F;&#x2F;身份证加密ID private String idcardCipherText;&#x2F;&#x2F;身份证密文 private String name;&#x2F;&#x2F;脱敏的姓名 private Long nameCipherId;&#x2F;&#x2F;姓名加密ID private String nameCipherText;&#x2F;&#x2F;姓名密文 &#125; 第二步，加密服务数据表保存加密ID、初始化向量和密钥。加密服务表中没有密文，实现了密文和密钥分离保存： @Data @Entity public class CipherData &#123; @Id @GeneratedValue(strategy &#x3D; AUTO) private Long id; private String iv;&#x2F;&#x2F;初始化向量 private String secureKey;&#x2F;&#x2F;密钥 &#125; 第三步，加密服务使用GCM模式（ Galois&#x2F;Counter Mode）的AES-256对称加密算法，也就是AES-256-GCM。 这是一种AEAD（Authenticated Encryption with Associated Data）认证加密算法，除了能实现普通加密算法提供的保密性之外，还能实现可认证性和密文完整性，是目前最推荐的AES模式。 使用类似GCM的AEAD算法进行加解密，除了需要提供初始化向量和密钥之外，还可以提供一个AAD（附加认证数据，additional authenticated data），用于验证未包含在明文中的附加信息，解密时不使用加密时的AAD将解密失败。其实，GCM模式的内部使用的就是CTR模式，只不过还使用了GMAC签名算法，对密文进行签名实现完整性校验。 接下来，我们实现基于AES-256-GCM的加密服务，包含下面的主要逻辑： 加密时允许外部传入一个AAD用于认证，加密服务每次都会使用新生成的随机值作为密钥和初始化向量。 在加密后，加密服务密钥和初始化向量保存到数据库中，返回加密ID作为本次加密的标识。 应用解密时，需要提供加密ID、密文和加密时的AAD来解密。加密服务使用加密ID，从数据库查询出密钥和初始化向量。 这段逻辑的实现代码比较长，我加了详细注释方便你仔细阅读： @Service public class CipherService &#123; &#x2F;&#x2F;密钥长度 public static final int AES_KEY_SIZE &#x3D; 256; &#x2F;&#x2F;初始化向量长度 public static final int GCM_IV_LENGTH &#x3D; 12; &#x2F;&#x2F;GCM身份认证Tag长度 public static final int GCM_TAG_LENGTH &#x3D; 16; @Autowired private CipherRepository cipherRepository; &#x2F;&#x2F;内部加密方法 public static byte[] doEncrypt(byte[] plaintext, SecretKey key, byte[] iv, byte[] aad) throws Exception &#123; &#x2F;&#x2F;加密算法 Cipher cipher &#x3D; Cipher.getInstance(&quot;AES&#x2F;GCM&#x2F;NoPadding&quot;); &#x2F;&#x2F;Key规范 SecretKeySpec keySpec &#x3D; new SecretKeySpec(key.getEncoded(), &quot;AES&quot;); &#x2F;&#x2F;GCM参数规范 GCMParameterSpec gcmParameterSpec &#x3D; new GCMParameterSpec(GCM_TAG_LENGTH * 8, iv); &#x2F;&#x2F;加密模式 cipher.init(Cipher.ENCRYPT_MODE, keySpec, gcmParameterSpec); &#x2F;&#x2F;设置aad if (aad !&#x3D; null) cipher.updateAAD(aad); &#x2F;&#x2F;加密 byte[] cipherText &#x3D; cipher.doFinal(plaintext); return cipherText; &#125; &#x2F;&#x2F;内部解密方法 public static String doDecrypt(byte[] cipherText, SecretKey key, byte[] iv, byte[] aad) throws Exception &#123; &#x2F;&#x2F;加密算法 Cipher cipher &#x3D; Cipher.getInstance(&quot;AES&#x2F;GCM&#x2F;NoPadding&quot;); &#x2F;&#x2F;Key规范 SecretKeySpec keySpec &#x3D; new SecretKeySpec(key.getEncoded(), &quot;AES&quot;); &#x2F;&#x2F;GCM参数规范 GCMParameterSpec gcmParameterSpec &#x3D; new GCMParameterSpec(GCM_TAG_LENGTH * 8, iv); &#x2F;&#x2F;解密模式 cipher.init(Cipher.DECRYPT_MODE, keySpec, gcmParameterSpec); &#x2F;&#x2F;设置aad if (aad !&#x3D; null) cipher.updateAAD(aad); &#x2F;&#x2F;解密 byte[] decryptedText &#x3D; cipher.doFinal(cipherText); return new String(decryptedText); &#125; &#x2F;&#x2F;加密入口 public CipherResult encrypt(String data, String aad) throws Exception &#123; &#x2F;&#x2F;加密结果 CipherResult encryptResult &#x3D; new CipherResult(); &#x2F;&#x2F;密钥生成器 KeyGenerator keyGenerator &#x3D; KeyGenerator.getInstance(&quot;AES&quot;); &#x2F;&#x2F;生成密钥 keyGenerator.init(AES_KEY_SIZE); SecretKey key &#x3D; keyGenerator.generateKey(); &#x2F;&#x2F;IV数据 byte[] iv &#x3D; new byte[GCM_IV_LENGTH]; &#x2F;&#x2F;随机生成IV SecureRandom random &#x3D; new SecureRandom(); random.nextBytes(iv); &#x2F;&#x2F;处理aad byte[] aaddata &#x3D; null; if (!StringUtils.isEmpty(aad)) aaddata &#x3D; aad.getBytes(); &#x2F;&#x2F;获得密文 encryptResult.setCipherText(Base64.getEncoder().encodeToString(doEncrypt(data.getBytes(), key, iv, aaddata))); &#x2F;&#x2F;加密上下文数据 CipherData cipherData &#x3D; new CipherData(); &#x2F;&#x2F;保存IV cipherData.setIv(Base64.getEncoder().encodeToString(iv)); &#x2F;&#x2F;保存密钥 cipherData.setSecureKey(Base64.getEncoder().encodeToString(key.getEncoded())); cipherRepository.save(cipherData); &#x2F;&#x2F;返回本地加密ID encryptResult.setId(cipherData.getId()); return encryptResult; &#125; &#x2F;&#x2F;解密入口 public String decrypt(long cipherId, String cipherText, String aad) throws Exception &#123; &#x2F;&#x2F;使用加密ID找到加密上下文数据 CipherData cipherData &#x3D; cipherRepository.findById(cipherId).orElseThrow(() -&gt; new IllegalArgumentException(&quot;invlaid cipherId&quot;)); &#x2F;&#x2F;加载密钥 byte[] decodedKey &#x3D; Base64.getDecoder().decode(cipherData.getSecureKey()); &#x2F;&#x2F;初始化密钥 SecretKey originalKey &#x3D; new SecretKeySpec(decodedKey, 0, decodedKey.length, &quot;AES&quot;); &#x2F;&#x2F;加载IV byte[] decodedIv &#x3D; Base64.getDecoder().decode(cipherData.getIv()); &#x2F;&#x2F;处理aad byte[] aaddata &#x3D; null; if (!StringUtils.isEmpty(aad)) aaddata &#x3D; aad.getBytes(); &#x2F;&#x2F;解密 return doDecrypt(Base64.getDecoder().decode(cipherText.getBytes()), originalKey, decodedIv, aaddata); &#125; &#125; 第四步，分别实现加密和解密接口用于测试。 我们可以让用户选择，如果需要保护二要素的话，就自己输入一个查询密码作为AAD。系统需要读取用户敏感信息的时候，还需要用户提供这个密码，否则无法解密。这样一来，即使黑客拿到了用户数据库的密文、加密服务的密钥和IV，也会因为缺少AAD无法解密： @Autowired private CipherService cipherService; &#x2F;&#x2F;加密 @GetMapping(&quot;right&quot;) public UserData right(@RequestParam(value &#x3D; &quot;name&quot;, defaultValue &#x3D; &quot;朱晔&quot;) String name, @RequestParam(value &#x3D; &quot;idcard&quot;, defaultValue &#x3D; &quot;300000000000001234&quot;) String idCard, @RequestParam(value &#x3D; &quot;aad&quot;, required &#x3D; false)String aad) throws Exception &#123; UserData userData &#x3D; new UserData(); userData.setId(1L); &#x2F;&#x2F;脱敏姓名 userData.setName(chineseName(name)); &#x2F;&#x2F;脱敏身份证 userData.setIdcard(idCard(idCard)); &#x2F;&#x2F;加密姓名 CipherResult cipherResultName &#x3D; cipherService.encrypt(name,aad); userData.setNameCipherId(cipherResultName.getId()); userData.setNameCipherText(cipherResultName.getCipherText()); &#x2F;&#x2F;加密身份证 CipherResult cipherResultIdCard &#x3D; cipherService.encrypt(idCard,aad); userData.setIdcardCipherId(cipherResultIdCard.getId()); userData.setIdcardCipherText(cipherResultIdCard.getCipherText()); return userRepository.save(userData); &#125; &#x2F;&#x2F;解密 @GetMapping(&quot;read&quot;) public void read(@RequestParam(value &#x3D; &quot;aad&quot;, required &#x3D; false)String aad) throws Exception &#123; &#x2F;&#x2F;查询用户信息 UserData userData &#x3D; userRepository.findById(1L).get(); &#x2F;&#x2F;使用AAD来解密姓名和身份证 log.info(&quot;name : &#123;&#125; idcard : &#123;&#125;&quot;, cipherService.decrypt(userData.getNameCipherId(), userData.getNameCipherText(),aad), cipherService.decrypt(userData.getIdcardCipherId(), userData.getIdcardCipherText(),aad)); &#125; &#x2F;&#x2F;脱敏身份证 private static String idCard(String idCard) &#123; String num &#x3D; StringUtils.right(idCard, 4); return StringUtils.leftPad(num, StringUtils.length(idCard), &quot;*&quot;); &#125; &#x2F;&#x2F;脱敏姓名 public static String chineseName(String chineseName) &#123; String name &#x3D; StringUtils.left(chineseName, 1); return StringUtils.rightPad(name, StringUtils.length(chineseName), &quot;*&quot;); 访问加密接口获得如下结果，可以看到数据库表中只有脱敏数据和密文： &#123;&quot;id&quot;:1,&quot;name&quot;:&quot;朱*&quot;,&quot;idcard&quot;:&quot;**************1234&quot;,&quot;idcardCipherId&quot;:26346,&quot;idcardCipherText&quot;:&quot;t&#x2F;wIh1XTj00wJP1Lt3aGzSvn9GcqQWEwthN58KKU4KZ4Tw&#x3D;&#x3D;&quot;,&quot;nameCipherId&quot;:26347,&quot;nameCipherText&quot;:&quot;+gHrk1mWmveBMVUo+CYon8Zjj9QAtw&#x3D;&#x3D;&quot;&#125; 访问解密接口，可以看到解密成功了： [21:46:00.079] [http-nio-45678-exec-6] [INFO ] [o.g.t.c.s.s.StoreIdCardController:102 ] - name : 朱晔 idcard : 300000000000001234 如果AAD输入不对，会得到如下异常： javax.crypto.AEADBadTagException: Tag mismatch! at com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:578) at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:853) at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:446) at javax.crypto.Cipher.doFinal(Cipher.java:2164) 经过这样的设计，二要素就比较安全了。黑客要查询用户二要素的话，需要同时拿到密文、IV+密钥、AAD。而这三者可能由三方掌管，要全部拿到比较困难。 用一张图说清楚HTTPS我们知道，HTTP协议传输数据使用的是明文。那在传输敏感信息的场景下，如果客户端和服务端中间有一个黑客作为中间人拦截请求，就可以窃听到这些数据，还可以修改客户端传过来的数据。这就是很大的安全隐患。 为解决这个安全隐患，有了HTTPS协议。HTTPS&#x3D;SSL&#x2F;TLS+HTTP，通过使用一系列加密算法来确保信息安全传输，以实现数据传输的机密性、完整性和权威性。 机密性：使用非对称加密来加密密钥，然后使用密钥来加密数据，既安全又解决了非对称加密大量数据慢的问题。你可以做一个实验来测试两者的差距。 完整性：使用散列算法对信息进行摘要，确保信息完整无法被中间人篡改。 权威性：使用数字证书，来确保我们是在和合法的服务端通信。 可以看出，理解HTTPS的流程，将有助于我们理解各种加密算法的区别，以及证书的意义。此外，SSL&#x2F;TLS还是混合加密系统的一个典范，如果你需要自己开发应用层数据加密系统，也可以参考它的流程。 那么，我们就来看看HTTPS TLS 1.2连接（RSA握手）的整个过程吧。 作为准备工作，网站管理员需要申请并安装CA证书到服务端。CA证书中包含非对称加密的公钥、网站域名等信息，密钥是服务端自己保存的，不会在任何地方公开。 建立HTTPS连接的过程，首先是TCP握手，然后是TLS握手的一系列工作，包括： 客户端告知服务端自己支持的密码套件（比如TLS_RSA_WITH_AES_256_GCM_SHA384，其中RSA是密钥交换的方式，AES_256_GCM是加密算法，SHA384是消息验证摘要算法），提供客户端随机数。 服务端应答选择的密码套件，提供服务端随机数。 服务端发送CA证书给客户端，客户端验证CA证书（后面详细说明）。 客户端生成PreMasterKey，并使用非对称加密+公钥加密PreMasterKey。 客户端把加密后的PreMasterKey传给服务端。 服务端使用非对称加密+私钥解密得到PreMasterKey，并使用PreMasterKey+两个随机数，生成MasterKey。 客户端也使用PreMasterKey+两个随机数生成MasterKey。 客户端告知服务端之后将进行加密传输。 客户端使用MasterKey配合对称加密算法，进行对称加密测试。 服务端也使用MasterKey配合对称加密算法，进行对称加密测试。 接下来，客户端和服务端的所有通信都是加密通信，并且数据通过签名确保无法篡改。你可能会问，客户端怎么验证CA证书呢？ 其实，CA证书是一个证书链，你可以看一下上图的左边部分： 从服务端拿到的CA证书是用户证书，我们需要通过证书中的签发人信息找到上级中间证书，再网上找到根证书。 根证书只有为数不多的权威机构才能生成，一般预置在OS中，根本无法伪造。 找到根证书后，提取其公钥来验证中间证书的签名，判断其权威性。 最后再拿到中间证书的公钥，验证用户证书的签名。 这，就验证了用户证书的合法性，然后再校验其有效期、域名等信息进一步验证有效性。 总结一下，TLS通过巧妙的流程和算法搭配解决了传输安全问题：使用对称加密加密数据，使用非对称加密算法确保密钥无法被中间人解密；使用CA证书链认证，确保中间人无法伪造自己的证书和公钥。 如果网站涉及敏感数据的传输，必须使用HTTPS协议。作为用户，如果你看到网站不是HTTPS的或者看到无效证书警告，也不应该继续使用这个网站，以免敏感信息被泄露。 重点回顾今天，我们一起学习了如何保存和传输敏感数据。我来带你回顾一下重点内容。 对于数据保存，你需要记住两点： 用户密码不能加密保存，更不能明文保存，需要使用全球唯一的、具有一定长度的、随机的盐，配合单向散列算法保存。使用BCrypt算法，是一个比较好的实践。 诸如姓名和身份证这种需要可逆解密查询的敏感信息，需要使用对称加密算法保存。我的建议是，把脱敏数据和密文保存在业务数据库，独立使用加密服务来做数据加解密；对称加密需要用到的密钥和初始化向量，可以和业务数据库分开保存。 对于数据传输，则务必通过SSL&#x2F;TLS进行传输。对于用于客户端到服务端传输数据的HTTP，我们需要使用基于SSL&#x2F;TLS的HTTPS。对于一些走TCP的RPC服务，同样可以使用SSL&#x2F;TLS来确保传输安全。 最后，我要提醒你的是，如果不确定应该如何实现加解密方案或流程，可以咨询公司内部的安全专家，或是参考业界各大云厂商的方案，切勿自己想当然地去设计流程，甚至创造加密算法。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 虽然我们把用户名和密码脱敏加密保存在数据库中，但日志中可能还存在明文的敏感数据。你有什么思路在框架或中间件层面，对日志进行脱敏吗？ 你知道HTTPS双向认证的目的是什么吗？流程上又有什么区别呢？ 关于各种加密算法，你还遇到过什么坑吗？你又是如何保存敏感数据的呢？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/31 _ 加餐1：带你吃透课程中Java 8的那些重要知识点（一）","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/31 _ 加餐1：带你吃透课程中Java 8的那些重要知识点（一）/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/31%20_%20%E5%8A%A0%E9%A4%901%EF%BC%9A%E5%B8%A6%E4%BD%A0%E5%90%83%E9%80%8F%E8%AF%BE%E7%A8%8B%E4%B8%ADJava%208%E7%9A%84%E9%82%A3%E4%BA%9B%E9%87%8D%E8%A6%81%E7%9F%A5%E8%AF%86%E7%82%B9%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"31 | 加餐1：带你吃透课程中Java 8的那些重要知识点（一）作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 Java 8是目前最常用的JDK版本，在增强代码可读性、简化代码方面，相比Java 7增加了很多功能，比如Lambda、Stream流操作、并行流（ParallelStream）、Optional可空类型、新日期时间类型等。 这个课程中的所有案例，都充分使用了Java 8的各种特性来简化代码。这也就意味着，如果你不了解这些特性的话，理解课程内的Demo可能会有些困难。因此，我将这些特性，单独拎了出来组成了两篇加餐。由于后面有单独一节课去讲Java 8的日期时间类型，所以这里就不赘述了。 如何在项目中用上Lambda表达式和Stream操作？Java 8的特性有很多，除了这两篇加餐外，我再给你推荐一本全面介绍Java 8的书，叫《Java实战（第二版）》。此外，有同学在留言区问，怎么把Lambda表达式和Stream操作运用到项目中。其实，业务代码中可以使用这些特性的地方有很多。 这里，为了帮助你学习，并把这些特性用到业务开发中，我有三个小建议。 第一，从List的操作开始，先尝试把遍历List来筛选数据和转换数据的操作，使用Stream的filter和map实现，这是Stream最常用、最基本的两个API。你可以重点看看接下来两节的内容来入门。 第二，使用高级的IDE来写代码，以此找到可以利用Java 8语言特性简化代码的地方。比如，对于IDEA，我们可以把匿名类型使用Lambda替换的检测规则，设置为Error级别严重程度： 这样运行IDEA的Inspect Code的功能，可以在Error级别的错误中看到这个问题，引起更多关注，帮助我们建立使用Lambda表达式的习惯： 第三，如果你不知道如何把匿名类转换为Lambda表达式，可以借助IDE来重构： 反过来，如果你在学习课程内案例时，如果感觉阅读Lambda表达式和Stream API比较吃力，同样可以借助IDE把Java 8的写法转换为使用循环的写法： 或者是把Lambda表达式替换为匿名类： Lambda表达式Lambda表达式的初衷是，进一步简化匿名类的语法（不过实现上，Lambda表达式并不是匿名类的语法糖），使Java走向函数式编程。对于匿名类，虽然没有类名，但还是要给出方法定义。这里有个例子，分别使用匿名类和Lambda表达式创建一个线程打印字符串： &#x2F;&#x2F;匿名类 new Thread(new Runnable()&#123; @Override public void run()&#123; System.out.println(&quot;hello1&quot;); &#125; &#125;).start(); &#x2F;&#x2F;Lambda表达式 new Thread(() -&gt; System.out.println(&quot;hello2&quot;)).start(); 那么，Lambda表达式如何匹配Java的类型系统呢？ 答案就是，函数式接口。 函数式接口是一种只有单一抽象方法的接口，使用@FunctionalInterface来描述，可以隐式地转换成 Lambda 表达式。使用Lambda表达式来实现函数式接口，不需要提供类名和方法定义，通过一行代码提供函数式接口的实例，就可以让函数成为程序中的头等公民，可以像普通数据一样作为参数传递，而不是作为一个固定的类中的固定方法。 那，函数式接口到底是什么样的呢？java.util.function包中定义了各种函数式接口。比如，用于提供数据的Supplier接口，就只有一个get抽象方法，没有任何入参、有一个返回值： @FunctionalInterface public interface Supplier&lt;T&gt; &#123; &#x2F;** * Gets a result. * * @return a result *&#x2F; T get(); &#125; 我们可以使用Lambda表达式或方法引用，来得到Supplier接口的实例： &#x2F;&#x2F;使用Lambda表达式提供Supplier接口实现，返回OK字符串 Supplier&lt;String&gt; stringSupplier &#x3D; ()-&gt;&quot;OK&quot;; &#x2F;&#x2F;使用方法引用提供Supplier接口实现，返回空字符串 Supplier&lt;String&gt; supplier &#x3D; String::new; 这样，是不是很方便？为了帮你掌握函数式接口及其用法，我再举几个使用Lambda表达式或方法引用来构建函数的例子： &#x2F;&#x2F;Predicate接口是输入一个参数，返回布尔值。我们通过and方法组合两个Predicate条件，判断是否值大于0并且是偶数 Predicate&lt;Integer&gt; positiveNumber &#x3D; i -&gt; i &gt; 0; Predicate&lt;Integer&gt; evenNumber &#x3D; i -&gt; i % 2 &#x3D;&#x3D; 0; assertTrue(positiveNumber.and(evenNumber).test(2)); &#x2F;&#x2F;Consumer接口是消费一个数据。我们通过andThen方法组合调用两个Consumer，输出两行abcdefg Consumer&lt;String&gt; println &#x3D; System.out::println; println.andThen(println).accept(&quot;abcdefg&quot;); &#x2F;&#x2F;Function接口是输入一个数据，计算后输出一个数据。我们先把字符串转换为大写，然后通过andThen组合另一个Function实现字符串拼接 Function&lt;String, String&gt; upperCase &#x3D; String::toUpperCase; Function&lt;String, String&gt; duplicate &#x3D; s -&gt; s.concat(s); assertThat(upperCase.andThen(duplicate).apply(&quot;test&quot;), is(&quot;TESTTEST&quot;)); &#x2F;&#x2F;Supplier是提供一个数据的接口。这里我们实现获取一个随机数 Supplier&lt;Integer&gt; random &#x3D; ()-&gt;ThreadLocalRandom.current().nextInt(); System.out.println(random.get()); &#x2F;&#x2F;BinaryOperator是输入两个同类型参数，输出一个同类型参数的接口。这里我们通过方法引用获得一个整数加法操作，通过Lambda表达式定义一个减法操作，然后依次调用 BinaryOperator&lt;Integer&gt; add &#x3D; Integer::sum; BinaryOperator&lt;Integer&gt; subtraction &#x3D; (a, b) -&gt; a - b; assertThat(subtraction.apply(add.apply(1, 2), 3), is(0)); Predicate、Function等函数式接口，还使用default关键字实现了几个默认方法。这样一来，它们既可以满足函数式接口只有一个抽象方法，又能为接口提供额外的功能： @FunctionalInterface public interface Function&lt;T, R&gt; &#123; R apply(T t); default &lt;V&gt; Function&lt;V, R&gt; compose(Function&lt;? super V, ? extends T&gt; before) &#123; Objects.requireNonNull(before); return (V v) -&gt; apply(before.apply(v)); &#125; default &lt;V&gt; Function&lt;T, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) &#123; Objects.requireNonNull(after); return (T t) -&gt; after.apply(apply(t)); &#125; &#125; 很明显，Lambda表达式给了我们复用代码的更多可能性：我们可以把一大段逻辑中变化的部分抽象出函数式接口，由外部方法提供函数实现，重用方法内的整体逻辑处理。 不过需要注意的是，在自定义函数式接口之前，可以先确认下java.util.function包中的43个标准函数式接口是否能满足需求，我们要尽可能重用这些接口，因为使用大家熟悉的标准接口可以提高代码的可读性。 使用Java 8简化代码这一部分，我会通过几个具体的例子，带你感受一下使用Java 8简化代码的三个重要方面： 使用Stream简化集合操作； 使用Optional简化判空逻辑； JDK8结合Lambda和Stream对各种类的增强。 使用Stream简化集合操作Lambda表达式可以帮我们用简短的代码实现方法的定义，给了我们复用代码的更多可能性。利用这个特性，我们可以把集合的投影、转换、过滤等操作抽象成通用的接口，然后通过Lambda表达式传入其具体实现，这也就是Stream操作。 我们看一个具体的例子。这里有一段20行左右的代码，实现了如下的逻辑： 把整数列表转换为Point2D列表； 遍历Point2D列表过滤出Y轴&gt;1的对象； 计算Point2D点到原点的距离； 累加所有计算出的距离，并计算距离的平均值。 private static double calc(List&lt;Integer&gt; ints) &#123; &#x2F;&#x2F;临时中间集合 List&lt;Point2D&gt; point2DList &#x3D; new ArrayList&lt;&gt;(); for (Integer i : ints) &#123; point2DList.add(new Point2D.Double((double) i % 3, (double) i &#x2F; 3)); &#125; &#x2F;&#x2F;临时变量，纯粹是为了获得最后结果需要的中间变量 double total &#x3D; 0; int count &#x3D; 0; for (Point2D point2D : point2DList) &#123; &#x2F;&#x2F;过滤 if (point2D.getY() &gt; 1) &#123; &#x2F;&#x2F;算距离 double distance &#x3D; point2D.distance(0, 0); total +&#x3D; distance; count++; &#125; &#125; &#x2F;&#x2F;注意count可能为0的可能 return count &gt;0 ? total &#x2F; count : 0; &#125; 现在，我们可以使用Stream配合Lambda表达式来简化这段代码。简化后一行代码就可以实现这样的逻辑，更重要的是代码可读性更强了，通过方法名就可以知晓大概是在做什么事情。比如： map方法传入的是一个Function，可以实现对象转换； filter方法传入一个Predicate，实现对象的布尔判断，只保留返回true的数据； mapToDouble用于把对象转换为double； 通过average方法返回一个OptionalDouble，代表可能包含值也可能不包含值的可空double。 下面的第三行代码，就实现了上面方法的所有工作： List&lt;Integer&gt; ints &#x3D; Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8); double average &#x3D; calc(ints); double streamResult &#x3D; ints.stream() .map(i -&gt; new Point2D.Double((double) i % 3, (double) i &#x2F; 3)) .filter(point -&gt; point.getY() &gt; 1) .mapToDouble(point -&gt; point.distance(0, 0)) .average() .orElse(0); &#x2F;&#x2F;如何用一行代码来实现，比较一下可读性 assertThat(average, is(streamResult)); 到这里，你可能会问了，OptionalDouble又是怎么回事儿？ 有关Optional可空类型其实，类似OptionalDouble、OptionalInt、OptionalLong等，是服务于基本类型的可空对象。此外，Java8还定义了用于引用类型的Optional类。使用Optional，不仅可以避免使用Stream进行级联调用的空指针问题；更重要的是，它提供了一些实用的方法帮我们避免判空逻辑。 如下是一些例子，演示了如何使用Optional来避免空指针，以及如何使用它的fluent API简化冗长的if-else判空逻辑： @Test(expected &#x3D; IllegalArgumentException.class) public void optional() &#123; &#x2F;&#x2F;通过get方法获取Optional中的实际值 assertThat(Optional.of(1).get(), is(1)); &#x2F;&#x2F;通过ofNullable来初始化一个null，通过orElse方法实现Optional中无数据的时候返回一个默认值 assertThat(Optional.ofNullable(null).orElse(&quot;A&quot;), is(&quot;A&quot;)); &#x2F;&#x2F;OptionalDouble是基本类型double的Optional对象，isPresent判断有无数据 assertFalse(OptionalDouble.empty().isPresent()); &#x2F;&#x2F;通过map方法可以对Optional对象进行级联转换，不会出现空指针，转换后还是一个Optional assertThat(Optional.of(1).map(Math::incrementExact).get(), is(2)); &#x2F;&#x2F;通过filter实现Optional中数据的过滤，得到一个Optional，然后级联使用orElse提供默认值 assertThat(Optional.of(1).filter(integer -&gt; integer % 2 &#x3D;&#x3D; 0).orElse(null), is(nullValue())); &#x2F;&#x2F;通过orElseThrow实现无数据时抛出异常 Optional.empty().orElseThrow(IllegalArgumentException::new); &#125; 我把Optional类的常用方法整理成了一张图，你可以对照案例再复习一下： Java 8类对于函数式API的增强除了Stream之外，Java 8中有很多类也都实现了函数式的功能。 比如，要通过HashMap实现一个缓存的操作，在Java 8之前我们可能会写出这样的getProductAndCache方法：先判断缓存中是否有值；如果没有值，就从数据库搜索取值；最后，把数据加入缓存。 private Map&lt;Long, Product&gt; cache &#x3D; new ConcurrentHashMap&lt;&gt;(); private Product getProductAndCache(Long id) &#123; Product product &#x3D; null; &#x2F;&#x2F;Key存在，返回Value if (cache.containsKey(id)) &#123; product &#x3D; cache.get(id); &#125; else &#123; &#x2F;&#x2F;不存在，则获取Value &#x2F;&#x2F;需要遍历数据源查询获得Product for (Product p : Product.getData()) &#123; if (p.getId().equals(id)) &#123; product &#x3D; p; break; &#125; &#125; &#x2F;&#x2F;加入ConcurrentHashMap if (product !&#x3D; null) cache.put(id, product); &#125; return product; &#125; @Test public void notcoolCache() &#123; getProductAndCache(1L); getProductAndCache(100L); System.out.println(cache); assertThat(cache.size(), is(1)); assertTrue(cache.containsKey(1L)); &#125; 而在Java 8中，我们利用ConcurrentHashMap的computeIfAbsent方法，用一行代码就可以实现这样的繁琐操作： private Product getProductAndCacheCool(Long id) &#123; return cache.computeIfAbsent(id, i -&gt; &#x2F;&#x2F;当Key不存在的时候提供一个Function来代表根据Key获取Value的过程 Product.getData().stream() .filter(p -&gt; p.getId().equals(i)) &#x2F;&#x2F;过滤 .findFirst() &#x2F;&#x2F;找第一个，得到Optional&lt;Product&gt; .orElse(null)); &#x2F;&#x2F;如果找不到Product，则使用null &#125; @Test public void coolCache() &#123; getProductAndCacheCool(1L); getProductAndCacheCool(100L); System.out.println(cache); assertThat(cache.size(), is(1)); assertTrue(cache.containsKey(1L)); &#125; computeIfAbsent方法在逻辑上相当于： if (map.get(key) &#x3D;&#x3D; null) &#123; V newValue &#x3D; mappingFunction.apply(key); if (newValue !&#x3D; null) map.put(key, newValue); &#125; 又比如，利用Files.walk返回一个Path的流，通过两行代码就能实现递归搜索+grep的操作。整个逻辑是：递归搜索文件夹，查找所有的.java文件；然后读取文件每一行内容，用正则表达式匹配public class关键字；最后输出文件名和这行内容。 @Test public void filesExample() throws IOException &#123; &#x2F;&#x2F;无限深度，递归遍历文件夹 try (Stream&lt;Path&gt; pathStream &#x3D; Files.walk(Paths.get(&quot;.&quot;))) &#123; pathStream.filter(Files::isRegularFile) &#x2F;&#x2F;只查普通文件 .filter(FileSystems.getDefault().getPathMatcher(&quot;glob:**&#x2F;*.java&quot;)::matches) &#x2F;&#x2F;搜索java源码文件 .flatMap(ThrowingFunction.unchecked(path -&gt; Files.readAllLines(path).stream() &#x2F;&#x2F;读取文件内容，转换为Stream&lt;List&gt; .filter(line -&gt; Pattern.compile(&quot;public class&quot;).matcher(line).find()) &#x2F;&#x2F;使用正则过滤带有public class的行 .map(line -&gt; path.getFileName() + &quot; &gt;&gt; &quot; + line))) &#x2F;&#x2F;把这行文件内容转换为文件名+行 .forEach(System.out::println); &#x2F;&#x2F;打印所有的行 &#125; &#125; 输出结果如下： 我再和你分享一个小技巧吧。因为Files.readAllLines方法会抛出一个受检异常（IOException），所以我使用了一个自定义的函数式接口，用ThrowingFunction包装这个方法，把受检异常转换为运行时异常，让代码更清晰： @FunctionalInterface public interface ThrowingFunction&lt;T, R, E extends Throwable&gt; &#123; static &lt;T, R, E extends Throwable&gt; Function&lt;T, R&gt; unchecked(ThrowingFunction&lt;T, R, E&gt; f) &#123; return t -&gt; &#123; try &#123; return f.apply(t); &#125; catch (Throwable e) &#123; throw new RuntimeException(e); &#125; &#125;; &#125; R apply(T t) throws E; &#125; 如果用Java 7实现类似逻辑的话，大概需要几十行代码，你可以尝试下。 并行流前面我们看到的Stream操作都是串行Stream，操作只是在一个线程中执行，此外Java 8还提供了并行流的功能：通过parallel方法，一键把Stream转换为并行操作提交到线程池处理。 比如，如下代码通过线程池来并行消费处理1到100： IntStream.rangeClosed(1,100).parallel().forEach(i-&gt;&#123; System.out.println(LocalDateTime.now() + &quot; : &quot; + i); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; &#125;); 并行流不确保执行顺序，并且因为每次处理耗时1秒，所以可以看到在8核机器上，数组是按照8个一组1秒输出一次： 在这个课程中，有很多类似使用threadCount个线程对某个方法总计执行taskCount次操作的案例，用于演示并发情况下的多线程问题或多线程处理性能。除了会用到并行流，我们有时也会使用线程池或直接使用线程进行类似操作。为了方便你对比各种实现，这里我一次性给出实现此类操作的五种方式。 为了测试这五种实现方式，我们设计一个场景：使用20个线程（threadCount）以并行方式总计执行10000次（taskCount）操作。因为单个任务单线程执行需要10毫秒（任务代码如下），也就是每秒吞吐量是100个操作，那20个线程QPS是2000，执行完10000次操作最少耗时5秒。 private void increment(AtomicInteger atomicInteger) &#123; atomicInteger.incrementAndGet(); try &#123; TimeUnit.MILLISECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 现在我们测试一下这五种方式，是否都可以利用更多的线程并行执行操作。 第一种方式是使用线程。直接把任务按照线程数均匀分割，分配到不同的线程执行，使用CountDownLatch来阻塞主线程，直到所有线程都完成操作。这种方式，需要我们自己分割任务： private int thread(int taskCount, int threadCount) throws InterruptedException &#123; &#x2F;&#x2F;总操作次数计数器 AtomicInteger atomicInteger &#x3D; new AtomicInteger(); &#x2F;&#x2F;使用CountDownLatch来等待所有线程执行完成 CountDownLatch countDownLatch &#x3D; new CountDownLatch(threadCount); &#x2F;&#x2F;使用IntStream把数字直接转为Thread IntStream.rangeClosed(1, threadCount).mapToObj(i -&gt; new Thread(() -&gt; &#123; &#x2F;&#x2F;手动把taskCount分成taskCount份，每一份有一个线程执行 IntStream.rangeClosed(1, taskCount &#x2F; threadCount).forEach(j -&gt; increment(atomicInteger)); &#x2F;&#x2F;每一个线程处理完成自己那部分数据之后，countDown一次 countDownLatch.countDown(); &#125;)).forEach(Thread::start); &#x2F;&#x2F;等到所有线程执行完成 countDownLatch.await(); &#x2F;&#x2F;查询计数器当前值 return atomicInteger.get(); &#125; 第二种方式是，使用Executors.newFixedThreadPool来获得固定线程数的线程池，使用execute提交所有任务到线程池执行，最后关闭线程池等待所有任务执行完成： private int threadpool(int taskCount, int threadCount) throws InterruptedException &#123; &#x2F;&#x2F;总操作次数计数器 AtomicInteger atomicInteger &#x3D; new AtomicInteger(); &#x2F;&#x2F;初始化一个线程数量&#x3D;threadCount的线程池 ExecutorService executorService &#x3D; Executors.newFixedThreadPool(threadCount); &#x2F;&#x2F;所有任务直接提交到线程池处理 IntStream.rangeClosed(1, taskCount).forEach(i -&gt; executorService.execute(() -&gt; increment(atomicInteger))); &#x2F;&#x2F;提交关闭线程池申请，等待之前所有任务执行完成 executorService.shutdown(); executorService.awaitTermination(1, TimeUnit.HOURS); &#x2F;&#x2F;查询计数器当前值 return atomicInteger.get(); &#125; 第三种方式是，使用ForkJoinPool而不是普通线程池执行任务。 ForkJoinPool和传统的ThreadPoolExecutor区别在于，前者对于n并行度有n个独立队列，后者是共享队列。如果有大量执行耗时比较短的任务，ThreadPoolExecutor的单队列就可能会成为瓶颈。这时，使用ForkJoinPool性能会更好。 因此，ForkJoinPool更适合大任务分割成许多小任务并行执行的场景，而ThreadPoolExecutor适合许多独立任务并发执行的场景。 在这里，我们先自定义一个具有指定并行数的ForkJoinPool，再通过这个ForkJoinPool并行执行操作： private int forkjoin(int taskCount, int threadCount) throws InterruptedException &#123; &#x2F;&#x2F;总操作次数计数器 AtomicInteger atomicInteger &#x3D; new AtomicInteger(); &#x2F;&#x2F;自定义一个并行度&#x3D;threadCount的ForkJoinPool ForkJoinPool forkJoinPool &#x3D; new ForkJoinPool(threadCount); &#x2F;&#x2F;所有任务直接提交到线程池处理 forkJoinPool.execute(() -&gt; IntStream.rangeClosed(1, taskCount).parallel().forEach(i -&gt; increment(atomicInteger))); &#x2F;&#x2F;提交关闭线程池申请，等待之前所有任务执行完成 forkJoinPool.shutdown(); forkJoinPool.awaitTermination(1, TimeUnit.HOURS); &#x2F;&#x2F;查询计数器当前值 return atomicInteger.get(); &#125; 第四种方式是，直接使用并行流，并行流使用公共的ForkJoinPool，也就是ForkJoinPool.commonPool()。 公共的ForkJoinPool默认的并行度是CPU核心数-1，原因是对于CPU绑定的任务分配超过CPU个数的线程没有意义。由于并行流还会使用主线程执行任务，也会占用一个CPU核心，所以公共ForkJoinPool的并行度即使-1也能用满所有CPU核心。 这里，我们通过配置强制指定（增大）了并行数，但因为使用的是公共ForkJoinPool，所以可能会存在干扰，你可以回顾下第3讲有关线程池混用产生的问题： private int stream(int taskCount, int threadCount) &#123; &#x2F;&#x2F;设置公共ForkJoinPool的并行度 System.setProperty(&quot;java.util.concurrent.ForkJoinPool.common.parallelism&quot;, String.valueOf(threadCount)); &#x2F;&#x2F;总操作次数计数器 AtomicInteger atomicInteger &#x3D; new AtomicInteger(); &#x2F;&#x2F;由于我们设置了公共ForkJoinPool的并行度，直接使用parallel提交任务即可 IntStream.rangeClosed(1, taskCount).parallel().forEach(i -&gt; increment(atomicInteger)); &#x2F;&#x2F;查询计数器当前值 return atomicInteger.get(); &#125; 第五种方式是，使用CompletableFuture来实现。CompletableFuture.runAsync方法可以指定一个线程池，一般会在使用CompletableFuture的时候用到： private int completableFuture(int taskCount, int threadCount) throws InterruptedException, ExecutionException &#123; &#x2F;&#x2F;总操作次数计数器 AtomicInteger atomicInteger &#x3D; new AtomicInteger(); &#x2F;&#x2F;自定义一个并行度&#x3D;threadCount的ForkJoinPool ForkJoinPool forkJoinPool &#x3D; new ForkJoinPool(threadCount); &#x2F;&#x2F;使用CompletableFuture.runAsync通过指定线程池异步执行任务 CompletableFuture.runAsync(() -&gt; IntStream.rangeClosed(1, taskCount).parallel().forEach(i -&gt; increment(atomicInteger)), forkJoinPool).get(); &#x2F;&#x2F;查询计数器当前值 return atomicInteger.get(); &#125; 上面这五种方法都可以实现类似的效果： 可以看到，这5种方式执行完10000个任务的耗时都在5.4秒到6秒之间。这里的结果只是证明并行度的设置是有效的，并不是性能比较。 如果你的程序对性能要求特别敏感，建议通过性能测试根据场景决定适合的模式。一般而言，使用线程池（第二种）和直接使用并行流（第四种）的方式在业务代码中比较常用。但需要注意的是，我们通常会重用线程池，而不会像Demo中那样在业务逻辑中直接声明新的线程池，等操作完成后再关闭。 另外需要注意的是，在上面的例子中我们一定是先运行stream方法再运行forkjoin方法，对公共ForkJoinPool默认并行度的修改才能生效。 这是因为ForkJoinPool类初始化公共线程池是在静态代码块里，加载类时就会进行的，如果forkjoin方法中先使用了ForkJoinPool，即便stream方法中设置了系统属性也不会起作用。因此我的建议是，设置ForkJoinPool公共线程池默认并行度的操作，应该放在应用启动时设置。 重点回顾今天，我和你简单介绍了Java 8中最重要的几个功能，包括Lambda表达式、Stream流式操作、Optional可空对象、并行流操作。这些特性，可以帮助我们写出简单易懂、可读性更强的代码。特别是使用Stream的链式方法，可以用一行代码完成之前几十行代码的工作。 因为Stream的API非常多，使用方法也是千变万化，因此我会在下一讲和你详细介绍Stream API的一些使用细节。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 检查下代码中是否有使用匿名类，以及通过遍历List进行数据过滤、转换和聚合的代码，看看能否使用Lambda表达式和Stream来重新实现呢？ 对于并行流部分的并行消费处理1到100的例子，如果把forEach替换为forEachOrdered，你觉得会发生什么呢？ 关于Java 8，你还有什么使用心得吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/32 _ 加餐2：带你吃透课程中Java 8的那些重要知识点（二）","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/32 _ 加餐2：带你吃透课程中Java 8的那些重要知识点（二）/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/32%20_%20%E5%8A%A0%E9%A4%902%EF%BC%9A%E5%B8%A6%E4%BD%A0%E5%90%83%E9%80%8F%E8%AF%BE%E7%A8%8B%E4%B8%ADJava%208%E7%9A%84%E9%82%A3%E4%BA%9B%E9%87%8D%E8%A6%81%E7%9F%A5%E8%AF%86%E7%82%B9%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"32 | 加餐2：带你吃透课程中Java 8的那些重要知识点（二）作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 上一讲的几个例子中，其实都涉及了Stream API的最基本使用方法。今天，我会与你详细介绍复杂、功能强大的Stream API。 Stream流式操作，用于对集合进行投影、转换、过滤、排序等，更进一步地，这些操作能链式串联在一起使用，类似于SQL语句，可以大大简化代码。可以说，Stream操作是Java 8中最重要的内容，也是这个课程大部分代码都会用到的操作。 我先说明下，有些案例可能不太好理解，建议你对着代码逐一到源码中查看Stream操作的方法定义，以及JDK中的代码注释。 Stream操作详解为了方便你理解Stream的各种操作，以及后面的案例，我先把这节课涉及的Stream操作汇总到了一张图中。你可以先熟悉一下。 在接下来的讲述中，我会围绕订单场景，给出如何使用Stream的各种API完成订单的统计、搜索、查询等功能，和你一起学习Stream流式操作的各种方法。你可以结合代码中的注释理解案例，也可以自己运行源码观察输出。 我们先定义一个订单类、一个订单商品类和一个顾客类，用作后续Demo代码的数据结构： &#x2F;&#x2F;订单类 @Data public class Order &#123; private Long id; private Long customerId;&#x2F;&#x2F;顾客ID private String customerName;&#x2F;&#x2F;顾客姓名 private List&lt;OrderItem&gt; orderItemList;&#x2F;&#x2F;订单商品明细 private Double totalPrice;&#x2F;&#x2F;总价格 private LocalDateTime placedAt;&#x2F;&#x2F;下单时间 &#125; &#x2F;&#x2F;订单商品类 @Data @AllArgsConstructor @NoArgsConstructor public class OrderItem &#123; private Long productId;&#x2F;&#x2F;商品ID private String productName;&#x2F;&#x2F;商品名称 private Double productPrice;&#x2F;&#x2F;商品价格 private Integer productQuantity;&#x2F;&#x2F;商品数量 &#125; &#x2F;&#x2F;顾客类 @Data @AllArgsConstructor public class Customer &#123; private Long id; private String name;&#x2F;&#x2F;顾客姓名 &#125; 在这里，我们有一个orders字段保存了一些模拟数据，类型是List。这里，我就不贴出生成模拟数据的代码了。这不会影响你理解后面的代码，你也可以自己下载源码阅读。 创建流要使用流，就要先创建流。创建流一般有五种方式： 通过stream方法把List或数组转换为流； 通过Stream.of方法直接传入多个元素构成一个流； 通过Stream.iterate方法使用迭代的方式构造一个无限流，然后使用limit限制流元素个数； 通过Stream.generate方法从外部传入一个提供元素的Supplier来构造无限流，然后使用limit限制流元素个数； 通过IntStream或DoubleStream构造基本类型的流。 &#x2F;&#x2F;通过stream方法把List或数组转换为流 @Test public void stream() &#123; Arrays.asList(&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;).stream().forEach(System.out::println); Arrays.stream(new int[]&#123;1, 2, 3&#125;).forEach(System.out::println); &#125; &#x2F;&#x2F;通过Stream.of方法直接传入多个元素构成一个流 @Test public void of() &#123; String[] arr &#x3D; &#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125;; Stream.of(arr).forEach(System.out::println); Stream.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;).forEach(System.out::println); Stream.of(1, 2, &quot;a&quot;).map(item -&gt; item.getClass().getName()).forEach(System.out::println); &#125; &#x2F;&#x2F;通过Stream.iterate方法使用迭代的方式构造一个无限流，然后使用limit限制流元素个数 @Test public void iterate() &#123; Stream.iterate(2, item -&gt; item * 2).limit(10).forEach(System.out::println); Stream.iterate(BigInteger.ZERO, n -&gt; n.add(BigInteger.TEN)).limit(10).forEach(System.out::println); &#125; &#x2F;&#x2F;通过Stream.generate方法从外部传入一个提供元素的Supplier来构造无限流，然后使用limit限制流元素个数 @Test public void generate() &#123; Stream.generate(() -&gt; &quot;test&quot;).limit(3).forEach(System.out::println); Stream.generate(Math::random).limit(10).forEach(System.out::println); &#125; &#x2F;&#x2F;通过IntStream或DoubleStream构造基本类型的流 @Test public void primitive() &#123; &#x2F;&#x2F;演示IntStream和DoubleStream IntStream.range(1, 3).forEach(System.out::println); IntStream.range(0, 3).mapToObj(i -&gt; &quot;x&quot;).forEach(System.out::println); IntStream.rangeClosed(1, 3).forEach(System.out::println); DoubleStream.of(1.1, 2.2, 3.3).forEach(System.out::println); &#x2F;&#x2F;各种转换，后面注释代表了输出结果 System.out.println(IntStream.of(1, 2).toArray().getClass()); &#x2F;&#x2F;class [I System.out.println(Stream.of(1, 2).mapToInt(Integer::intValue).toArray().getClass()); &#x2F;&#x2F;class [I System.out.println(IntStream.of(1, 2).boxed().toArray().getClass()); &#x2F;&#x2F;class [Ljava.lang.Object; System.out.println(IntStream.of(1, 2).asDoubleStream().toArray().getClass()); &#x2F;&#x2F;class [D System.out.println(IntStream.of(1, 2).asLongStream().toArray().getClass()); &#x2F;&#x2F;class [J &#x2F;&#x2F;注意基本类型流和装箱后的流的区别 Arrays.asList(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;).stream() &#x2F;&#x2F; Stream&lt;String&gt; .mapToInt(String::length) &#x2F;&#x2F; IntStream .asLongStream() &#x2F;&#x2F; LongStream .mapToDouble(x -&gt; x &#x2F; 10.0) &#x2F;&#x2F; DoubleStream .boxed() &#x2F;&#x2F; Stream&lt;Double&gt; .mapToLong(x -&gt; 1L) &#x2F;&#x2F; LongStream .mapToObj(x -&gt; &quot;&quot;) &#x2F;&#x2F; Stream&lt;String&gt; .collect(Collectors.toList()); &#125; filterfilter方法可以实现过滤操作，类似SQL中的where。我们可以使用一行代码，通过filter方法实现查询所有订单中最近半年金额大于40的订单，通过连续叠加filter方法进行多次条件过滤： &#x2F;&#x2F;最近半年的金额大于40的订单 orders.stream() .filter(Objects::nonNull) &#x2F;&#x2F;过滤null值 .filter(order -&gt; order.getPlacedAt().isAfter(LocalDateTime.now().minusMonths(6))) &#x2F;&#x2F;最近半年的订单 .filter(order -&gt; order.getTotalPrice() &gt; 40) &#x2F;&#x2F;金额大于40的订单 .forEach(System.out::println); 如果不使用Stream的话，必然需要一个中间集合来收集过滤后的结果，而且所有的过滤条件会堆积在一起，代码冗长且不易读。 mapmap操作可以做转换（或者说投影），类似SQL中的select。为了对比，我用两种方式统计订单中所有商品的数量，前一种是通过两次遍历实现，后一种是通过两次mapToLong+sum方法实现： &#x2F;&#x2F;计算所有订单商品数量 &#x2F;&#x2F;通过两次遍历实现 LongAdder longAdder &#x3D; new LongAdder(); orders.stream().forEach(order -&gt; order.getOrderItemList().forEach(orderItem -&gt; longAdder.add(orderItem.getProductQuantity()))); &#x2F;&#x2F;使用两次mapToLong+sum方法实现 assertThat(longAdder.longValue(), is(orders.stream().mapToLong(order -&gt; order.getOrderItemList().stream() .mapToLong(OrderItem::getProductQuantity).sum()).sum())); 显然，后一种方式无需中间变量longAdder，更直观。 这里再补充一下，使用for循环生成数据，是我们平时常用的操作，也是这个课程会大量用到的。现在，我们可以用一行代码使用IntStream配合mapToObj替代for循环来生成数据，比如生成10个Product元素构成List： &#x2F;&#x2F;把IntStream通过转换Stream&lt;Project&gt; System.out.println(IntStream.rangeClosed(1,10) .mapToObj(i-&gt;new Product((long)i, &quot;product&quot;+i, i*100.0)) .collect(toList())); flatMap接下来，我们看看flatMap展开或者叫扁平化操作，相当于map+flat，通过map把每一个元素替换为一个流，然后展开这个流。 比如，我们要统计所有订单的总价格，可以有两种方式： 直接通过原始商品列表的商品个数*商品单价统计的话，可以先把订单通过flatMap展开成商品清单，也就是把Order替换为Stream，然后对每一个OrderItem用mapToDouble转换获得商品总价，最后进行一次sum求和； 利用flatMapToDouble方法把列表中每一项展开替换为一个DoubleStream，也就是直接把每一个订单转换为每一个商品的总价，然后求和。 &#x2F;&#x2F;直接展开订单商品进行价格统计 System.out.println(orders.stream() .flatMap(order -&gt; order.getOrderItemList().stream()) .mapToDouble(item -&gt; item.getProductQuantity() * item.getProductPrice()).sum()); &#x2F;&#x2F;另一种方式flatMap+mapToDouble&#x3D;flatMapToDouble System.out.println(orders.stream() .flatMapToDouble(order -&gt; order.getOrderItemList() .stream().mapToDouble(item -&gt; item.getProductQuantity() * item.getProductPrice())) .sum()); 这两种方式可以得到相同的结果，并无本质区别。 sortedsorted操作可以用于行内排序的场景，类似SQL中的order by。比如，要实现大于50元订单的按价格倒序取前5，可以通过Order::getTotalPrice方法引用直接指定需要排序的依据字段，通过reversed()实现倒序： &#x2F;&#x2F;大于50的订单,按照订单价格倒序前5 orders.stream().filter(order -&gt; order.getTotalPrice() &gt; 50) .sorted(comparing(Order::getTotalPrice).reversed()) .limit(5) .forEach(System.out::println); distinctdistinct操作的作用是去重，类似SQL中的distinct。比如下面的代码实现： 查询去重后的下单用户。使用map从订单提取出购买用户，然后使用distinct去重。 查询购买过的商品名。使用flatMap+map提取出订单中所有的商品名，然后使用distinct去重。 &#x2F;&#x2F;去重的下单用户 System.out.println(orders.stream().map(order -&gt; order.getCustomerName()).distinct().collect(joining(&quot;,&quot;))); &#x2F;&#x2F;所有购买过的商品 System.out.println(orders.stream() .flatMap(order -&gt; order.getOrderItemList().stream()) .map(OrderItem::getProductName) .distinct().collect(joining(&quot;,&quot;))); skip &amp; limitskip和limit操作用于分页，类似MySQL中的limit。其中，skip实现跳过一定的项，limit用于限制项总数。比如下面的两段代码： 按照下单时间排序，查询前2个订单的顾客姓名和下单时间； 按照下单时间排序，查询第3和第4个订单的顾客姓名和下单时间。 &#x2F;&#x2F;按照下单时间排序，查询前2个订单的顾客姓名和下单时间 orders.stream() .sorted(comparing(Order::getPlacedAt)) .map(order -&gt; order.getCustomerName() + &quot;@&quot; + order.getPlacedAt()) .limit(2).forEach(System.out::println); &#x2F;&#x2F;按照下单时间排序，查询第3和第4个订单的顾客姓名和下单时间 orders.stream() .sorted(comparing(Order::getPlacedAt)) .map(order -&gt; order.getCustomerName() + &quot;@&quot; + order.getPlacedAt()) .skip(2).limit(2).forEach(System.out::println); collectcollect是收集操作，对流进行终结（终止）操作，把流导出为我们需要的数据结构。“终结”是指，导出后，无法再串联使用其他中间操作，比如filter、map、flatmap、sorted、distinct、limit、skip。 在Stream操作中，collect是最复杂的终结操作，比较简单的终结操作还有forEach、toArray、min、max、count、anyMatch等，我就不再展开了，你可以查询JDK文档，搜索terminal operation或intermediate operation。 接下来，我通过6个案例，来演示下几种比较常用的collect操作： 第一个案例，实现了字符串拼接操作，生成一定位数的随机字符串。 第二个案例，通过Collectors.toSet静态方法收集为Set去重，得到去重后的下单用户，再通过Collectors.joining静态方法实现字符串拼接。 第三个案例，通过Collectors.toCollection静态方法获得指定类型的集合，比如把List转换为LinkedList。 第四个案例，通过Collectors.toMap静态方法将对象快速转换为Map，Key是订单ID、Value是下单用户名。 第五个案例，通过Collectors.toMap静态方法将对象转换为Map。Key是下单用户名，Value是下单时间，一个用户可能多次下单，所以直接在这里进行了合并，只获取最近一次的下单时间。 第六个案例，使用Collectors.summingInt方法对商品数量求和，再使用Collectors.averagingInt方法对结果求平均值，以统计所有订单平均购买的商品数量。 &#x2F;&#x2F;生成一定位数的随机字符串 System.out.println(random.ints(48, 122) .filter(i -&gt; (i &lt; 57 || i &gt; 65) &amp;&amp; (i &lt; 90 || i &gt; 97)) .mapToObj(i -&gt; (char) i) .limit(20) .collect(StringBuilder::new, StringBuilder::append, StringBuilder::append) .toString()); &#x2F;&#x2F;所有下单的用户，使用toSet去重后实现字符串拼接 System.out.println(orders.stream() .map(order -&gt; order.getCustomerName()).collect(toSet()) .stream().collect(joining(&quot;,&quot;, &quot;[&quot;, &quot;]&quot;))); &#x2F;&#x2F;用toCollection收集器指定集合类型 System.out.println(orders.stream().limit(2).collect(toCollection(LinkedList::new)).getClass()); &#x2F;&#x2F;使用toMap获取订单ID+下单用户名的Map orders.stream() .collect(toMap(Order::getId, Order::getCustomerName)) .entrySet().forEach(System.out::println); &#x2F;&#x2F;使用toMap获取下单用户名+最近一次下单时间的Map orders.stream() .collect(toMap(Order::getCustomerName, Order::getPlacedAt, (x, y) -&gt; x.isAfter(y) ? x : y)) .entrySet().forEach(System.out::println); &#x2F;&#x2F;订单平均购买的商品数量 System.out.println(orders.stream().collect(averagingInt(order -&gt; order.getOrderItemList().stream() .collect(summingInt(OrderItem::getProductQuantity))))); 可以看到，这6个操作使用Stream方式一行代码就可以实现，但使用非Stream方式实现的话，都需要几行甚至十几行代码。 有关Collectors类的一些常用静态方法，我总结到了一张图中，你可以再整理一下思路： 其中，groupBy和partitionBy比较复杂，我和你举例介绍。 groupBygroupBy是分组统计操作，类似SQL中的group by子句。它和后面介绍的partitioningBy都是特殊的收集器，同样也是终结操作。分组操作比较复杂，为帮你理解得更透彻，我准备了8个案例： 第一个案例，按照用户名分组，使用Collectors.counting方法统计每个人的下单数量，再按照下单数量倒序输出。 第二个案例，按照用户名分组，使用Collectors.summingDouble方法统计订单总金额，再按总金额倒序输出。 第三个案例，按照用户名分组，使用两次Collectors.summingInt方法统计商品采购数量，再按总数量倒序输出。 第四个案例，统计被采购最多的商品。先通过flatMap把订单转换为商品，然后把商品名作为Key、Collectors.summingInt作为Value分组统计采购数量，再按Value倒序获取第一个Entry，最后查询Key就得到了售出最多的商品。 第五个案例，同样统计采购最多的商品。相比第四个案例排序Map的方式，这次直接使用Collectors.maxBy收集器获得最大的Entry。 第六个案例，按照用户名分组，统计用户下的金额最高的订单。Key是用户名，Value是Order，直接通过Collectors.maxBy方法拿到金额最高的订单，然后通过collectingAndThen实现Optional.get的内容提取，最后遍历Key&#x2F;Value即可。 第七个案例，根据下单年月分组统计订单ID列表。Key是格式化成年月后的下单时间，Value直接通过Collectors.mapping方法进行了转换，把订单列表转换为订单ID构成的List。 第八个案例，根据下单年月+用户名两次分组统计订单ID列表，相比上一个案例多了一次分组操作，第二次分组是按照用户名进行分组。 &#x2F;&#x2F;按照用户名分组，统计下单数量 System.out.println(orders.stream().collect(groupingBy(Order::getCustomerName, counting())) .entrySet().stream().sorted(Map.Entry.&lt;String, Long&gt;comparingByValue().reversed()).collect(toList())); &#x2F;&#x2F;按照用户名分组，统计订单总金额 System.out.println(orders.stream().collect(groupingBy(Order::getCustomerName, summingDouble(Order::getTotalPrice))) .entrySet().stream().sorted(Map.Entry.&lt;String, Double&gt;comparingByValue().reversed()).collect(toList())); &#x2F;&#x2F;按照用户名分组，统计商品采购数量 System.out.println(orders.stream().collect(groupingBy(Order::getCustomerName, summingInt(order -&gt; order.getOrderItemList().stream() .collect(summingInt(OrderItem::getProductQuantity))))) .entrySet().stream().sorted(Map.Entry.&lt;String, Integer&gt;comparingByValue().reversed()).collect(toList())); &#x2F;&#x2F;统计最受欢迎的商品，倒序后取第一个 orders.stream() .flatMap(order -&gt; order.getOrderItemList().stream()) .collect(groupingBy(OrderItem::getProductName, summingInt(OrderItem::getProductQuantity))) .entrySet().stream() .sorted(Map.Entry.&lt;String, Integer&gt;comparingByValue().reversed()) .map(Map.Entry::getKey) .findFirst() .ifPresent(System.out::println); &#x2F;&#x2F;统计最受欢迎的商品的另一种方式，直接利用maxBy orders.stream() .flatMap(order -&gt; order.getOrderItemList().stream()) .collect(groupingBy(OrderItem::getProductName, summingInt(OrderItem::getProductQuantity))) .entrySet().stream() .collect(maxBy(Map.Entry.comparingByValue())) .map(Map.Entry::getKey) .ifPresent(System.out::println); &#x2F;&#x2F;按照用户名分组，选用户下的总金额最大的订单 orders.stream().collect(groupingBy(Order::getCustomerName, collectingAndThen(maxBy(comparingDouble(Order::getTotalPrice)), Optional::get))) .forEach((k, v) -&gt; System.out.println(k + &quot;#&quot; + v.getTotalPrice() + &quot;@&quot; + v.getPlacedAt())); &#x2F;&#x2F;根据下单年月分组，统计订单ID列表 System.out.println(orders.stream().collect (groupingBy(order -&gt; order.getPlacedAt().format(DateTimeFormatter.ofPattern(&quot;yyyyMM&quot;)), mapping(order -&gt; order.getId(), toList())))); &#x2F;&#x2F;根据下单年月+用户名两次分组，统计订单ID列表 System.out.println(orders.stream().collect (groupingBy(order -&gt; order.getPlacedAt().format(DateTimeFormatter.ofPattern(&quot;yyyyMM&quot;)), groupingBy(order -&gt; order.getCustomerName(), mapping(order -&gt; order.getId(), toList()))))); 如果不借助Stream转换为普通的Java代码，实现这些复杂的操作可能需要几十行代码。 partitionBypartitioningBy用于分区，分区是特殊的分组，只有true和false两组。比如，我们把用户按照是否下单进行分区，给partitioningBy方法传入一个Predicate作为数据分区的区分，输出是Map&lt;Boolean, List&gt;： public static &lt;T&gt; Collector&lt;T, ?, Map&lt;Boolean, List&lt;T&gt;&gt;&gt; partitioningBy(Predicate&lt;? super T&gt; predicate) &#123; return partitioningBy(predicate, toList()); &#125; 测试一下，partitioningBy配合anyMatch，可以把用户分为下过订单和没下过订单两组： &#x2F;&#x2F;根据是否有下单记录进行分区 System.out.println(Customer.getData().stream().collect( partitioningBy(customer -&gt; orders.stream().mapToLong(Order::getCustomerId) .anyMatch(id -&gt; id &#x3D;&#x3D; customer.getId())))); 重点回顾今天，我用了大量的篇幅和案例，和你展开介绍了Stream中很多具体的流式操作方法。有些案例可能不太好理解，我建议你对着代码逐一到源码中查看这些操作的方法定义，以及JDK中的代码注释。 最后，我建议你思考下，在日常工作中还会使用SQL统计哪些信息，这些SQL是否也可以用Stream来改写呢？Stream的API博大精深，但其中又有规律可循。这其中的规律主要就是，理清楚这些API传参的函数式接口定义，就能搞明白到底是需要我们提供数据、消费数据、还是转换数据等。那，掌握Stream的方法便是，多测试多练习，以强化记忆、加深理解。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 使用Stream可以非常方便地对List做各种操作，那有没有什么办法可以实现在整个过程中观察数据变化呢？比如，我们进行filter+map操作，如何观察filter后map的原始数据呢？ Collectors类提供了很多现成的收集器，那我们有没有办法实现自定义的收集器呢？比如，实现一个MostPopularCollector，来得到List中出现次数最多的元素，满足下面两个测试用例： assertThat(Stream.of(1, 1, 2, 2, 2, 3, 4, 5, 5).collect(new MostPopularCollector&lt;&gt;()).get(), is(2)); assertThat(Stream.of(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;c&#39;, &#39;d&#39;).collect(new MostPopularCollector&lt;&gt;()).get(), is(&#39;c&#39;)); 关于Java 8，你还有什么使用心得吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/33 _ 加餐3：定位应用问题，排错套路很重要","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/33 _ 加餐3：定位应用问题，排错套路很重要/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/33%20_%20%E5%8A%A0%E9%A4%903%EF%BC%9A%E5%AE%9A%E4%BD%8D%E5%BA%94%E7%94%A8%E9%97%AE%E9%A2%98%EF%BC%8C%E6%8E%92%E9%94%99%E5%A5%97%E8%B7%AF%E5%BE%88%E9%87%8D%E8%A6%81/","excerpt":"","text":"33 | 加餐3：定位应用问题，排错套路很重要作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 咱们这个课程已经更新13讲了，感谢各位同学一直在坚持学习，并在评论区留下了很多高质量的留言。这些留言，有的是分享自己曾经踩的坑，有的是对课后思考题的详细解答，还有的是提出了非常好的问题，进一步丰富了这个课程的内容。 有同学说，这个课程的案例非常实用，都是工作中会遇到的。正如我在开篇词中所说，这个课程涉及的100个案例、约130个小坑，有40%来自于我经历过或者是见过的200多个线上生产事故，剩下的60%来自于我开发业务项目，以及日常审核别人的代码发现的问题。确实，我在整理这些案例上花费了很多精力，也特别感谢各位同学的认可，更希望你们能继续坚持学习，继续在评论区和我交流。 也有同学反馈，排查问题的思路很重要，希望自己遇到问题时，也能够从容、高效地定位到根因。因此，今天这一讲，我就与你说说我在应急排错方面积累的心得。这都是我多年担任技术负责人和架构师自己总结出来的，希望对你有所帮助。当然了，也期待你能留言与我说说，自己平时的排错套路。 在不同环境排查问题，有不同的方式要说排查问题的思路，我们首先得明白是在什么环境排错。 如果是在自己的开发环境排查问题，那你几乎可以使用任何自己熟悉的工具来排查，甚至可以进行单步调试。只要问题能重现，排查就不会太困难，最多就是把程序调试到JDK或三方类库内部进行分析。 如果是在测试环境排查问题，相比开发环境少的是调试，不过你可以使用JDK自带的jvisualvm或阿里的Arthas，附加到远程的JVM进程排查问题。另外，测试环境允许造数据、造压力模拟我们需要的场景，因此遇到偶发问题时，我们可以尝试去造一些场景让问题更容易出现，方便测试。 如果是在生产环境排查问题，往往比较难：一方面，生产环境权限管控严格，一般不允许调试工具从远程附加进程；另一方面，生产环境出现问题要求以恢复为先，难以留出充足的时间去慢慢排查问题。但，因为生产环境的流量真实、访问量大、网络权限管控严格、环境复杂，因此更容易出问题，也是出问题最多的环境。 接下来，我就与你详细说说，如何在生产环境排查问题吧。 生产问题的排查很大程度依赖监控其实，排查问题就像在破案，生产环境出现问题时，因为要尽快恢复应用，就不可能保留完整现场用于排查和测试。因此，是否有充足的信息可以了解过去、还原现场就成了破案的关键。这里说的信息，主要就是日志、监控和快照。 日志就不用多说了，主要注意两点： 确保错误、异常信息可以被完整地记录到文件日志中； 确保生产上程序的日志级别是INFO以上。记录日志要使用合理的日志优先级，DEBUG用于开发调试、INFO用于重要流程信息、WARN用于需要关注的问题、ERROR用于阻断流程的错误。 对于监控，在生产环境排查问题时，首先就需要开发和运维团队做好充足的监控，而且是多个层次的监控。 主机层面，对CPU、内存、磁盘、网络等资源做监控。如果应用部署在虚拟机或Kubernetes集群中，那么除了对物理机做基础资源监控外，还要对虚拟机或Pod做同样的监控。监控层数取决于应用的部署方案，有一层OS就要做一层监控。 网络层面，需要监控专线带宽、交换机基本情况、网络延迟。 所有的中间件和存储都要做好监控，不仅仅是监控进程对CPU、内存、磁盘IO、网络使用的基本指标，更重要的是监控组件内部的一些重要指标。比如，著名的监控工具Prometheus，就提供了大量的exporter来对接各种中间件和存储系统。 应用层面，需要监控JVM进程的类加载、内存、GC、线程等常见指标（比如使用Micrometer来做应用监控），此外还要确保能够收集、保存应用日志、GC日志。 我们再来看看快照。这里的“快照”是指，应用进程在某一时刻的快照。通常情况下，我们会为生产环境的Java应用设置-XX:+HeapDumpOnOutOfMemoryError和-XX:HeapDumpPath&#x3D;…这2个JVM参数，用于在出现OOM时保留堆快照。这个课程中，我们也多次使用MAT工具来分析堆快照。 了解过去、还原现场后，接下来我们就看看定位问题的套路。 分析定位问题的套路定位问题，首先要定位问题出在哪个层次上。比如，是Java应用程序自身的问题还是外部因素导致的问题。我们可以先查看程序是否有异常，异常信息一般比较具体，可以马上定位到大概的问题方向；如果是一些资源消耗型的问题可能不会有异常，我们可以通过指标监控配合显性问题点来定位。 一般情况下，程序的问题来自以下三个方面。 第一，程序发布后的Bug，回滚后可以立即解决。这类问题的排查，可以回滚后再慢慢分析版本差异。 第二，外部因素，比如主机、中间件或数据库的问题。这类问题的排查方式，按照主机层面的问题、中间件或存储（统称组件）的问题分为两类。 主机层面的问题，可以使用工具排查： CPU相关问题，可以使用top、vmstat、pidstat、ps等工具排查； 内存相关问题，可以使用free、top、ps、vmstat、cachestat、sar等工具排查； IO相关问题，可以使用lsof、iostat、pidstat、sar、iotop、df、du等工具排查； 网络相关问题，可以使用ifconfig、ip、nslookup、dig、ping、tcpdump、iptables等工具排查。 组件的问题，可以从以下几个方面排查： 排查组件所在主机是否有问题； 排查组件进程基本情况，观察各种监控指标； 查看组件的日志输出，特别是错误日志； 进入组件控制台，使用一些命令查看其运作情况。 第三，因为系统资源不够造成系统假死的问题，通常需要先通过重启和扩容解决问题，之后再进行分析，不过最好能留一个节点作为现场。系统资源不够，一般体现在CPU使用高、内存泄漏或OOM的问题、IO问题、网络相关问题这四个方面。 对于CPU使用高的问题，如果现场还在，具体的分析流程是： 首先，在Linux服务器上运行top -Hp pid命令，来查看进程中哪个线程CPU使用高； 然后，输入大写的P将线程按照 CPU 使用率排序，并把明显占用CPU的线程ID转换为16进制； 最后，在jstack命令输出的线程栈中搜索这个线程ID，定位出问题的线程当时的调用栈。 如果没有条件直接在服务器上运行top命令的话，我们可以用采样的方式定位问题：间隔固定秒数（比如10秒）运行一次jstack命令，采样几次后，对比采样得出哪些线程始终处于运行状态，分析出问题的线程。 如果现场没有了，我们可以通过排除法来分析。CPU使用高，一般是由下面的因素引起的： 突发压力。这类问题，我们可以通过应用之前的负载均衡的流量或日志量来确认，诸如Nginx等反向代理都会记录URL，可以依靠代理的Access Log进行细化定位，也可以通过监控观察JVM线程数的情况。压力问题导致CPU使用高的情况下，如果程序的各资源使用没有明显不正常，之后可以通过压测+Profiler（jvisualvm就有这个功能）进一步定位热点方法；如果资源使用不正常，比如产生了几千个线程，就需要考虑调参。 GC。这种情况，我们可以通过JVM监控GC相关指标、GC Log进行确认。如果确认是GC的压力，那么内存使用也很可能会不正常，需要按照内存问题分析流程做进一步分析。 程序中死循环逻辑或不正常的处理流程。这类问题，我们可以结合应用日志分析。一般情况下，应用执行过程中都会产生一些日志，可以重点关注日志量异常部分。 对于内存泄露或OOM的问题，最简单的分析方式，就是堆转储后使用MAT分析。堆转储，包含了堆现场全貌和线程栈信息，一般观察支配树图、直方图就可以马上看到占用大量内存的对象，可以快速定位到内存相关问题。这一点我们会在第5篇加餐中详细介绍。 需要注意的是，Java进程对内存的使用不仅仅是堆区，还包括线程使用的内存（线程个数*每一个线程的线程栈）和元数据区。每一个内存区都可能产生OOM，可以结合监控观察线程数、已加载类数量等指标分析。另外，我们需要注意看一下，JVM参数的设置是否有明显不合理的地方，限制了资源使用。 IO相关的问题，除非是代码问题引起的资源不释放等问题，否则通常都不是由Java进程内部因素引发的。 网络相关的问题，一般也是由外部因素引起的。对于连通性问题，结合异常信息通常比较容易定位；对于性能或瞬断问题，可以先尝试使用ping等工具简单判断，如果不行再使用tcpdump或Wireshark来分析。 分析和定位问题需要注意的九个点有些时候，我们分析和定位问题时，会陷入误区或是找不到方向。遇到这种情况，你可以借鉴下我的九个心得。 第一，考虑“鸡”和“蛋”的问题。比如，发现业务逻辑执行很慢且线程数增多的情况时，我们需要考虑两种可能性： 一是，程序逻辑有问题或外部依赖慢，使得业务逻辑执行慢，在访问量不变的情况下需要更多的线程数来应对。比如，10TPS的并发原先一次请求1s可以执行完成，10个线程可以支撑；现在执行完成需要10s，那就需要100个线程。 二是，有可能是请求量增大了，使得线程数增多，应用本身的CPU资源不足，再加上上下文切换问题导致处理变慢了。 出现问题的时候，我们需要结合内部表现和入口流量一起看，确认这里的“慢”到底是根因还是结果。 第二，考虑通过分类寻找规律。在定位问题没有头绪的时候，我们可以尝试总结规律。 比如，我们有10台应用服务器做负载均衡，出问题时可以通过日志分析是否是均匀分布的，还是问题都出现在1台机器。又比如，应用日志一般会记录线程名称，出问题时我们可以分析日志是否集中在某一类线程上。再比如，如果发现应用开启了大量TCP连接，通过netstat我们可以分析出主要集中连接到哪个服务。 如果能总结出规律，很可能就找到了突破点。 第三，分析问题需要根据调用拓扑来，不能想当然。比如，我们看到Nginx返回502错误，一般可以认为是下游服务的问题导致网关无法完成请求转发。对于下游服务，不能想当然就认为是我们的Java程序，比如在拓扑上可能Nginx代理的是Kubernetes的Traefik Ingress，链路是Nginx-&gt;Traefik-&gt;应用，如果一味排查Java程序的健康情况，那么始终不会找到根因。 又比如，我们虽然使用了Spring Cloud Feign来进行服务调用，出现连接超时也不一定就是服务端的问题，有可能是客户端通过URL来调用服务端，并不是通过Eureka的服务发现实现的客户端负载均衡。换句话说，客户端连接的是Nginx代理而不是直接连接应用，客户端连接服务出现的超时，其实是Nginx代理宕机所致。 第四，考虑资源限制类问题。观察各种曲线指标，如果发现曲线慢慢上升然后稳定在一个水平线上，那么一般就是资源达到了限制或瓶颈。 比如，在观察网络带宽曲线的时候，如果发现带宽上升到120MB左右不动了，那么很可能就是打满了1GB的网卡或传输带宽。又比如，观察到数据库活跃连接数上升到10个就不动了，那么很可能是连接池打满了。观察监控一旦看到任何这样的曲线，都要引起重视。 第五，考虑资源相互影响。CPU、内存、IO和网络，这四类资源就像人的五脏六腑，是相辅相成的，一个资源出现了明显的瓶颈，很可能会引起其他资源的连锁反应。 比如，内存泄露后对象无法回收会造成大量Full GC，此时CPU会大量消耗在GC上从而引起CPU使用增加。又比如，我们经常会把数据缓存在内存队列中进行异步IO处理，网络或磁盘出现问题时，就很可能会引起内存的暴涨。因此，出问题的时候，我们要考虑到这一点，以避免误判。 第六，排查网络问题要考虑三个方面，到底是客户端问题，还是服务端问题，还是传输问题。比如，出现数据库访问慢的现象，可能是客户端的原因，连接池不够导致连接获取慢、GC停顿、CPU占满等；也可能是传输环节的问题，包括光纤、防火墙、路由表设置等问题；也可能是真正的服务端问题，需要逐一排查来进行区分。 服务端慢一般可以看到MySQL出慢日志，传输慢一般可以通过ping来简单定位，排除了这两个可能，并且仅仅是部分客户端出现访问慢的情况，就需要怀疑是客户端本身的问题。对于第三方系统、服务或存储访问出现慢的情况，不能完全假设是服务端的问题。 第七，快照类工具和趋势类工具需要结合使用。比如，jstat、top、各种监控曲线是趋势类工具，可以让我们观察各个指标的变化情况，定位大概的问题点；而jstack和分析堆快照的MAT是快照类工具，用于详细分析某一时刻应用程序某一个点的细节。 一般情况下，我们会先使用趋势类工具来总结规律，再使用快照类工具来分析问题。如果反过来可能就会误判，因为快照类工具反映的只是一个瞬间程序的情况，不能仅仅通过分析单一快照得出结论，如果缺少趋势类工具的帮助，那至少也要提取多个快照来对比。 第八，不要轻易怀疑监控。我曾看过一个空难事故的分析，飞行员在空中发现仪表显示飞机所有油箱都处于缺油的状态，他第一时间的怀疑是油表出现故障了，始终不愿意相信是真的缺油，结果飞行不久后引擎就断油熄火了。同样地，在应用出现问题时，我们会查看各种监控系统，但有些时候我们宁愿相信自己的经验，也不相信监控图表的显示。这可能会导致我们完全朝着错误的方向来排查问题。 如果你真的怀疑是监控系统有问题，可以看一下这套监控系统对于不出问题的应用显示是否正常，如果正常那就应该相信监控而不是自己的经验。 第九，如果因为监控缺失等原因无法定位到根因的话，相同问题就有再出现的风险，需要做好三项工作： 做好日志、监控和快照补漏工作，下次遇到问题时可以定位根因； 针对问题的症状做好实时报警，确保出现问题后可以第一时间发现； 考虑做一套热备的方案，出现问题后可以第一时间切换到热备系统快速解决问题，同时又可以保留老系统的现场。 重点回顾今天，我和你总结分享了分析生产环境问题的套路。 第一，分析问题一定是需要依据的，靠猜是猜不出来的，需要提前做好基础监控的建设。监控的话，需要在基础运维层、应用层、业务层等多个层次进行。定位问题的时候，我们同样需要参照多个监控层的指标表现综合分析。 第二，定位问题要先对原因进行大致分类，比如是内部问题还是外部问题、CPU相关问题还是内存相关问题、仅仅是A接口的问题还是整个应用的问题，然后再去进一步细化探索，一定是从大到小来思考问题；在追查问题遇到瓶颈的时候，我们可以先退出细节，再从大的方面捋一下涉及的点，再重新来看问题。 第三，分析问题很多时候靠的是经验，很难找到完整的方法论。遇到重大问题的时候，往往也需要根据直觉来第一时间找到最有可能的点，这里甚至有运气成分。我还和你分享了我的九条经验，建议你在平时解决问题的时候多思考、多总结，提炼出更多自己分析问题的套路和拿手工具。 最后，值得一提的是，定位到问题原因后，我们要做好记录和复盘。每一次故障和问题都是宝贵的资源，复盘不仅仅是记录问题，更重要的是改进。复盘时，我们需要做到以下四点： 记录完整的时间线、处理措施、上报流程等信息； 分析问题的根本原因； 给出短、中、长期改进方案，包括但不限于代码改动、SOP、流程，并记录跟踪每一个方案进行闭环； 定期组织团队回顾过去的故障。 思考与讨论 如果你现在打开一个App后发现首页展示了一片空白，那这到底是客户端兼容性的问题，还是服务端的问题呢？如果是服务端的问题，又如何进一步细化定位呢？你有什么分析思路吗？ 对于分析定位问题，你会做哪些监控或是使用哪些工具呢？ 你有没有遇到过什么花了很长时间才定位到的，或是让你印象深刻的问题或事故呢？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/34 _ 加餐4：分析定位Java问题，一定要用好这些工具（一）","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/34 _ 加餐4：分析定位Java问题，一定要用好这些工具（一）/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/34%20_%20%E5%8A%A0%E9%A4%904%EF%BC%9A%E5%88%86%E6%9E%90%E5%AE%9A%E4%BD%8DJava%E9%97%AE%E9%A2%98%EF%BC%8C%E4%B8%80%E5%AE%9A%E8%A6%81%E7%94%A8%E5%A5%BD%E8%BF%99%E4%BA%9B%E5%B7%A5%E5%85%B7%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"34 | 加餐4：分析定位Java问题，一定要用好这些工具（一）作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我要和你分享的内容是分析定位Java问题常用的一些工具。 到这里，我们的课程更新17讲了，已经更新过半了。在学习过程中，你会发现我在介绍各种坑的时候，并不是直接给出问题的结论，而是通过工具来亲眼看到问题。 为什么这么做呢？因为我始终认为，遇到问题尽量不要去猜，一定要眼见为实。只有通过日志、监控或工具真正看到问题，然后再回到代码中进行比对确认，我们才能认为是找到了根本原因。 你可能一开始会比较畏惧使用复杂的工具去排查问题，又或者是打开了工具感觉无从下手，但是随着实践越来越多，对Java程序和各种框架的运作越来越熟悉，你会发现使用这些工具越来越顺手。 其实呢，工具只是我们定位问题的手段，要用好工具主要还是得对程序本身的运作有大概的认识，这需要长期的积累。 因此，我会通过两篇加餐，和你分享4个案例，分别展示使用JDK自带的工具来排查JVM参数配置问题、使用Wireshark来分析网络问题、通过MAT来分析内存问题，以及使用Arthas来分析CPU使用高的问题。这些案例也只是冰山一角，你可以自己再通过些例子进一步学习和探索。 在今天这篇加餐中，我们就先学习下如何使用JDK自带工具、Wireshark来分析和定位Java程序的问题吧。 使用JDK自带工具查看JVM情况JDK自带了很多命令行甚至是图形界面工具，帮助我们查看JVM的一些信息。比如，在我的机器上运行ls命令，可以看到JDK 8提供了非常多的工具或程序： 接下来，我会与你介绍些常用的监控工具。你也可以先通过下面这张图了解下各种工具的基本作用： 为了测试这些工具，我们先来写一段代码：启动10个死循环的线程，每个线程分配一个10MB左右的字符串，然后休眠10秒。可以想象到，这个程序会对GC造成压力。 &#x2F;&#x2F;启动10个线程 IntStream.rangeClosed(1, 10).mapToObj(i -&gt; new Thread(() -&gt; &#123; while (true) &#123; &#x2F;&#x2F;每一个线程都是一个死循环，休眠10秒，打印10M数据 String payload &#x3D; IntStream.rangeClosed(1, 10000000) .mapToObj(__ -&gt; &quot;a&quot;) .collect(Collectors.joining(&quot;&quot;)) + UUID.randomUUID().toString(); try &#123; TimeUnit.SECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(payload.length()); &#125; &#125;)).forEach(Thread::start); TimeUnit.HOURS.sleep(1); 修改pom.xml，配置spring-boot-maven-plugin插件打包的Java程序的main方法类： &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;&#x2F;artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;org.geekbang.time.commonmistakes.troubleshootingtools.jdktool.CommonMistakesApplication &lt;&#x2F;mainClass&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; 然后使用java -jar启动进程，设置JVM参数，让堆最小最大都是1GB： java -jar common-mistakes-0.0.1-SNAPSHOT.jar -Xms1g -Xmx1g 完成这些准备工作后，我们就可以使用JDK提供的工具，来观察分析这个测试程序了。 jps首先，使用jps得到Java进程列表，这会比使用ps来的方便： ➜ ~ jps 12707 22261 Launcher 23864 common-mistakes-0.0.1-SNAPSHOT.jar 15608 RemoteMavenServer36 23243 Main 23868 Jps 22893 KotlinCompileDaemon jinfo然后，可以使用jinfo打印JVM的各种参数： ➜ ~ jinfo 23864 Java System Properties: #Wed Jan 29 12:49:47 CST 2020 ... user.name&#x3D;zhuye path.separator&#x3D;\\: os.version&#x3D;10.15.2 java.runtime.name&#x3D;Java(TM) SE Runtime Environment file.encoding&#x3D;UTF-8 java.vm.name&#x3D;Java HotSpot(TM) 64-Bit Server VM ... VM Flags: -XX:CICompilerCount&#x3D;4 -XX:ConcGCThreads&#x3D;2 -XX:G1ConcRefinementThreads&#x3D;8 -XX:G1HeapRegionSize&#x3D;1048576 -XX:GCDrainStackTargetSize&#x3D;64 -XX:InitialHeapSize&#x3D;268435456 -XX:MarkStackSize&#x3D;4194304 -XX:MaxHeapSize&#x3D;4294967296 -XX:MaxNewSize&#x3D;2576351232 -XX:MinHeapDeltaBytes&#x3D;1048576 -XX:NonNMethodCodeHeapSize&#x3D;5835340 -XX:NonProfiledCodeHeapSize&#x3D;122911450 -XX:ProfiledCodeHeapSize&#x3D;122911450 -XX:ReservedCodeCacheSize&#x3D;251658240 -XX:+SegmentedCodeCache -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC VM Arguments: java_command: common-mistakes-0.0.1-SNAPSHOT.jar -Xms1g -Xmx1g java_class_path (initial): common-mistakes-0.0.1-SNAPSHOT.jar Launcher Type: SUN_STANDARD 查看第15行和19行可以发现，我们设置JVM参数的方式不对，-Xms1g和-Xmx1g这两个参数被当成了Java程序的启动参数，整个JVM目前最大内存是4GB左右，而不是1GB。 因此，当我们怀疑JVM的配置很不正常的时候，要第一时间使用工具来确认参数。除了使用工具确认JVM参数外，你也可以打印VM参数和程序参数： System.out.println(&quot;VM options&quot;); System.out.println(ManagementFactory.getRuntimeMXBean().getInputArguments().stream().collect(Collectors.joining(System.lineSeparator()))); System.out.println(&quot;Program arguments&quot;); System.out.println(Arrays.stream(args).collect(Collectors.joining(System.lineSeparator()))); 把JVM参数放到-jar之前，重新启动程序，可以看到如下输出，从输出也可以确认这次JVM参数的配置正确了： ➜ target git:(master) ✗ java -Xms1g -Xmx1g -jar common-mistakes-0.0.1-SNAPSHOT.jar test VM options -Xms1g -Xmx1g Program arguments test jvisualvm然后，启动另一个重量级工具jvisualvm观察一下程序，可以在概述面板再次确认JVM参数设置成功了： 继续观察监视面板可以看到，JVM的GC活动基本是10秒发生一次，堆内存在250MB到900MB之间波动，活动线程数是22。我们可以在监视面板看到JVM的基本情况，也可以直接在这里进行手动GC和堆Dump操作： jconsole如果希望看到各个内存区的GC曲线图，可以使用jconsole观察。jconsole也是一个综合性图形界面监控工具，比jvisualvm更方便的一点是，可以用曲线的形式监控各种数据，包括MBean中的属性值： jstat同样，如果没有条件使用图形界面（毕竟在Linux服务器上，我们主要使用命令行工具），又希望看到GC趋势的话，我们可以使用jstat工具。 jstat工具允许以固定的监控频次输出JVM的各种监控指标，比如使用-gcutil输出GC和内存占用汇总信息，每隔5秒输出一次，输出100次，可以看到Young GC比较频繁，而Full GC基本10秒一次： ➜ ~ jstat -gcutil 23940 5000 100 S0 S1 E O M CCS YGC YGCT FGC FGCT CGC CGCT GCT 0.00 100.00 0.36 87.63 94.30 81.06 539 14.021 33 3.972 837 0.976 18.968 0.00 100.00 0.60 69.51 94.30 81.06 540 14.029 33 3.972 839 0.978 18.979 0.00 0.00 0.50 99.81 94.27 81.03 548 14.143 34 4.002 840 0.981 19.126 0.00 100.00 0.59 70.47 94.27 81.03 549 14.177 34 4.002 844 0.985 19.164 0.00 100.00 0.57 99.85 94.32 81.09 550 14.204 34 4.002 845 0.990 19.196 0.00 100.00 0.65 77.69 94.32 81.09 559 14.469 36 4.198 847 0.993 19.659 0.00 100.00 0.65 77.69 94.32 81.09 559 14.469 36 4.198 847 0.993 19.659 0.00 100.00 0.70 35.54 94.32 81.09 567 14.763 37 4.378 853 1.001 20.142 0.00 100.00 0.70 41.22 94.32 81.09 567 14.763 37 4.378 853 1.001 20.142 0.00 100.00 1.89 96.76 94.32 81.09 574 14.943 38 4.487 859 1.007 20.438 0.00 100.00 1.39 39.20 94.32 81.09 575 14.946 38 4.487 861 1.010 20.442 其中，S0表示Survivor0区占用百分比，S1表示Survivor1区占用百分比，E表示Eden区占用百分比，O表示老年代占用百分比，M表示元数据区占用百分比，YGC表示年轻代回收次数，YGCT表示年轻代回收耗时，FGC表示老年代回收次数，FGCT表示老年代回收耗时。 jstat命令的参数众多，包含-class、-compiler、-gc等。Java 8、Linux&#x2F;Unix平台jstat工具的完整介绍，你可以查看这里。jstat定时输出的特性，可以方便我们持续观察程序的各项指标。 继续来到线程面板可以看到，大量以Thread开头的线程基本都是有节奏的10秒运行一下，其他时间都在休眠，和我们的代码逻辑匹配： 点击面板的线程Dump按钮，可以查看线程瞬时的线程栈： jstack通过命令行工具jstack，也可以实现抓取线程栈的操作： ➜ ~ jstack 23940 2020-01-29 13:08:15 Full thread dump Java HotSpot(TM) 64-Bit Server VM (11.0.3+12-LTS mixed mode): ... &quot;main&quot; #1 prio&#x3D;5 os_prio&#x3D;31 cpu&#x3D;440.66ms elapsed&#x3D;574.86s tid&#x3D;0x00007ffdd9800000 nid&#x3D;0x2803 waiting on condition [0x0000700003849000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(java.base@11.0.3&#x2F;Native Method) at java.lang.Thread.sleep(java.base@11.0.3&#x2F;Thread.java:339) at java.util.concurrent.TimeUnit.sleep(java.base@11.0.3&#x2F;TimeUnit.java:446) at org.geekbang.time.commonmistakes.troubleshootingtools.jdktool.CommonMistakesApplication.main(CommonMistakesApplication.java:41) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@11.0.3&#x2F;Native Method) at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@11.0.3&#x2F;NativeMethodAccessorImpl.java:62) at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@11.0.3&#x2F;DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(java.base@11.0.3&#x2F;Method.java:566) at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) at org.springframework.boot.loader.Launcher.launch(Launcher.java:51) at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:52) &quot;Thread-1&quot; #13 prio&#x3D;5 os_prio&#x3D;31 cpu&#x3D;17851.77ms elapsed&#x3D;574.41s tid&#x3D;0x00007ffdda029000 nid&#x3D;0x9803 waiting on condition [0x000070000539d000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(java.base@11.0.3&#x2F;Native Method) at java.lang.Thread.sleep(java.base@11.0.3&#x2F;Thread.java:339) at java.util.concurrent.TimeUnit.sleep(java.base@11.0.3&#x2F;TimeUnit.java:446) at org.geekbang.time.commonmistakes.troubleshootingtools.jdktool.CommonMistakesApplication.lambda$null$1(CommonMistakesApplication.java:33) at org.geekbang.time.commonmistakes.troubleshootingtools.jdktool.CommonMistakesApplication$$Lambda$41&#x2F;0x00000008000a8c40.run(Unknown Source) at java.lang.Thread.run(java.base@11.0.3&#x2F;Thread.java:834) ... 抓取后可以使用类似fastthread这样的在线分析工具来分析线程栈。 jcmd最后，我们来看一下Java HotSpot虚拟机的NMT功能。 通过NMT，我们可以观察细粒度内存使用情况，设置-XX:NativeMemoryTracking&#x3D;summary&#x2F;detail可以开启NMT功能，开启后可以使用jcmd工具查看NMT数据。 我们重新启动一次程序，这次加上JVM参数以detail方式开启NMT： -Xms1g -Xmx1g -XX:ThreadStackSize&#x3D;256k -XX:NativeMemoryTracking&#x3D;detail 在这里，我们还增加了-XX:ThreadStackSize参数，并将其值设置为256k，也就是期望把线程栈设置为256KB。我们通过NMT观察一下设置是否成功。 启动程序后执行如下jcmd命令，以概要形式输出NMT结果。可以看到，当前有32个线程，线程栈总共保留了差不多4GB左右的内存。我们明明配置线程栈最大256KB啊，为什么会出现4GB这么夸张的数字呢，到底哪里出了问题呢？ ➜ ~ jcmd 24404 VM.native_memory summary 24404: Native Memory Tracking: Total: reserved&#x3D;6635310KB, committed&#x3D;5337110KB - Java Heap (reserved&#x3D;1048576KB, committed&#x3D;1048576KB) (mmap: reserved&#x3D;1048576KB, committed&#x3D;1048576KB) - Class (reserved&#x3D;1066233KB, committed&#x3D;15097KB) (classes #902) (malloc&#x3D;9465KB #908) (mmap: reserved&#x3D;1056768KB, committed&#x3D;5632KB) - Thread (reserved&#x3D;4209797KB, committed&#x3D;4209797KB) (thread #32) (stack: reserved&#x3D;4209664KB, committed&#x3D;4209664KB) (malloc&#x3D;96KB #165) (arena&#x3D;37KB #59) - Code (reserved&#x3D;249823KB, committed&#x3D;2759KB) (malloc&#x3D;223KB #730) (mmap: reserved&#x3D;249600KB, committed&#x3D;2536KB) - GC (reserved&#x3D;48700KB, committed&#x3D;48700KB) (malloc&#x3D;10384KB #135) (mmap: reserved&#x3D;38316KB, committed&#x3D;38316KB) - Compiler (reserved&#x3D;186KB, committed&#x3D;186KB) (malloc&#x3D;56KB #105) (arena&#x3D;131KB #7) - Internal (reserved&#x3D;9693KB, committed&#x3D;9693KB) (malloc&#x3D;9661KB #2585) (mmap: reserved&#x3D;32KB, committed&#x3D;32KB) - Symbol (reserved&#x3D;2021KB, committed&#x3D;2021KB) (malloc&#x3D;1182KB #334) (arena&#x3D;839KB #1) - Native Memory Tracking (reserved&#x3D;85KB, committed&#x3D;85KB) (malloc&#x3D;5KB #53) (tracking overhead&#x3D;80KB) - Arena Chunk (reserved&#x3D;196KB, committed&#x3D;196KB) (malloc&#x3D;196KB) 重新以VM.native_memory detail参数运行jcmd： jcmd 24404 VM.native_memory detail 可以看到，有16个可疑线程，每一个线程保留了262144KB内存，也就是256MB（通过下图红框可以看到，使用关键字262144KB for Thread Stack from搜索到了16个结果）： 其实，ThreadStackSize参数的单位是KB，所以我们如果要设置线程栈256KB，那么应该设置256而不是256k。重新设置正确的参数后，使用jcmd再次验证下： 除了用于查看NMT外，jcmd还有许多功能。我们可以通过help，看到它的所有功能： jcmd 24781 help 对于其中每一种功能，我们都可以进一步使用help来查看介绍。比如，使用GC.heap_info命令可以打印Java堆的一些信息： jcmd 24781 help GC.heap_info 除了jps、jinfo、jcmd、jstack、jstat、jconsole、jvisualvm外，JDK中还有一些工具，你可以通过官方文档查看完整介绍。 使用Wireshark分析SQL批量插入慢的问题我之前遇到过这样一个案例：有一个数据导入程序需要导入大量的数据，开发同学就想到了使用Spring JdbcTemplate的批量操作功能进行数据批量导入，但是发现性能非常差，和普通的单条SQL执行性能差不多。 我们重现下这个案例。启动程序后，首先创建一个testuser表，其中只有一列name，然后使用JdbcTemplate的batchUpdate方法，批量插入10000条记录到testuser表： @SpringBootApplication @Slf4j public class BatchInsertAppliation implements CommandLineRunner &#123; @Autowired private JdbcTemplate jdbcTemplate; public static void main(String[] args) &#123; SpringApplication.run(BatchInsertApplication.class, args); &#125; @PostConstruct public void init() &#123; &#x2F;&#x2F;初始化表 jdbcTemplate.execute(&quot;drop table IF EXISTS &#96;testuser&#96;;&quot;); jdbcTemplate.execute(&quot;create TABLE &#96;testuser&#96; (\\n&quot; + &quot; &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT,\\n&quot; + &quot; &#96;name&#96; varchar(255) NOT NULL,\\n&quot; + &quot; PRIMARY KEY (&#96;id&#96;)\\n&quot; + &quot;) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4;&quot;); &#125; @Override public void run(String... args) &#123; long begin &#x3D; System.currentTimeMillis(); String sql &#x3D; &quot;INSERT INTO &#96;testuser&#96; (&#96;name&#96;) VALUES (?)&quot;; &#x2F;&#x2F;使用JDBC批量更新 jdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() &#123; @Override public void setValues(PreparedStatement preparedStatement, int i) throws SQLException &#123; &#x2F;&#x2F;第一个参数(索引从1开始)，也就是name列赋值 preparedStatement.setString(1, &quot;usera&quot; + i); &#125; @Override public int getBatchSize() &#123; &#x2F;&#x2F;批次大小为10000 return 10000; &#125; &#125;); log.info(&quot;took : &#123;&#125; ms&quot;, System.currentTimeMillis() - begin); &#125; &#125; 执行程序后可以看到，1万条数据插入耗时26秒： [14:44:19.094] [main] [INFO ] [o.g.t.c.t.network.BatchInsertApplication:52 ] - took : 26144 ms 其实，对于批量操作，我们希望程序可以把多条insert SQL语句合并成一条，或至少是一次性提交多条语句到数据库，以减少和MySQL交互次数、提高性能。那么，我们的程序是这样运作的吗？ 我在加餐3中提到一条原则，“分析问题一定是需要依据的，靠猜是猜不出来的”。现在，我们就使用网络分析工具Wireshark来分析一下这个案例，眼见为实。 首先，我们可以在这里下载Wireshark，启动后选择某个需要捕获的网卡。由于我们连接的是本地的MySQL，因此选择loopback回环网卡： 然后，Wireshark捕捉这个网卡的所有网络流量。我们可以在上方的显示过滤栏输入tcp.port &#x3D;&#x3D; 6657，来过滤出所有6657端口的TCP请求（因为我们是通过6657端口连接MySQL的）。 可以看到，程序运行期间和MySQL有大量交互。因为Wireshark直接把TCP数据包解析为了MySQL协议，所以下方窗口可以直接显示MySQL请求的SQL查询语句。我们看到，testuser表的每次insert操作，插入的都是一行记录： 如果列表中的Protocol没有显示MySQL的话，你可以手动点击Analyze菜单的Decode As菜单，然后加一条规则，把6657端口设置为MySQL协议： 这就说明，我们的程序并不是在做批量插入操作，和普通的单条循环插入没有区别。调试程序进入ClientPreparedStatement类，可以看到执行批量操作的是executeBatchInternal方法。executeBatchInternal方法的源码如下： @Override protected long[] executeBatchInternal() throws SQLException &#123; synchronized (checkClosed().getConnectionMutex()) &#123; if (this.connection.isReadOnly()) &#123; throw new SQLException(Messages.getString(&quot;PreparedStatement.25&quot;) + Messages.getString(&quot;PreparedStatement.26&quot;), MysqlErrorNumbers.SQL_STATE_ILLEGAL_ARGUMENT); &#125; if (this.query.getBatchedArgs() &#x3D;&#x3D; null || this.query.getBatchedArgs().size() &#x3D;&#x3D; 0) &#123; return new long[0]; &#125; &#x2F;&#x2F; we timeout the entire batch, not individual statements int batchTimeout &#x3D; getTimeoutInMillis(); setTimeoutInMillis(0); resetCancelledState(); try &#123; statementBegins(); clearWarnings(); if (!this.batchHasPlainStatements &amp;&amp; this.rewriteBatchedStatements.getValue()) &#123; if (((PreparedQuery&lt;?&gt;) this.query).getParseInfo().canRewriteAsMultiValueInsertAtSqlLevel()) &#123; return executeBatchedInserts(batchTimeout); &#125; if (!this.batchHasPlainStatements &amp;&amp; this.query.getBatchedArgs() !&#x3D; null &amp;&amp; this.query.getBatchedArgs().size() &gt; 3 &#x2F;* cost of option setting rt-wise *&#x2F;) &#123; return executePreparedBatchAsMultiStatement(batchTimeout); &#125; &#125; return executeBatchSerially(batchTimeout); &#125; finally &#123; this.query.getStatementExecuting().set(false); clearBatch(); &#125; &#125; &#125; 注意第18行，判断了rewriteBatchedStatements参数是否为true，是才会开启批量的优化。优化方式有2种： 如果有条件的话，优先把insert语句优化为一条语句，也就是executeBatchedInserts方法； 如果不行的话，再尝试把insert语句优化为多条语句一起提交，也就是executePreparedBatchAsMultiStatement方法。 到这里就明朗了，实现批量提交优化的关键，在于rewriteBatchedStatements参数。我们修改连接字符串，并将其值设置为true： spring.datasource.url&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:6657&#x2F;common_mistakes?characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;rewriteBatchedStatements&#x3D;true 重新按照之前的步骤打开Wireshark验证，可以看到： 这次insert SQL语句被拼接成了一条语句（如第二个红框所示）； 这个TCP包因为太大被分割成了11个片段传输，#699请求是最后一个片段，其实际内容是insert语句的最后一部分内容（如第一和第三个红框显示）。 为了查看整个TCP连接的所有数据包，你可以在请求上点击右键，选择Follow-&gt;TCP Stream： 打开后可以看到，从MySQL认证开始到insert语句的所有数据包的内容： 查看最开始的握手数据包可以发现，TCP的最大分段大小（MSS）是16344字节，而我们的MySQL超长insert的数据一共138933字节，因此被分成了11段传输，其中最大的一段是16332字节，低于MSS要求的16344字节。 最后可以看到插入1万条数据仅耗时253毫秒，性能提升了100倍： [20:19:30.185] [main] [INFO ] [o.g.t.c.t.network.BatchInsertApplication:52 ] - took : 253 ms 虽然我们一直在使用MySQL，但我们很少会考虑MySQL Connector Java是怎么和MySQL交互的，实际发送给MySQL的SQL语句又是怎样的。有没有感觉到，MySQL协议其实并不遥远，我们完全可以使用Wireshark来观察、分析应用程序与MySQL交互的整个流程。 重点回顾今天，我就使用JDK自带工具查看JVM情况、使用Wireshark分析SQL批量插入慢的问题，和你展示了一些工具及其用法。 首先，JDK自带的一些监控和故障诊断工具中，有命令行工具也有图形工具。其中，命令行工具更适合在服务器上使用，图形界面工具用于本地观察数据更直观。为了帮助你用好这些工具，我们带你使用这些工具，分析了程序错误设置JVM参数的两个问题，并且观察了GC工作的情况。 然后，我们使用Wireshark分析了MySQL批量insert操作慢的问题。我们看到，通过Wireshark分析网络包可以让一切变得如此透明。因此，学好Wireshark，对我们排查C&#x2F;S网络程序的Bug或性能问题，会有非常大的帮助。 比如，遇到诸如Connection reset、Broken pipe等网络问题的时候，你可以利用Wireshark来定位问题，观察客户端和服务端之间到底出了什么问题。 此外，如果你需要开发网络程序的话，Wireshark更是分析协议、确认程序是否正确实现的必备工具。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 JDK中还有一个jmap工具，我们会使用jmap -dump命令来进行堆转储。那么，这条命令和jmap -dump:live有什么区别呢？你能否设计一个实验，来证明下它们的区别呢？ 你有没有想过，客户端是如何和MySQL进行认证的呢？你能否对照MySQL的文档，使用Wireshark观察分析这一过程呢？ 在平时工作中，你还会使用什么工具来分析排查Java应用程序的问题呢？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/35 _ 加餐5：分析定位Java问题，一定要用好这些工具（二）","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/35 _ 加餐5：分析定位Java问题，一定要用好这些工具（二）/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/35%20_%20%E5%8A%A0%E9%A4%905%EF%BC%9A%E5%88%86%E6%9E%90%E5%AE%9A%E4%BD%8DJava%E9%97%AE%E9%A2%98%EF%BC%8C%E4%B8%80%E5%AE%9A%E8%A6%81%E7%94%A8%E5%A5%BD%E8%BF%99%E4%BA%9B%E5%B7%A5%E5%85%B7%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"35 | 加餐5：分析定位Java问题，一定要用好这些工具（二）作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 在上一篇加餐中，我们介绍了使用JDK内置的一些工具、网络抓包工具Wireshark去分析、定位Java程序的问题。很多同学看完这一讲，留言反馈说是“打开了一片新天地，之前没有关注过JVM”“利用JVM工具发现了生产OOM的原因”。 其实，工具正是帮助我们深入到框架和组件内部，了解其运作方式和原理的重要抓手。所以，我们一定要用好它们。 今天，我继续和你介绍如何使用JVM堆转储的工具MAT来分析OOM问题，以及如何使用全能的故障诊断工具Arthas来分析、定位高CPU问题。 使用MAT分析OOM问题对于排查OOM问题、分析程序堆内存使用情况，最好的方式就是分析堆转储。 堆转储，包含了堆现场全貌和线程栈信息（Java 6 Update 14开始包含）。我们在上一篇加餐中看到，使用jstat等工具虽然可以观察堆内存使用情况的变化，但是对程序内到底有多少对象、哪些是大对象还一无所知，也就是说只能看到问题但无法定位问题。而堆转储，就好似得到了病人在某个瞬间的全景核磁影像，可以拿着慢慢分析。 Java的OutOfMemoryError是比较严重的问题，需要分析出根因，所以对生产应用一般都会这样设置JVM参数，方便发生OOM时进行堆转储： -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath&#x3D;. 上一篇加餐中我们提到的jvisualvm工具，同样可以进行一键堆转储后，直接打开这个dump查看。但是，jvisualvm的堆转储分析功能并不是很强大，只能查看类使用内存的直方图，无法有效跟踪内存使用的引用关系，所以我更推荐使用Eclipse的Memory Analyzer（也叫做MAT）做堆转储的分析。你可以点击这个链接，下载MAT。 使用MAT分析OOM问题，一般可以按照以下思路进行： 通过支配树功能或直方图功能查看消耗内存最大的类型，来分析内存泄露的大概原因； 查看那些消耗内存最大的类型、详细的对象明细列表，以及它们的引用链，来定位内存泄露的具体点； 配合查看对象属性的功能，可以脱离源码看到对象的各种属性的值和依赖关系，帮助我们理清程序逻辑和参数； 辅助使用查看线程栈来看OOM问题是否和过多线程有关，甚至可以在线程栈看到OOM最后一刻出现异常的线程。 比如，我手头有一个OOM后得到的转储文件java_pid29569.hprof，现在要使用MAT的直方图、支配树、线程栈、OQL等功能来分析此次OOM的原因。 首先，用MAT打开后先进入的是概览信息界面，可以看到整个堆是437.6MB： 那么，这437.6MB都是什么对象呢？ 如图所示，工具栏的第二个按钮可以打开直方图，直方图按照类型进行分组，列出了每个类有多少个实例，以及占用的内存。可以看到，char[]字节数组占用内存最多，对象数量也很多，结合第二位的String类型对象数量也很多，大概可以猜出（String使用char[]作为实际数据存储）程序可能是被字符串占满了内存，导致OOM。 我们继续分析下，到底是不是这样呢。 在char[]上点击右键，选择List objects-&gt;with incoming references，就可以列出所有的char[]实例，以及每个char[]的整个引用关系链： 随机展开一个char[]，如下图所示： 接下来，我们按照红色框中的引用链来查看，尝试找到这些大char[]的来源： 在①处看到，这些char[]几乎都是10000个字符、占用20000字节左右（char是UTF-16，每一个字符占用2字节）； 在②处看到，char[]被String的value字段引用，说明char[]来自字符串； 在③处看到，String被ArrayList的elementData字段引用，说明这些字符串加入了一个ArrayList中； 在④处看到，ArrayList又被FooService的data字段引用，这个ArrayList整个RetainedHeap列的值是431MB。 Retained Heap（深堆）代表对象本身和对象关联的对象占用的内存，Shallow Heap（浅堆）代表对象本身占用的内存。比如，我们的FooService中的data这个ArrayList对象本身只有16字节，但是其所有关联的对象占用了431MB内存。这些就可以说明，肯定有哪里在不断向这个List中添加String数据，导致了OOM。 左侧的蓝色框可以查看每一个实例的内部属性，图中显示FooService有一个data属性，类型是ArrayList。 如果我们希望看到字符串完整内容的话，可以右键选择Copy-&gt;Value，把值复制到剪贴板或保存到文件中： 这里，我们复制出的是10000个字符a（下图红色部分可以看到）。对于真实案例，查看大字符串、大数据的实际内容对于识别数据来源，有很大意义： 看到这些，我们已经基本可以还原出真实的代码是怎样的了。 其实，我们之前使用直方图定位FooService，已经走了些弯路。你可以点击工具栏中第三个按钮（下图左上角的红框所示）进入支配树界面（有关支配树的具体概念参考这里）。这个界面会按照对象保留的Retained Heap倒序直接列出占用内存最大的对象。 可以看到，第一位就是FooService，整个路径是FooSerice-&gt;ArrayList-&gt;Object[]-&gt;String-&gt;char[]（蓝色框部分），一共有21523个字符串（绿色方框部分）： 这样，我们就从内存角度定位到FooService是根源了。那么，OOM的时候，FooService是在执行什么逻辑呢？ 为解决这个问题，我们可以点击工具栏的第五个按钮（下图红色框所示）。打开线程视图，首先看到的就是一个名为main的线程（Name列），展开后果然发现了FooService： 先执行的方法先入栈，所以线程栈最上面是线程当前执行的方法，逐一往下看能看到整个调用路径。因为我们希望了解FooService.oom()方法，看看是谁在调用它，它的内部又调用了谁，所以选择以FooService.oom()方法（蓝色框）为起点来分析这个调用栈。 往下看整个绿色框部分，oom()方法被OOMApplication的run方法调用，而这个run方法又被SpringAppliction.callRunner方法调用。看到参数中的CommandLineRunner你应该能想到，OOMApplication其实是实现了CommandLineRunner接口，所以是SpringBoot应用程序启动后执行的。 以FooService为起点往上看，从紫色框中的Collectors和IntPipeline，你大概也可以猜出，这些字符串是由Stream操作产生的。再往上看，可以发现在StringBuilder的append操作的时候，出现了OutOfMemoryError异常（黑色框部分），说明这这个线程抛出了OOM异常。 我们看到，整个程序是Spring Boot应用程序，那么FooService是不是Spring的Bean呢，又是不是单例呢？如果能分析出这点的话，就更能确认是因为反复调用同一个FooService的oom方法，然后导致其内部的ArrayList不断增加数据的。 点击工具栏的第四个按钮（如下图红框所示），来到OQL界面。在这个界面，我们可以使用类似SQL的语法，在dump中搜索数据（你可以直接在MAT帮助菜单搜索OQL Syntax，来查看OQL的详细语法）。 比如，输入如下语句搜索FooService的实例： SELECT * FROM org.geekbang.time.commonmistakes.troubleshootingtools.oom.FooService 可以看到只有一个实例，然后我们通过List objects功能搜索引用FooService的对象： 得到以下结果： 可以看到，一共两处引用： 第一处是，OOMApplication使用了FooService，这个我们已经知道了。 第二处是一个ConcurrentHashMap。可以看到，这个HashMap是DefaultListableBeanFactory的singletonObjects字段，可以证实FooService是Spring容器管理的单例的Bean。 你甚至可以在这个HashMap上点击右键，选择Java Collections-&gt;Hash Entries功能，来查看其内容： 这样就列出了所有的Bean，可以在Value上的Regex进一步过滤。输入FooService后可以看到，类型为FooService的Bean只有一个，其名字是fooService： 到现在为止，我们虽然没看程序代码，但是已经大概知道程序出现OOM的原因和大概的调用栈了。我们再贴出程序来对比一下，果然和我们看到得一模一样： @SpringBootApplication public class OOMApplication implements CommandLineRunner &#123; @Autowired FooService fooService; public static void main(String[] args) &#123; SpringApplication.run(OOMApplication.class, args); &#125; @Override public void run(String... args) throws Exception &#123; &#x2F;&#x2F;程序启动后，不断调用Fooservice.oom()方法 while (true) &#123; fooService.oom(); &#125; &#125; &#125; @Component public class FooService &#123; List&lt;String&gt; data &#x3D; new ArrayList&lt;&gt;(); public void oom() &#123; &#x2F;&#x2F;往同一个ArrayList中不断加入大小为10KB的字符串 data.add(IntStream.rangeClosed(1, 10_000) .mapToObj(__ -&gt; &quot;a&quot;) .collect(Collectors.joining(&quot;&quot;))); &#125; &#125; 到这里，我们使用MAT工具从对象清单、大对象、线程栈等视角，分析了一个OOM程序的堆转储。可以发现，有了堆转储，几乎相当于拿到了应用程序的源码+当时那一刻的快照，OOM的问题无从遁形。 使用Arthas分析高CPU问题Arthas是阿里开源的Java诊断工具，相比JDK内置的诊断工具，要更人性化，并且功能强大，可以实现许多问题的一键定位，而且可以一键反编译类查看源码，甚至是直接进行生产代码热修复，实现在一个工具内快速定位和修复问题的一站式服务。今天，我就带你使用Arthas定位一个CPU使用高的问题，系统学习下这个工具的使用。 首先，下载并启动Arthas： curl -O https:&#x2F;&#x2F;alibaba.github.io&#x2F;arthas&#x2F;arthas-boot.jar java -jar arthas-boot.jar 启动后，直接找到我们要排查的JVM进程，然后可以看到Arthas附加进程成功： [INFO] arthas-boot version: 3.1.7 [INFO] Found existing java process, please choose one and hit RETURN. * [1]: 12707 [2]: 30724 org.jetbrains.jps.cmdline.Launcher [3]: 30725 org.geekbang.time.commonmistakes.troubleshootingtools.highcpu.HighCPUApplication [4]: 24312 sun.tools.jconsole.JConsole [5]: 26328 org.jetbrains.jps.cmdline.Launcher [6]: 24106 org.netbeans.lib.profiler.server.ProfilerServer 3 [INFO] arthas home: &#x2F;Users&#x2F;zhuye&#x2F;.arthas&#x2F;lib&#x2F;3.1.7&#x2F;arthas [INFO] Try to attach process 30725 [INFO] Attach process 30725 success. [INFO] arthas-client connect 127.0.0.1 3658 ,---. ,------. ,--------.,--. ,--. ,---. ,---. &#x2F; O \\ | .--. &#39;&#39;--. .--&#39;| &#39;--&#39; | &#x2F; O \\ &#39; .-&#39; | .-. || &#39;--&#39;.&#39; | | | .--. || .-. |&#96;. &#96;-. | | | || |\\ \\ | | | | | || | | |.-&#39; | &#96;--&#39; &#96;--&#39;&#96;--&#39; &#39;--&#39; &#96;--&#39; &#96;--&#39; &#96;--&#39;&#96;--&#39; &#96;--&#39;&#96;-----&#39; wiki https:&#x2F;&#x2F;alibaba.github.io&#x2F;arthas tutorials https:&#x2F;&#x2F;alibaba.github.io&#x2F;arthas&#x2F;arthas-tutorials version 3.1.7 pid 30725 time 2020-01-30 15:48:33 输出help命令，可以看到所有支持的命令列表。今天，我们会用到dashboard、thread、jad、watch、ognl命令，来定位这个HighCPUApplication进程。你可以通过官方文档，查看这些命令的完整介绍： dashboard命令用于整体展示进程所有线程、内存、GC等情况，其输出如下： 可以看到，CPU高并不是GC引起的，占用CPU较多的线程有8个，其中7个是ForkJoinPool.commonPool。学习过加餐1的话，你应该就知道了，ForkJoinPool.commonPool是并行流默认使用的线程池。所以，此次CPU高的问题，应该出现在某段并行流的代码上。 接下来，要查看最繁忙的线程在执行的线程栈，可以使用thread -n命令。这里，我们查看下最忙的8个线程： thread -n 8 输出如下： 可以看到，由于这些线程都在处理MD5的操作，所以占用了大量CPU资源。我们希望分析出代码中哪些逻辑可能会执行这个操作，所以需要从方法栈上找出我们自己写的类，并重点关注。 由于主线程也参与了ForkJoinPool的任务处理，因此我们可以通过主线程的栈看到需要重点关注org.geekbang.time.commonmistakes.troubleshootingtools.highcpu.HighCPUApplication类的doTask方法。 接下来，使用jad命令直接对HighCPUApplication类反编译： jad org.geekbang.time.commonmistakes.troubleshootingtools.highcpu.HighCPUApplication 可以看到，调用路径是main-&gt;task()-&gt;doTask()，当doTask方法接收到的int参数等于某个常量的时候，会进行1万次的MD5操作，这就是耗费CPU的来源。那么，这个魔法值到底是多少呢？ 你可能想到了，通过jad命令继续查看User类即可。这里因为是Demo，所以我没有给出很复杂的逻辑。在业务逻辑很复杂的代码中，判断逻辑不可能这么直白，我们可能还需要分析出doTask的“慢”会慢在什么入参上。 这时，我们可以使用watch命令来观察方法入参。如下命令，表示需要监控耗时超过100毫秒的doTask方法的入参，并且输出入参，展开2层入参参数： watch org.geekbang.time.commonmistakes.troubleshootingtools.highcpu.HighCPUApplication doTask &#39;&#123;params&#125;&#39; &#39;#cost&gt;100&#39; -x 2 可以看到，所有耗时较久的doTask方法的入参都是0，意味着User.ADMN_ID常量应该是0。 最后，我们使用ognl命令来运行一个表达式，直接查询User类的ADMIN_ID静态字段来验证是不是这样，得到的结果果然是0： [arthas@31126]$ ognl &#39;@org.geekbang.time.commonmistakes.troubleshootingtools.highcpu.User@ADMIN_ID&#39; @Integer[0] 需要额外说明的是，由于monitor、trace、watch等命令是通过字节码增强技术来实现的，会在指定类的方法中插入一些切面来实现数据统计和观测，因此诊断结束要执行shutdown来还原类或方法字节码，然后退出Arthas。 在这个案例中，我们通过Arthas工具排查了高CPU的问题： 首先，通过dashboard + thread命令，基本可以在几秒钟内一键定位问题，找出消耗CPU最多的线程和方法栈； 然后，直接jad反编译相关代码，来确认根因； 此外，如果调用入参不明确的话，可以使用watch观察方法入参，并根据方法执行时间来过滤慢请求的入参。 可见，使用Arthas来定位生产问题根本用不着原始代码，也用不着通过增加日志来帮助我们分析入参，一个工具即可完成定位问题、分析问题的全套流程。 对于应用故障分析，除了阿里Arthas之外，还可以关注去哪儿的Bistoury工具，其提供了可视化界面，并且可以针对多台机器进行管理，甚至提供了在线断点调试等功能，模拟IDE的调试体验。 重点回顾最后，我再和你分享一个案例吧。 有一次开发同学遇到一个OOM问题，通过查监控、查日志、查调用链路排查了数小时也无法定位问题，但我拿到堆转储文件后，直接打开支配树图一眼就看到了可疑点。Mybatis每次查询都查询出了几百万条数据，通过查看线程栈马上可以定位到出现Bug的方法名，然后来到代码果然发现因为参数条件为null导致了全表查询，整个定位过程不足5分钟。 从这个案例我们看到，使用正确的工具、正确的方法来分析问题，几乎可以在几分钟内定位到问题根因。今天，我和你介绍的MAT正是分析Java堆内存问题的利器，而Arthas是快速定位分析Java程序生产Bug的利器。利用好这两个工具，就可以帮助我们在分钟级定位生产故障。 思考与讨论 在介绍线程池的时候，我们模拟了两种可能的OOM情况，一种是使用Executors.newFixedThreadPool，一种是使用Executors.newCachedThreadPool，你能回忆起OOM的原因吗？假设并不知道OOM的原因，拿到了这两种OOM后的堆转储，你能否尝试使用MAT分析堆转储来定位问题呢？ Arthas还有一个强大的热修复功能。比如，遇到高CPU问题时，我们定位出是管理员用户会执行很多次MD5，消耗大量CPU资源。这时，我们可以直接在服务器上进行热修复，步骤是：jad命令反编译代码-&gt;使用文本编辑器（比如Vim）直接修改代码-&gt;使用sc命令查找代码所在类的ClassLoader-&gt;使用redefine命令热更新代码。你可以尝试使用这个流程，直接修复程序（注释doTask方法中的相关代码）吗？ 在平时工作中，你还会使用什么工具来分析排查Java应用程序的问题呢？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/36 _ 加餐6：这15年来，我是如何在工作中学习技术和英语的？","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/36 _ 加餐6：这15年来，我是如何在工作中学习技术和英语的？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/36%20_%20%E5%8A%A0%E9%A4%906%EF%BC%9A%E8%BF%9915%E5%B9%B4%E6%9D%A5%EF%BC%8C%E6%88%91%E6%98%AF%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E5%92%8C%E8%8B%B1%E8%AF%AD%E7%9A%84%EF%BC%9F/","excerpt":"","text":"36 | 加餐6：这15年来，我是如何在工作中学习技术和英语的？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来和你聊聊如何在工作中，让自己成长得更快。 工作这些年来，经常会有同学来找我沟通学习和成长，他们的问题可以归结为两个。 一是，长期参与CRUD业务开发项目，技术提升出现瓶颈，学不到新知识，完全没有办法实践各种新技术，以后会不会被淘汰、找不到工作？ 二是，英语学得比较晚，大学的时候也只是为了应试，英语水平很低，看不了英文的技术资料，更别说去外企找工作了。 不知道你是不是也面临这两个问题呢？今天，我就通过自己的经历和你分享一下，如何利用有限的环境、有限的时间来学习技术和英语？ 学好技术在我看来，知识网络的搭建就是在造楼房：基础也就是地基的承载力，决定了你能把楼造多高；广度就像是把房子造大、造宽；深度就是楼房的高度。因此，如果你想要提升自己的水平，那这三个方面的发展缺一不可。 第一，学习必须靠自觉。虽说工作经历和项目经验是实践技术、提升技术的一个重要手段，但不可能所有的工作经历和项目都能持续地提升我们的技术。所以，我们要想提升自己的技术水平，就必须打消仅仅通过工作经历来提升的念头，要靠业余时间主动地持续学习和积累来提升。 比如，你可以针对项目中用到的技术，全面阅读官方文档，做各种Demo来论证其技术特性。在这个过程中，你一定还会产生许多技术疑问，那就继续展开学习。 第二，不要吝啬分享。刚毕业那会，我花了很多时间和精力在CSDN回答问题，积极写博客、写书和翻译书。这些经历对我的技术成长，帮助非常大。 很多知识点我们自认为完全掌握了，但其实并不是那么回事儿。当我们要说出来教别人的时候，就必须100%了解每一个细节。因此，分享不仅是帮助自己进一步理清每一个知识点、锻炼自己的表达能力，更是一种强迫自己学习的手段，因为你要保证按时交付。 当然了，分享的过程也需要些正向激励，让自己保持分享的激情。就比如说，我获得的几次微软MVP、CSDN TOP3专家等荣誉，就对我激励很大，可以让我保持热情去不断地学习并帮助别人。 第三，不要停留在舒适区。分享一段我的真实经历吧。我加入一家公司组建新团队后，在做技术选型的时候，考虑到成本等因素，放弃了从事了七年的.NET技术，转型Java。有了.NET的积累，我自己转型Java只用了两周。其实，一开始做这个决定非常痛苦，但是突破自己的舒适区并没有想象得那么困难。随后，我又自学了iOS、深度学习、Python等技术或语言。 随着掌握的技术越来越多，这些技术不但让我触类旁通，更让我理解了技术只是工具，解决问题需要使用合适的技术。因此，我也建议你，利用业余时间多学习几门不同类型的编程语言，比如Java、Python和Go。 有些时候，我们因为恐惧跳出舒适区而不愿意学习和引入合适的新技术来解决问题，虽然省去了前期的学习转型成本，但是后期却会投入更多的时间来弥补技术上的短板。 第四，打好基础很重要。这里的“基础”是指，和编程语言无关的那部分知识，包括硬件基础、操作系统原理、TCP&#x2F;IP、HTTP、数据结构和算法、安全基础、设计模式、数据库原理等。学习基础知识是比较枯燥的过程，需要大块的时间来系统阅读相关书籍，并要尝试把学到的知识付诸实践。只有实践过的技术才能映入脑子里，否则只是书本上的知识。 比如，学习TCP&#x2F;IP的时候，我们可以使用Wireshark来观察网络数据。又比如，学习设计模式的时候，我们可以结合身边的业务案例来思考下，是否有对应的适用场景，如果没有能否模拟一个场景，然后使用所有设计模式和自己熟悉的语言开发一个实际的Demo。 这些看似和我们日常业务开发关系不大的基础知识，是我们是否能够深入理解技术的最重要的基石。 第五，想办法积累技术深度。对开发者而言，技术深度体现在从一个框架、组件或SDK的使用者转变为开发者。 虽然不建议大家重复去造轮子、造框架，但我们完全可以阅读各种框架的源码去了解其实现，并亲手实现一些框架的原型。比如，你可以尝试把MVC、RPC、ORM、IoC、AOP等框架，都实现一个最基本功能点原型。在实现的过程中，你一定会遇到很多问题和难点，然后再继续研究一下Spring、Hibernate、Dubbo等框架是如何实现的。 当把自己的小框架实现出来的那一刻，你收获的不仅是满满的成就感，更是在技术深度积累上的更进一步。在这个过程中，你肯定会遇到不少问题、解决不少问题。有了这些积累，之后再去学习甚至二次开发那些流行的开源框架，就会更容易了。 除了实现一些框架外，我还建议你选择一个中间件（比如Redis、RocketMQ）来练手学习网络知识。 我们可以先实现它的客户端，用Netty实现TCP通信层的功能，之后参照官方文档实现协议封装、客户端连接池等功能。在实现的过程中，你可以对自己实现的客户端进行压测，分析和官方实现的性能差距。这样一来，你不仅可以对TCP&#x2F;IP网络有更深入的了解，还可以获得很多网络方面的优化经验。 然后，再尝试实现服务端，进一步加深对网络的认识。最后，尝试把服务端扩展为支持高可用的集群，来加深对分布式通信技术的理解。 在实现这样一个分布式C&#x2F;S中间件的过程中，你对技术的理解肯定会深入了许多。在这个过程中，你会发现，技术深度的“下探”和基础知识的积累息息相关。基础知识不扎实，往深了走往往会步履维艰。这时，你可以再回过头来，重新系统学习一些基础理论。 第六，扩大技术广度也重要。除了之前提到的多学几门编程语言之外，在技术广度的拓展上，我们还可以在两个方面下功夫。 第一，阅读大量的技术书籍。新出来的各种技术图书（不只是编程相关的），一般我都会买。十几年来，我买了500多本技术图书，大概有三分之一是完整看过的，还有三分之一只翻了一个大概，还有三分之一只看了目录。 广泛的阅读，让我能够了解目前各种技术的主流框架和平台。这样的好处是，在整体看技术方案的时候，我可以知道大家都在做什么，不至于只能理解方案中的一部分。对于看不完的、又比较有价值的书，我会做好标签，等空闲的时候再看。 第二，在开发程序的时候，我们会直接使用运维搭建的数据库（比如Elasticsearch、MySQL）、中间件（比如RabbitMQ、ZooKeeper）、容器云（比如Kubernetes）。但，如果我们只会使用这些组件而不会搭建的话，对它们的理解很可能只是停留在API或客户端层面。 因此，我建议你去尝试下从头搭建和配置这些组件，在遇到性能问题的时候自己着手分析一下。把实现技术的前后打通，遇到问题时我们就不至于手足无措了。我通常会购买公有云按小时收费的服务器，来构建一些服务器集群，尝试搭建和测试这些系统，加深对运维的理解。 学好英语为啥要单独说英语的学习方法呢，这是因为学好英语对做技术的同学非常重要： 国外的社区环境比较好，许多技术问题只有通过英文关键字才能在Google或Stackoverflow上搜到答案； 可以第一时间学习各种新技术、阅读第一手资料，中文翻译资料往往至少有半年左右的延迟； 参与或研究各种开源项目，和老外沟通需要使用英语来提问，以及阅读别人的答复。 所以说，学好英语可以整体拓宽个人视野。不过，对于上班族来说，我们可能没有太多的大块时间投入英语学习，那如何利用碎片时间、相对休闲地学习英语呢？还有一个问题是，学好英语需要大量的练习和训练，但不在外企工作就连个英语环境都没有，那如何解决这样的矛盾呢？ 接下来，我将从读、听、写和说四个方面，和你分享一下我学习英语的方法。 读方面读对于我们这些搞技术的人来说是最重要的，并且也是最容易掌握的。我建议你这么学： 先从阅读身边的技术文档开始，有英语文档的一定要选择阅读英语文档。一来，贴近实际工作，是我们真正用得到的知识，比较容易有兴趣去读；二来，这些文档中大部分词汇，我们日常基本都接触过，难度不会太大。 技术书籍的常用词汇量不大，有了一些基础后，你可以正式或非正式地参与翻译一些英语书籍或文档。从我的经验来看，翻译过一本书之后，你在日常阅读任何技术资料时基本都不需要查字典了。 订阅一些英语报纸，比如ChinaDaily。第一，贴近日常生活，都是我们身边发生的事儿，不会很枯燥；第二，可以进一步积累词汇量。在这个过程中，你肯定需要大量查字典打断阅读，让你感觉很痛苦。但一般来说，一个单词最多查三次也就记住了，所以随着时间推移，你慢慢可以摆脱字典，词汇量也可以上一个台阶了。 技术方面阅读能力的培养，通常只需要三个月左右的时间，但生活方面资料的阅读可能需要一年甚至更长的时间。 听方面读需要积累词汇量，听力的训练需要通过时间来磨耳朵。每个人都可以选择适合自己的材料来磨耳朵，比如我是通过看美剧来训练听力的。 我就以看美剧为例，说说练听力的几个关键点。 量变到质变的过程，需要1000小时的量。如果一部美剧是100小时，那么看前9部的时候可能都很痛苦，直到某一天你突然觉得一下子都可以听懂了。 需要确保看美剧时没有中文字幕，否则很难忍住不看，看了字幕就无法起到训练听力的效果。 在美剧的选择上，可以先选择对话比较少，也可以选择自己感兴趣的题材，这样不容易放弃。如果第一次听下来，听懂率低于30%，连理解剧情都困难，那么可以先带着中文字幕看一遍，然后再脱离字幕看。 看美剧不在乎看的多少，而是要找适合的素材反复训练。有人说，反复看100遍《老友记》，英语的听说能力可以接近母语是英语的人的水平。 如果看美剧不适合你的话，你可以选择其他方式，比如开车或坐地铁的时候听一些感兴趣的PodCast等。 总而言之，选择自己喜欢的材料和内容，从简单开始，不断听。如果你有一定词汇量的话，查字典其实不是必须的，很多时候不借助字典，同一个单词出现10遍后我们也就知道它的意思了。 一定要记住，在积累1000小时之前，别轻易放弃。 写方面如果有外企经历，那么平时写英语邮件和文档基本就可以让你的工作英语过关；如果没有外企经历也没关系，你可以尝试通过下面的方式锻炼写作： 每天写英语日记。日记是自己看的，没人会嘲笑你，可以从简单的开始。 在保持写作的同时，需要确保自己能有持续的一定量的阅读。因为，写作要实现从正确到准确到优雅，离不开阅读的积累。 写程序的时候使用英语注释，或者尝试写英语博客，总之利用好一切写的机会，来提升自己的英语表达。 再和你分享一个小技巧。当你要通过查词典知道中文的英语翻译时，尽量不要直接用找到的英文单词，最好先在英语例句中确认这个翻译的准确性再使用，以免闹笑话。 说方面训练说英语的机会是最少的，毕竟身边说英语的人少，很难自己主动练习。 这里我和你分享两个方法吧。 第一是，买外教的1-1对话课程来训练。这些课程一般按小时计费，由母语是英语的人在线和你聊一些话题，帮助你训练对话能力。 买不买课程不重要，只要能有母语是英语的人来帮你提升就可以。同时，大量的听力训练也可以帮助你提升说的能力，很多英语短句经过反复强化会成为脱口而出的下意识反应。所以，你会发现在听力达到质变的时候，说的能力也会上一个台阶。 第二，大胆说，不要担心有语法错误、单词发音问题、表达不流畅问题而被嘲笑。其实，你可以反过来想想，老外说中文时出现这些问题，你会嘲笑他吗。 这里有一个技巧是，尽量选用简单的表达和词汇去说，先尝试把内容说出来，甚至是只说几个关键字，而不是憋着在脑子里尝试整理一个完整的句子。灵活运用有限的单词，尽可能地流畅、准确表达，才是聪明的做法。 总结最后我想说，如果你感觉学得很累、进步很慢，也不要放弃，坚持下来就会越来越好。我刚毕业那会儿，有一阵子也对OOP很迷茫，感觉根本无法理解OOP的理念，写出的代码完全是过程化的代码。但我没有放弃，参与写了几年的复杂业务程序，再加上系统自学设计模式，到某一个时刻我突然就能写出OOP的业务代码了。 学习一定是一个日积月累、量变到质变的过程，希望我分享的学习方法能对你有启发。不过，每个人的情况都不同，一定要找到适合自己的学习方式，才更容易坚持下去。 持续学习很重要，不一定要短时间突击学习，而最好是慢慢学、持续积累，积累越多学习就会越轻松。如果学习遇到瓶颈感觉怎么都学不会，也不要沮丧，这其实还是因为积累不够。你一定也有过这样的经验：一本去年觉得很难啃的书，到今年再看会觉得恰到好处，明年就会觉得比较简单，就是这个道理。 我是朱晔，欢迎在评论区与我留言分享你学习技术和英语的心得，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/37 _ 加餐7：程序员成长28计","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/37 _ 加餐7：程序员成长28计/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/37%20_%20%E5%8A%A0%E9%A4%907%EF%BC%9A%E7%A8%8B%E5%BA%8F%E5%91%98%E6%88%90%E9%95%BF28%E8%AE%A1/","excerpt":"","text":"37 | 加餐7：程序员成长28计作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 今天直播，我准备和你聊聊程序员成长的话题。我从毕业后入行到现在作为一个高级管理者，已经在互联网领域拼搏了15年了。我把这些年自己的成长历程、心得体会整理成了“程序员成长28计”。 今天，我就和你分别聊聊这28计。 入门 0.5年第1计：不要过于纠结方向选择问题。 开始入门的时候，我们可能都会纠结于选择前端还是后端，选择了后端还犹豫到底选Java、Go还是Python。 其实，我觉得不用过于纠结。如果说你对偏前端的内容感兴趣，那就从前端入手；对数据库方面的内容感兴趣，那就从后端入手。等你真正入门以后，你再去转方向、转技术栈都会非常容易，因为技术都是相通的。 第2计：学习一定要敢于踏出真正的第一步。 这里我说的第一步，不是说开始看某个领域的书了，而是真正把IDE下载好、把编程环境搭建好，并实现一个最简单的程序。我一直觉得，把编程环境搭建好，就已经算是入门一半了。 如果你只是停留在看书这个层次上的话，是永远入不了门的。因为这些知识只是停留在书上，还没有真正变成你自己的。只有自己写过、实践过，才能真正掌握。 第3计：找人给你指一下方向。 刚入门的时候，面对各种各样的语言、技术你很可能会迷茫。就比如说，刚入门后端的时候，Spring全家桶有十几样，还有各种数据库方面的，Java程序本身的语法和框架，那到底先学什么呢？这个时候，只要找人给你指点一下学习的顺序，以及按照怎样的主线来学习，就会事半功倍。否则，你会在大量的资料里花费大量的时间、消耗大量的精力。 第4计：找准合适的入门资料。 在我看来，选择入门资料，需要注意两点： 一定要选择手把手的资料，也就是从搭环境开始怎么一步步地去操作，并带一些实战项目。这样看，视频课程可能更合适。 要难度合适。 那怎么理解“难度合适”呢？举个例子，你看的这本书的知识深度在70分，而你自己的知识深度在60分，那这本书非常合适。因为从60到65的感觉是非常爽的。在60分的时候，你有能力去汲取70分深度的书里面的知识点，然后你会变成65分。而如果你现在的知识深度在20分去看70分的书，或者你的知识深度在75分却去看70分的书，就不会有任何感觉、任何收益。所以，很多同学看我的专栏课程会有共鸣，也是这个原因。 程序员2年第5计：想办法系统性地学习。 步入两年这个阶段后，我们要开始想办法系统性地学习了，比如系统性地学习设计模式、算法、数据库。只有系统性地学习，才能给我们建立起完整的知识框架。因为一定是先有知识网络，才能在网络上继续铺更多的东西。 那怎么才能有系统性学习的动力呢？ 第一，分享可以让自己有动力。比如，你说要写一个什么系列的文章，那话说出去了，就会逼着自己去实现。 第二，花钱买课程，做系统性的学习。当你花了几百甚至几千块钱去买课程的时候，就会逼着自己的学习，不然钱就浪费掉了。 第6计：选择一份好工作。 选择一份好工作，也就是选择一个好的项目，从而积累一些人脉资源，是非常重要的，可能要比技术成长更重要些。 比如说，你能够进入到一个相对较大的公司，它能带给你的最最主要的就是人脉资源，也就是你能够认识更多、更优秀的人。认识这些人，就是你日后的机会。 第7计：学习必须靠自觉。 我们不能期望项目经验一定或者说一直会给自己带来技术提升。即使是你能接触一些高并发的、比较复杂的项目，它们带来的提升也是有限的，或者说持续的时间通常会比较短。 因为大多数公司在乎的都是你的输出，输出你的能力和经验。所以说，学习和成长这件事儿，必须靠自觉，包括自觉地去想如何系统性地学习、如何有计划地学习，以及平时要多问为什么。 第8计：看适合自己的书。 这里也是说，我们在看书的过程中，要注意去鉴别书的层次，选择难度合适的书。 其实，在做程序员前两年的时间里，我不太建议去广泛地看书，要先想办法能够专注些，打好自己主要的编程语言的基础；然后，围绕着自己主要的编程语言或者主要使用的技术去看书。 第9计：想办法积累技术广度。 将来踏上技术管理路线之后，你有可能管的团队不是你这个领域，比如你是后端出身可能要带领移动团队。如果你不知道移动端最基本的东西的话，是没有办法跟团队成员沟通的。所以说，你可以有自己的一个专长，但是你要知道其他领域最基本的东西。 积累技术广度的方式，主要有下面三种。 第一，体验全栈。如果你是做后端的，就应该去大概了解下客户端、移动端，或者说大前端；可以了解下测试和运维怎么做，了解运维的话帮助可能会更大。你还可以动手做一个自己的项目，就从云服务器的采购开始。在搭建项目部署的过程中，你可以自己去搭建运维相关的部分，甚至是自己搭建一些中间件。 因为在大厂，一般都有自动化发布系统、有工程化平台、有自己的运维体系、有自己的监控系统等等。但是，如果只是使用这些工具的话，我们是没法建立一个全局观的，因为我们不知道它们是怎么运作的。 第二，多学一些编程语言。但是学了几门编程语言后，你会发现每门语言都有自己的特色和软肋。这就会引发你很多的思考，比如为什么这个语言没有这个特性，又怎么样去解决。另外，每门语言他都有自己的技术栈，你会来回地比较。这些思考和比较，对自己的成长都很有用。 如果你对一个语言的掌握比较透彻的话，再去学其他语言不会花很久。我刚毕业是做.net，后来转了Java，再后来又去学Python。因为高级语言的特性基本上都差不多，你只要学一些语法，用到的时候再去查更多的内容，然后做个项目，所以学一门语言可能也就需要一个月甚至会更快一些。 第三，广泛看书。 第10计：想办法积累技术深度。 主要的方式是造轮子、看源码和学底层。 第一，造轮子。所谓的造轮子，不一定是要造完要用，你可以拿造轮子来练手，比如徒手写一个框架。在这个过程中，你会遇到很多困难，然后可能会想办法去学习一些现有技术的源码，这对技术深度的理解是非常有帮助的。 第二，看一些源码。如果你能够理清楚一些源码的主线，然后你能积累很多设计模式的知识。 第三，学一些偏向于底层的东西，可以帮助你理解技术的本质。上层的技术都依赖于底层的技术，所以你学完了底层的技术后，就会发现上层的技术再变也没有什么本质上的区别，然后学起来就会非常快。 第11计：学会使用搜索引擎。 对于程序员来说，最好可以使用Google来搜索，也就是说要使用英文的关键字来搜索。一方面，通过Google你可以搜到更多的内容，另一方面国外的技术圈或者网站关于纯技术的讨论会多一些。 第12计：学会和适应画图、写文档。 我觉得，写文档是在锻炼自己的总结能力和表达能力，画图更多的是在锻炼自己的抽象能力。写文档、画架构图，不仅仅是架构师需要具备的能力，还是你准确表达自己观点的必备方式。所以，我们不要觉得，宁肯写100行代码，也不愿意写一句话。 架构师3年第13计：注意软素质的提升。 这时候你已经有了好几年的经验了，那除了技术方面，还要注意软素质，比如沟通、自我驱动、总结等能力的提升。比如说沟通能力，就是你能不能很流畅地表达自己的观点，能不能比较主动地去沟通。 这些素质在日常工作中还是挺重要的，因为你做了架构师之后，免不了要去跟业务方和技术团队，甚至是其他的团队的架构师去沟通。 如果你的这些软素质不过硬，那可能你的方案就得不到认可，没办法达成自己的目标。 第14计：积累领域经验也很重要。 当你在一个领域工作几年之后，你就会对这个领域的产品非常熟悉，甚至比产品经理更懂产品。也就是说，即使这个产品没有别人的帮助，你也可以确保它朝着正确的方向发展。如果你想一直在这个领域工作的话，这种领域经验的积累就对自己的发展非常有帮助。 所以说，有些人做的是业务架构师，他可能在技术上并不是特别擅长，但对这个领域的系统设计或者说产品设计特别在行。如果说，你不想纯做技术的话，可以考虑积累更多的领域经验。 第15计：架构工作要接地气。 我以前做架构师的时候发现，有些架构师给出的方案非常漂亮，但就是不接地气、很难去落地。所以，在我看来，架构工作必须要接地气，包括三个方面：产出符合实际情况的方案、方案要落地实际项目、不要太技术化。 这里其实会有一个矛盾点：如果你想要提升自己的经验、技术，很多时候就需要去引入一些新技术，但是这些新技术的引入需要成本。而这里的成本不仅仅是你自己学习的成本，还需要整个团队有一定的经验。 比如Kubernetes，不是你引入了团队用就完事儿，整个团队的技术都需要得到提升，才能够驾驭这个系统。如果我们是为了自己的利益去引入一些不太符合公司实际情况的技术的话，其实对公司来说是不负责任的，而且这个方案很大程度上有可能会失败。 所以说，我觉得做架构工作是要产出一些更接地气的方案。比如同样是解决一个问题，有些架构方式或设计比较“老土”，但往往是很稳定的；而一些复杂的技术，虽然有先进的理念和设计，但你要驾驭它就需要很多成本，而且因为它的“新”往往还会存在各种各样的问题。 这也就是说，我们在设计架构的时候，必须要权衡方案是否接地气。 第16计：打造个人品牌。 我觉得，个人品牌包括口碑和影响力两个方面。 口碑就是你日常工作的态度，包括你的能力和沟通，会让人知道你靠不靠谱、能力是不是够强。好的口碑再加上宝贵的人脉，就是你非常重要的资源。口碑好的人基本上是不需要主动去找工作的，因为一直会有一些老领导或者朋友、同事会千方百计地想要给你机会。 很多人的技术非常不错，但就是没人知道他，问题就出在影响力上。而提升影响力的方法，无外乎就是参加技术大会、做分享、写博客、写书等等。 有了影响力和口碑，让更多的人能接触到你、认识你，你就会有更多的机会。 技术管理第17计：掌握管事的方法。 “管事”就是你怎样去安排，这里包括了制定项目管理流程、制定技术标准、工具化和自动化三个方面。 刚转做技术管理时容易犯的一个错的是，把事情都抓在自己手里。这时，你一定要想通，不是你自己在干活，你的产出是靠团队的。与其说什么事情都自己干，还不如说你去制定规范、流程和方向，然后让团队去做，否则你很容易就成了整个团队的瓶颈。 第18计：掌握带团队的方法。 第一，招人&amp;放权。带团队的话，最重要是招到优秀的人，然后就是放权。不要因为担心招到的人会比自己优秀，就想要找“弱”一些的。只有团队的事情做得更好了，你的整个团队的产出才是最高。 第二，工程师文化。通过建立工程师文化，让大家去互相交流、学习，从而建立一个良好的学习工作氛围。 第三，适当的沟通汇报制度。这也属于制定流程里面的，也是要建立一个沟通汇报的制度。 第19计：关注前沿技术，思考技术创新。 做了技术管理之后，你的视角要更高。你团队的成员，可能只是看到、接触到这一个部分、这一个模块，没有更多的信息，也没办法想得更远。这时，你就必须去创新、去关注更多的前沿技术，去思考自己的项目能不能用上这些技术。 第20计：关注产品。 在我看来，一个产品的形态很多时候决定了公司的命运，在产品上多想一些点子，往往要比技术上的重构带来的收益更大。这里不仅仅包括这个产品是怎么运作的，还包括产品中包含的创新、你能否挖掘一些衍生品。 高级技术管理在这个层次上面，我们更高级的技术管理可能是总监级别甚至以上，我以前在两家百人以上的小公司做过CTO。我当时的感觉是，所做的事情不能仅限于产品技术本身了。 第21计：搭建团队最重要。 这和招人还不太一样，招人肯定招的是下属，而搭建团队是必须让团队有一个梯队。一旦你把一些核心的人固化下来以后，整个团队就发展起来了。所以，你要在招人方面花费更多的精力，当然不仅仅是指面试。 搭建团队最重要的是你自己要有一个想法，知道自己需要一个什么样的职位来填补空缺，这个岗位上又需要什么样的人。 第22计：打造技术文化。 虽然在做技术管理的时候，我强调说要建立制度，但文化会更高于制度，而且文化没有那么强势。因为制度其实是列出来，要求大家去遵守，有“强迫”的感觉；而文化更强调潜移默化，通过耳濡目染获得大同感。这样一来，大家慢慢地就不会觉得这是文化了，而是说我现在就是这么干事儿的。 第23计：提炼价值观。 价值观是说公司按照这个理念去运作，希望有一些志同道合的人在一起干活。所以价值观又会高于文化，是整个公司层面的，对大家的影响也会更多。 虽然说价值观不会那么显性，但可以长久地确保公司里面的整个团队的心都是齐的，大家都知道公司是怎么运作的，有相同的目标。 第24计：关注运营和财务。 到了高级技术管理的位置，你就不仅仅是一个打工的了，你的命运是和公司紧紧绑定在一起的。所以，你需要更多地关注公司的运营和财务。 当你觉得自己的团队很小却要做那么多项目的时候，可以站在更高的角度去换位思考下。这时你可能就发现，你的团队做的事情并没有那么重要，对整个公司的发展来说你的团队规模已经足够了。如果说我们再大量招人的话，那么财务上就会入不敷出，整个公司的情况肯定也不会好。 职场心得第25计：掌握工作汇报的方式方法。 首先，我们不要把汇报当作负担、当作浪费时间。汇报其实是双向的，你跟上级多沟通的话，他可以反馈给你更多的信息，这个信息可能是你工作的方向，也可能是给你的一些资源，还可能是告诉你上级想要什么。因为你和你的上级其实在一个信息层面上是不对等的，他能收到更上级的信息，比如公司策略方面的信息。 第26计：坚持+信念。 第一，如果说你的目标就是成功的话，那没有什么可以阻挡你。职场上的扯皮和甩锅，都是避免不了的。举个例子吧。 我以前在一家公司工作的时候，别人不愿意配合我的工作。那怎么办呢，我知道自己的目标是把这件事儿做成。当时，这个项目的很多内容，比如说运维，都不在我这边，需要其他同事来负责。但人家就是不配合，群里艾特也不看，打电话也不接，那我怎么办呢？多打两次呗，实在不行我就发邮件抄送大家的上级。总之，就是想尽办法去沟通，因为你的目标就是成功。 第二，很多时候，创新就是相信一定可以实现才有的。 很多时候，你觉得这个事情是做不成的，然后直接拒绝掉了，创新就没有了。但如果相信这个事情一定是可以做成的，你就会想方设法去实现它，这个时候你想出来的东西就是有开创性的，就是创新。 第27计：持续的思考和总结。 在职场上提炼方法论是非常重要的。你要去思考自己在工作中对各种各样的事情的处理，是不是妥当，是不是能够总结出一些方法论。把这些方法论提炼保留下来，将来是能够帮到你的。很多东西，比如复盘自己的工作经历、复盘自己的选择，都要动脑子、都要去写，不能说过去了就过去了。这些经历提炼出的方法论，都是你的经验，是非常有价值的。 第28计：有关和平级同事的相处。 和平级同事之间，要以帮助别人的心态来合作。我们和上下级的同事来沟通，一般是不会有什么问题的，但跟平级的，尤其是跨部门的平级同事去沟通的时候，往往会因为利益问题，不会很愉快。 我觉得，这里最重要的就是以帮助别人的心态来合作。 比如这样说“你有什么困难的话，可以来问我”“你人手是不是不够，我可以帮你一起把这个项目做好”。这样大家的合作会比较顺畅，别人也不会有那么多戒心。 人和人的沟通，还在于有一层纱，突破了这层纱以后，大家就都会相信你，觉得你是一个靠谱的人。这样，平级同事也会愿意和你分享一些东西，因为他放心。 管理格言接下来，我要推荐的8条管理格言，是曹操管理和用人的理念，不是我自己总结出来的。 第一，真心诚意，以情感人。人和人之间去沟通的时候，不管是和上级或者下级的沟通，都要以非常诚恳的态度去沟通。 第二，推心置腹，以诚待人。有事情不要藏在心里，做“城府很深”的管理者。我觉得更好的方式是，让大家尽可能地知道更多的事儿，统一战线，站在一个角度来考虑问题。 第三，开诚布公，以理服人。把管理策略公布出来，不管是奖励也好惩罚也罢，让团队成员感觉公平公正， 第四，言行一致，以信取人。说到做到，对于管理下属、和别人沟通都非常重要。 第五，令行禁止，依法治人。管理上，你要制定好相关的制度，而且要公开出来。如果触犯了制度就需要惩罚，做得好了就要有奖赏。 第六，设身处地，以宽容人。很多时候，我们和别人的矛盾是没有足够的换位思考，没有设身处地地去想。如果说你的下属犯了错，还是要想一想是不是多给些机会，是不是能宽容一些。 第七，扬人责己，以功归人。这是非常重要的一点。事情是团队一起做的话，那就是团队的功劳，甚至下属的功劳。如果别人做得好的话，就要多表扬一些。对自己要严格一些，很多时候团队的问题就是管理者的问题，跟下属没太多关系。 第八，论功行赏，以奖励人。做得好了，要多给别人一些奖励。这也是公平公正的，大家都能看得到。 最后，我将关于程序员成长的28计整理在了一张思维导图上，以方便你收藏、转发。 我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/38 _ 加餐8：Java程序从虚拟机迁移到Kubernetes的一些坑","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/38 _ 加餐8：Java程序从虚拟机迁移到Kubernetes的一些坑/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/38%20_%20%E5%8A%A0%E9%A4%908%EF%BC%9AJava%E7%A8%8B%E5%BA%8F%E4%BB%8E%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB%E5%88%B0Kubernetes%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/","excerpt":"","text":"38 | 加餐8：Java程序从虚拟机迁移到Kubernetes的一些坑作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔，我们又见面了。结课并不意味着结束，我非常高兴能持续把好的内容分享给你，也希望你能继续在留言区与我保持交流，分享你的学习心得和实践经验。 使用Kubernetes大规模部署应用程序，可以提升整体资源利用率，提高集群稳定性，还能提供快速的集群扩容能力，甚至还可以实现集群根据压力自动扩容。因此，现在越来越多的公司开始把程序从虚拟机（VM）迁移到Kubernetes了。 在大多数的公司中，Kubernetes集群由运维来搭建，而程序的发布一般也是由CI&#x2F;CD平台完成。从虚拟机到Kubernetes的整个迁移过程，基本不需要修改任何代码，可能只是重新发布一次而已。所以，我们Java开发人员可能对迁移这个事情本身感知不强烈，认为Kubernetes只是运维需要知道的事情。但是程序一旦部署到了Kubernetes集群中，在容器环境中运行，总是会出现各种各样之前没有的奇怪的问题。 今天的加餐，就让我们一起看下这其中大概会遇到哪些“坑”，还有相应的“避坑方法”。 Pod IP不固定带来的坑Pod是Kubernetes中能够创建和部署应用的最小单元，我们可以通过Pod IP来访问到某一个应用实例，但需要注意的是，如果没有经过特殊配置，Pod IP并不是固定不变的，会在Pod重启后会发生变化。 不过好在，通常我们的Java微服务都是没有状态的，我们并不需要通过Pod IP来访问到某一个特定的Java服务实例。通常来说，要访问到部署在Kubernetes中的微服务集群，有两种服务发现和访问的方式： 通过Kubernetes来实现。也就是通过Service进行内部服务的互访，通过Ingress从外部访问到服务集群。 通过微服务注册中心（比如Eureka）来实现。也就是服务之间的互访通过客户端负载均衡后+直接访问Pod IP进行，外部访问到服务集群通过微服务网关转发请求。 使用这两种方式进行微服务的访问，我们都没有和Pod IP直接打交道，也不会把Pod IP记录持久化，所以一般不需要太关注Pod IP变动的问题。不过，在一些场景下，Pod IP的变动会造成一些问题。 之前我就遇到过这样的情况：某任务调度中间件会记录被调度节点的IP到数据库，随后通过访问节点IP查看任务节点执行日志的时候，如果节点部署在Kubernetes中，那么节点重启后Pod IP就会变动。这样，之前记录在数据库中的老节点的Pod IP必然访问不到，那么就会发生无法查看任务日志的情况。 遇到这种情况，我们应该怎么做呢？这时候，可能就需要修改这个中间件，把任务执行日志也进行持久化，从而避免这种访问任务节点来查看日志的行为。 总之，我们需要意识到Pod IP不固定的问题，并且进行“避坑操作”：在迁移到Kubernetes集群之前，摸排一下是否会存在需要通过IP访问到老节点的情况，如果有的话需要进行改造。 程序因为OOM被杀进程的坑在Kubernetes集群中部署程序的时候，我们通常会为容器设置一定的内存限制（limit），容器不可以使用超出其资源limit属性所设置的资源量。如果容器内的Java程序使用了大量内存，可能会出现各种OOM的情况。 第一种情况，是OS OOM Kill问题。如果过量内存导致操作系统Kernel不稳定，操作系统可能就会杀死Java进程。这时候，你能在操作系统&#x2F;var&#x2F;log&#x2F;messages日志中找到类似oom_kill_process的关键字。 第二种情况，是我们最常遇到的Java程序的OOM问题。程序超出堆内存的限制申请内存，导致Heap OOM，后续可能会因为健康检测没有通过被Kubernetes重启Pod。 在Kubernetes中部署Java程序时，这两种情况都很常见，表现出的症状也都是OOM关键字+重启。所以，当运维同学说程序因为OOM被杀死或重启的时候，我们一定要和运维同学一起去区分清楚，到底是哪一种情况，然后再对症处理。 对于情况1，问题的原因往往不是Java堆内存不够，更可能是程序使用了太多的堆外内存，超过了内存限制。这个时候，调大JVM最大堆内存只会让问题更严重，因为堆内存是可以通过GC回收的。我们需要分析Java进程哪部分区域内存占用过大，是不是合理，以及是否可能存在内存泄露问题。Java进程的内存占用除了堆之外，还包括 直接内存 元数据区 线程栈大小 Xss * 线程数 JIT代码缓存 GC、编译器使用额外空间 …… 我们可以使用NMT打印各部分区域大小，从而判断到底是哪部分内存区域占用了过多内存，或是可能有内存泄露问题： java -XX:NativeMemoryTracking=smmary/detail -XX:+UnlockDiagnosticVMOptions -XX:+PrintNMTStatistics 如果你确认OOM是情况2，那么我同样不建议直接调大堆内存的限制，防止之后再出现情况1。我会更建议你把堆内存限制为容器内存限制的50%~70%，预留出足够多的内存给堆外和OS核心。如果需要扩容堆内存的话，那么也需要同步扩容容器的内存limit。此外，也需要通过Heap Dump（你可以回顾下第35讲的相关内容）等手段来排查为什么堆内存占用会这么大，排除潜在的内存泄露的可能性。 内存和CPU资源配置不适配容器的坑刚刚我们提到了，堆内存扩容需要结合容器内存limit同步进行。其实，我们更希望的是，Java程序的堆内存配置能随着容器的资源配置，实现自动扩容或缩容，而不是写死Xmx和Xms。这样一来，运维同学可以更方便地针对整个集群进行扩容或缩容。 对于JDK&gt;8u191的版本，我们可以设置下面这些JVM参数，来让JVM自动根据容器内存限制来设置堆内存用量。比如，下面配置相当于把Xmx和Xms都设置为了容器内存limit的50%： XX:MaxRAMPercentage=50.0 -XX:InitialRAMPercentage=50.0 -XX:MinRAMPercentage=50.0 接下来，我们看看CPU资源配置不适配容器的坑，以及对应的解决方案。 对于CPU资源的使用，我们主要需要注意的是，代码中的各种组件甚至是JVM本身，会根据CPU数来配置并发数等重要参数指标，那么： 如果这个值因为JVM对容器的兼容性问题取到了Kubernetes工作节点的CPU数量，那么这个数量可能就不是4或8，而是128以上，进而导致并发数过高。 对于JDK&gt;8u191的版本可能会对容器兼容性较好，但是其获取到的Runtime.getRuntime().availableProcessors() 其实是request的值而不是limit的值（比如我们设置request为2、limit为8、CICompilerCount和ParallelGCThreads可能只是2），那么可能并发数就会过低，进而影响JVM的GC或编译性能。 所以，我的建议是： 第一，通过-XX:+PrintFlagsFinal开关，来确认ActiveProcessorCount是不是符合我们的期望，并且确认CICompilerCount、ParallelGCThreads等重要参数配置是否合理。 第二，直接设置CPU的request和limit一致，或是对于JDK&gt;8u191的版本可以通过-XX:ActiveProcessorCount&#x3D;xxx直接把ActiveProcessorCount设置为容器的CPU limit。 Pod重启以及重启后没有现场的坑除非宿主机有问题，否则虚拟机不太会自己重启或被重启，而Kubernetes中Pod的重启绝非小概率事件。在存活检测不通过、Pod重新进行节点调度等情况下，Pod都会进行重启。对于Pod的重启，我们需要关注两个问题。 第一个问题是，分析Pod为什么会重启。 其中，除了“程序因为OOM被杀进程的坑”这部分提到的OOM的问题之外，我们还需要关注存活检查不通过的情况。 Kubernetes有readinessProbe和livenessProbe两个探针，前者用于检查应用是否已经启动完成，后者用于持续探活。一般而言，运维同学会配置这2个探针为一个健康检测的断点，如果健康检测访问一次需要消耗比较长的时间（比如涉及到存储或外部服务可用性检测），那么很可能可以通过readinessProbe的检查但不通过livenessProbe检查（毕竟我们通常会为readinessProbe设置比较长的超时时间，而对于livenessProbe则没有那么宽容）。此外，健康检测也可能会受到Full GC的干扰导致超时。所以，我们需要和运维同学一起确认livenessProbe的配置地址和超时时间设置是否合理，防止偶发的livenessProbe探活失败导致的Pod重启。 第二个问题是，要理解Pod和虚拟机不同。 虚拟机一般都是有状态的，即便部署在虚拟机内的Java程序重启了，我们始终能有现场。而对于Pod重启来说，则是新建一个Pod，这就意味着老的Pod无法进入。因此，如果因为堆OOM问题导致重启，我们希望事后查看当时OS的一些日志或是在现场执行一些命令来分析问题，就不太可能了。 所以，我们需要想办法在Pod关闭之前尽可能保留现场，比如： 对于程序的应用日志、标准输出、GC日志等可以直接挂载到持久卷，不要保存在容器内部。 对于程序的堆栈现场保留，可以配置-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath在堆OOM的时候生成Dump；还可以让JVM调用任一个shell脚本，通过脚本来保留线程栈等信息： -XX:OnOutOfMemoryError=saveinfo.sh 对于容器的现场保留，可以让运维配置preStop钩子，在Pod关闭之前把必要的信息上传到持久卷或云上。 重点回顾今天，我们探讨了Java应用部署到Kubernetes集群会遇到的4类问题。 第一类问题是，我们需要理解应用的IP会动态变化，因此要在设计上解除对Pod IP的强依赖，使用依赖服务发现来定位到应用。 第二类问题是，在出现OOM问题的时候，首先要区分OOM的原因来自Java进程层面还是容器层面。如果是容器层面的话，我们还需要进一步分析到底是哪个内存区域占用了过多内存，定位到问题后再根据容器资源设置合理的JVM参数或进行资源扩容。 第三类问题是，需要确保程序使用的内存和CPU资源匹配容器的资源限制，既要确保程序所“看”到的主机资源信息是容器本身的而不是物理机的，又要确保程序能尽可能随着容器扩容而扩容其资源限制。 第四类问题是，我们需要重点关注程序非发布期重启的问题，并且针对Pod的重启问题做好现场保留的准备工作，排除资源配置不合理、存活检查不通过等可能性，以避免因为程序频繁重启导致的偶发性能问题或可用性问题。 只有解决了这些隐患，才能让Kubernetes集群更好地发挥作用。 思考与讨论在你的工作中，还遇到过Java+Kubernetes中的其他坑吗？ 我是朱晔，欢迎在评论区与我留言分享，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/开篇词、业务代码真的会有这么多坑？","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/开篇词、业务代码真的会有这么多坑？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/%E5%BC%80%E7%AF%87%E8%AF%8D%E3%80%81%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E7%9C%9F%E7%9A%84%E4%BC%9A%E6%9C%89%E8%BF%99%E4%B9%88%E5%A4%9A%E5%9D%91%EF%BC%9F/","excerpt":"","text":"开篇词 | 业务代码真的会有这么多坑？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔，贝壳金服的资深架构师。 我先和你说说我这15年的工作经历吧，以加深彼此的了解。前7年，我专注于.NET领域，负责业务项目的同时，也做了很多社区工作。在CSDN做版主期间，我因为回答了大量有关.NET的问题，并把很多问题的答案总结成了博客，获得了3次微软MVP的称号。 后来，我转到了Java领域，也从程序员变为了架构师，更关注开源项目和互联网架构设计。在空中网，我整体负责了百万人在线的大型MMO网游《激战》技术平台的架构设计，期间和团队开发了许多性能和稳定性都不错的Java框架；在饿了么，我负责过日千万订单量的物流平台的开发管理和架构工作，遇到了许多只有高并发下才会出现的问题，积累了大量的架构经验；现在，我在贝壳金服的基础架构团队，负责基础组件、中间件、基础服务开发规划，制定一些流程和规范，带领团队自研Java后端开发框架、微服务治理平台等，在落地Spring Cloud结合Kubernetes容器云平台技术体系的过程中，摸索出了很多适合公司项目的基础组件和最佳实践。 这15年来，我一直没有脱离编码工作，接触过大大小小的项目不下400个，自己亲身经历的、见别人踩过的坑不计其数。我感触很深的一点是，业务代码中真的有太多的坑：有些是看似非常简单的知识点反而容易屡次踩坑，比如Spring声明式事务不生效的问题；而有些坑因为“潜伏期”长，引发的线上事故造成了大量的人力和资金损失。因此，我系统梳理了这些案例和坑点，最终筛选出100个案例，涉及130多个坑点，组成了这个课程。 意识不到业务代码的坑，很危险我想看到100、130这两个数字，你不禁要问了：“我写了好几年的业务代码了，遇到问题时上网搜一下就有答案，遇到最多的问题就是服务器不稳定，重启一下基本就可以解决，哪里会有这么多坑呢？”带着这个问题，你继续听我往下说吧。 据我观察，很多开发同学没意识到这些坑，有以下三种可能： 意识不到坑的存在，比如所谓的服务器不稳定很可能是代码问题导致的，很多时候遇到OOM、死锁、超时问题在运维层面通过改配置、重启、扩容等手段解决了，没有反推到开发层面去寻找根本原因。 有些问题只会在特定情况下暴露。比如，缓存击穿、在多线程环境使用非线程安全的类，只有在多线程或高并发的情况才会暴露问题。 有些性能问题不会导致明显的Bug，只会让程序运行缓慢、内存使用增加，但会在量变到质变的瞬间爆发。 而正是因为没有意识到这些坑和问题，采用了错误的处理方式，最后问题一旦爆发，处理起来就非常棘手，这是非常可怕的。下面这些场景有没有感觉似曾相识呢？ 比如，我曾听说过有一个订单量很大的项目，每天总有上千份订单的状态或流程有问题，需要花费大量的时间来核对数据，修复订单状态。开发同学因为每天牵扯太多精力在排查问题上，根本没时间开发新需求。技术负责人为此头痛不已，无奈之下招了专门的技术支持人员。最后痛定思痛，才决定开启明细日志彻查这个问题，结果发现是自调用方法导致事务没生效的坑。 再比如，有个朋友告诉我，他们的金融项目计算利息的代码中，使用了float类型而不是BigDecimal类来保存和计算金额，导致给用户结算的每一笔利息都多了几分钱。好在，日终对账及时发现了问题。试想一下，结算的有上千个用户，每个用户有上千笔小订单，如果等月终对账的时候再发现，可能已经损失了几百万。 再比如，我们使用RabbitMQ做异步处理，业务处理失败的消息会循环不断地进入MQ。问题爆发之前，可能只影响了消息处理的时效性。但等MQ彻底瘫痪时，面对MQ中堆积的、混杂了死信和正常消息的几百万条数据，你除了清空又能怎么办。但清空MQ，就意味着要花费几小时甚至几十小时的时间，来补正常的业务数据，对业务影响时间很长。 像这样由一个小坑引发的重大事故，不仅仅会给公司造成损失，还会因为自责影响工作状态，降低编码的自信心。我就曾遇到过一位比较负责的核心开发同学，因为一个Bug给公司带来数万元的经济损失，最后心理上承受不住提出了辞职。 其实，很多时候不是我们不想从根本上解决问题，只是不知道问题到底在了哪里。要避开这些坑、找到这些定时炸弹，第一步就是得知道它们是什么、在哪里、为什么会出现。而讲清楚这些坑点和相关的最佳实践，正是本课程的主要内容。 这个课程是什么？如果用几个关键词概括这个课程的话，那我会选择“Java”“业务开发”“避坑100例”这3个。接下来，我就和你详细说说这个课程是什么，以及有什么特点。 第一个关键词是“Java”，指的是课程内所有Demo都是基于Java语言的。 如果你熟悉Java，那可以100%体会到这些坑点，也可以直接用这些Demo去检查你的业务代码是否也有类似的错误实现。 如果你不熟悉Java问题也不大，现在大部分高级语言的特性和结构都差不多，许多都是共性问题。此外“设计篇”“安全篇”的内容，基本是脱离具体语言层面的、高层次的问题。因此，即使不使用Java，你也可以有不少收获，这也是本课程的第一个特点。 讲到这里，我要说明的是，这个课程是围绕坑点而不是Java语言体系展开的，因此不是系统学习Java的教材。 第二个关键词是“业务开发”，也就是说课程内容限定在业务项目的开发，侧重业务项目开发时可能遇到的坑。 我们先看“业务”这个词。做业务开发时间长的同学尤其知道，业务项目有两大特点： 工期紧、逻辑复杂，开发人员会更多地考虑主流程逻辑的正确实现，忽略非主流程逻辑，或保障、补偿、一致性逻辑的实现； 往往缺乏详细的设计、监控和容量规划的闭环，结果就是随着业务发展出现各种各样的事故。 根据这些性质，我总结出了近30个方面的内容，力求覆盖业务项目开发的关键问题。案例的全面性，是本课程的第二大特点。 这些案例可以看作是Java业务代码的避坑大全，帮助你写出更好的代码，也能帮你进一步补全知识网增加面试的信心。你甚至可以把二级目录当作代码审核的Checklist，帮助业务项目一起成长和避坑。 我们再看“开发”这个词。为了更聚焦，也更有针对性，我把专栏内容限定在业务开发，不会过多地讨论架构、测试、部署运维等阶段的问题。而“设计篇”，重在讲述架构设计上可能会遇到的坑，不会全面、完整地介绍高可用、高并发、可伸缩性等架构因素。 第三个关键词是“避坑100例”。坑就是容易犯的错，避坑就是踩坑后分析根因，避免重复踩同样的坑。 整个课程30篇文章，涉及100个案例、约130个小坑，其中40%来自于我经历过或者是见过的200多个线上生产事故，剩下的60%来自于我开发业务项目，以及日常审核别人的代码发现的问题。贴近实际，而不是讲述过时的或日常开发根本用不到的技术或框架，就是本课程的第三大特点了。 大部分案例我会配合一个可执行的Demo来演示，Demo中不仅有错误实现（踩坑），还有修正后的正确实现（避坑）。完整且连续、授人以渔，是本课程的第四大特点。 完整且连续，知其所以然。我会按照“知识介绍-&gt;还原业务场景-&gt;错误实现-&gt;正确实现-&gt;原理分析-&gt;小总结 ”来讲解每个案例，针对每个坑点我至少会给出一个解决方案，并会挑选核心的点和你剖析源码。这样一来，你不仅能避坑，更能知道产生坑的根本原因，提升自己的技术能力。 授人以渔。在遇到问题的时候，我们一定是先通过经验和工具来定位分析问题，然后才能定位到坑，并不是一开始就知道为什么的。在这个课程中，我会尽可能地把分析问题的过程完整地呈现给你，而不是直接告诉你为什么，这样你以后遇到问题时也能有解决问题的思路。 这也是为什么，网络上虽然有很多关于Java代码踩坑的资料，但很多同学却和我反馈说，看过之后印象不深刻，也因为没吃透导致在一个知识点上重复踩坑。鉴于此，我还会与你分析我根据多年经验和思考，梳理出的一些最佳实践。 看到这里，是不是迫不及待地想要看看这个专栏的内容都会涉及哪些坑点了呢？那就看看下面这张思维导图吧： 鉴于这个专栏的内容和特点，我再和你说说最佳的学习方式是什么。 学习课程的最佳方法我们都知道，编程是一门实践科学，只看不练、不思考，效果通常不会太好。因此，我建议你打开每篇文章后，能够按照下面的方式深入学习： 对于每一个坑点，实际运行调试一下源码，使用文中提到的工具和方法重现问题，眼见为实。 对于每一个坑点，再思考下除了文内的解决方案和思路外，是否还有其他修正方式。 对于坑点根因中涉及的JDK或框架源码分析，你可以找到相关类再系统阅读一下源码。 实践课后思考题。这些思考题，有的是对文章内容的补充，有的是额外容易踩的坑。 理解了课程涉及的所有案例后，你应该就对业务代码大部分容易犯错的点了如指掌了，不仅仅自己可以写出更高质量的业务代码，还可以在审核别人代码时发现可能存在的问题，帮助整个团队成长。 当然了，你从这个课程收获的将不仅是解决案例中那些问题的方法，还可以提升自己分析定位问题、阅读源码的能力。当你再遇到其他诡异的坑时，也能有清晰的解决思路，也可以成长为一名救火专家，帮助大家一起定位、分析问题。 好了，以上就是我今天想要和你分享的内容了。请赶快跟随我们的课程开启避坑之旅吧，也欢迎你留言说说自己的情况，你都踩过哪些坑、对写业务代码又有哪些困惑？我们下一讲见！","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/答疑篇：代码篇思考题集锦（三）","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/答疑篇：代码篇思考题集锦（三）/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/%E7%AD%94%E7%96%91%E7%AF%87%EF%BC%9A%E4%BB%A3%E7%A0%81%E7%AF%87%E6%80%9D%E8%80%83%E9%A2%98%E9%9B%86%E9%94%A6%EF%BC%88%E4%B8%89%EF%BC%89/","excerpt":"","text":"答疑篇：代码篇思考题集锦（三）作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 今天，我们继续一起分析这门课第13~20讲的课后思考题。这些题目涉及了日志、文件IO、序列化、Java 8日期时间类、OOM、Java高级特性（反射、注解和泛型）和Spring框架的16道问题。 接下来，我们就一一具体分析吧。 13 | 日志：日志记录真没你想象的那么简单问题1：在讲“为什么我的日志会重复记录？”的案例时，我们把INFO级别的日志存放到_info.log中，把WARN和ERROR级别的日志存放到_error.log中。如果现在要把INFO和WARN级别的日志存放到_info.log中，把ERROR日志存放到_error.log中，应该如何配置Logback呢？ 答：要实现这个配置有两种方式，分别是：直接使用EvaluatorFilter和自定义一个Filter。我们分别看一下。 第一种方式是，直接使用logback自带的EvaluatorFilter： &lt;filter class&#x3D;&quot;ch.qos.logback.core.filter.EvaluatorFilter&quot;&gt; &lt;evaluator class&#x3D;&quot;ch.qos.logback.classic.boolex.GEventEvaluator&quot;&gt; &lt;expression&gt; e.level.toInt() &#x3D;&#x3D; WARN.toInt() || e.level.toInt() &#x3D;&#x3D; INFO.toInt() &lt;&#x2F;expression&gt; &lt;&#x2F;evaluator&gt; &lt;OnMismatch&gt;DENY&lt;&#x2F;OnMismatch&gt; &lt;OnMatch&gt;NEUTRAL&lt;&#x2F;OnMatch&gt; &lt;&#x2F;filter&gt; 第二种方式是，自定义一个Filter，实现解析配置中的“|”字符分割的多个Level： public class MultipleLevelsFilter extends Filter&lt;ILoggingEvent&gt; &#123; @Getter @Setter private String levels; private List&lt;Integer&gt; levelList; @Override public FilterReply decide(ILoggingEvent event) &#123; if (levelList &#x3D;&#x3D; null &amp;&amp; !StringUtils.isEmpty(levels)) &#123; &#x2F;&#x2F;把由|分割的多个Level转换为List&lt;Integer&gt; levelList &#x3D; Arrays.asList(levels.split(&quot;\\\\|&quot;)).stream() .map(item -&gt; Level.valueOf(item)) .map(level -&gt; level.toInt()) .collect(Collectors.toList()); &#125; &#x2F;&#x2F;如果levelList包含当前日志的级别，则接收否则拒绝 if (levelList.contains(event.getLevel().toInt())) return FilterReply.ACCEPT; else return FilterReply.DENY; &#125; &#125; 然后，在配置文件中使用这个MultipleLevelsFilter就可以了（完整的配置代码参考这里）： &lt;filter class&#x3D;&quot;org.geekbang.time.commonmistakes.logging.duplicate.MultipleLevelsFilter&quot;&gt; &lt;levels&gt;INFO|WARN&lt;&#x2F;levels&gt; &lt;&#x2F;filter&gt; 问题2：生产级项目的文件日志肯定需要按时间和日期进行分割和归档处理，以避免单个文件太大，同时保留一定天数的历史日志，你知道如何配置吗？可以在官方文档找到答案。 答：参考配置如下，使用SizeAndTimeBasedRollingPolicy来实现按照文件大小和历史文件保留天数，进行文件分割和归档： &lt;rollingPolicy class&#x3D;&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;30&lt;&#x2F;MaxHistory&gt; &lt;!--日志文件最大的大小--&gt; &lt;MaxFileSize&gt;100MB&lt;&#x2F;MaxFileSize&gt; &lt;!--日志整体最大 可选的totalSizeCap属性控制所有归档文件的总大小。当超过总大小上限时，将异步删除最旧的存档。 totalSizeCap属性也需要设置maxHistory属性。此外，“最大历史”限制总是首先应用，“总大小上限”限制其次应用。 --&gt; &lt;totalSizeCap&gt;10GB&lt;&#x2F;totalSizeCap&gt; &lt;&#x2F;rollingPolicy&gt; 14 | 文件IO：实现高效正确的文件读写并非易事问题1：Files.lines方法进行流式处理，需要使用try-with-resources进行资源释放。那么，使用Files类中其他返回Stream包装对象的方法进行流式处理，比如newDirectoryStream方法返回DirectoryStream，list、walk和find方法返回Stream，也同样有资源释放问题吗？ 答：使用Files类中其他返回Stream包装对象的方法进行流式处理，也同样会有资源释放问题。 因为，这些接口都需要使用try-with-resources模式来释放。正如文中所说，如果不显式释放，那么可能因为底层资源没有及时关闭造成资源泄露。 问题2：Java的File类和Files类提供的文件复制、重命名、删除等操作，是原子性的吗？ 答：Java的File和Files类的文件复制、重命名、删除等操作，都不是原子性的。原因是，文件类操作基本都是调用操作系统本身的API，一般来说这些文件API并不像数据库有事务机制（也很难办到），即使有也很可能有平台差异性。 比如，File.renameTo方法的文档中提到： Many aspects of the behavior of this method are inherently platform-dependent: The rename operation might not be able to move a file from one filesystem to another, it might not be atomic, and it might not succeed if a file with the destination abstract pathname already exists. The return value should always be checked to make sure that the rename operation was successful. 又比如，Files.copy方法的文档中提到： Copying a file is not an atomic operation. If an IOException is thrown, then it is possible that the target file is incomplete or some of its file attributes have not been copied from the source file. When the REPLACE_EXISTING option is specified and the target file exists, then the target file is replaced. The check for the existence of the file and the creation of the new file may not be atomic with respect to other file system activities. 15 | 序列化：一来一回你还是原来的你吗？问题1：在讨论Redis序列化方式的时候，我们自定义了RedisTemplate，让Key使用String序列化、让Value使用JSON序列化，从而使Redis获得的Value可以直接转换为需要的对象类型。那么，使用RedisTemplate&lt;String, Long&gt;能否存取Value是Long的数据呢？这其中有什么坑吗？ 答：使用RedisTemplate&lt;String, Long&gt;，不一定能存取Value是Long的数据。在Integer区间内返回的是Integer，超过这个区间返回Long。测试代码如下： @GetMapping(&quot;wrong2&quot;) public void wrong2() &#123; String key &#x3D; &quot;testCounter&quot;; &#x2F;&#x2F;测试一下设置在Integer范围内的值 countRedisTemplate.opsForValue().set(key, 1L); log.info(&quot;&#123;&#125; &#123;&#125;&quot;, countRedisTemplate.opsForValue().get(key), countRedisTemplate.opsForValue().get(key) instanceof Long); Long l1 &#x3D; getLongFromRedis(key); &#x2F;&#x2F;测试一下设置超过Integer范围的值 countRedisTemplate.opsForValue().set(key, Integer.MAX_VALUE + 1L); log.info(&quot;&#123;&#125; &#123;&#125;&quot;, countRedisTemplate.opsForValue().get(key), countRedisTemplate.opsForValue().get(key) instanceof Long); &#x2F;&#x2F;使用getLongFromRedis转换后的值必定是Long Long l2 &#x3D; getLongFromRedis(key); log.info(&quot;&#123;&#125; &#123;&#125;&quot;, l1, l2); &#125; private Long getLongFromRedis(String key) &#123; Object o &#x3D; countRedisTemplate.opsForValue().get(key); if (o instanceof Integer) &#123; return ((Integer) o).longValue(); &#125; if (o instanceof Long) &#123; return (Long) o; &#125; return null; &#125; 会得到如下输出： 1 false 2147483648 true 1 2147483648 可以看到，值设置1的时候类型不是Long，设置2147483648的时候是Long。也就是使用RedisTemplate&lt;String, Long&gt;不一定就代表获取的到的Value是Long。 所以，这边我写了一个getLongFromRedis方法来做转换避免出错，判断当值是Integer的时候转换为Long。 问题2：你可以看一下Jackson2ObjectMapperBuilder类源码的实现（注意configure方法），分析一下其除了关闭FAIL_ON_UNKNOWN_PROPERTIES外，还做了什么吗？ 答：除了关闭FAIL_ON_UNKNOWN_PROPERTIES外，Jackson2ObjectMapperBuilder类源码还主要做了以下两方面的事儿。 第一，设置Jackson的一些默认值，比如： MapperFeature.DEFAULT_VIEW_INCLUSION设置为禁用； DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES设置为禁用。 第二，自动注册classpath中存在的一些jackson模块，比如： jackson-datatype-jdk8，支持JDK8的一些类型，比如Optional； jackson-datatype-jsr310， 支持JDK8的日期时间一些类型。 jackson-datatype-joda，支持Joda-Time类型。 jackson-module-kotlin，支持Kotlin。 16 | 用好Java 8的日期时间类，少踩一些“老三样”的坑问题1：在这一讲中，我多次强调了Date是一个时间戳，是UTC时间、没有时区概念。那，为什么调用其toString方法，会输出类似CST之类的时区字样呢？ 答：关于这个问题，参考toString中的相关源码，你可以看到会获取当前时区（取不到则显示GMT）进行格式化： public String toString() &#123; BaseCalendar.Date date &#x3D; normalize(); ... TimeZone zi &#x3D; date.getZone(); if (zi !&#x3D; null) &#123; sb.append(zi.getDisplayName(date.isDaylightTime(), TimeZone.SHORT, Locale.US)); &#x2F;&#x2F; zzz &#125; else &#123; sb.append(&quot;GMT&quot;); &#125; sb.append(&#39; &#39;).append(date.getYear()); &#x2F;&#x2F; yyyy return sb.toString(); &#125; private final BaseCalendar.Date normalize() &#123; if (cdate &#x3D;&#x3D; null) &#123; BaseCalendar cal &#x3D; getCalendarSystem(fastTime); cdate &#x3D; (BaseCalendar.Date) cal.getCalendarDate(fastTime, TimeZone.getDefaultRef()); return cdate; &#125; &#x2F;&#x2F; Normalize cdate with the TimeZone in cdate first. This is &#x2F;&#x2F; required for the compatible behavior. if (!cdate.isNormalized()) &#123; cdate &#x3D; normalize(cdate); &#125; &#x2F;&#x2F; If the default TimeZone has changed, then recalculate the &#x2F;&#x2F; fields with the new TimeZone. TimeZone tz &#x3D; TimeZone.getDefaultRef(); if (tz !&#x3D; cdate.getZone()) &#123; cdate.setZone(tz); CalendarSystem cal &#x3D; getCalendarSystem(cdate); cal.getCalendarDate(fastTime, cdate); &#125; return cdate; &#125; 其实说白了，这里显示的时区仅仅用于呈现，并不代表Date类内置了时区信息。 问题2：日期时间数据始终要保存到数据库中，MySQL中有两种数据类型datetime和timestamp可以用来保存日期时间。你能说说它们的区别吗，它们是否包含时区信息呢？ 答：datetime和timestamp的区别，主要体现在占用空间、表示的时间范围和时区三个方面。 占用空间：datetime占用8字节；timestamp占用4字节。 表示的时间范围：datetime表示的范围是从“1000-01-01 00:00:00.000000”到“9999-12-31 23:59:59.999999”；timestamp表示的范围是从“1970-01-01 00:00:01.000000”到“2038-01-19 03:14:07.999999”。 时区：timestamp保存的时候根据当前时区转换为UTC，查询的时候再根据当前时区从UTC转回来；而datetime就是一个死的字符串时间（仅仅对MySQL本身而言）表示。 需要注意的是，我们说datetime不包含时区是固定的时间表示，仅仅是指MySQL本身。使用timestamp，需要考虑Java进程的时区和MySQL连接的时区。而使用datetime类型，则只需要考虑Java进程的时区（因为MySQL datetime没有时区信息了，JDBC时间戳转换成MySQL datetime，会根据MySQL的serverTimezone做一次转换）。 如果你的项目有国际化需求，我推荐使用时间戳，并且要确保你的应用服务器和数据库服务器设置了正确的匹配当地时区的时区配置。 其实，即便你的项目没有国际化需求，至少是应用服务器和数据库服务器设置一致的时区，也是需要的。 17 | 别以为“自动挡”就不可能出现OOM问题1：Spring的ConcurrentReferenceHashMap，针对Key和Value支持软引用和弱引用两种方式。你觉得哪种方式更适合做缓存呢？ 答：软引用和弱引用的区别在于：若一个对象是弱引用可达，无论当前内存是否充足它都会被回收，而软引用可达的对象在内存不充足时才会被回收。因此，软引用要比弱引用“强”一些。 那么，使用弱引用作为缓存就会让缓存的生命周期过短，所以软引用更适合作为缓存。 问题2：当我们需要动态执行一些表达式时，可以使用Groovy动态语言实现：new出一个GroovyShell类，然后调用evaluate方法动态执行脚本。这种方式的问题是，会重复产生大量的类，增加Metaspace区的GC负担，有可能会引起OOM。你知道如何避免这个问题吗？ 答：调用evaluate方法动态执行脚本会产生大量的类，要避免可能因此导致的OOM问题，我们可以把脚本包装为一个函数，先调用parse函数来得到Script对象，然后缓存起来，以后直接使用invokeMethod方法调用这个函数即可： private Object rightGroovy(String script, String method, Object... args) &#123; Script scriptObject; if (SCRIPT_CACHE.containsKey(script)) &#123; &#x2F;&#x2F;如果脚本已经生成过Script则直接使用 scriptObject &#x3D; SCRIPT_CACHE.get(script); &#125; else &#123; &#x2F;&#x2F;否则把脚本解析为Script scriptObject &#x3D; shell.parse(script); SCRIPT_CACHE.put(script, scriptObject); &#125; return scriptObject.invokeMethod(method, args); &#125; 我在源码中提供了一个测试程序，你可以直接去看一下。 18 | 当反射、注解和泛型遇到OOP时，会有哪些坑？问题1：泛型类型擦除后会生成一个bridge方法，这个方法同时又是synthetic方法。除了泛型类型擦除，你知道还有什么情况编译器会生成synthetic方法吗？ 答：Synthetic方法是编译器自动生成的方法（在源码中不出现）。除了文中提到的泛型类型擦除外，Synthetic方法还可能出现的一个比较常见的场景，是内部类和顶层类需要相互访问对方的private字段或方法的时候。 编译后的内部类和普通类没有区别，遵循private字段或方法对外部类不可见的原则，但语法上内部类和顶层类的私有字段需要可以相互访问。为了解决这个矛盾，编译器就只能生成桥接方法，也就是Synthetic方法，来把private成员转换为package级别的访问限制。 比如如下代码，InnerClassApplication类的test方法需要访问内部类MyInnerClass类的私有字段name，而内部类MyInnerClass类的test方法需要访问外部类InnerClassApplication类的私有字段gender。 public class InnerClassApplication &#123; private String gender &#x3D; &quot;male&quot;; public static void main(String[] args) throws Exception &#123; InnerClassApplication application &#x3D; new InnerClassApplication(); application.test(); &#125; private void test()&#123; MyInnerClass myInnerClass &#x3D; new MyInnerClass(); System.out.println(myInnerClass.name); myInnerClass.test(); &#125; class MyInnerClass &#123; private String name &#x3D; &quot;zhuye&quot;; void test()&#123; System.out.println(gender); &#125; &#125; &#125; 编译器会为InnerClassApplication和MyInnerClass都生成桥接方法。 如下图所示，InnerClassApplication的test方法，其实调用的是内部类的access$000静态方法： 这个access$000方法是Synthetic方法： 而Synthetic方法的实现转接调用了内部类的name字段： 反过来，内部类的test方法也是通过外部类InnerClassApplication类的桥接方法access$100调用到其私有字段： 问题2：关于注解继承问题，你觉得Spring的常用注解@Service、@Controller是否支持继承呢？ 答：Spring的常用注解@Service、@Controller，不支持继承。这些注解只支持放到具体的（非接口非抽象）顶层类上（来让它们成为Bean），如果支持继承会非常不灵活而且容易出错。 19 | Spring框架：IoC和AOP是扩展的核心问题1：除了通过@Autowired注入Bean外，还可以使用@Inject或@Resource来注入Bean。你知道这三种方式的区别是什么吗？ 答：我们先说一下使用@Autowired、@Inject和@Resource这三种注解注入Bean的方式： @Autowired，是Spring的注解，优先按照类型注入。当无法确定具体注入类型的时候，可以通过@Qualifier注解指定Bean名称。 @Inject：是JSR330规范的实现，也是根据类型进行自动装配的，这一点和@Autowired类似。如果需要按名称进行装配，则需要配合使用@Named。@Autowired和@Inject的区别在于，前者可以使用required&#x3D;false允许注入null，后者允许注入一个Provider实现延迟注入。 @Resource：JSR250规范的实现，如果不指定name优先根据名称进行匹配（然后才是类型），如果指定name则仅根据名称匹配。 问题2：当Bean产生循环依赖时，比如BeanA的构造方法依赖BeanB作为成员需要注入，BeanB也依赖BeanA，你觉得会出现什么问题呢？又有哪些解决方式呢？ 答：Bean产生循环依赖，主要包括两种情况：一种是注入属性或字段涉及循环依赖，另一种是构造方法注入涉及循环依赖。接下来，我分别和你讲一讲。 第一种，注入属性或字段涉及循环依赖，比如TestA和TestB相互依赖： @Component public class TestA &#123; @Autowired @Getter private TestB testB; &#125; @Component public class TestB &#123; @Autowired @Getter private TestA testA; &#125; 针对这个问题，Spring内部通过三个Map的方式解决了这个问题，不会出错。基本原理是，因为循环依赖，所以实例的初始化无法一次到位，需要分步进行： 创建A（仅仅实例化，不注入依赖）； 创建B（仅仅实例化，不注入依赖）； 为B注入A（此时B已健全）； 为A注入B（此时A也健全）。 网上有很多相关的分析，我找了一篇比较详细的，可供你参考。 第二种，构造方法注入涉及循环依赖。遇到这种情况的话，程序无法启动，比如TestC和TestD的相互依赖： @Component public class TestC &#123; @Getter private TestD testD; @Autowired public TestC(TestD testD) &#123; this.testD &#x3D; testD; &#125; &#125; @Component public class TestD &#123; @Getter private TestC testC; @Autowired public TestD(TestC testC) &#123; this.testC &#x3D; testC; &#125; &#125; 这种循环依赖的主要解决方式，有2种： 改为属性或字段注入； 使用@Lazy延迟注入。比如如下代码： @Component public class TestC &#123; @Getter private TestD testD; @Autowired public TestC(@Lazy TestD testD) &#123; this.testD &#x3D; testD; &#125; &#125; 其实，这种@Lazy方式注入的就不是实际的类型了，而是代理类，获取的时候通过代理去拿值（实例化）。所以，它可以解决循环依赖无法实例化的问题。 20 | Spring框架：框架帮我们做了很多工作也带来了复杂度问题1：除了Spring框架这两讲涉及的execution、within、@within、@annotation 四个指示器外，Spring AOP 还支持 this、target、args、@target、@args。你能说说后面五种指示器的作用吗？ 答：关于这些指示器的作用，你可以参考官方文档，文档里已经写的很清晰。 总结一下，按照使用场景，建议使用下面这些指示器： 针对方法签名，使用execution； 针对类型匹配，使用within（匹配类型）、this（匹配代理类实例）、target（匹配代理背后的目标类实例）、args（匹配参数）； 针对注解匹配，使用@annotation（使用指定注解标注的方法）、@target（使用指定注解标注的类）、@args（使用指定注解标注的类作为某个方法的参数）。 你可能会问，@within怎么没有呢？ 其实，对于Spring默认的基于动态代理或CGLIB的AOP，因为切点只能是方法，使用@within和@target指示器并无区别；但需要注意如果切换到AspectJ，那么使用@within和@target这两个指示器的行为就会有所区别了，@within会切入更多的成员的访问（比如静态构造方法、字段访问），一般而言使用@target指示器即可。 问题2：Spring 的 Environment 中的 PropertySources 属性可以包含多个 PropertySource，越往前优先级越高。那，我们能否利用这个特点实现配置文件中属性值的自动赋值呢？比如，我们可以定义 %%MYSQL.URL%%、%%MYSQL.USERNAME%% 和 %%MYSQL.PASSWORD%%，分别代表数据库连接字符串、用户名和密码。在配置数据源时，我们只要设置其值为占位符，框架就可以自动根据当前应用程序名 application.name，统一把占位符替换为真实的数据库信息。这样，生产的数据库信息就不需要放在配置文件中了，会更安全。 答：我们利用PropertySource具有优先级的特点，实现配置文件中属性值的自动赋值。主要逻辑是，遍历现在的属性值，找出能匹配到占位符的属性，并把这些属性的值替换为实际的数据库信息，然后再把这些替换后的属性值构成新的PropertiesPropertySource，加入PropertySources的第一个。这样，我们这个PropertiesPropertySource中的值就可以生效了。 主要源码如下： public static void main(String[] args) &#123; Utils.loadPropertySource(CommonMistakesApplication.class, &quot;db.properties&quot;); new SpringApplicationBuilder() .sources(CommonMistakesApplication.class) .initializers(context -&gt; initDbUrl(context.getEnvironment())) .run(args); &#125; private static final String MYSQL_URL_PLACEHOLDER &#x3D; &quot;%%MYSQL.URL%%&quot;; private static final String MYSQL_USERNAME_PLACEHOLDER &#x3D; &quot;%%MYSQL.USERNAME%%&quot;; private static final String MYSQL_PASSWORD_PLACEHOLDER &#x3D; &quot;%%MYSQL.PASSWORD%%&quot;; private static void initDbUrl(ConfigurableEnvironment env) &#123; String dataSourceUrl &#x3D; env.getProperty(&quot;spring.datasource.url&quot;); String username &#x3D; env.getProperty(&quot;spring.datasource.username&quot;); String password &#x3D; env.getProperty(&quot;spring.datasource.password&quot;); if (dataSourceUrl !&#x3D; null &amp;&amp; !dataSourceUrl.contains(MYSQL_URL_PLACEHOLDER)) throw new IllegalArgumentException(&quot;请使用占位符&quot; + MYSQL_URL_PLACEHOLDER + &quot;来替换数据库URL配置！&quot;); if (username !&#x3D; null &amp;&amp; !username.contains(MYSQL_USERNAME_PLACEHOLDER)) throw new IllegalArgumentException(&quot;请使用占位符&quot; + MYSQL_USERNAME_PLACEHOLDER + &quot;来替换数据库账号配置！&quot;); if (password !&#x3D; null &amp;&amp; !password.contains(MYSQL_PASSWORD_PLACEHOLDER)) throw new IllegalArgumentException(&quot;请使用占位符&quot; + MYSQL_PASSWORD_PLACEHOLDER + &quot;来替换数据库密码配置！&quot;); &#x2F;&#x2F;这里我把值写死了，实际应用中可以从外部服务来获取 Map&lt;String, String&gt; property &#x3D; new HashMap&lt;&gt;(); property.put(MYSQL_URL_PLACEHOLDER, &quot;jdbc:mysql:&#x2F;&#x2F;localhost:6657&#x2F;common_mistakes?characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&quot;); property.put(MYSQL_USERNAME_PLACEHOLDER, &quot;root&quot;); property.put(MYSQL_PASSWORD_PLACEHOLDER, &quot;kIo9u7Oi0eg&quot;); &#x2F;&#x2F;保存修改后的配置属性 Properties modifiedProps &#x3D; new Properties(); &#x2F;&#x2F;遍历现在的属性值，找出能匹配到占位符的属性，并把这些属性的值替换为实际的数据库信息 StreamSupport.stream(env.getPropertySources().spliterator(), false) .filter(ps -&gt; ps instanceof EnumerablePropertySource) .map(ps -&gt; ((EnumerablePropertySource) ps).getPropertyNames()) .flatMap(Arrays::stream) .forEach(propKey -&gt; &#123; String propValue &#x3D; env.getProperty(propKey); property.entrySet().forEach(item -&gt; &#123; &#x2F;&#x2F;如果原先配置的属性值包含我们定义的占位符 if (propValue.contains(item.getKey())) &#123; &#x2F;&#x2F;那么就把实际的配置信息加入modifiedProps modifiedProps.put(propKey, propValue.replaceAll(item.getKey(), item.getValue())); &#125; &#125;); &#125;); if (!modifiedProps.isEmpty()) &#123; log.info(&quot;modifiedProps: &#123;&#125;&quot;, modifiedProps); env.getPropertySources().addFirst(new PropertiesPropertySource(&quot;mysql&quot;, modifiedProps)); &#125; &#125; 我在GitHub上第20讲对应的源码中更新了我的实现，你可以点击这里查看。有一些同学会问，这么做的意义到底在于什么，为何不直接使用类似Apollo这样的配置框架呢？ 其实，我们的目的就是不希望让开发人员手动配置数据库信息，希望程序启动的时候自动替换占位符实现自动配置（从CMDB直接拿着应用程序ID来换取对应的数据库信息。你可能会问了，一个应用程序ID对应多个数据库怎么办？其实，一般对于微服务系统来说，一个应用就应该对应一个数据库）。这样一来，除了程序其他人都不会接触到生产的数据库信息，会更安全。 以上，就是咱们这门课的第13~20讲的思考题答案了。 关于这些题目，以及背后涉及的知识点，如果你还有哪里感觉不清楚的，欢迎在评论区与我留言，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/答疑篇：代码篇思考题集锦（一）","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/答疑篇：代码篇思考题集锦（一）/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/%E7%AD%94%E7%96%91%E7%AF%87%EF%BC%9A%E4%BB%A3%E7%A0%81%E7%AF%87%E6%80%9D%E8%80%83%E9%A2%98%E9%9B%86%E9%94%A6%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"答疑篇：代码篇思考题集锦（一）作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 在回复《Java 业务开发常见错误100例》这门课留言的过程中，我看到有些同学特别想看一看咱们这个课程所有思考题的答案。因此呢，我特地将这个课程涉及的思考题进行了梳理，把其中的67个问题的答案或者说解题思路，详细地写了出来，并整理成了一个“答疑篇”模块。 我把这些问题拆分为了6篇分别更新，你可以根据自己的时间来学习，以保证学习效果。你可以通过这些回答，再来回顾下这些知识点，以求温故而知新；同时，你也可以对照着我的回答，对比下自己的解题思路，看看有没有什么不一样的地方，并留言给我。 今天是答疑篇的第一讲，我们一起来分析下咱们这门课前6讲的课后思考题。这些题目涉及了并发工具、代码加锁、线程池、连接池、HTTP调用和Spring声明式事务的12道思考题。 接下来，我们就一一具体分析吧。 01 | 使用了并发工具类库，线程安全就高枕无忧了吗？问题1：ThreadLocalRandom是Java 7引入的一个生成随机数的类。你觉得可以把它的实例设置到静态变量中，在多线程情况下重用吗？ 答：不能。 ThreadLocalRandom文档里有这么一条： Usages of this class should typically be of the form: ThreadLocalRandom.current().nextX(…) (where X is Int, Long, etc). When all usages are of this form, it is never possible to accidently share a ThreadLocalRandom across multiple threads. 那为什么规定要ThreadLocalRandom.current().nextX(…)这样来使用呢？我来分析下原因吧。 current()的时候初始化一个初始化种子到线程，每次nextseed再使用之前的种子生成新的种子： UNSAFE.putLong(t &#x3D; Thread.currentThread(), SEED, r &#x3D; UNSAFE.getLong(t, SEED) + GAMMA); 如果你通过主线程调用一次current生成一个ThreadLocalRandom的实例保存起来，那么其它线程来获取种子的时候必然取不到初始种子，必须是每一个线程自己用的时候初始化一个种子到线程。你可以在nextSeed方法设置一个断点来测试： UNSAFE.getLong(Thread.currentThread(),SEED); 问题2：ConcurrentHashMap还提供了putIfAbsent方法，你能否通过查阅JDK文档，说说computeIfAbsent和putIfAbsent方法的区别？ 答：computeIfAbsent和putIfAbsent这两个方法，都是判断值不存在的时候为Map进行赋值的原子方法，它们的区别具体包括以下三个方面： 当Key存在的时候，如果Value的获取比较昂贵的话，putIfAbsent方法就会白白浪费时间在获取这个昂贵的Value上（这个点特别注意），而computeIfAbsent则会因为传入的是Lambda表达式而不是实际值不会有这个问题。 Key不存在的时候，putIfAbsent会返回null，这时候我们要小心空指针；而computeIfAbsent会返回计算后的值，不存在空指针的问题。 当Key不存在的时候，putIfAbsent允许put null进去，而computeIfAbsent不能（当然了，此条针对HashMap，ConcurrentHashMap不允许put null value进去）。 我写了一段代码来证明这三点，你可以点击这里的GitHub链接查看。 02 | 代码加锁：不要让“锁”事成为烦心事问题1：在这一讲开头的例子里，我们为变量a、b都使用了volatile关键字进行修饰，你知道volatile关键字的作用吗？我之前遇到过这样一个坑：我们开启了一个线程无限循环来跑一些任务，有一个bool类型的变量来控制循环的退出，默认为true代表执行，一段时间后主线程将这个变量设置为了false。如果这个变量不是volatile修饰的，子线程可以退出吗？你能否解释其中的原因呢？ 答：不能退出。比如下面的代码，3秒后另一个线程把b设置为false，但是主线程无法退出： private static boolean b &#x3D; true; public static void main(String[] args) throws InterruptedException &#123; new Thread(()-&gt;&#123; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; &#125; b &#x3D;false; &#125;).start(); while (b) &#123; TimeUnit.MILLISECONDS.sleep(0); &#125; System.out.println(&quot;done&quot;); &#125; 其实，这是可见性的问题。 虽然另一个线程把b设置为了false，但是这个字段在CPU缓存中，另一个线程（主线程）还是读不到最新的值。使用volatile关键字，可以让数据刷新到主内存中去。准确来说，让数据刷新到主内存中去是两件事情： 将当前处理器缓存行的数据，写回到系统内存； 这个写回内存的操作会导致其他CPU里缓存了该内存地址的数据变为无效。 当然，使用AtomicBoolean等关键字来修改变量b也行。但相比volatile来说，AtomicBoolean等关键字除了确保可见性，还提供了CAS方法，具有更多的功能，在本例的场景中用不到。 问题2：关于代码加锁还有两个坑，一是加锁和释放没有配对的问题，二是分布式锁自动释放导致的重复逻辑执行的问题。你有什么方法来发现和解决这两个问题吗？ 答：针对加解锁没有配对的问题，我们可以用一些代码质量工具或代码扫描工具（比如Sonar）来帮助排查。这个问题在编码阶段就能发现。 针对分布式锁超时自动释放问题，可以参考Redisson的RedissonLock的锁续期机制。锁续期是每次续一段时间，比如30秒，然后10秒执行一次续期。虽然是无限次续期，但即使客户端崩溃了也没关系，不会无限期占用锁，因为崩溃后无法自动续期自然最终会超时。 03 | 线程池：业务代码最常用也最容易犯错的组件问题1：在讲线程池的管理策略时我们提到，或许一个激进创建线程的弹性线程池更符合我们的需求，你能给出相关的实现吗？实现后再测试一下，是否所有的任务都可以正常处理完成呢？ 答：我们按照文中提到的两个思路来实现一下激进线程池： 由于线程池在工作队列满了无法入队的情况下会扩容线程池，那么我们可以重写队列的 offer 方法，造成这个队列已满的假象； 由于我们 Hack 了队列，在达到了最大线程后势必会触发拒绝策略，那么我们还需要实现一个自定义的拒绝策略处理程序，这个时候再把任务真正插入队列。 完整的实现代码以及相应的测试代码如下： @GetMapping(&quot;better&quot;) public int better() throws InterruptedException &#123; &#x2F;&#x2F;这里开始是激进线程池的实现 BlockingQueue&lt;Runnable&gt; queue &#x3D; new LinkedBlockingQueue&lt;Runnable&gt;(10) &#123; @Override public boolean offer(Runnable e) &#123; &#x2F;&#x2F;先返回false，造成队列满的假象，让线程池优先扩容 return false; &#125; &#125;; ThreadPoolExecutor threadPool &#x3D; new ThreadPoolExecutor( 2, 5, 5, TimeUnit.SECONDS, queue, new ThreadFactoryBuilder().setNameFormat(&quot;demo-threadpool-%d&quot;).get(), (r, executor) -&gt; &#123; try &#123; &#x2F;&#x2F;等出现拒绝后再加入队列 &#x2F;&#x2F;如果希望队列满了阻塞线程而不是抛出异常，那么可以注释掉下面三行代码，修改为executor.getQueue().put(r); if (!executor.getQueue().offer(r, 0, TimeUnit.SECONDS)) &#123; throw new RejectedExecutionException(&quot;ThreadPool queue full, failed to offer &quot; + r.toString()); &#125; &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); &#125; &#125;); &#x2F;&#x2F;激进线程池实现结束 printStats(threadPool); &#x2F;&#x2F;每秒提交一个任务，每个任务耗时10秒执行完成，一共提交20个任务 &#x2F;&#x2F;任务编号计数器 AtomicInteger atomicInteger &#x3D; new AtomicInteger(); IntStream.rangeClosed(1, 20).forEach(i -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; int id &#x3D; atomicInteger.incrementAndGet(); try &#123; threadPool.submit(() -&gt; &#123; log.info(&quot;&#123;&#125; started&quot;, id); try &#123; TimeUnit.SECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; &#125; log.info(&quot;&#123;&#125; finished&quot;, id); &#125;); &#125; catch (Exception ex) &#123; log.error(&quot;error submitting task &#123;&#125;&quot;, id, ex); atomicInteger.decrementAndGet(); &#125; &#125;); TimeUnit.SECONDS.sleep(60); return atomicInteger.intValue(); &#125; 使用这个激进的线程池可以处理完这20个任务，因为我们优先开启了更多线程来处理任务。 [10:57:16.092] [demo-threadpool-4] [INFO ] [o.g.t.c.t.t.ThreadPoolOOMController:157 ] - 20 finished [10:57:17.062] [pool-8-thread-1] [INFO ] [o.g.t.c.t.t.ThreadPoolOOMController:22 ] - &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [10:57:17.062] [pool-8-thread-1] [INFO ] [o.g.t.c.t.t.ThreadPoolOOMController:23 ] - Pool Size: 5 [10:57:17.062] [pool-8-thread-1] [INFO ] [o.g.t.c.t.t.ThreadPoolOOMController:24 ] - Active Threads: 0 [10:57:17.062] [pool-8-thread-1] [INFO ] [o.g.t.c.t.t.ThreadPoolOOMController:25 ] - Number of Tasks Completed: 20 [10:57:17.062] [pool-8-thread-1] [INFO ] [o.g.t.c.t.t.ThreadPoolOOMController:26 ] - Number of Tasks in Queue: 0 [10:57:17.062] [pool-8-thread-1] [INFO ] [o.g.t.c.t.t.ThreadPoolOOMController:28 ] - &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 问题2：在讲“务必确认清楚线程池本身是不是复用”时，我们改进了ThreadPoolHelper使其能够返回复用的线程池。如果我们不小心每次都创建了这样一个自定义的线程池（10核心线程，50最大线程，2秒回收的），反复执行测试接口线程，最终可以被回收吗？会出现OOM问题吗？ 答：会因为创建过多线程导致OOM，因为默认情况下核心线程不会回收，并且ThreadPoolExecutor也回收不了。 我们可以看看它的源码，工作线程Worker是内部类，只要它活着，换句话说就是线程在跑，就会阻止ThreadPoolExecutor回收： public class ThreadPoolExecutor extends AbstractExecutorService &#123; private final class Worker extends AbstractQueuedSynchronizer implements Runnable &#123; &#125; &#125; 因此，我们不能认为ThreadPoolExecutor没有引用，就能回收。 04 | 连接池：别让连接池帮了倒忙问题1：有了连接池之后，获取连接是从连接池获取，没有足够连接时连接池会创建连接。这时，获取连接操作往往有两个超时时间：一个是从连接池获取连接的最长等待时间，通常叫作请求连接超时connectRequestTimeout，或连接等待超时connectWaitTimeout；一个是连接池新建TCP连接三次握手的连接超时，通常叫作连接超时connectTimeout。针对JedisPool、Apache HttpClient和Hikari数据库连接池，你知道如何设置这2个参数吗？ 答：假设我们希望设置连接超时5s、请求连接超时10s，下面我来演示下，如何配置Hikari、Jedis和HttpClient的两个超时参数。 针对Hikari，设置两个超时时间的方式，是修改数据库连接字符串中的connectTimeout属性和配置文件中的hikari配置的connection-timeout： spring.datasource.hikari.connection-timeout&#x3D;10000 spring.datasource.url&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:6657&#x2F;common_mistakes?connectTimeout&#x3D;5000&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;rewriteBatchedStatements&#x3D;true 针对Jedis，是设置JedisPoolConfig的MaxWaitMillis属性和设置创建JedisPool时的timeout属性： JedisPoolConfig config &#x3D; new JedisPoolConfig(); config.setMaxWaitMillis(10000); try (JedisPool jedisPool &#x3D; new JedisPool(config, &quot;127.0.0.1&quot;, 6379, 5000); Jedis jedis &#x3D; jedisPool.getResource()) &#123; return jedis.set(&quot;test&quot;, &quot;test&quot;); &#125; 针对HttpClient，是设置RequestConfig的ConnectionRequestTimeout和ConnectTimeout属性： RequestConfig requestConfig &#x3D; RequestConfig.custom() .setConnectTimeout(5000) .setConnectionRequestTimeout(10000) .build(); HttpGet httpGet &#x3D; new HttpGet(&quot;http:&#x2F;&#x2F;127.0.0.1:45678&#x2F;twotimeoutconfig&#x2F;test&quot;); httpGet.setConfig(requestConfig); try (CloseableHttpResponse response &#x3D; httpClient.execute(httpGet)) &#123; return EntityUtils.toString(response.getEntity()); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; return null; 也可以直接参考我放在GitHub上的源码。 问题2：对于带有连接池的SDK的使用姿势，最主要的是鉴别其内部是否实现了连接池，如果实现了连接池要尽量复用Client。对于NoSQL中的MongoDB来说，使用MongoDB Java驱动时，MongoClient类应该是每次都创建还是复用呢？你能否在官方文档中找到答案呢？ 答：官方文档里有这么一段话： Typically you only create one MongoClient instance for a given MongoDB deployment (e.g. standalone, replica set, or a sharded cluster) and use it across your application. However, if you do create multiple instances: All resource usage limits (e.g. max connections, etc.) apply per MongoClient instance. To dispose of an instance, call MongoClient.close() to clean up resources. MongoClient类应该尽可能复用（一个MongoDB部署只使用一个MongoClient），不过复用不等于在任何情况下就只用一个。正如文档里所说，每一个MongoClient示例有自己独立的资源限制。 05 | HTTP调用：你考虑到超时、重试、并发了吗？问题1：在“配置连接超时和读取超时参数的学问”这一节中，我们强调了要注意连接超时和读取超时参数的配置，大多数的HTTP客户端也都有这两个参数。有读就有写，但为什么我们很少看到“写入超时”的概念呢？ 答：其实写入操作只是将数据写入TCP的发送缓冲区，已经发送到网络的数据依然需要暂存在发送缓冲区中，只有收到对方的ack后，操作系统内核才从缓冲区中清除这一部分数据，为后续发送数据腾出空间。 如果接收端从socket读取数据的速度太慢，可能会导致发送端发送缓冲区满，导致写入操作阻塞，产生写入超时。但是，因为有滑动窗口的控制，通常不太容易发生发送缓冲区满导致写入超时的情况。相反，读取超时包含了服务端处理数据执行业务逻辑的时间，所以读取超时是比较容易发生的。 这也就是为什么我们一般都会比较重视读取超时而不是写入超时的原因了。 问题2：除了Ribbon的AutoRetriesNextServer重试机制，Nginx也有类似的重试功能。你了解Nginx相关的配置吗？ 答：关于Nginx的重试功能，你可以参考这里，了解下Nginx的proxy_next_upstream配置。 proxy_next_upstream，用于指定在什么情况下Nginx会将请求转移到其他服务器上。其默认值是proxy_next_upstream error timeout，即发生网络错误以及超时，才会重试其他服务器。也就是说，默认情况下，服务返回500状态码是不会重试的。 如果我们想在请求返回500状态码时也进行重试，可以配置： proxy_next_upstream error timeout http_500; 需要注意的是，proxy_next_upstream配置中有一个选项non_idempotent，一定要小心开启。通常情况下，如果请求使用非等幂方法（POST、PATCH），请求失败后不会再到其他服务器进行重试。但是，加上non_idempotent这个选项后，即使是非幂等请求类型（例如POST请求），发生错误后也会重试。 06 | 20%的业务代码的Spring声明式事务，可能都没处理正确问题1：考虑到Demo的简洁，这一讲中所有数据访问使用的都是Spring Data JPA。国内大多数互联网业务项目是使用MyBatis进行数据访问的，使用MyBatis配合Spring的声明式事务也同样需要注意这一讲中提到的这些点。你可以尝试把今天的Demo改为MyBatis做数据访问实现，看看日志中是否可以体现出这些坑？ 答：使用mybatis-spring-boot-starter无需做任何配置，即可使MyBatis整合Spring的声明式事务。在GitHub上的课程源码中，我更新了一个使用MyBatis配套嵌套事务的例子，实现的效果是主方法出现异常，子方法的嵌套事务也会回滚。 我来和你解释下这个例子中的核心代码： @Transactional public void createUser(String name) &#123; createMainUser(name); try &#123; subUserService.createSubUser(name); &#125; catch (Exception ex) &#123; log.error(&quot;create sub user error:&#123;&#125;&quot;, ex.getMessage()); &#125; &#x2F;&#x2F;如果createSubUser是NESTED模式，这里抛出异常会导致嵌套事务无法“提交” throw new RuntimeException(&quot;create main user error&quot;); &#125; 子方法使用了NESTED事务传播模式： @Transactional(propagation &#x3D; Propagation.NESTED) public void createSubUser(String name) &#123; userDataMapper.insert(name, &quot;sub&quot;); &#125; 执行日志如下图所示： 每个NESTED事务执行前，会将当前操作保存下来，叫做savepoint（保存点）。NESTED事务在外部事务提交以后自己才会提交，如果当前NESTED事务执行失败，则回滚到之前的保存点。 问题2：在讲“小心 Spring 的事务可能没有生效”时我们提到，如果要针对private方法启用事务，动态代理方式的AOP不可行，需要使用静态织入方式的AOP，也就是在编译期间织入事务增强代码，可以配置Spring框架使用AspectJ来实现AOP。你能否参阅Spring的文档“Using @Transactional with AspectJ”试试呢？注意：AspectJ配合lombok使用，还可能会踩一些坑。 答：我们需要加入aspectj的依赖和配置aspectj-maven-plugin插件，并且需要设置Spring开启AspectJ事务管理模式。具体的实现方式，包括如下4步。 第一步，引入spring-aspects依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-aspects&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 第二步，加入lombok和aspectj插件： &lt;plugin&gt; &lt;groupId&gt;org.projectlombok&lt;&#x2F;groupId&gt; &lt;artifactId&gt;lombok-maven-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;1.18.0.0&lt;&#x2F;version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;generate-sources&lt;&#x2F;phase&gt; &lt;goals&gt; &lt;goal&gt;delombok&lt;&#x2F;goal&gt; &lt;&#x2F;goals&gt; &lt;&#x2F;execution&gt; &lt;&#x2F;executions&gt; &lt;configuration&gt; &lt;addOutputDirectory&gt;false&lt;&#x2F;addOutputDirectory&gt; &lt;sourceDirectory&gt;src&#x2F;main&#x2F;java&lt;&#x2F;sourceDirectory&gt; &lt;&#x2F;configuration&gt; &lt;&#x2F;plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;&#x2F;groupId&gt; &lt;artifactId&gt;aspectj-maven-plugin&lt;&#x2F;artifactId&gt; &lt;version&gt;1.10&lt;&#x2F;version&gt; &lt;configuration&gt; &lt;complianceLevel&gt;1.8&lt;&#x2F;complianceLevel&gt; &lt;source&gt;1.8&lt;&#x2F;source&gt; &lt;aspectLibraries&gt; &lt;aspectLibrary&gt; &lt;groupId&gt;org.springframework&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-aspects&lt;&#x2F;artifactId&gt; &lt;&#x2F;aspectLibrary&gt; &lt;&#x2F;aspectLibraries&gt; &lt;&#x2F;configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;&#x2F;goal&gt; &lt;goal&gt;test-compile&lt;&#x2F;goal&gt; &lt;&#x2F;goals&gt; &lt;&#x2F;execution&gt; &lt;&#x2F;executions&gt; &lt;&#x2F;plugin&gt; 使用delombok插件的目的是，把代码中的Lombok注解先编译为代码，这样AspectJ编译不会有问题，同时需要设置中的sourceDirectory为delombok目录： &lt;sourceDirectory&gt;$&#123;project.build.directory&#125;&#x2F;generated-sources&#x2F;delombok&lt;&#x2F;sourceDirectory&gt; 第三步，设置@EnableTransactionManagement注解，开启事务管理走AspectJ模式： @SpringBootApplication @EnableTransactionManagement(mode &#x3D; AdviceMode.ASPECTJ) public class CommonMistakesApplication &#123; 第四步，使用Maven编译项目，编译后查看createUserPrivate方法的源码，可以发现AspectJ帮我们做编译时织入（Compile Time Weaving）： 运行程序，观察日志可以发现createUserPrivate（私有）方法同样应用了事务，出异常后事务回滚： [14:21:39.155] [http-nio-45678-exec-2] [DEBUG] [o.s.orm.jpa.JpaTransactionManager:370 ] - Creating new transaction with name [org.geekbang.time.commonmistakes.transaction.transactionproxyfailed.UserService.createUserPrivate]: PROPAGATION_REQUIRED,ISOLATION_DEFAULT [14:21:39.155] [http-nio-45678-exec-2] [DEBUG] [o.s.orm.jpa.JpaTransactionManager:393 ] - Opened new EntityManager [SessionImpl(1087443072&lt;open&gt;)] for JPA transaction [14:21:39.158] [http-nio-45678-exec-2] [DEBUG] [o.s.orm.jpa.JpaTransactionManager:421 ] - Exposing JPA transaction as JDBC [org.springframework.orm.jpa.vendor.HibernateJpaDialect$HibernateConnectionHandle@4e16e6ea] [14:21:39.159] [http-nio-45678-exec-2] [DEBUG] [o.s.orm.jpa.JpaTransactionManager:356 ] - Found thread-bound EntityManager [SessionImpl(1087443072&lt;open&gt;)] for JPA transaction [14:21:39.159] [http-nio-45678-exec-2] [DEBUG] [o.s.orm.jpa.JpaTransactionManager:471 ] - Participating in existing transaction [14:21:39.173] [http-nio-45678-exec-2] [DEBUG] [o.s.orm.jpa.JpaTransactionManager:834 ] - Initiating transaction rollback [14:21:39.173] [http-nio-45678-exec-2] [DEBUG] [o.s.orm.jpa.JpaTransactionManager:555 ] - Rolling back JPA transaction on EntityManager [SessionImpl(1087443072&lt;open&gt;)] [14:21:39.176] [http-nio-45678-exec-2] [DEBUG] [o.s.orm.jpa.JpaTransactionManager:620 ] - Closing JPA EntityManager [SessionImpl(1087443072&lt;open&gt;)] after transaction [14:21:39.176] [http-nio-45678-exec-2] [ERROR] [o.g.t.c.t.t.UserService:28 ] - create user failed because invalid username! [14:21:39.177] [http-nio-45678-exec-2] [DEBUG] [o.s.o.j.SharedEntityManagerCreator$SharedEntityManagerInvocationHandler:305 ] - Creating new EntityManager for shared EntityManager invocation 以上，就是咱们这门课前6讲的思考题答案了。 关于这些题目，以及背后涉及的知识点，如果你还有哪里感觉不清楚的，欢迎在评论区与我留言，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/答疑篇：代码篇思考题集锦（二）","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/答疑篇：代码篇思考题集锦（二）/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/%E7%AD%94%E7%96%91%E7%AF%87%EF%BC%9A%E4%BB%A3%E7%A0%81%E7%AF%87%E6%80%9D%E8%80%83%E9%A2%98%E9%9B%86%E9%94%A6%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"答疑篇：代码篇思考题集锦（二）作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 今天，我们继续一起分析这门课第7~12讲的课后思考题。这些题目涉及了数据库索引、判等问题、数值计算、集合类、空值处理和异常处理的12道问题。 接下来，我们就一一具体分析吧。 07 | 数据库索引：索引并不是万能药问题1：在介绍二级索引代价时，我们通过EXPLAIN命令看到了索引覆盖和回表的两种情况。你能用optimizer trace来分析一下这两种情况的成本差异吗？ 答：如下代码所示，打开optimizer_trace后，再执行SQL就可以查询information_schema.OPTIMIZER_TRACE表查看执行计划了，最后可以关闭optimizer_trace功能： SET optimizer_trace&#x3D;&quot;enabled&#x3D;on&quot;; SELECT * FROM person WHERE NAME &gt;&#39;name84059&#39; AND create_time&gt;&#39;2020-01-24 05:00:00&#39;; SELECT * FROM information_schema.OPTIMIZER_TRACE; SET optimizer_trace&#x3D;&quot;enabled&#x3D;off&quot;; 假设我们为表person的NAME和SCORE列建了联合索引，那么下面第二条语句应该可以走索引覆盖，而第一条语句需要回表： explain select * from person where NAME&#x3D;&#39;name1&#39;; explain select NAME,SCORE from person where NAME&#x3D;&#39;name1&#39;; 通过观察OPTIMIZER_TRACE的输出可以看到，索引覆盖（index_only&#x3D;true）的成本是1.21而回表查询（index_only&#x3D;false）的是2.21，也就是索引覆盖节省了回表的成本1。 索引覆盖： analyzing_range_alternatives&quot;: &#123; &quot;range_scan_alternatives&quot;: [ &#123; &quot;index&quot;: &quot;name_score&quot;, &quot;ranges&quot;: [ &quot;name1 &lt;&#x3D; name &lt;&#x3D; name1&quot; ] &#x2F;* ranges *&#x2F;, &quot;index_dives_for_eq_ranges&quot;: true, &quot;rowid_ordered&quot;: false, &quot;using_mrr&quot;: false, &quot;index_only&quot;: true, &quot;rows&quot;: 1, &quot;cost&quot;: 1.21, &quot;chosen&quot;: true &#125; ] 回表： &quot;range_scan_alternatives&quot;: [ &#123; &quot;index&quot;: &quot;name_score&quot;, &quot;ranges&quot;: [ &quot;name1 &lt;&#x3D; name &lt;&#x3D; name1&quot; ] &#x2F;* ranges *&#x2F;, &quot;index_dives_for_eq_ranges&quot;: true, &quot;rowid_ordered&quot;: false, &quot;using_mrr&quot;: false, &quot;index_only&quot;: false, &quot;rows&quot;: 1, &quot;cost&quot;: 2.21, &quot;chosen&quot;: true &#125; ] 问题2：索引除了可以用于加速搜索外，还可以在排序时发挥作用，你能通过EXPLAIN来证明吗？你知道，针对排序在什么情况下，索引会失效吗？ 答：排序使用到索引，在执行计划中的体现就是key这一列。如果没有用到索引，会在Extra中看到Using filesort，代表使用了内存或磁盘进行排序。而具体走内存还是磁盘，是由sort_buffer_size和排序数据大小决定的。 排序无法使用到索引的情况有： 对于使用联合索引进行排序的场景，多个字段排序ASC和DESC混用； a+b作为联合索引，按照a范围查询后按照b排序； 排序列涉及到的多个字段不属于同一个联合索引； 排序列使用了表达式。 其实，这些原因都和索引的结构有关。你可以再有针对性地复习下第07讲的聚簇索引和二级索引部分。 08 | 判等问题：程序里如何确定你就是你？问题1：在实现equals时，我是先通过getClass方法判断两个对象的类型，你可能会想到还可以使用instanceof来判断。你能说说这两种实现方式的区别吗？ 答：事实上，使用getClass和instanceof这两种方案都是可以判断对象类型的。它们的区别就是，getClass限制了这两个对象只能属于同一个类，而instanceof却允许两个对象是同一个类或其子类。 正是因为这种区别，不同的人对这两种方案有不同的喜好，争论也很多。在我看来，你只需要根据自己的要求去选择。补充说明一下，Lombok使用的是instanceof的方案。 问题2：在“hashCode 和 equals 要配对实现”这一节的例子中，我演示了可以通过HashSet的contains方法判断元素是否在HashSet中。那同样是Set的TreeSet，其contains方法和HashSet的contains方法有什么区别吗？ 答：HashSet基于HashMap，数据结构是哈希表。所以，HashSet的contains方法，其实就是根据hashcode和equals去判断相等的。 TreeSet基于TreeMap，数据结构是红黑树。所以，TreeSet的contains方法，其实就是根据compareTo去判断相等的。 09 | 数值计算：注意精度、舍入和溢出问题问题1：BigDecimal提供了8种舍入模式，你能通过一些例子说说它们的区别吗？ 答：@Darren同学的留言非常全面，梳理得也非常清楚了。这里，我对他的留言稍加修改，就是这个问题的答案了。 第一种，ROUND_UP，舍入远离零的舍入模式，在丢弃非零部分之前始终增加数字（始终对非零舍弃部分前面的数字加1）。 需要注意的是，此舍入模式始终不会减少原始值。 第二种，ROUND_DOWN，接近零的舍入模式，在丢弃某部分之前始终不增加数字（从不对舍弃部分前面的数字加1，即截断）。 需要注意的是，此舍入模式始终不会增加原始值。 第三种，ROUND_CEILING，接近正无穷大的舍入模式。 如果 BigDecimal 为正，则舍入行为与 ROUND_UP 相同； 如果为负，则舍入行为与 ROUND_DOWN 相同。 需要注意的是，此舍入模式始终不会减少原始值。 第四种，ROUND_FLOOR，接近负无穷大的舍入模式。 如果 BigDecimal 为正，则舍入行为与 ROUND_DOWN 相同； 如果为负，则舍入行为与 ROUND_UP 相同。 需要注意的是，此舍入模式始终不会增加原始值。 第五种，ROUND_HALF_UP，向“最接近的”数字舍入。如果舍弃部分 &gt;&#x3D; 0.5，则舍入行为与 ROUND_UP 相同；否则，舍入行为与 ROUND_DOWN 相同。 需要注意的是，这是我们大多数人在小学时就学过的舍入模式（四舍五入）。 第六种，ROUND_HALF_DOWN，向“最接近的”数字舍入。如果舍弃部分 &gt; 0.5，则舍入行为与 ROUND_UP 相同；否则，舍入行为与 ROUND_DOWN 相同（五舍六入）。 第七种，ROUND_HALF_EVEN，向“最接近的”数字舍入。这种算法叫做银行家算法，具体规则是，四舍六入，五则看前一位，如果是偶数舍入，如果是奇数进位，比如5.5 -&gt; 6，2.5 -&gt; 2。 第八种，ROUND_UNNECESSARY，假设请求的操作具有精确的结果，也就是不需要进行舍入。如果计算结果产生不精确的结果，则抛出ArithmeticException。 问题2：数据库（比如MySQL）中的浮点数和整型数字，你知道应该怎样定义吗？又如何实现浮点数的准确计算呢？ 答：MySQL中的整数根据能表示的范围有TINYINT、SMALLINT、MEDIUMINT、INTEGER、BIGINT等类型，浮点数包括单精度浮点数FLOAT和双精度浮点数DOUBLE和Java中的float&#x2F;double一样，同样有精度问题。 要解决精度问题，主要有两个办法： 第一，使用DECIMAL类型（和那些INT类型一样，都属于严格数值数据类型），比如DECIMAL(13, 2)或DECIMAL(13, 4)。 第二，使用整数保存到分，这种方式容易出错，万一读的时候忘记&#x2F;100或者是存的时候忘记*100，可能会引起重大问题。当然了，我们也可以考虑将整数和小数分开保存到两个整数字段。 10 | 集合类：坑满地的List列表操作问题1：调用类型是Integer的ArrayList的remove方法删除元素，传入一个Integer包装类的数字和传入一个int基本类型的数字，结果一样吗？ 答：传int基本类型的remove方法是按索引值移除，返回移除的值；传Integer包装类的remove方法是按值移除，返回列表移除项目之前是否包含这个值（是否移除成功）。 为了验证两个remove方法重载的区别，我们写一段测试代码比较一下： private static void removeByIndex(int index) &#123; List&lt;Integer&gt; list &#x3D; IntStream.rangeClosed(1, 10).boxed().collect(Collectors.toCollection(ArrayList::new)); System.out.println(list.remove(index)); System.out.println(list); &#125; private static void removeByValue(Integer index) &#123; List&lt;Integer&gt; list &#x3D; IntStream.rangeClosed(1, 10).boxed().collect(Collectors.toCollection(ArrayList::new)); System.out.println(list.remove(index)); System.out.println(list); &#125; 测试一下removeByIndex(4)，通过输出可以看到第五项被移除了，返回5： 5 [1, 2, 3, 4, 6, 7, 8, 9, 10] 而调用removeByValue(Integer.valueOf(4))，通过输出可以看到值4被移除了，返回true： true [1, 2, 3, 5, 6, 7, 8, 9, 10] 问题2：循环遍历List，调用remove方法删除元素，往往会遇到ConcurrentModificationException，原因是什么，修复方式又是什么呢？ 答：原因是，remove的时候会改变modCount，通过迭代器遍历就会触发ConcurrentModificationException。我们看下ArrayList类内部迭代器的相关源码： public E next() &#123; checkForComodification(); int i &#x3D; cursor; if (i &gt;&#x3D; size) throw new NoSuchElementException(); Object[] elementData &#x3D; ArrayList.this.elementData; if (i &gt;&#x3D; elementData.length) throw new ConcurrentModificationException(); cursor &#x3D; i + 1; return (E) elementData[lastRet &#x3D; i]; &#125; final void checkForComodification() &#123; if (modCount !&#x3D; expectedModCount) throw new ConcurrentModificationException(); &#125; 要修复这个问题，有以下两种解决方案。 第一种，通过ArrayList的迭代器remove。迭代器的remove方法会维护一个expectedModCount，使其与 ArrayList 的modCount保持一致： List&lt;String&gt; list &#x3D; IntStream.rangeClosed(1, 10).mapToObj(String::valueOf).collect(Collectors.toCollection(ArrayList::new)); for (Iterator&lt;String&gt; iterator &#x3D; list.iterator(); iterator.hasNext(); ) &#123; String next &#x3D; iterator.next(); if (&quot;2&quot;.equals(next)) &#123; iterator.remove(); &#125; &#125; System.out.println(list); 第二种，直接使用removeIf方法，其内部使用了迭代器的remove方法： List&lt;String&gt; list &#x3D; IntStream.rangeClosed(1, 10).mapToObj(String::valueOf).collect(Collectors.toCollection(ArrayList::new)); list.removeIf(item -&gt; item.equals(&quot;2&quot;)); System.out.println(list); 11 | 空值处理：分不清楚的null和恼人的空指针问题1：ConcurrentHashMap的Key和Value都不能为null，而HashMap却可以，你知道这么设计的原因是什么吗？TreeMap、Hashtable等Map的Key和Value是否支持null呢？ 答：原因正如ConcurrentHashMap的作者所说： The main reason that nulls aren’t allowed in ConcurrentMaps (ConcurrentHashMaps, ConcurrentSkipListMaps) is that ambiguities that may be just barely tolerable in non-concurrent maps can’t be accommodated. The main one is that if map.get(key) returns null, you can’t detect whether the key explicitly maps to null vs the key isn’t mapped. In a non-concurrent map, you can check this via map.contains(key), but in a concurrent one, the map might have changed between calls. 如果Value为null会增加二义性，也就是说多线程情况下map.get(key)返回null，我们无法区分Value原本就是null还是Key没有映射，Key也是类似的原因。此外，我也更同意他的观点，就是普通的Map允许null是否是一个正确的做法，也值得商榷，因为这会增加犯错的可能性。 Hashtable也是线程安全的，所以Key和Value不可以是null。 TreeMap是线程不安全的，但是因为需要排序，需要进行key的compareTo方法，所以Key不能是null，而Value可以是null。 问题2：对于Hibernate框架，我们可以使用@DynamicUpdate注解实现字段的动态更新。那么，对于MyBatis框架来说，要如何实现类似的动态SQL功能，实现插入和修改SQL只包含POJO中的非空字段呢？ 答：MyBatis可以通过动态SQL实现： &lt;select id&#x3D;&quot;findUser&quot; resultType&#x3D;&quot;User&quot;&gt; SELECT * FROM USER WHERE 1&#x3D;1 &lt;if test&#x3D;&quot;name !&#x3D; null&quot;&gt; AND name like #&#123;name&#125; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;email !&#x3D; null&quot;&gt; AND email &#x3D; #&#123;email&#125; &lt;&#x2F;if&gt; &lt;&#x2F;select&gt; 如果使用MyBatisPlus的话，实现类似的动态SQL功能会更方便。我们可以直接在字段上加@TableField注解来实现，可以设置insertStrategy、updateStrategy、whereStrategy属性。关于这三个属性的使用方式，你可以参考如下源码，或是这里的官方文档： &#x2F;** * 字段验证策略之 insert: 当insert操作时，该字段拼接insert语句时的策略 * IGNORED: 直接拼接 insert into table_a(column) values (#&#123;columnProperty&#125;); * NOT_NULL: insert into table_a(&lt;if test&#x3D;&quot;columnProperty !&#x3D; null&quot;&gt;column&lt;&#x2F;if&gt;) values (&lt;if test&#x3D;&quot;columnProperty !&#x3D; null&quot;&gt;#&#123;columnProperty&#125;&lt;&#x2F;if&gt;) * NOT_EMPTY: insert into table_a(&lt;if test&#x3D;&quot;columnProperty !&#x3D; null and columnProperty!&#x3D;&#39;&#39;&quot;&gt;column&lt;&#x2F;if&gt;) values (&lt;if test&#x3D;&quot;columnProperty !&#x3D; null and columnProperty!&#x3D;&#39;&#39;&quot;&gt;#&#123;columnProperty&#125;&lt;&#x2F;if&gt;) * * @since 3.1.2 *&#x2F; FieldStrategy insertStrategy() default FieldStrategy.DEFAULT; &#x2F;** * 字段验证策略之 update: 当更新操作时，该字段拼接set语句时的策略 * IGNORED: 直接拼接 update table_a set column&#x3D;#&#123;columnProperty&#125;, 属性为null&#x2F;空string都会被set进去 * NOT_NULL: update table_a set &lt;if test&#x3D;&quot;columnProperty !&#x3D; null&quot;&gt;column&#x3D;#&#123;columnProperty&#125;&lt;&#x2F;if&gt; * NOT_EMPTY: update table_a set &lt;if test&#x3D;&quot;columnProperty !&#x3D; null and columnProperty!&#x3D;&#39;&#39;&quot;&gt;column&#x3D;#&#123;columnProperty&#125;&lt;&#x2F;if&gt; * * @since 3.1.2 *&#x2F; FieldStrategy updateStrategy() default FieldStrategy.DEFAULT; &#x2F;** * 字段验证策略之 where: 表示该字段在拼接where条件时的策略 * IGNORED: 直接拼接 column&#x3D;#&#123;columnProperty&#125; * NOT_NULL: &lt;if test&#x3D;&quot;columnProperty !&#x3D; null&quot;&gt;column&#x3D;#&#123;columnProperty&#125;&lt;&#x2F;if&gt; * NOT_EMPTY: &lt;if test&#x3D;&quot;columnProperty !&#x3D; null and columnProperty!&#x3D;&#39;&#39;&quot;&gt;column&#x3D;#&#123;columnProperty&#125;&lt;&#x2F;if&gt; * * @since 3.1.2 *&#x2F; FieldStrategy whereStrategy() default FieldStrategy.DEFAULT; 12 | 异常处理：别让自己在出问题的时候变为瞎子问题1：关于在finally代码块中抛出异常的坑，如果在finally代码块中返回值，你觉得程序会以try或catch中的返回值为准，还是以finally中的返回值为准呢？ 答：以finally中的返回值为准。 从语义上来说，finally是做方法收尾资源释放处理的，我们不建议在finally中有return，这样逻辑会很混乱。这是因为，实现上finally中的代码块会被复制多份，分别放到try和catch调用return和throw异常之前，所以finally中如果有返回值，会覆盖try中的返回值。 问题2：对于手动抛出的异常，不建议直接使用Exception或RuntimeException，通常建议复用JDK中的一些标准异常，比如IllegalArgumentException、IllegalStateException、UnsupportedOperationException。你能说说它们的适用场景，并列出更多常见的可重用标准异常吗？ 答：我们先分别看看IllegalArgumentException、IllegalStateException、UnsupportedOperationException这三种异常的适用场景。 IllegalArgumentException：参数不合法异常，适用于传入的参数不符合方法要求的场景。 IllegalStateException：状态不合法异常，适用于状态机的状态的无效转换，当前逻辑的执行状态不适合进行相应操作等场景。 UnsupportedOperationException：操作不支持异常，适用于某个操作在实现或环境下不支持的场景。 还可以重用的异常有IndexOutOfBoundsException、NullPointerException、ConcurrentModificationException等。 以上，就是咱们这门课第7~12讲的思考题答案了。 关于这些题目，以及背后涉及的知识点，如果你还有哪里感觉不清楚的，欢迎在评论区与我留言，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/答疑篇：安全篇思考题答案合集","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/答疑篇：安全篇思考题答案合集/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/%E7%AD%94%E7%96%91%E7%AF%87%EF%BC%9A%E5%AE%89%E5%85%A8%E7%AF%87%E6%80%9D%E8%80%83%E9%A2%98%E7%AD%94%E6%A1%88%E5%90%88%E9%9B%86/","excerpt":"","text":"答疑篇：安全篇思考题答案合集作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 今天，我们继续一起分析这门课“安全篇”模块的第27~30讲的课后思考题。这些题目涉及了数据源头、安全兜底、数据和代码、敏感数据相关的4大知识点。 接下来，我们就一一具体分析吧。 27 | 数据源头：任何客户端的东西都不可信任问题1：在讲述用户标识不能从客户端获取这个要点的时候，我提到开发同学可能会因为用户信息未打通而通过前端来传用户ID。那我们有什么好办法，来打通不同的系统甚至不同网站的用户标识吗？ 答：打通用户在不同系统之间的登录，大致有以下三种方案。 第一种，把用户身份放在统一的服务端，每一个系统都需要到这个服务端来做登录状态的确认，确认后在自己网站的Cookie中保存会话，这就是单点登录的做法。这种方案要求所有关联系统都对接一套中央认证服务器（中央保存用户会话），在未登录的时候跳转到中央认证服务器进行登录或登录状态确认。因此，这种方案适合一个公司内部的不同域名下的网站。 第二种，把用户身份信息直接放在Token中，在客户端任意传递，Token由服务端进行校验（如果共享密钥话，甚至不需要同一个服务端进行校验），无需采用中央认证服务器，相对比较松耦合，典型的标准是JWT。这种方案适合异构系统的跨系统用户认证打通，而且相比单点登录的方案，用户体验会更好一些。 第三种，如果需要打通不同公司系统的用户登录状态，那么一般都会采用OAuth 2.0的标准中的授权码模式，基本流程如下： 第三方网站客户端转到授权服务器，上送ClientID、重定向地址RedirectUri等信息。 用户在授权服务器进行登录并且进行授权批准（授权批准这步可以配置为自动完成）。 授权完成后，重定向回到之前客户端提供的重定向地址，附上授权码。 第三方网站服务端通过授权码+ClientID+ClientSecret去授权服务器换取Token。这里的Token包含访问Token和刷新Token，访问Token过期后用刷新Token去获得新的访问Token。 因为我们不会对外暴露ClientSecret，也不会对外暴露访问Token，同时使用授权码换取Token的过程是服务端进行的，客户端拿到的只是一次性的授权码，所以这种模式比较安全。 问题2：还有一类和客户端数据相关的漏洞非常重要，那就是URL地址中的数据。在把匿名用户重定向到登录页面的时候，我们一般会带上redirectUrl，这样用户登录后可以快速返回之前的页面。黑客可能会伪造一个活动链接，由真实的网站+钓鱼的redirectUrl构成，发邮件诱导用户进行登录。用户登录时访问的其实是真的网站，所以不容易察觉到redirectUrl是钓鱼网站，登录后却来到了钓鱼网站，用户可能会不知不觉就把重要信息泄露了。这种安全问题，我们叫做开放重定向问题。你觉得，从代码层面应该怎么预防开放重定向问题呢？ 答：要从代码层面预防开放重定向问题，有以下三种做法可供参考： 第一种，固定重定向的目标URL。 第二种，可采用编号方式指定重定向的目标URL，也就是重定向的目标URL只能是在我们的白名单内的。 第三种，用合理充分的校验方式来校验跳转的目标地址，如果是非己方地址，就告知用户跳转有风险，小心钓鱼网站的威胁。 28 | 安全兜底：涉及钱时，必须考虑防刷、限量和防重问题1：防重、防刷都是事前手段，如果我们的系统正在被攻击或利用，你有什么办法及时发现问题吗？ 答：对于及时发现系统正在被攻击或利用，监控是较好的手段，关键点在于报警阈值怎么设置。我觉得可以对比昨天同时、上周同时的量，发现差异达到一定百分比报警，而且报警需要有升级机制。此外，有的时候大盘很大的话，活动给整个大盘带来的变化不明显，如果进行整体监控可能出了问题也无法及时发现，因此可以考虑对于活动做独立的监控报警。 问题2：任何三方资源的使用一般都会定期对账，如果在对账中发现我们系统记录的调用量低于对方系统记录的使用量，你觉得一般是什么问题引起的呢？ 答：我之前遇到的情况是，在事务内调用外部接口，调用超时后本地事务回滚本地就没有留下数据。更合适的做法是： 请求发出之前先记录请求数据提交事务，记录状态为未知。 发布调用外部接口的请求，如果可以拿到明确的结果，则更新数据库中记录的状态为成功或失败。如果出现超时或未知异常，不能假设第三方接口调用失败，需要通过查询接口查询明确的结果。 写一个定时任务补偿数据库中所有未知状态的记录，从第三方接口同步结果。 值得注意的是，对账的时候一定要对两边，不管哪方数据缺失都可能是因为程序逻辑有bug，需要重视。此外，任何涉及第三方系统的交互，都建议在数据库中保持明细的请求&#x2F;响应报文，方便在出问题的时候定位Bug根因。 29 | 数据和代码：数据就是数据，代码就是代码问题1：在讨论SQL注入案例时，最后那次测试我们看到sqlmap返回了4种注入方式。其中，布尔盲注、时间盲注和报错注入，我都介绍过了。你知道联合查询注入，是什么吗？ 答：联合查询注入，也就是通过UNION来实现我们需要的信息露出，一般属于回显的注入方式。我们知道，UNION可以用于合并两个SELECT查询的结果集，因此可以把注入脚本来UNION到原始的SELECT后面。这样就可以查询我们需要的数据库元数据以及表数据了。 注入的关键点在于： 第一，UNION的两个SELECT语句的列数和字段类型需要一致。 第二，需要探查UNION后的结果和页面回显呈现数据的对应关系。 问题2：在讨论XSS的时候，对于Thymeleaf模板引擎，我们知道如何让文本进行HTML转义显示。FreeMarker也是Java中很常用的模板引擎，你知道如何处理转义吗？ 答：其实，现在大多数的模板引擎都使用了黑名单机制，而不是白名单机制来做HTML转义，这样更能有效防止XSS漏洞。也就是，默认开启HTML转义，如果某些情况你不需要转义可以临时关闭。 比如，FreeMarker（2.3.24以上版本）默认对HTML、XHTML、XML等文件类型（输出格式）设置了各种转义规则，你可以使用?no_esc： &lt;#-- 假设默认是HTML输出 --&gt; $&#123;&#39;&lt;b&gt;test&lt;&#x2F;b&gt;&#39;&#125; &lt;#-- 输出: &lt;b&gt;test&lt;&#x2F;b&gt; --&gt; $&#123;&#39;&lt;b&gt;test&lt;&#x2F;b&gt;&#39;?no_esc&#125; &lt;#-- 输出: &lt;b&gt;test&lt;&#x2F;b&gt; --&gt; 或noautoesc指示器： $&#123;&#39;&amp;&#39;&#125; &lt;#-- 输出: &amp; --&gt; &lt;#noautoesc&gt; $&#123;&#39;&amp;&#39;&#125; &lt;#-- 输出: &amp; --&gt; ... $&#123;&#39;&amp;&#39;&#125; &lt;#-- 输出: &amp; --&gt; &lt;&#x2F;#noautoesc&gt; $&#123;&#39;&amp;&#39;&#125; &lt;#-- 输出: &amp; --&gt; 来临时关闭转义。又比如，对于模板引擎Mustache，可以使用三个花括号而不是两个花括号，来取消变量自动转义： 模板: * &#123;&#123;name&#125;&#125; * &#123;&#123;company&#125;&#125; * &#123;&#123;&#123;company&#125;&#125;&#125; 数据: &#123; &quot;name&quot;: &quot;Chris&quot;, &quot;company&quot;: &quot;&lt;b&gt;GitHub&lt;&#x2F;b&gt;&quot; &#125; 输出： * Chris * * &lt;b&gt;GitHub&lt;&#x2F;b&gt; * &lt;b&gt;GitHub&lt;&#x2F;b&gt; 30 | 如何正确保存和传输敏感数据？问题1：虽然我们把用户名和密码脱敏加密保存在数据库中，但日志中可能还存在明文的敏感数据。你有什么思路在框架或中间件层面，对日志进行脱敏吗？ 答：如果我们希望在日志的源头进行脱敏，那么可以在日志框架层面做。比如对于logback日志框架，我们可以自定义MessageConverter，通过正则表达式匹配敏感信息脱敏。 需要注意的是，这种方式有两个缺点。 第一，正则表达式匹配敏感信息的格式不一定精确，会出现误杀漏杀的现象。一般来说，这个问题不会很严重。要实现精确脱敏的话，就只能提供各种脱敏工具类，然后让业务应用在日志中记录敏感信息的时候，先手动调用工具类进行脱敏。 第二，如果数据量比较大的话，脱敏操作可能会增加业务应用的CPU和内存使用，甚至会导致应用不堪负荷出现不可用。考虑到目前大部分公司都引入了ELK来集中收集日志，并且一般而言都不允许上服务器直接看文件日志，因此我们可以考虑在日志收集中间件中（比如logstash）写过滤器进行脱敏。这样可以把脱敏的消耗转义到ELK体系中，不过这种方式同样有第一点提到的字段不精确匹配导致的漏杀误杀的缺点。 问题2：你知道HTTPS双向认证的目的是什么吗？流程上又有什么区别呢？ 答：单向认证一般用于Web网站，浏览器只需要验证服务端的身份。对于移动端App，如果我们希望有更高的安全性，可以引入HTTPS双向认证，也就是除了客户端验证服务端身份之外，服务端也验证客户端的身份。 单向认证和双向认证的流程区别，主要包括以下三个方面。 第一，不仅仅服务端需要有CA证书，客户端也需要有CA证书。 第二，双向认证的流程中，客户端校验服务端CA证书之后，客户端会把自己的CA证书发给服务端，然后服务端需要校验客户端CA证书的真实性。 第三，客户端给服务端的消息会使用自己的私钥签名，服务端可以使用客户端CA证书中的公钥验签。 这里还想补充一点，对于移动应用程序考虑到更强的安全性，我们一般也会把服务端的公钥配置在客户端中，这种方式的叫做SSL Pinning。也就是说由客户端直接校验服务端证书的合法性，而不是通过证书信任链来校验。采用SSL Pinning，由于客户端绑定了服务端公钥，因此我们无法通过在移动设备上信用根证书实现抓包。不过这种方式的缺点是需要小心服务端CA证书过期后续证书注意不要修改公钥。 好了，以上就是咱们整个《Java 业务开发常见错误100例》这门课的30讲正文的思考题答案或者解题思路了。 关于这些题目，以及背后涉及的知识点，如果你还有哪里感觉不清楚的，欢迎在评论区与我留言，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/答疑篇：设计篇思考题答案合集","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/答疑篇：设计篇思考题答案合集/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/%E7%AD%94%E7%96%91%E7%AF%87%EF%BC%9A%E8%AE%BE%E8%AE%A1%E7%AF%87%E6%80%9D%E8%80%83%E9%A2%98%E7%AD%94%E6%A1%88%E5%90%88%E9%9B%86/","excerpt":"","text":"答疑篇：设计篇思考题答案合集作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 今天，我们继续一起分析这门课“设计篇”模块的第21~26讲的课后思考题。这些题目涉及了代码重复、接口设计、缓存设计、生产就绪、异步处理和数据存储这6大知识点。 接下来，我们就一一具体分析吧。 21 | 代码重复：搞定代码重复的三个绝招问题1：除了模板方法设计模式是减少重复代码的一把好手，观察者模式也常用于减少代码重复（并且是松耦合方式），Spring 也提供了类似工具（点击这里查看），你能想到观察者模式有哪些应用场景吗？ 答：其实，和使用MQ来解耦系统和系统的调用类似，应用内各个组件之间的调用我们也可以使用观察者模式来解耦，特别是当你的应用是一个大单体的时候。观察者模式除了是让组件之间可以更松耦合，还能更有利于消除重复代码。 其原因是，对于一个复杂的业务逻辑，里面必然涉及到大量其它组件的调用，虽然我们没有重复写这些组件内部处理逻辑的代码，但是这些复杂调用本身就构成了重复代码。 我们可以考虑把代码逻辑抽象一下，抽象出许多事件，围绕这些事件来展开处理，那么这种处理模式就从“命令式”变为了“环境感知式”，每一个组件就好像活在一个场景中，感知场景中的各种事件，然后又把发出处理结果作为另一个事件。 经过这种抽象，复杂组件之间的调用逻辑就变成了“事件抽象+事件发布+事件订阅”，整个代码就会更简化。 补充说明一下，除了观察者模式我们还经常听到发布订阅模式，那么它们有什么区别呢？ 其实，观察者模式也可以叫做发布订阅模式。不过在严格定义上，前者属于松耦合，后者必须要MQ Broker的介入，实现发布者订阅者的完全解耦。 问题2：关于 Bean 属性复制工具，除了最简单的 Spring 的 BeanUtils 工具类的使用，你还知道哪些对象映射类库吗？它们又有什么功能呢？ 答：在众多对象映射工具中，MapStruct更具特色一点。它基于JSR 269的Java注解处理器实现（你可以理解为，它是编译时的代码生成器），使用的是纯Java方法而不是反射进行属性赋值，并且做到了编译时类型安全。 如果你使用IDEA的话，可以进一步安装 IDEA MapStruct Support插件，实现映射配置的自动完成、跳转到定义等功能。关于这个插件的具体功能，你可以参考这里。 22 | 接口设计：系统间对话的语言，一定要统一问题1：在“接口的响应要明确表示接口的处理结果”这一节的例子中，接口响应结构体中的code字段代表执行结果的错误码，对于业务特别复杂的接口，可能会有很多错误情况，code可能会有几十甚至几百个。客户端开发人员需要根据每一种错误情况逐一写if-else进行不同交互处理，会非常麻烦，你觉得有什么办法来改进吗？作为服务端，是否有必要告知客户端接口执行的错误码呢？ 答：服务端把错误码反馈给客户端有两个目的，一是客户端可以展示错误码方便排查问题，二是客户端可以根据不同的错误码来做交互区分。 对于第一点方便客户端排查问题，服务端应该进行适当的收敛和规整错误码，而不是把服务内可能遇到的、来自各个系统各个层次的错误码，一股脑地扔给客户端提示给用户。 我的建议是，开发一个错误码服务来专门治理错误码，实现错误码的转码、分类和收敛逻辑，甚至可以开发后台，让产品来录入需要的错误码提示消息。 此外，我还建议错误码由一定的规则构成，比如错误码第一位可以是错误类型（比如A表示错误来源于用户；B表示错误来源于当前系统，往往是业务逻辑出错，或程序健壮性差等问题；C表示错误来源于第三方服务），第二、第三位可以是错误来自的系统编号（比如01来自用户服务，02来自商户服务等等），后面三位是自增错误码ID。 对于第二点对不同错误码的交互区分，我觉得更好的做法是服务端驱动模式，让服务端告知客户端如何处理，说白了就是客户端只需要照做即可，不需要感知错误码的含义（即便客户端显示错误码，也只是用于排错）。 比如，服务端的返回可以包含actionType和actionInfo两个字段，前者代表客户端应该做的交互动作，后者代表客户端完成这个交互动作需要的信息。其中，actionType可以是toast（无需确认的消息提示）、alert（需要确认的弹框提示）、redirectView（转到另一个视图）、redirectWebView（打开Web视图）等；actionInfo就是toast的信息、alert的信息、redirect的URL等。 由服务端来明确客户端在请求API后的交互行为，主要的好处是灵活和统一两个方面。 灵活在于两个方面：第一，在紧急的时候还可以通过redirect方式进行救急。比如，遇到特殊情况需要紧急进行逻辑修改的情况时，我们可以直接在不发版的情况下切换到H5实现。第二是，我们可以提供后台，让产品或运营来配置交互的方式和信息（而不是改交互，改提示还需要客户端发版）。 统一：有的时候会遇到不同的客户端（比如iOS、Android、前端），对于交互的实现不统一的情况，如果API结果可以规定这部分内容，那就可以彻底避免这个问题。 问题2：在“要考虑接口变迁的版本控制策略”这一节的例子中，我们在类或方法上标记@APIVersion自定义注解，实现了URL方式统一的接口版本定义。你可以用类似的方式（也就是自定义RequestMappingHandlerMapping），来实现一套统一的基于请求头方式的版本控制吗？ 答：我在GitHub上第21讲的源码中更新了我的实现，你可以点击这里查看。主要原理是，定义自己的RequestCondition来做请求头的匹配： public class APIVersionCondition implements RequestCondition&lt;APIVersionCondition&gt; &#123; @Getter private String apiVersion; @Getter private String headerKey; public APIVersionCondition(String apiVersion, String headerKey) &#123; this.apiVersion &#x3D; apiVersion; this.headerKey &#x3D; headerKey; &#125; @Override public APIVersionCondition combine(APIVersionCondition other) &#123; return new APIVersionCondition(other.getApiVersion(), other.getHeaderKey()); &#125; @Override public APIVersionCondition getMatchingCondition(HttpServletRequest request) &#123; String version &#x3D; request.getHeader(headerKey); return apiVersion.equals(version) ? this : null; &#125; @Override public int compareTo(APIVersionCondition other, HttpServletRequest request) &#123; return 0; &#125; &#125; 并且自定义RequestMappingHandlerMapping，来把方法关联到自定义的RequestCondition： public class APIVersionHandlerMapping extends RequestMappingHandlerMapping &#123; @Override protected boolean isHandler(Class&lt;?&gt; beanType) &#123; return AnnotatedElementUtils.hasAnnotation(beanType, Controller.class); &#125; @Override protected RequestCondition&lt;APIVersionCondition&gt; getCustomTypeCondition(Class&lt;?&gt; handlerType) &#123; APIVersion apiVersion &#x3D; AnnotationUtils.findAnnotation(handlerType, APIVersion.class); return createCondition(apiVersion); &#125; @Override protected RequestCondition&lt;APIVersionCondition&gt; getCustomMethodCondition(Method method) &#123; APIVersion apiVersion &#x3D; AnnotationUtils.findAnnotation(method, APIVersion.class); return createCondition(apiVersion); &#125; private RequestCondition&lt;APIVersionCondition&gt; createCondition(APIVersion apiVersion) &#123; return apiVersion &#x3D;&#x3D; null ? null : new APIVersionCondition(apiVersion.value(), apiVersion.headerKey()); &#125; &#125; 23 | 缓存设计：缓存可以锦上添花也可以落井下石问题1：在聊到缓存并发问题时，我们说到热点 Key 回源会对数据库产生的压力问题，如果 Key 特别热的话，可能缓存系统也无法承受，毕竟所有的访问都集中打到了一台缓存服务器。如果我们使用 Redis 来做缓存，那可以把一个热点 Key 的缓存查询压力，分散到多个 Redis 节点上吗？ 答：Redis 4.0以上如果开启了LFU算法作为maxmemory-policy，那么可以使用–hotkeys配合redis-cli命令行工具来探查热点Key。此外，我们还可以通过MONITOR命令来收集Redis执行的所有命令，然后配合redis-faina工具来分析热点Key、热点前缀等信息。 对于如何分散热点Key对于Redis单节点的压力的问题，我们可以考虑为Key加上一定范围的随机数作为后缀，让一个Key变为多个Key，相当于对热点Key进行分区操作。 当然，除了分散Redis压力之外，我们也可以考虑再做一层短时间的本地缓存，结合Redis的Keyspace通知功能，来处理本地缓存的数据同步。 问题2：大 Key 也是数据缓存容易出现的一个问题。如果一个 Key 的 Value 特别大，那么可能会对 Redis 产生巨大的性能影响，因为 Redis 是单线程模型，对大 Key 进行查询或删除等操作，可能会引起 Redis 阻塞甚至是高可用切换。你知道怎么查询 Redis 中的大 Key，以及如何在设计上实现大 Key 的拆分吗？ 答：Redis的大Key可能会导致集群内存分布不均问题，并且大Key的操作可能也会产生阻塞。 关于查询Redis中的大Key，我们可以使用redis-cli –bigkeys命令来实时探查大Key。此外，我们还可以使用redis-rdb-tools工具来分析Redis的RDB快照，得到包含Key的字节数、元素个数、最大元素长度等信息的CSV文件。然后，我们可以把这个CSV文件导入MySQL中，写SQL去分析。 针对大Key，我们可以考虑两方面的优化： 第一，是否有必要在Redis保存这么多数据。一般情况下，我们在缓存系统中保存面向呈现的数据，而不是原始数据；对于原始数据的计算，我们可以考虑其它文档型或搜索型的NoSQL数据库。 第二，考虑把具有二级结构的Key（比如List、Set、Hash）拆分成多个小Key，来独立获取（或是用MGET获取）。 此外值得一提的是，大Key的删除操作可能会产生较大性能问题。从Redis 4.0开始，我们可以使用UNLINK命令而不是DEL命令在后台删除大Key；而对于4.0之前的版本，我们可以考虑使用游标删除大Key中的数据，而不是直接使用DEL命令，比如对于Hash使用HSCAN+HDEL结合管道功能来删除。 24 | 业务代码写完，就意味着生产就绪了？问题1：Spring Boot Actuator提供了大量内置端点，你觉得端点和自定义一个@RestController有什么区别呢？你能否根据官方文档，开发一个自定义端点呢？ 答：Endpoint是Spring Boot Actuator抽象出来的一个概念，主要用于监控和配置。使用@Endpoint注解自定义端点，配合方法上的@ReadOperation、@WriteOperation、@DeleteOperation注解，分分钟就可以开发出自动通过HTTP或JMX进行暴露的监控点。 如果只希望通过HTTP暴露的话，可以使用@WebEndpoint注解；如果只希望通过JMX暴露的话，可以使用@JmxEndpoint注解。 而使用@RestController一般用于定义业务接口，如果数据需要暴露到JMX的话需要手动开发。 比如，下面这段代码展示了如何定义一个累加器端点，提供了读取操作和累加两个操作： @Endpoint(id &#x3D; &quot;adder&quot;) @Component public class TestEndpoint &#123; private static AtomicLong atomicLong &#x3D; new AtomicLong(); &#x2F;&#x2F;读取值 @ReadOperation public String get() &#123; return String.valueOf(atomicLong.get()); &#125; &#x2F;&#x2F;累加值 @WriteOperation public String increment() &#123; return String.valueOf(atomicLong.incrementAndGet()); &#125; &#125; 然后，我们可以通过HTTP或JMX来操作这个累加器。这样，我们就实现了一个自定义端点，并且可以通过JMX来操作： 问题2：在介绍指标Metrics时我们看到，InfluxDB中保存了由Micrometer框架自动帮我们收集的一些应用指标。你能否参考源码中两个Grafana配置的JSON文件，把这些指标在Grafana中配置出一个完整的应用监控面板呢？ 答：我们可以参考Micrometer源码中的binder包下面的类，来了解Micrometer帮我们自动做的一些指标。 JVM在线时间：process.uptime 系统CPU使用：system.cpu.usage JVM进程CPU使用：process.cpu.usage 系统1分钟负载：system.load.average.1m JVM使用内存：jvm.memory.used JVM提交内存：jvm.memory.committed JVM最大内存：jvm.memory.max JVM线程情况：jvm.threads.states JVM GC暂停：jvm.gc.pause、jvm.gc.concurrent.phase.time 剩余磁盘：disk.free Logback日志数量：logback.events Tomcat线程情况（最大、繁忙、当前）：tomcat.threads.config.max、tomcat.threads.busy、tomcat.threads.current 具体的面板配置方式，第24讲中已有说明。这里，我只和你分享在配置时会用到的两个小技巧。 第一个小技巧是，把公共的标签配置为下拉框固定在页头显示：一般来说，我们会配置一个面板给所有的应用使用（每一个指标中我们都会保存应用名称、IP地址等信息，这个功能可以使用Micrometer的CommonTags实现，参考文档的5.2节），我们可以利用Grafana的Variables功能把应用名称和IP展示为两个下拉框显示，同时提供一个adhoc筛选器自由增加筛选条件： 来到Variables面板，可以看到我配置的三个变量： Application和IP两个变量的查询语句如下： SHOW TAG VALUES FROM jvm_memory_used WITH KEY &#x3D; &quot;application_name&quot; SHOW TAG VALUES FROM jvm_memory_used WITH KEY &#x3D; &quot;ip&quot; WHERE application_name&#x3D;~ &#x2F;^$Application$&#x2F; 第二个小技巧是，利用GROUP BY功能展示一些明细的曲线：类似jvm_threads_states、jvm.gc.pause等指标中包含了更细节的一些状态区分标签，比如jvm_threads_states中的state标签代表了线程状态。一般而言，我们在展现图表的时候需要按照线程状态分组分曲线显示： 配置的InfluxDB查询语句是： SELECT max(&quot;value&quot;) FROM &quot;jvm_threads_states&quot; WHERE (&quot;application_name&quot; &#x3D;~ &#x2F;^$Application$&#x2F; AND &quot;ip&quot; &#x3D;~ &#x2F;^$IP$&#x2F;) AND $timeFilter GROUP BY time($__interval), &quot;state&quot; fill(none) 这里可以看到，application_name和ip两个条件的值，是关联到刚才我们配置的两个变量的，在GROUP BY中增加了按照state的分组。 25 | 异步处理好用，但非常容易用错问题1：在用户注册后发送消息到MQ，然后会员服务监听消息进行异步处理的场景下，有些时候我们会发现，虽然用户服务先保存数据再发送MQ，但会员服务收到消息后去查询数据库，却发现数据库中还没有新用户的信息。你觉得，这可能是什么问题呢，又该如何解决呢？ 答：我先来分享下，我遇到这个问题的真实情况。 当时，我们是因为业务代码把保存数据和发MQ消息放在了一个事务中，收到消息的时候有可能事务还没有提交完成。为了解决这个问题，开发同学当时的处理方式是，收MQ消息的时候Sleep 1秒再去处理。这样虽然解决了问题，但却大大降低了消息处理的吞吐量。 更好的做法是先提交事务，完成后再发MQ消息。但是，这又引申出来一个问题：MQ消息发送失败怎么办，如何确保发送消息和本地事务有整体事务性？这就需要进一步考虑建立本地消息表来确保MQ消息可补偿，把业务处理和保存MQ消息到本地消息表的操作，放在相同事务内处理，然后异步发送和补偿消息表中的消息到MQ。 问题2：除了使用Spring AMQP实现死信消息的重投递外，RabbitMQ 2.8.0 后支持的死信交换器DLX也可以实现类似功能。你能尝试用DLX实现吗，并比较下这两种处理机制？ 答：其实RabbitMQ的DLX死信交换器和普通交换器没有什么区别，只不过它有一个特点是，可以把其它队列关联到这个DLX交换器上，然后消息过期后自动会转发到DLX交换器。那么，我们就可以利用这个特点来实现延迟消息重投递，经过一定次数之后还是处理失败则作为死信处理。 实现结构如下图所示： 关于这个实现架构图，我需要说明的是： 为了简单起见，图中圆柱体代表交换器+队列，并省去了RoutingKey。 WORKER作为DLX用于处理消息，BUFFER用于临时存放需要延迟重试的消息，WORKER和BUFFER绑定在一起。 DEAD用于存放超过重试次数的死信。 在这里WORKER其实是一个DLX，我们把它绑定到BUFFER实现延迟重试。 通过RabbitMQ实现具有延迟重试功能的消息重试以及最后进入死信队列的整个流程如下： 客户端发送记录到WORKER； Handler收到消息后处理失败； 第一次重试，发送消息到BUFFER； 3秒后消息过期，自动转发到WORKER； Handler再次收到消息后处理失败； 第二次重试，发送消息到BUFFER； 3秒后消息过期，还是自动转发到WORKER； Handler再次收到消息后处理失败，达到最大重试次数； 发送消息到DEAD（作为死信消息）； DeadHandler收到死信处理（比如进行人工处理）。 整个程序的日志输出如下，可以看到输出日志和我们前面贴出的结构图、详细解释的流程一致： [21:59:48.625] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.a.r.DeadLetterController:24 ] - Client 发送消息 msg1 [21:59:48.640] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.rabbitmqdlx.MQListener:27 ] - Handler 收到消息：msg1 [21:59:48.641] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.rabbitmqdlx.MQListener:33 ] - Handler 消费消息：msg1 异常，准备重试第1次 [21:59:51.643] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.rabbitmqdlx.MQListener:27 ] - Handler 收到消息：msg1 [21:59:51.644] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.rabbitmqdlx.MQListener:33 ] - Handler 消费消息：msg1 异常，准备重试第2次 [21:59:54.646] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.rabbitmqdlx.MQListener:27 ] - Handler 收到消息：msg1 [21:59:54.646] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#0-1] [INFO ] [o.g.t.c.a.rabbitmqdlx.MQListener:40 ] - Handler 消费消息：msg1 异常，已重试 2 次，发送到死信队列处理！ [21:59:54.649] [org.springframework.amqp.rabbit.RabbitListenerEndpointContainer#1-1] [ERROR] [o.g.t.c.a.rabbitmqdlx.MQListener:62 ] - DeadHandler 收到死信消息： msg1 接下来，我们再对比下这种实现方式和第25讲中Spring重试的区别。其实，这两种实现方式的差别很大，体现在下面两点。 第一点，Spring的重试是在处理的时候，在线程内休眠进行延迟重试，消息不会重发到MQ；我们这个方案中处理失败的消息会发送到RMQ，由RMQ做延迟处理。 第二点，Spring的重试方案，只涉及普通队列和死信队列两个队列（或者说交换器）；我们这个方案的实现中涉及工作队列、缓冲队列（用于存放等待延迟重试的消息）和死信队列（真正需要人工处理的消息）三个队列。 当然了，如果你希望把存放正常消息的队列和把存放需要重试处理消息的队列区分开的话，可以把我们这个方案中的队列再拆分下，变为四个队列，也就是工作队列、重试队列、缓冲队列（关联到重试队列作为DLX）和死信队列。 这里我再强调一下，虽然说我们利用了RMQ的DLX死信交换器的功能，但是我们把DLX当做了工作队列来使用，因为我们利用的是其能自动（从BUFFER缓冲队列）接收过期消息的特性。 这部分源码比较长，我直接放在GitHub上了。感兴趣的话，你可以点击这里的链接查看。 26 | 数据存储：NoSQL与RDBMS如何取长补短、相辅相成？问题1：我们提到，InfluxDB不能包含太多tag。你能写一段测试代码，来模拟这个问题，并观察下InfluxDB的内存使用情况吗？ 答：我们写一段如下的测试代码：向InfluxDB写入大量指标，每一条指标关联10个Tag，每一个Tag都是100000以内的随机数，这种方式会造成high series cardinality问题，从而大量占用InfluxDB的内存。 @GetMapping(&quot;influxdbwrong&quot;) public void influxdbwrong() &#123; OkHttpClient.Builder okHttpClientBuilder &#x3D; new OkHttpClient().newBuilder() .connectTimeout(1, TimeUnit.SECONDS) .readTimeout(60, TimeUnit.SECONDS) .writeTimeout(60, TimeUnit.SECONDS); try (InfluxDB influxDB &#x3D; InfluxDBFactory.connect(&quot;http:&#x2F;&#x2F;127.0.0.1:8086&quot;, &quot;root&quot;, &quot;root&quot;, okHttpClientBuilder)) &#123; influxDB.setDatabase(&quot;performance&quot;); &#x2F;&#x2F;插入100000条记录 IntStream.rangeClosed(1, 100000).forEach(i -&gt; &#123; Map&lt;String, String&gt; tags &#x3D; new HashMap&lt;&gt;(); &#x2F;&#x2F;每条记录10个tag，tag的值是100000以内随机值 IntStream.rangeClosed(1, 10).forEach(j -&gt; tags.put(&quot;tagkey&quot; + i, &quot;tagvalue&quot; + ThreadLocalRandom.current().nextInt(100000))); Point point &#x3D; Point.measurement(&quot;bad&quot;) .tag(tags) .addField(&quot;value&quot;, ThreadLocalRandom.current().nextInt(10000)) .time(System.currentTimeMillis(), TimeUnit.MILLISECONDS) .build(); influxDB.write(point); &#125;); &#125; &#125; 不过因为InfluxDB的默认参数配置限制了Tag的值数量以及数据库Series数量： max-values-per-tag &#x3D; 100000 max-series-per-database &#x3D; 1000000 所以这个程序很快就会出错，无法形成OOM，你可以把这两个参数改为0来解除这个限制。 继续运行程序，我们可以发现InfluxDB占用大量内存最终出现OOM。 问题2：文档数据库MongoDB，也是一种常用的NoSQL。你觉得MongoDB的优势和劣势是什么呢？它适合用在什么场景下呢？ 答：MongoDB是目前比较火的文档型NoSQL。虽然MongoDB 在4.0版本后具有了事务功能，但是它整体的稳定性相比MySQL还是有些差距。因此，MongoDB不太适合作为重要数据的主数据库，但可以用来存储日志、爬虫等数据重要程度不那么高，但写入并发量又很大的场景。 虽然MongoDB的写入性能较高，但复杂查询性能却相比Elasticsearch来说没啥优势；虽然MongoDB有Sharding功能，但是还不太稳定。因此，我个人建议在数据写入量不大、更新不频繁，并且不需要考虑事务的情况下，使用Elasticsearch来替换MongoDB。 以上，就是咱们这门课的第21~26讲的思考题答案了。 关于这些题目，以及背后涉及的知识点，如果你还有哪里感觉不清楚的，欢迎在评论区与我留言，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/结束语 _ 写代码时，如何才能尽量避免踩坑？","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/结束语 _ 写代码时，如何才能尽量避免踩坑？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/%E7%BB%93%E6%9D%9F%E8%AF%AD%20_%20%E5%86%99%E4%BB%A3%E7%A0%81%E6%97%B6%EF%BC%8C%E5%A6%82%E4%BD%95%E6%89%8D%E8%83%BD%E5%B0%BD%E9%87%8F%E9%81%BF%E5%85%8D%E8%B8%A9%E5%9D%91%EF%BC%9F/","excerpt":"","text":"结束语 | 写代码时，如何才能尽量避免踩坑？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 这个课程要告一段落了，在这里我要特别感谢你一直以来的认可与陪伴。于我而言，虽然这半年多以来我几乎所有的业余时间都用了在这个课程的创作，以及回答你的问题上，很累很辛苦，但是看到你的认真学习和对课程内容的好评，看到你不仅收获了知识还燃起了钻研源码的热情，我也非常高兴，深觉一切的辛苦付出都是甜蜜的。 相信一路走来，你不仅理解了业务代码开发中常见的130多个坑点的解决方式，也知道了其根本原因，以及如何使用一些常用工具来分析问题。这样在以后遇到各种坑的时候，你就更加能有方法、有信心来解决问题。 如何尽量避免踩坑？不过，学习、分析这些坑点并不是我们的最终目的，在写业务代码时如何尽量避免踩坑才是。所以，接下来，我要重点和你聊聊避免踩坑的一些方法。 所谓坑，往往就是我们意识不到的陷进。虽然这个课程覆盖了130多个业务开发时可能会出错的点，但我相信在整个Java开发领域还有成千上万个可能会踩的坑。同时，随着Java语言以及各种新框架、新技术的产生，我们还会不断遇到各种坑，很难有一种方式确保永远不会遇到新问题。 而我们能做的，就是尽可能少踩坑，或者减少踩坑给我们带来的影响。鉴于此，我还有10条建议要分享给你。 第一，遇到自己不熟悉的新类，在不了解之前不要随意使用。 比如，我在并发工具这一讲中提到的CopyOnWriteArrayList。如果你仅仅认为CopyOnWriteArrayList是ArrayList的线程安全版本，在不知晓原理之前把它用于大量写操作的场景，那么很可能会遇到性能问题。 JDK或各种框架随着时间的推移会不断推出各种特殊类，用于极致化各种细化场景下的程序性能。在使用这些类之前，我们需要认清楚这些类的由来，以及要解决的问题，在确认自己的场景符合的情况下再去使用。 而且，越普适的工具类通常用起来越简单，越高级的类用起来越复杂，也更容易踩坑。比如，代码加锁这一讲中提到的，锁工具类StampedLock就比ReentrantLock或者synchronized的用法复杂得多，很容易踩坑。 第二，尽量使用更高层次的框架。 通常情况下，偏底层的框架趋向于提供更多细节的配置，尽可能让使用者根据自己的需求来进行不同的配置，而较少考虑最佳实践的问题；而高层次的框架，则会更多地考虑怎么方便开发者开箱即用。 比如，在HTTP请求这一讲中，我们谈到Apache HttpClient的并发数限制问题。如果你使用Spring Cloud Feign搭配HttpClient，就不会遇到单域名默认2个并发连接的问题。因为，Spring Cloud Feign已经把这个参数设置为了50，足够应对一般场景了。 第三，关注各种框架和组件的安全补丁和版本更新。 比如，我们使用的Tomcat服务器、序列化框架等，就是黑客关注的安全突破口。我们需要及时关注这些组件和框架的稳定大版本和补丁，并及时更新升级，以避免组件和框架本身的性能问题或安全问题带来的大坑。 第四，尽量少自己造轮子，使用流行的框架。 流行框架最大的好处是成熟，在经过大量用户的使用打磨后，你能想到、能遇到的所有问题几乎别人都遇到了，框架中也有了解决方案。很多时候我们会以“轻量级”为由来造轮子，但其实很多复杂的框架，一开始也是轻量的。只不过是，这些框架经过各种迭代解决了各种问题，做了很多可扩展性预留之后，才变得越来越复杂，而并不一定是框架本身的设计臃肿。 如果我们自己去开发框架的话，很可能会踩一些别人已经踩过的坑。比如，直接使用JDK NIO来开发网络程序或网络框架的话，我们可能会遇到epoll的selector空轮询Bug，最终导致 CPU 100%。而Netty规避了这些问题，因此使用Netty开发NIO网络程序，不但简单而且可以少踩很多坑。 第五，开发的时候遇到错误，除了搜索解决方案外，更重要的是理解原理。 比如，在OOM这一讲，我提到的配置超大server.max-http-header-size参数导致的OOM问题，可能就是来自网络的解决方案。网络上别人给出的解决方案，可能只是适合“自己”，不一定适合所有人。并且，各种框架迭代很频繁，今天有效的解决方案，明天可能就无效了；今天有效的参数配置，新版本可能就不再建议使用甚至失效了。 因此，只有知其所以然，才能从根本上避免踩坑。 第六，网络上的资料有很多，但不一定可靠，最可靠的还是官方文档。 比如，搜索Java 8的一些介绍，你可以看到有些资料提到了在Java 8中Files.lines方法进行文件读取更高效，但是Demo代码并没使用try-with-resources来释放资源。在文件IO这一讲中，我和你讲解了这么做会导致文件句柄无法释放。 其实，网上的各种资料，本来就是大家自己学习分享的经验和心得，不一定都是对的。另外，这些资料给出的都是Demo，演示的是某个类在某方面的功能，不一定会面面俱到地考虑到资源释放、并发等问题。 因此，对于系统学习某个组件或框架，我最推荐的还是JDK或者三方库的官方文档。这些文档基本不会出现错误的示例，一般也会提到使用的最佳实践，以及最需要注意的点。 第七，做好单元测试和性能测试。 如果你开发的是一个偏底层的服务或框架，有非常多的受众和分支流程，那么单元测试（或者是自动化测试）就是必须的。 人工测试一般针对主流程和改动点，只有单元测试才可以确保任何一次改动不会影响现有服务的每一个细节点。此外，许多坑都涉及线程安全、资源使用，这些问题只有在高并发的情况下才会产生。没有经过性能测试的代码，只能认为是完成了功能，还不能确保健壮性、可扩展性和可靠性。 第八，做好设计评审和代码审查工作。 人都会犯错，而且任何一个人的知识都有盲区。因此，项目的设计如果能提前有专家组进行评审，每一段代码都能有至少三个人进行代码审核，就可以极大地减少犯错的可能性。 比如，对于熟悉IO的开发者来说，他肯定知道文件的读写需要基于缓冲区。如果他看到另一个同事提交的代码，是以单字节的方式来读写文件，就可以提前发现代码的性能问题。 又比如，一些比较老的资料仍然提倡使用MD5摘要来保存密码。但是，现在MD5已经不安全了。如果项目设计已经由公司内安全经验丰富的架构师和安全专家评审过，就可以提前避免安全疏漏。 第九，借助工具帮我们避坑。 其实，我们犯很多低级错误时，并不是自己不知道，而是因为疏忽。就好像是，即使我们知道可能存在这100个坑，但如果让我们一条一条地确认所有代码是否有这些坑，我们也很难办到。但是，如果我们可以把规则明确的坑使用工具来检测，就可以避免大量的低级错误。 比如，使用YYYY进行日期格式化的坑、使用&#x3D;&#x3D;进行判等的坑、List.subList原List和子List相互影响的坑等，都可以通过阿里P3C代码规约扫描插件发现。我也建议你为IDE安装这个插件。 此外，我还建议在CI流程中集成Sonarqube代码静态扫描平台，对需要构建发布的代码进行全面的代码质量扫描。 第十，做好完善的监控报警。 诸如内存泄露、文件句柄不释放、线程泄露等消耗型问题，往往都是量变积累成为质变，最后才会造成进程崩溃。如果一开始我们就可以对应用程序的内存使用、文件句柄使用、IO使用量、网络带宽、TCP连接、线程数等各种指标进行监控，并且基于合理阈值设置报警，那么可能就能在事故的婴儿阶段及时发现问题、解决问题。 此外，在遇到报警的时候，我们不能凭经验想当然地认为这些问题都是已知的，对报警置之不理。我们要牢记，所有报警都需要处理和记录。 以上，就是我要分享给你的10条建议了。用好这10条建议，可以帮助我们很大程度提前发现Java开发中的一些坑、避免一些压力引起的生产事故，或是减少踩坑的影响。 最后，正所谓师傅领进门，修行靠个人，希望你在接下来学习技术和写代码的过程中，能够养成多研究原理、多思考总结问题的习惯，点点滴滴补全自己的知识网络。对代码精益求精，写出健壮的代码，线上问题少了，不但自己的心情好了，也能得到更多认可，并有更多时间来学习提升。这样，我们的个人成长就会比较快，形成正向循环。 另外，如果你有时间，我想请你帮我填个课程问卷，和我反馈你对这个课程的想法和建议。今天虽然是结课，但我还会继续关注你的留言，也希望你能继续学习这个课程的内容，并会通过留言区和你互动。 你还可以继续把这个课程分享给身边的朋友和同事，我们继续交流、讨论在写Java业务代码时可能会犯的错儿。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/结课测试 _ 关于Java业务开发的100个常见错误，你都明白其中缘由了吗？","date":"2024-06-17T01:04:53.999Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/结课测试 _ 关于Java业务开发的100个常见错误，你都明白其中缘由了吗？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/%E7%BB%93%E8%AF%BE%E6%B5%8B%E8%AF%95%20_%20%E5%85%B3%E4%BA%8EJava%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E7%9A%84100%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%EF%BC%8C%E4%BD%A0%E9%83%BD%E6%98%8E%E7%99%BD%E5%85%B6%E4%B8%AD%E7%BC%98%E7%94%B1%E4%BA%86%E5%90%97%EF%BC%9F/","excerpt":"","text":"结课测试 | 关于Java业务开发的100个常见错误，你都明白其中缘由了吗？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 《Java业务开发常见错误100例》这门课程已经全部结束了。我给你准备了一套结课测试题。它既可以是对你学习效果的一个检验，也可以被看作对于课程内容的一个系统性回顾。 这套测试题共有 20 道题目，包括 8道单选题和 12道多选题，满分 100 分，系统自动评分。 还等什么，点击下面按钮开始测试吧！","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/01、使用了并发工具类库，线程安全就高枕无忧了吗？","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/01、使用了并发工具类库，线程安全就高枕无忧了吗？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/01%E3%80%81%E4%BD%BF%E7%94%A8%E4%BA%86%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BB%E5%BA%93%EF%BC%8C%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E5%B0%B1%E9%AB%98%E6%9E%95%E6%97%A0%E5%BF%A7%E4%BA%86%E5%90%97%EF%BC%9F/","excerpt":"","text":"01 | 使用了并发工具类库，线程安全就高枕无忧了吗？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。作为课程的第一讲，我今天要和你聊聊使用并发工具类库相关的话题。 在代码审核讨论的时候，我们有时会听到有关线程安全和并发工具的一些片面的观点和结论，比如“把HashMap改为ConcurrentHashMap，就可以解决并发问题了呀”“要不我们试试无锁的CopyOnWriteArrayList吧，性能更好”。事实上，这些说法都不太准确。 的确，为了方便开发者进行多线程编程，现代编程语言会提供各种并发工具类。但如果我们没有充分了解它们的使用场景、解决的问题，以及最佳实践的话，盲目使用就可能会导致一些坑，小则损失性能，大则无法确保多线程情况下业务逻辑的正确性。 我需要先说明下，这里的并发工具类是指用来解决多线程环境下并发问题的工具类库。一般而言并发工具包括同步器和容器两大类，业务代码中使用并发容器的情况会多一些，我今天分享的例子也会侧重并发容器。 接下来，我们就看看在使用并发工具时，最常遇到哪些坑，以及如何解决、避免这些坑吧。 没有意识到线程重用导致用户信息错乱的Bug之前有业务同学和我反馈，在生产上遇到一个诡异的问题，有时获取到的用户信息是别人的。查看代码后，我发现他使用了ThreadLocal来缓存获取到的用户信息。 我们知道，ThreadLocal适用于变量在线程间隔离，而在方法或类间共享的场景。如果用户信息的获取比较昂贵（比如从数据库查询用户信息），那么在ThreadLocal中缓存数据是比较合适的做法。但，这么做为什么会出现用户信息错乱的Bug呢？ 我们看一个具体的案例吧。 使用Spring Boot创建一个Web应用程序，使用ThreadLocal存放一个Integer的值，来暂且代表需要在线程中保存的用户信息，这个值初始是null。在业务逻辑中，我先从ThreadLocal获取一次值，然后把外部传入的参数设置到ThreadLocal中，来模拟从当前上下文获取到用户信息的逻辑，随后再获取一次值，最后输出两次获得的值和线程名称。 private static final ThreadLocal&lt;Integer&gt; currentUser &#x3D; ThreadLocal.withInitial(() -&gt; null); @GetMapping(&quot;wrong&quot;) public Map wrong(@RequestParam(&quot;userId&quot;) Integer userId) &#123; &#x2F;&#x2F;设置用户信息之前先查询一次ThreadLocal中的用户信息 String before &#x3D; Thread.currentThread().getName() + &quot;:&quot; + currentUser.get(); &#x2F;&#x2F;设置用户信息到ThreadLocal currentUser.set(userId); &#x2F;&#x2F;设置用户信息之后再查询一次ThreadLocal中的用户信息 String after &#x3D; Thread.currentThread().getName() + &quot;:&quot; + currentUser.get(); &#x2F;&#x2F;汇总输出两次查询结果 Map result &#x3D; new HashMap(); result.put(&quot;before&quot;, before); result.put(&quot;after&quot;, after); return result; &#125; 按理说，在设置用户信息之前第一次获取的值始终应该是null，但我们要意识到，程序运行在Tomcat中，执行程序的线程是Tomcat的工作线程，而Tomcat的工作线程是基于线程池的。 顾名思义，线程池会重用固定的几个线程，一旦线程重用，那么很可能首次从ThreadLocal获取的值是之前其他用户的请求遗留的值。这时，ThreadLocal中的用户信息就是其他用户的信息。 为了更快地重现这个问题，我在配置文件中设置一下Tomcat的参数，把工作线程池最大线程数设置为1，这样始终是同一个线程在处理请求： server.tomcat.max-threads&#x3D;1 运行程序后先让用户1来请求接口，可以看到第一和第二次获取到用户ID分别是null和1，符合预期： 随后用户2来请求接口，这次就出现了Bug，第一和第二次获取到用户ID分别是1和2，显然第一次获取到了用户1的信息，原因就是Tomcat的线程池重用了线程。从图中可以看到，两次请求的线程都是同一个线程：http-nio-8080-exec-1。 这个例子告诉我们，在写业务代码时，首先要理解代码会跑在什么线程上： 我们可能会抱怨学多线程没用，因为代码里没有开启使用多线程。但其实，可能只是我们没有意识到，在Tomcat这种Web服务器下跑的业务代码，本来就运行在一个多线程环境（否则接口也不可能支持这么高的并发），并不能认为没有显式开启多线程就不会有线程安全问题。 因为线程的创建比较昂贵，所以Web服务器往往会使用线程池来处理请求，这就意味着线程会被重用。这时，使用类似ThreadLocal工具来存放一些数据时，需要特别注意在代码运行完后，显式地去清空设置的数据。如果在代码中使用了自定义的线程池，也同样会遇到这个问题。 理解了这个知识点后，我们修正这段代码的方案是，在代码的finally代码块中，显式清除ThreadLocal中的数据。这样一来，新的请求过来即使使用了之前的线程也不会获取到错误的用户信息了。修正后的代码如下： @GetMapping(&quot;right&quot;) public Map right(@RequestParam(&quot;userId&quot;) Integer userId) &#123; String before &#x3D; Thread.currentThread().getName() + &quot;:&quot; + currentUser.get(); currentUser.set(userId); try &#123; String after &#x3D; Thread.currentThread().getName() + &quot;:&quot; + currentUser.get(); Map result &#x3D; new HashMap(); result.put(&quot;before&quot;, before); result.put(&quot;after&quot;, after); return result; &#125; finally &#123; &#x2F;&#x2F;在finally代码块中删除ThreadLocal中的数据，确保数据不串 currentUser.remove(); &#125; &#125; 重新运行程序可以验证，再也不会出现第一次查询用户信息查询到之前用户请求的Bug： ThreadLocal是利用独占资源的方式，来解决线程安全问题，那如果我们确实需要有资源在线程之间共享，应该怎么办呢？这时，我们可能就需要用到线程安全的容器了。 使用了线程安全的并发工具，并不代表解决了所有线程安全问题JDK 1.5后推出的ConcurrentHashMap，是一个高性能的线程安全的哈希表容器。“线程安全”这四个字特别容易让人误解，因为ConcurrentHashMap只能保证提供的原子性读写操作是线程安全的。 我在相当多的业务代码中看到过这个误区，比如下面这个场景。有一个含900个元素的Map，现在再补充100个元素进去，这个补充操作由10个线程并发进行。开发人员误以为使用了ConcurrentHashMap就不会有线程安全问题，于是不加思索地写出了下面的代码：在每一个线程的代码逻辑中先通过size方法拿到当前元素数量，计算ConcurrentHashMap目前还需要补充多少元素，并在日志中输出了这个值，然后通过putAll方法把缺少的元素添加进去。 为方便观察问题，我们输出了这个Map一开始和最后的元素个数。 &#x2F;&#x2F;线程个数 private static int THREAD_COUNT &#x3D; 10; &#x2F;&#x2F;总元素数量 private static int ITEM_COUNT &#x3D; 1000; &#x2F;&#x2F;帮助方法，用来获得一个指定元素数量模拟数据的ConcurrentHashMap private ConcurrentHashMap&lt;String, Long&gt; getData(int count) &#123; return LongStream.rangeClosed(1, count) .boxed() .collect(Collectors.toConcurrentMap(i -&gt; UUID.randomUUID().toString(), Function.identity(), (o1, o2) -&gt; o1, ConcurrentHashMap::new)); &#125; @GetMapping(&quot;wrong&quot;) public String wrong() throws InterruptedException &#123; ConcurrentHashMap&lt;String, Long&gt; concurrentHashMap &#x3D; getData(ITEM_COUNT - 100); &#x2F;&#x2F;初始900个元素 log.info(&quot;init size:&#123;&#125;&quot;, concurrentHashMap.size()); ForkJoinPool forkJoinPool &#x3D; new ForkJoinPool(THREAD_COUNT); &#x2F;&#x2F;使用线程池并发处理逻辑 forkJoinPool.execute(() -&gt; IntStream.rangeClosed(1, 10).parallel().forEach(i -&gt; &#123; &#x2F;&#x2F;查询还需要补充多少个元素 int gap &#x3D; ITEM_COUNT - concurrentHashMap.size(); log.info(&quot;gap size:&#123;&#125;&quot;, gap); &#x2F;&#x2F;补充元素 concurrentHashMap.putAll(getData(gap)); &#125;)); &#x2F;&#x2F;等待所有任务完成 forkJoinPool.shutdown(); forkJoinPool.awaitTermination(1, TimeUnit.HOURS); &#x2F;&#x2F;最后元素个数会是1000吗？ log.info(&quot;finish size:&#123;&#125;&quot;, concurrentHashMap.size()); return &quot;OK&quot;; &#125; 访问接口后程序输出的日志内容如下： 从日志中可以看到： 初始大小900符合预期，还需要填充100个元素。 worker1线程查询到当前需要填充的元素为36，竟然还不是100的倍数。 worker13线程查询到需要填充的元素数是负的，显然已经过度填充了。 最后HashMap的总项目数是1536，显然不符合填充满1000的预期。 针对这个场景，我们可以举一个形象的例子。ConcurrentHashMap就像是一个大篮子，现在这个篮子里有900个桔子，我们期望把这个篮子装满1000个桔子，也就是再装100个桔子。有10个工人来干这件事儿，大家先后到岗后会计算还需要补多少个桔子进去，最后把桔子装入篮子。 ConcurrentHashMap这个篮子本身，可以确保多个工人在装东西进去时，不会相互影响干扰，但无法确保工人A看到还需要装100个桔子但是还未装的时候，工人B就看不到篮子中的桔子数量。更值得注意的是，你往这个篮子装100个桔子的操作不是原子性的，在别人看来可能会有一个瞬间篮子里有964个桔子，还需要补36个桔子。 回到ConcurrentHashMap，我们需要注意ConcurrentHashMap对外提供的方法或能力的限制： 使用了ConcurrentHashMap，不代表对它的多个操作之间的状态是一致的，是没有其他线程在操作它的，如果需要确保需要手动加锁。 诸如size、isEmpty和containsValue等聚合方法，在并发情况下可能会反映ConcurrentHashMap的中间状态。因此在并发情况下，这些方法的返回值只能用作参考，而不能用于流程控制。显然，利用size方法计算差异值，是一个流程控制。 诸如putAll这样的聚合方法也不能确保原子性，在putAll的过程中去获取数据可能会获取到部分数据。 代码的修改方案很简单，整段逻辑加锁即可： @GetMapping(&quot;right&quot;) public String right() throws InterruptedException &#123; ConcurrentHashMap&lt;String, Long&gt; concurrentHashMap &#x3D; getData(ITEM_COUNT - 100); log.info(&quot;init size:&#123;&#125;&quot;, concurrentHashMap.size()); ForkJoinPool forkJoinPool &#x3D; new ForkJoinPool(THREAD_COUNT); forkJoinPool.execute(() -&gt; IntStream.rangeClosed(1, 10).parallel().forEach(i -&gt; &#123; &#x2F;&#x2F;下面的这段复合逻辑需要锁一下这个ConcurrentHashMap synchronized (concurrentHashMap) &#123; int gap &#x3D; ITEM_COUNT - concurrentHashMap.size(); log.info(&quot;gap size:&#123;&#125;&quot;, gap); concurrentHashMap.putAll(getData(gap)); &#125; &#125;)); forkJoinPool.shutdown(); forkJoinPool.awaitTermination(1, TimeUnit.HOURS); log.info(&quot;finish size:&#123;&#125;&quot;, concurrentHashMap.size()); return &quot;OK&quot;; &#125; 重新调用接口，程序的日志输出结果符合预期： 可以看到，只有一个线程查询到了需要补100个元素，其他9个线程查询到不需要补元素，最后Map大小为1000。 到了这里，你可能又要问了，使用ConcurrentHashMap全程加锁，还不如使用普通的HashMap呢。 其实不完全是这样。 ConcurrentHashMap提供了一些原子性的简单复合逻辑方法，用好这些方法就可以发挥其威力。这就引申出代码中常见的另一个问题：在使用一些类库提供的高级工具类时，开发人员可能还是按照旧的方式去使用这些新类，因为没有使用其特性，所以无法发挥其威力。 没有充分了解并发工具的特性，从而无法发挥其威力我们来看一个使用Map来统计Key出现次数的场景吧，这个逻辑在业务代码中非常常见。 使用ConcurrentHashMap来统计，Key的范围是10。 使用最多10个并发，循环操作1000万次，每次操作累加随机的Key。 如果Key不存在的话，首次设置值为1。 代码如下： &#x2F;&#x2F;循环次数 private static int LOOP_COUNT &#x3D; 10000000; &#x2F;&#x2F;线程数量 private static int THREAD_COUNT &#x3D; 10; &#x2F;&#x2F;元素数量 private static int ITEM_COUNT &#x3D; 10; private Map&lt;String, Long&gt; normaluse() throws InterruptedException &#123; ConcurrentHashMap&lt;String, Long&gt; freqs &#x3D; new ConcurrentHashMap&lt;&gt;(ITEM_COUNT); ForkJoinPool forkJoinPool &#x3D; new ForkJoinPool(THREAD_COUNT); forkJoinPool.execute(() -&gt; IntStream.rangeClosed(1, LOOP_COUNT).parallel().forEach(i -&gt; &#123; &#x2F;&#x2F;获得一个随机的Key String key &#x3D; &quot;item&quot; + ThreadLocalRandom.current().nextInt(ITEM_COUNT); synchronized (freqs) &#123; if (freqs.containsKey(key)) &#123; &#x2F;&#x2F;Key存在则+1 freqs.put(key, freqs.get(key) + 1); &#125; else &#123; &#x2F;&#x2F;Key不存在则初始化为1 freqs.put(key, 1L); &#125; &#125; &#125; )); forkJoinPool.shutdown(); forkJoinPool.awaitTermination(1, TimeUnit.HOURS); return freqs; &#125; 我们吸取之前的教训，直接通过锁的方式锁住Map，然后做判断、读取现在的累计值、加1、保存累加后值的逻辑。这段代码在功能上没有问题，但无法充分发挥ConcurrentHashMap的威力，改进后的代码如下： private Map&lt;String, Long&gt; gooduse() throws InterruptedException &#123; ConcurrentHashMap&lt;String, LongAdder&gt; freqs &#x3D; new ConcurrentHashMap&lt;&gt;(ITEM_COUNT); ForkJoinPool forkJoinPool &#x3D; new ForkJoinPool(THREAD_COUNT); forkJoinPool.execute(() -&gt; IntStream.rangeClosed(1, LOOP_COUNT).parallel().forEach(i -&gt; &#123; String key &#x3D; &quot;item&quot; + ThreadLocalRandom.current().nextInt(ITEM_COUNT); &#x2F;&#x2F;利用computeIfAbsent()方法来实例化LongAdder，然后利用LongAdder来进行线程安全计数 freqs.computeIfAbsent(key, k -&gt; new LongAdder()).increment(); &#125; )); forkJoinPool.shutdown(); forkJoinPool.awaitTermination(1, TimeUnit.HOURS); &#x2F;&#x2F;因为我们的Value是LongAdder而不是Long，所以需要做一次转换才能返回 return freqs.entrySet().stream() .collect(Collectors.toMap( e -&gt; e.getKey(), e -&gt; e.getValue().longValue()) ); &#125; 在这段改进后的代码中，我们巧妙利用了下面两点： 使用ConcurrentHashMap的原子性方法computeIfAbsent来做复合逻辑操作，判断Key是否存在Value，如果不存在则把Lambda表达式运行后的结果放入Map作为Value，也就是新创建一个LongAdder对象，最后返回Value。 由于computeIfAbsent方法返回的Value是LongAdder，是一个线程安全的累加器，因此可以直接调用其increment方法进行累加。 这样在确保线程安全的情况下达到极致性能，把之前7行代码替换为了1行。 我们通过一个简单的测试比较一下修改前后两段代码的性能： @GetMapping(&quot;good&quot;) public String good() throws InterruptedException &#123; StopWatch stopWatch &#x3D; new StopWatch(); stopWatch.start(&quot;normaluse&quot;); Map&lt;String, Long&gt; normaluse &#x3D; normaluse(); stopWatch.stop(); &#x2F;&#x2F;校验元素数量 Assert.isTrue(normaluse.size() &#x3D;&#x3D; ITEM_COUNT, &quot;normaluse size error&quot;); &#x2F;&#x2F;校验累计总数 Assert.isTrue(normaluse.entrySet().stream() .mapToLong(item -&gt; item.getValue()).reduce(0, Long::sum) &#x3D;&#x3D; LOOP_COUNT , &quot;normaluse count error&quot;); stopWatch.start(&quot;gooduse&quot;); Map&lt;String, Long&gt; gooduse &#x3D; gooduse(); stopWatch.stop(); Assert.isTrue(gooduse.size() &#x3D;&#x3D; ITEM_COUNT, &quot;gooduse size error&quot;); Assert.isTrue(gooduse.entrySet().stream() .mapToLong(item -&gt; item.getValue()) .reduce(0, Long::sum) &#x3D;&#x3D; LOOP_COUNT , &quot;gooduse count error&quot;); log.info(stopWatch.prettyPrint()); return &quot;OK&quot;; &#125; 这段测试代码并无特殊之处，使用StopWatch来测试两段代码的性能，最后跟了一个断言判断Map中元素的个数以及所有Value的和，是否符合预期来校验代码的正确性。测试结果如下： 可以看到，优化后的代码，相比使用锁来操作ConcurrentHashMap的方式，性能提升了10倍。 你可能会问，computeIfAbsent为什么如此高效呢？ 答案就在源码最核心的部分，也就是Java自带的Unsafe实现的CAS。它在虚拟机层面确保了写入数据的原子性，比加锁的效率高得多： static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSetObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); &#125; 像ConcurrentHashMap这样的高级并发工具的确提供了一些高级API，只有充分了解其特性才能最大化其威力，而不能因为其足够高级、酷炫盲目使用。 没有认清并发工具的使用场景，因而导致性能问题除了ConcurrentHashMap这样通用的并发工具类之外，我们的工具包中还有些针对特殊场景实现的生面孔。一般来说，针对通用场景的通用解决方案，在所有场景下性能都还可以，属于“万金油”；而针对特殊场景的特殊实现，会有比通用解决方案更高的性能，但一定要在它针对的场景下使用，否则可能会产生性能问题甚至是Bug。 之前在排查一个生产性能问题时，我们发现一段简单的非数据库操作的业务逻辑，消耗了超出预期的时间，在修改数据时操作本地缓存比回写数据库慢许多。查看代码发现，开发同学使用了CopyOnWriteArrayList来缓存大量的数据，而数据变化又比较频繁。 CopyOnWrite是一个时髦的技术，不管是Linux还是Redis都会用到。在Java中，CopyOnWriteArrayList虽然是一个线程安全的ArrayList，但因为其实现方式是，每次修改数据时都会复制一份数据出来，所以有明显的适用场景，即读多写少或者说希望无锁读的场景。 如果我们要使用CopyOnWriteArrayList，那一定是因为场景需要而不是因为足够酷炫。如果读写比例均衡或者有大量写操作的话，使用CopyOnWriteArrayList的性能会非常糟糕。 我们写一段测试代码，来比较下使用CopyOnWriteArrayList和普通加锁方式ArrayList的读写性能吧。在这段代码中我们针对并发读和并发写分别写了一个测试方法，测试两者一定次数的写或读操作的耗时。 &#x2F;&#x2F;测试并发写的性能 @GetMapping(&quot;write&quot;) public Map testWrite() &#123; List&lt;Integer&gt; copyOnWriteArrayList &#x3D; new CopyOnWriteArrayList&lt;&gt;(); List&lt;Integer&gt; synchronizedList &#x3D; Collections.synchronizedList(new ArrayList&lt;&gt;()); StopWatch stopWatch &#x3D; new StopWatch(); int loopCount &#x3D; 100000; stopWatch.start(&quot;Write:copyOnWriteArrayList&quot;); &#x2F;&#x2F;循环100000次并发往CopyOnWriteArrayList写入随机元素 IntStream.rangeClosed(1, loopCount).parallel().forEach(__ -&gt; copyOnWriteArrayList.add(ThreadLocalRandom.current().nextInt(loopCount))); stopWatch.stop(); stopWatch.start(&quot;Write:synchronizedList&quot;); &#x2F;&#x2F;循环100000次并发往加锁的ArrayList写入随机元素 IntStream.rangeClosed(1, loopCount).parallel().forEach(__ -&gt; synchronizedList.add(ThreadLocalRandom.current().nextInt(loopCount))); stopWatch.stop(); log.info(stopWatch.prettyPrint()); Map result &#x3D; new HashMap(); result.put(&quot;copyOnWriteArrayList&quot;, copyOnWriteArrayList.size()); result.put(&quot;synchronizedList&quot;, synchronizedList.size()); return result; &#125; &#x2F;&#x2F;帮助方法用来填充List private void addAll(List&lt;Integer&gt; list) &#123; list.addAll(IntStream.rangeClosed(1, 1000000).boxed().collect(Collectors.toList())); &#125; &#x2F;&#x2F;测试并发读的性能 @GetMapping(&quot;read&quot;) public Map testRead() &#123; &#x2F;&#x2F;创建两个测试对象 List&lt;Integer&gt; copyOnWriteArrayList &#x3D; new CopyOnWriteArrayList&lt;&gt;(); List&lt;Integer&gt; synchronizedList &#x3D; Collections.synchronizedList(new ArrayList&lt;&gt;()); &#x2F;&#x2F;填充数据 addAll(copyOnWriteArrayList); addAll(synchronizedList); StopWatch stopWatch &#x3D; new StopWatch(); int loopCount &#x3D; 1000000; int count &#x3D; copyOnWriteArrayList.size(); stopWatch.start(&quot;Read:copyOnWriteArrayList&quot;); &#x2F;&#x2F;循环1000000次并发从CopyOnWriteArrayList随机查询元素 IntStream.rangeClosed(1, loopCount).parallel().forEach(__ -&gt; copyOnWriteArrayList.get(ThreadLocalRandom.current().nextInt(count))); stopWatch.stop(); stopWatch.start(&quot;Read:synchronizedList&quot;); &#x2F;&#x2F;循环1000000次并发从加锁的ArrayList随机查询元素 IntStream.range(0, loopCount).parallel().forEach(__ -&gt; synchronizedList.get(ThreadLocalRandom.current().nextInt(count))); stopWatch.stop(); log.info(stopWatch.prettyPrint()); Map result &#x3D; new HashMap(); result.put(&quot;copyOnWriteArrayList&quot;, copyOnWriteArrayList.size()); result.put(&quot;synchronizedList&quot;, synchronizedList.size()); return result; &#125; 运行程序可以看到，大量写的场景（10万次add操作），****CopyOnWriteArray几乎比同步的ArrayList慢一百倍： 而在大量读的场景下（100万次get操作），CopyOnWriteArray又比同步的ArrayList快五倍以上： 你可能会问，为何在大量写的场景下，CopyOnWriteArrayList会这么慢呢？ 答案就在源码中。以add方法为例，每次add时，都会用Arrays.copyOf创建一个新数组，频繁add时内存的申请释放消耗会很大： &#x2F;** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) *&#x2F; public boolean add(E e) &#123; synchronized (lock) &#123; Object[] elements &#x3D; getArray(); int len &#x3D; elements.length; Object[] newElements &#x3D; Arrays.copyOf(elements, len + 1); newElements[len] &#x3D; e; setArray(newElements); return true; &#125; &#125; 重点回顾今天，我主要与你分享了，开发人员使用并发工具来解决线程安全问题时容易犯的四类错。 一是，只知道使用并发工具，但并不清楚当前线程的来龙去脉，解决多线程问题却不了解线程。比如，使用ThreadLocal来缓存数据，以为ThreadLocal在线程之间做了隔离不会有线程安全问题，没想到线程重用导致数据串了。请务必记得，在业务逻辑结束之前清理ThreadLocal中的数据。 二是，误以为使用了并发工具就可以解决一切线程安全问题，期望通过把线程不安全的类替换为线程安全的类来一键解决问题。比如，认为使用了ConcurrentHashMap就可以解决线程安全问题，没对复合逻辑加锁导致业务逻辑错误。如果你希望在一整段业务逻辑中，对容器的操作都保持整体一致性的话，需要加锁处理。 三是，没有充分了解并发工具的特性，还是按照老方式使用新工具导致无法发挥其性能。比如，使用了ConcurrentHashMap，但没有充分利用其提供的基于CAS安全的方法，还是使用锁的方式来实现逻辑。你可以阅读一下ConcurrentHashMap的文档，看一下相关原子性操作API是否可以满足业务需求，如果可以则优先考虑使用。 四是，没有了解清楚工具的适用场景，在不合适的场景下使用了错误的工具导致性能更差。比如，没有理解CopyOnWriteArrayList的适用场景，把它用在了读写均衡或者大量写操作的场景下，导致性能问题。对于这种场景，你可以考虑是用普通的List。 其实，这四类坑之所以容易踩到，原因可以归结为，我们在使用并发工具的时候，并没有充分理解其可能存在的问题、适用场景等。所以最后，我还要和你分享两点建议： 一定要认真阅读官方文档（比如Oracle JDK文档）。充分阅读官方文档，理解工具的适用场景及其API的用法，并做一些小实验。了解之后再去使用，就可以避免大部分坑。 如果你的代码运行在多线程环境下，那么就会有并发问题，并发问题不那么容易重现，可能需要使用压力测试模拟并发场景，来发现其中的Bug或性能问题。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 今天我们多次用到了ThreadLocalRandom，你觉得是否可以把它的实例设置到静态变量中，在多线程情况下重用呢？ ConcurrentHashMap还提供了putIfAbsent方法，你能否通过查阅JDK文档，说说computeIfAbsent和putIfAbsent方法的区别？ 你在使用并发工具时，还遇到过其他坑吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/02、代码加锁：不要让“锁”事成为烦心事","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/02、代码加锁：不要让“锁”事成为烦心事/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/02%E3%80%81%E4%BB%A3%E7%A0%81%E5%8A%A0%E9%94%81%EF%BC%9A%E4%B8%8D%E8%A6%81%E8%AE%A9%E2%80%9C%E9%94%81%E2%80%9D%E4%BA%8B%E6%88%90%E4%B8%BA%E7%83%A6%E5%BF%83%E4%BA%8B/","excerpt":"","text":"02 | 代码加锁：不要让“锁”事成为烦心事作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。 在上一讲中，我与你介绍了使用并发容器等工具解决线程安全的误区。今天，我们来看看解决线程安全问题的另一种重要手段——锁，在使用上比较容易犯哪些错。 我先和你分享一个有趣的案例吧。有一天，一位同学在群里说“见鬼了，疑似遇到了一个JVM的Bug”，我们都很好奇是什么Bug。 于是，他贴出了这样一段代码：在一个类里有两个int类型的字段a和b，有一个add方法循环1万次对a和b进行++操作，有另一个compare方法，同样循环1万次判断a是否小于b，条件成立就打印a和b的值，并判断a&gt;b是否成立。 @Slf4j public class Interesting &#123; volatile int a &#x3D; 1; volatile int b &#x3D; 1; public void add() &#123; log.info(&quot;add start&quot;); for (int i &#x3D; 0; i &lt; 10000; i++) &#123; a++; b++; &#125; log.info(&quot;add done&quot;); &#125; public void compare() &#123; log.info(&quot;compare start&quot;); for (int i &#x3D; 0; i &lt; 10000; i++) &#123; &#x2F;&#x2F;a始终等于b吗？ if (a &lt; b) &#123; log.info(&quot;a:&#123;&#125;,b:&#123;&#125;,&#123;&#125;&quot;, a, b, a &gt; b); &#x2F;&#x2F;最后的a&gt;b应该始终是false吗？ &#125; &#125; log.info(&quot;compare done&quot;); &#125; &#125; 他起了两个线程来分别执行add和compare方法： Interesting interesting &#x3D; new Interesting(); new Thread(() -&gt; interesting.add()).start(); new Thread(() -&gt; interesting.compare()).start(); 按道理，a和b同样进行累加操作，应该始终相等，compare中的第一次判断应该始终不会成立，不会输出任何日志。但，执行代码后发现不但输出了日志，而且更诡异的是，compare方法在判断a&lt;b成立的情况下还输出了a&gt;b也成立： 群里一位同学看到这个问题笑了，说：“这哪是JVM的Bug，分明是线程安全问题嘛。很明显，你这是在操作两个字段a和b，有线程安全问题，应该为add方法加上锁，确保a和b的++是原子性的，就不会错乱了。”随后，他为add方法加上了锁： public synchronized void add() 但，加锁后问题并没有解决。 我们来仔细想一下，为什么锁可以解决线程安全问题呢。因为只有一个线程可以拿到锁，所以加锁后的代码中的资源操作是线程安全的。但是，这个案例中的add方法始终只有一个线程在操作，显然只为add方法加锁是没用的。 之所以出现这种错乱，是因为两个线程是交错执行add和compare方法中的业务逻辑，而且这些业务逻辑不是原子性的：a++和b++操作中可以穿插在compare方法的比较代码中；更需要注意的是，a&lt;b这种比较操作在字节码层面是加载a、加载b和比较三步，代码虽然是一行但也不是原子性的。 所以，正确的做法应该是，为add和compare都加上方法锁，确保add方法执行时，compare无法读取a和b： public synchronized void add() public synchronized void compare() 所以，使用锁解决问题之前一定要理清楚，我们要保护的是什么逻辑，多线程执行的情况又是怎样的。 加锁前要清楚锁和被保护的对象是不是一个层面的除了没有分析清线程、业务逻辑和锁三者之间的关系随意添加无效的方法锁外，还有一种比较常见的错误是，没有理清楚锁和要保护的对象是否是一个层面的。 我们知道静态字段属于类，类级别的锁才能保护；而非静态字段属于类实例，实例级别的锁就可以保护。 先看看这段代码有什么问题：在类Data中定义了一个静态的int字段counter和一个非静态的wrong方法，实现counter字段的累加操作。 class Data &#123; @Getter private static int counter &#x3D; 0; public static int reset() &#123; counter &#x3D; 0; return counter; &#125; public synchronized void wrong() &#123; counter++; &#125; &#125; 写一段代码测试下： @GetMapping(&quot;wrong&quot;) public int wrong(@RequestParam(value &#x3D; &quot;count&quot;, defaultValue &#x3D; &quot;1000000&quot;) int count) &#123; Data.reset(); &#x2F;&#x2F;多线程循环一定次数调用Data类不同实例的wrong方法 IntStream.rangeClosed(1, count).parallel().forEach(i -&gt; new Data().wrong()); return Data.getCounter(); &#125; 因为默认运行100万次，所以执行后应该输出100万，但页面输出的是639242： 我们来分析下为什么会出现这个问题吧。 在非静态的wrong方法上加锁，只能确保多个线程无法执行同一个实例的wrong方法，却不能保证不会执行不同实例的wrong方法。而静态的counter在多个实例中共享，所以必然会出现线程安全问题。 理清思路后，修正方法就很清晰了：同样在类中定义一个Object类型的静态字段，在操作counter之前对这个字段加锁。 class Data &#123; @Getter private static int counter &#x3D; 0; private static Object locker &#x3D; new Object(); public void right() &#123; synchronized (locker) &#123; counter++; &#125; &#125; &#125; 你可能要问了，把wrong方法定义为静态不就可以了，这个时候锁是类级别的。可以是可以，但我们不可能为了解决线程安全问题改变代码结构，把实例方法改为静态方法。 感兴趣的同学还可以从字节码以及JVM的层面继续探索一下，代码块级别的synchronized和方法上标记synchronized关键字，在实现上有什么区别。 加锁要考虑锁的粒度和场景问题在方法上加synchronized关键字实现加锁确实简单，也因此我曾看到一些业务代码中几乎所有方法都加了synchronized，但这种滥用synchronized的做法： 一是，没必要。通常情况下60%的业务代码是三层架构，数据经过无状态的Controller、Service、Repository流转到数据库，没必要使用synchronized来保护什么数据。 二是，可能会极大地降低性能。使用Spring框架时，默认情况下Controller、Service、Repository是单例的，加上synchronized会导致整个程序几乎就只能支持单线程，造成极大的性能问题。 即使我们确实有一些共享资源需要保护，也要尽可能降低锁的粒度，仅对必要的代码块甚至是需要保护的资源本身加锁。 比如，在业务代码中，有一个ArrayList因为会被多个线程操作而需要保护，又有一段比较耗时的操作（代码中的slow方法）不涉及线程安全问题，应该如何加锁呢？ 错误的做法是，给整段业务逻辑加锁，把slow方法和操作ArrayList的代码同时纳入synchronized代码块；更合适的做法是，把加锁的粒度降到最低，只在操作ArrayList的时候给这个ArrayList加锁。 private List&lt;Integer&gt; data &#x3D; new ArrayList&lt;&gt;(); &#x2F;&#x2F;不涉及共享资源的慢方法 private void slow() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#x2F;&#x2F;错误的加锁方法 @GetMapping(&quot;wrong&quot;) public int wrong() &#123; long begin &#x3D; System.currentTimeMillis(); IntStream.rangeClosed(1, 1000).parallel().forEach(i -&gt; &#123; &#x2F;&#x2F;加锁粒度太粗了 synchronized (this) &#123; slow(); data.add(i); &#125; &#125;); log.info(&quot;took:&#123;&#125;&quot;, System.currentTimeMillis() - begin); return data.size(); &#125; &#x2F;&#x2F;正确的加锁方法 @GetMapping(&quot;right&quot;) public int right() &#123; long begin &#x3D; System.currentTimeMillis(); IntStream.rangeClosed(1, 1000).parallel().forEach(i -&gt; &#123; slow(); &#x2F;&#x2F;只对List加锁 synchronized (data) &#123; data.add(i); &#125; &#125;); log.info(&quot;took:&#123;&#125;&quot;, System.currentTimeMillis() - begin); return data.size(); &#125; 执行这段代码，同样是1000次业务操作，正确加锁的版本耗时1.4秒，而对整个业务逻辑加锁的话耗时11秒。 如果精细化考虑了锁应用范围后，性能还无法满足需求的话，我们就要考虑另一个维度的粒度问题了，即：区分读写场景以及资源的访问冲突，考虑使用悲观方式的锁还是乐观方式的锁。 一般业务代码中，很少需要进一步考虑这两种更细粒度的锁，所以我只和你分享几个大概的结论，你可以根据自己的需求来考虑是否有必要进一步优化： 对于读写比例差异明显的场景，考虑使用ReentrantReadWriteLock细化区分读写锁，来提高性能。 如果你的JDK版本高于1.8、共享资源的冲突概率也没那么大的话，考虑使用StampedLock的乐观读的特性，进一步提高性能。 JDK里ReentrantLock和ReentrantReadWriteLock都提供了公平锁的版本，在没有明确需求的情况下不要轻易开启公平锁特性，在任务很轻的情况下开启公平锁可能会让性能下降上百倍。 多把锁要小心死锁问题刚才我们聊到锁的粒度够用就好，这就意味着我们的程序逻辑中有时会存在一些细粒度的锁。但一个业务逻辑如果涉及多把锁，容易产生死锁问题。 之前我遇到过这样一个案例：下单操作需要锁定订单中多个商品的库存，拿到所有商品的锁之后进行下单扣减库存操作，全部操作完成之后释放所有的锁。代码上线后发现，下单失败概率很高，失败后需要用户重新下单，极大影响了用户体验，还影响到了销量。 经排查发现是死锁引起的问题，背后原因是扣减库存的顺序不同，导致并发的情况下多个线程可能相互持有部分商品的锁，又等待其他线程释放另一部分商品的锁，于是出现了死锁问题。 接下来，我们剖析一下核心的业务代码。 首先，定义一个商品类型，包含商品名、库存剩余和商品的库存锁三个属性，每一种商品默认库存1000个；然后，初始化10个这样的商品对象来模拟商品清单： @Data @RequiredArgsConstructor static class Item &#123; final String name; &#x2F;&#x2F;商品名 int remaining &#x3D; 1000; &#x2F;&#x2F;库存剩余 @ToString.Exclude &#x2F;&#x2F;ToString不包含这个字段 ReentrantLock lock &#x3D; new ReentrantLock(); &#125; 随后，写一个方法模拟在购物车进行商品选购，每次从商品清单（items字段）中随机选购三个商品（为了逻辑简单，我们不考虑每次选购多个同类商品的逻辑，购物车中不体现商品数量）： private List&lt;Item&gt; createCart() &#123; return IntStream.rangeClosed(1, 3) .mapToObj(i -&gt; &quot;item&quot; + ThreadLocalRandom.current().nextInt(items.size())) .map(name -&gt; items.get(name)).collect(Collectors.toList()); &#125; 下单代码如下：先声明一个List来保存所有获得的锁，然后遍历购物车中的商品依次尝试获得商品的锁，最长等待10秒，获得全部锁之后再扣减库存；如果有无法获得锁的情况则解锁之前获得的所有锁，返回false下单失败。 private boolean createOrder(List&lt;Item&gt; order) &#123; &#x2F;&#x2F;存放所有获得的锁 List&lt;ReentrantLock&gt; locks &#x3D; new ArrayList&lt;&gt;(); for (Item item : order) &#123; try &#123; &#x2F;&#x2F;获得锁10秒超时 if (item.lock.tryLock(10, TimeUnit.SECONDS)) &#123; locks.add(item.lock); &#125; else &#123; locks.forEach(ReentrantLock::unlock); return false; &#125; &#125; catch (InterruptedException e) &#123; &#125; &#125; &#x2F;&#x2F;锁全部拿到之后执行扣减库存业务逻辑 try &#123; order.forEach(item -&gt; item.remaining--); &#125; finally &#123; locks.forEach(ReentrantLock::unlock); &#125; return true; &#125; 我们写一段代码测试这个下单操作。模拟在多线程情况下进行100次创建购物车和下单操作，最后通过日志输出成功的下单次数、总剩余的商品个数、100次下单耗时，以及下单完成后的商品库存明细： @GetMapping(&quot;wrong&quot;) public long wrong() &#123; long begin &#x3D; System.currentTimeMillis(); &#x2F;&#x2F;并发进行100次下单操作，统计成功次数 long success &#x3D; IntStream.rangeClosed(1, 100).parallel() .mapToObj(i -&gt; &#123; List&lt;Item&gt; cart &#x3D; createCart(); return createOrder(cart); &#125;) .filter(result -&gt; result) .count(); log.info(&quot;success:&#123;&#125; totalRemaining:&#123;&#125; took:&#123;&#125;ms items:&#123;&#125;&quot;, success, items.entrySet().stream().map(item -&gt; item.getValue().remaining).reduce(0, Integer::sum), System.currentTimeMillis() - begin, items); return success; &#125; 运行程序，输出如下日志： 可以看到，100次下单操作成功了65次，10种商品总计10000件，库存总计为9805，消耗了195件符合预期（65次下单成功，每次下单包含三件商品），总耗时50秒。 为什么会这样呢？ 使用JDK自带的VisualVM工具来跟踪一下，重新执行方法后不久就可以看到，线程Tab中提示了死锁问题，根据提示点击右侧线程Dump按钮进行线程抓取操作： 查看抓取出的线程栈，在页面中部可以看到如下日志： 显然，是出现了死锁，线程4在等待的一个锁被线程3持有，线程3在等待的另一把锁被线程4持有。 那为什么会有死锁问题呢？ 我们仔细回忆一下购物车添加商品的逻辑，随机添加了三种商品，假设一个购物车中的商品是item1和item2，另一个购物车中的商品是item2和item1，一个线程先获取到了item1的锁，同时另一个线程获取到了item2的锁，然后两个线程接下来要分别获取item2和item1的锁，这个时候锁已经被对方获取了，只能相互等待一直到10秒超时。 其实，避免死锁的方案很简单，为购物车中的商品排一下序，让所有的线程一定是先获取item1的锁然后获取item2的锁，就不会有问题了。所以，我只需要修改一行代码，对createCart获得的购物车按照商品名进行排序即可： @GetMapping(&quot;right&quot;) public long right() &#123; ... . long success &#x3D; IntStream.rangeClosed(1, 100).parallel() .mapToObj(i -&gt; &#123; List&lt;Item&gt; cart &#x3D; createCart().stream() .sorted(Comparator.comparing(Item::getName)) .collect(Collectors.toList()); return createOrder(cart); &#125;) .filter(result -&gt; result) .count(); ... return success; &#125; 测试一下right方法，不管执行多少次都是100次成功下单，而且性能相当高，达到了3000以上的TPS： 这个案例中，虽然产生了死锁问题，但因为尝试获取锁的操作并不是无限阻塞的，所以没有造成永久死锁，之后的改进就是避免循环等待，通过对购物车的商品进行排序来实现有顺序的加锁，避免循环等待。 重点回顾我们一起总结回顾下，使用锁来解决多线程情况下线程安全问题的坑吧。 第一，使用synchronized加锁虽然简单，但我们首先要弄清楚共享资源是类还是实例级别的、会被哪些线程操作，synchronized关联的锁对象或方法又是什么范围的。 第二，加锁尽可能要考虑粒度和场景，锁保护的代码意味着无法进行多线程操作。对于Web类型的天然多线程项目，对方法进行大范围加锁会显著降级并发能力，要考虑尽可能地只为必要的代码块加锁，降低锁的粒度；而对于要求超高性能的业务，还要细化考虑锁的读写场景，以及悲观优先还是乐观优先，尽可能针对明确场景精细化加锁方案，可以在适当的场景下考虑使用ReentrantReadWriteLock、StampedLock等高级的锁工具类。 第三，业务逻辑中有多把锁时要考虑死锁问题，通常的规避方案是，避免无限等待和循环等待。 此外，如果业务逻辑中锁的实现比较复杂的话，要仔细看看加锁和释放是否配对，是否有遗漏释放或重复释放的可能性；并且对于分布式锁要考虑锁自动超时释放了，而业务逻辑却还在进行的情况下，如果别的线线程或进程拿到了相同的锁，可能会导致重复执行。 为演示方便，今天的案例是在Controller的逻辑中开新的线程或使用线程池进行并发模拟，我们当然可以意识到哪些对象是并发操作的。但对于Web应用程序的天然多线程场景，你可能更容易忽略这点，并且也可能因为误用锁降低应用整体的吞吐量。如果你的业务代码涉及复杂的锁操作，强烈建议Mock相关外部接口或数据库操作后对应用代码进行压测，通过压测排除锁误用带来的性能问题和死锁问题。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 本文开头的例子里，变量a、b都使用了volatile关键字，你知道原因吗？我之前遇到过这样一个坑：我们开启了一个线程无限循环来跑一些任务，有一个bool类型的变量来控制循环的退出，默认为true代表执行，一段时间后主线程将这个变量设置为了false。如果这个变量不是volatile修饰的，子线程可以退出吗？你能否解释其中的原因呢？ 文末我们又提了两个坑，一是加锁和释放没有配对的问题，二是锁自动释放导致的重复逻辑执行的问题。你有什么方法来发现和解决这两种问题吗？ 在使用锁的过程中，你还遇到过其他坑吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/03、线程池：业务代码最常用也最容易犯错的组件","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/03、线程池：业务代码最常用也最容易犯错的组件/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/03%E3%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0%EF%BC%9A%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E6%9C%80%E5%B8%B8%E7%94%A8%E4%B9%9F%E6%9C%80%E5%AE%B9%E6%98%93%E7%8A%AF%E9%94%99%E7%9A%84%E7%BB%84%E4%BB%B6/","excerpt":"","text":"03 | 线程池：业务代码最常用也最容易犯错的组件作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来讲讲使用线程池需要注意的一些问题。 在程序中，我们会用各种池化技术来缓存创建昂贵的对象，比如线程池、连接池、内存池。一般是预先创建一些对象放入池中，使用的时候直接取出使用，用完归还以便复用，还会通过一定的策略调整池中缓存对象的数量，实现池的动态伸缩。 由于线程的创建比较昂贵，随意、没有控制地创建大量线程会造成性能问题，因此短平快的任务一般考虑使用线程池来处理，而不是直接创建线程。 今天，我们就针对线程池这个话题展开讨论，通过三个生产事故，来看看使用线程池应该注意些什么。 线程池的声明需要手动进行Java中的Executors类定义了一些快捷的工具方法，来帮助我们快速创建线程池。《阿里巴巴Java开发手册》中提到，禁止使用这些方法来创建线程池，而应该手动new ThreadPoolExecutor来创建线程池。这一条规则的背后，是大量血淋淋的生产事故，最典型的就是newFixedThreadPool和newCachedThreadPool，可能因为资源耗尽导致OOM问题。 首先，我们来看一下newFixedThreadPool为什么可能会出现OOM的问题。 我们写一段测试代码，来初始化一个单线程的FixedThreadPool，循环1亿次向线程池提交任务，每个任务都会创建一个比较大的字符串然后休眠一小时： @GetMapping(&quot;oom1&quot;) public void oom1() throws InterruptedException &#123; ThreadPoolExecutor threadPool &#x3D; (ThreadPoolExecutor) Executors.newFixedThreadPool(1); &#x2F;&#x2F;打印线程池的信息，稍后我会解释这段代码 printStats(threadPool); for (int i &#x3D; 0; i &lt; 100000000; i++) &#123; threadPool.execute(() -&gt; &#123; String payload &#x3D; IntStream.rangeClosed(1, 1000000) .mapToObj(__ -&gt; &quot;a&quot;) .collect(Collectors.joining(&quot;&quot;)) + UUID.randomUUID().toString(); try &#123; TimeUnit.HOURS.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; log.info(payload); &#125;); &#125; threadPool.shutdown(); threadPool.awaitTermination(1, TimeUnit.HOURS); &#125; 执行程序后不久，日志中就出现了如下OOM： Exception in thread &quot;http-nio-45678-ClientPoller&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded 翻看newFixedThreadPool方法的源码不难发现，线程池的工作队列直接new了一个LinkedBlockingQueue，而默认构造方法的LinkedBlockingQueue是一个Integer.MAX_VALUE长度的队列，可以认为是无界的： public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); &#125; public class LinkedBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123; ... &#x2F;** * Creates a &#123;@code LinkedBlockingQueue&#125; with a capacity of * &#123;@link Integer#MAX_VALUE&#125;. *&#x2F; public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE); &#125; ... &#125; 虽然使用newFixedThreadPool可以把工作线程控制在固定的数量上，但任务队列是无界的。如果任务较多并且执行较慢的话，队列可能会快速积压，撑爆内存导致OOM。 我们再把刚才的例子稍微改一下，改为使用newCachedThreadPool方法来获得线程池。程序运行不久后，同样看到了如下OOM异常： [11:30:30.487] [http-nio-45678-exec-1] [ERROR] [.a.c.c.C.[.[.[&#x2F;].[dispatcherServlet]:175 ] - Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Handler dispatch failed; nested exception is java.lang.OutOfMemoryError: unable to create new native thread] with root cause java.lang.OutOfMemoryError: unable to create new native thread 从日志中可以看到，这次OOM的原因是无法创建线程，翻看newCachedThreadPool的源码可以看到，这种线程池的最大线程数是Integer.MAX_VALUE，可以认为是没有上限的，而其工作队列SynchronousQueue是一个没有存储空间的阻塞队列。这意味着，只要有请求到来，就必须找到一条工作线程来处理，如果当前没有空闲的线程就再创建一条新的。 由于我们的任务需要1小时才能执行完成，大量的任务进来后会创建大量的线程。我们知道线程是需要分配一定的内存空间作为线程栈的，比如1MB，因此无限制创建线程必然会导致OOM： public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); 其实，大部分Java开发同学知道这两种线程池的特性，只是抱有侥幸心理，觉得只是使用线程池做一些轻量级的任务，不可能造成队列积压或开启大量线程。 但，现实往往是残酷的。我之前就遇到过这么一个事故：用户注册后，我们调用一个外部服务去发送短信，发送短信接口正常时可以在100毫秒内响应，TPS 100的注册量，CachedThreadPool能稳定在占用10个左右线程的情况下满足需求。在某个时间点，外部短信服务不可用了，我们调用这个服务的超时又特别长，比如1分钟，1分钟可能就进来了6000用户，产生6000个发送短信的任务，需要6000个线程，没多久就因为无法创建线程导致了OOM，整个应用程序崩溃。 因此，我同样不建议使用Executors提供的两种快捷的线程池，原因如下： 我们需要根据自己的场景、并发情况来评估线程池的几个核心参数，包括核心线程数、最大线程数、线程回收策略、工作队列的类型，以及拒绝策略，确保线程池的工作行为符合需求，一般都需要设置有界的工作队列和可控的线程数。 任何时候，都应该为自定义线程池指定有意义的名称，以方便排查问题。当出现线程数量暴增、线程死锁、线程占用大量CPU、线程执行出现异常等问题时，我们往往会抓取线程栈。此时，有意义的线程名称，就可以方便我们定位问题。 除了建议手动声明线程池以外，我还建议用一些监控手段来观察线程池的状态。线程池这个组件往往会表现得任劳任怨、默默无闻，除非是出现了拒绝策略，否则压力再大都不会抛出一个异常。如果我们能提前观察到线程池队列的积压，或者线程数量的快速膨胀，往往可以提早发现并解决问题。 线程池线程管理策略详解在之前的Demo中，我们用一个printStats方法实现了最简陋的监控，每秒输出一次线程池的基本内部信息，包括线程数、活跃线程数、完成了多少任务，以及队列中还有多少积压任务等信息： private void printStats(ThreadPoolExecutor threadPool) &#123; Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; &#123; log.info(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;); log.info(&quot;Pool Size: &#123;&#125;&quot;, threadPool.getPoolSize()); log.info(&quot;Active Threads: &#123;&#125;&quot;, threadPool.getActiveCount()); log.info(&quot;Number of Tasks Completed: &#123;&#125;&quot;, threadPool.getCompletedTaskCount()); log.info(&quot;Number of Tasks in Queue: &#123;&#125;&quot;, threadPool.getQueue().size()); log.info(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;); &#125;, 0, 1, TimeUnit.SECONDS); &#125; 接下来，我们就利用这个方法来观察一下线程池的基本特性吧。 首先，自定义一个线程池。这个线程池具有2个核心线程、5个最大线程、使用容量为10的ArrayBlockingQueue阻塞队列作为工作队列，使用默认的AbortPolicy拒绝策略，也就是任务添加到线程池失败会抛出RejectedExecutionException。此外，我们借助了Jodd类库的ThreadFactoryBuilder方法来构造一个线程工厂，实现线程池线程的自定义命名。 然后，我们写一段测试代码来观察线程池管理线程的策略。测试代码的逻辑为，每次间隔1秒向线程池提交任务，循环20次，每个任务需要10秒才能执行完成，代码如下： @GetMapping(&quot;right&quot;) public int right() throws InterruptedException &#123; &#x2F;&#x2F;使用一个计数器跟踪完成的任务数 AtomicInteger atomicInteger &#x3D; new AtomicInteger(); &#x2F;&#x2F;创建一个具有2个核心线程、5个最大线程，使用容量为10的ArrayBlockingQueue阻塞队列作为工作队列的线程池，使用默认的AbortPolicy拒绝策略 ThreadPoolExecutor threadPool &#x3D; new ThreadPoolExecutor( 2, 5, 5, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10), new ThreadFactoryBuilder().setNameFormat(&quot;demo-threadpool-%d&quot;).get(), new ThreadPoolExecutor.AbortPolicy()); printStats(threadPool); &#x2F;&#x2F;每隔1秒提交一次，一共提交20次任务 IntStream.rangeClosed(1, 20).forEach(i -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; int id &#x3D; atomicInteger.incrementAndGet(); try &#123; threadPool.submit(() -&gt; &#123; log.info(&quot;&#123;&#125; started&quot;, id); &#x2F;&#x2F;每个任务耗时10秒 try &#123; TimeUnit.SECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; &#125; log.info(&quot;&#123;&#125; finished&quot;, id); &#125;); &#125; catch (Exception ex) &#123; &#x2F;&#x2F;提交出现异常的话，打印出错信息并为计数器减一 log.error(&quot;error submitting task &#123;&#125;&quot;, id, ex); atomicInteger.decrementAndGet(); &#125; &#125;); TimeUnit.SECONDS.sleep(60); return atomicInteger.intValue(); &#125; 60秒后页面输出了17，有3次提交失败了： 并且日志中也出现了3次类似的错误信息： [14:24:52.879] [http-nio-45678-exec-1] [ERROR] [.t.c.t.demo1.ThreadPoolOOMController:103 ] - error submitting task 18 java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@163a2dec rejected from java.util.concurrent.ThreadPoolExecutor@18061ad2[Running, pool size &#x3D; 5, active threads &#x3D; 5, queued tasks &#x3D; 10, completed tasks &#x3D; 2] 我们把printStats方法打印出的日志绘制成图表，得出如下曲线： 至此，我们可以总结出线程池默认的工作行为： 不会初始化corePoolSize个线程，有任务来了才创建工作线程； 当核心线程满了之后不会立即扩容线程池，而是把任务堆积到工作队列中； 当工作队列满了后扩容线程池，一直到线程个数达到maximumPoolSize为止； 如果队列已满且达到了最大线程后还有任务进来，按照拒绝策略处理； 当线程数大于核心线程数时，线程等待keepAliveTime后还是没有任务需要处理的话，收缩线程到核心线程数。 了解这个策略，有助于我们根据实际的容量规划需求，为线程池设置合适的初始化参数。当然，我们也可以通过一些手段来改变这些默认工作行为，比如： 声明线程池后立即调用prestartAllCoreThreads方法，来启动所有核心线程； 传入true给allowCoreThreadTimeOut方法，来让线程池在空闲的时候同样回收核心线程。 不知道你有没有想过：Java线程池是先用工作队列来存放来不及处理的任务，满了之后再扩容线程池。当我们的工作队列设置得很大时，最大线程数这个参数显得没有意义，因为队列很难满，或者到满的时候再去扩容线程池已经于事无补了。 那么，我们有没有办法让线程池****更激进一点，优先开启更多的线程，而把队列当成一个后备方案呢？比如我们这个例子，任务执行得很慢，需要10秒，如果线程池可以优先扩容到5个最大线程，那么这些任务最终都可以完成，而不会因为线程池扩容过晚导致慢任务来不及处理。 限于篇幅，这里我只给你一个大致思路： 由于线程池在工作队列满了无法入队的情况下会扩容线程池，那么我们是否可以重写队列的offer方法，造成这个队列已满的假象呢？ 由于我们Hack了队列，在达到了最大线程后势必会触发拒绝策略，那么能否实现一个自定义的拒绝策略处理程序，这个时候再把任务真正插入队列呢？ 接下来，就请你动手试试看如何实现这样一个“弹性”线程池吧。Tomcat线程池也实现了类似的效果，可供你借鉴。 务必确认清楚线程池本身是不是复用的不久之前我遇到了这样一个事故：某项目生产环境时不时有报警提示线程数过多，超过2000个，收到报警后查看监控发现，瞬时线程数比较多但过一会儿又会降下来，线程数抖动很厉害，而应用的访问量变化不大。 为了定位问题，我们在线程数比较高的时候进行线程栈抓取，抓取后发现内存中有1000多个自定义线程池。一般而言，线程池肯定是复用的，有5个以内的线程池都可以认为正常，而1000多个线程池肯定不正常。 在项目代码里，我们没有搜到声明线程池的地方，搜索execute关键字后定位到，原来是业务代码调用了一个类库来获得线程池，类似如下的业务代码：调用ThreadPoolHelper的getThreadPool方法来获得线程池，然后提交数个任务到线程池处理，看不出什么异常。 @GetMapping(&quot;wrong&quot;) public String wrong() throws InterruptedException &#123; ThreadPoolExecutor threadPool &#x3D; ThreadPoolHelper.getThreadPool(); IntStream.rangeClosed(1, 10).forEach(i -&gt; &#123; threadPool.execute(() -&gt; &#123; ... try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; &#125;); &#125;); return &quot;OK&quot;; &#125; 但是，来到ThreadPoolHelper的实现让人大跌眼镜，getThreadPool方法居然是每次都使用Executors.newCachedThreadPool来创建一个线程池。 class ThreadPoolHelper &#123; public static ThreadPoolExecutor getThreadPool() &#123; &#x2F;&#x2F;线程池没有复用 return (ThreadPoolExecutor) Executors.newCachedThreadPool(); &#125; &#125; 通过上一小节的学习，我们可以想到newCachedThreadPool会在需要时创建必要多的线程，业务代码的一次业务操作会向线程池提交多个慢任务，这样执行一次业务操作就会开启多个线程。如果业务操作并发量较大的话，的确有可能一下子开启几千个线程。 那，为什么我们能在监控中看到线程数量会下降，而不会撑爆内存呢？ 回到newCachedThreadPool的定义就会发现，它的核心线程数是0，而keepAliveTime是60秒，也就是在60秒之后所有的线程都是可以回收的。好吧，就因为这个特性，我们的业务程序死得没太难看。 要修复这个Bug也很简单，使用一个静态字段来存放线程池的引用，返回线程池的代码直接返回这个静态字段即可。这里一定要记得我们的最佳实践，手动创建线程池。修复后的ThreadPoolHelper类如下： class ThreadPoolHelper &#123; private static ThreadPoolExecutor threadPoolExecutor &#x3D; new ThreadPoolExecutor( 10, 50, 2, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(1000), new ThreadFactoryBuilder().setNameFormat(&quot;demo-threadpool-%d&quot;).get()); public static ThreadPoolExecutor getRightThreadPool() &#123; return threadPoolExecutor; &#125; &#125; 需要仔细斟酌线程池的混用策略线程池的意义在于复用，那这是不是意味着程序应该始终使用一个线程池呢？ 当然不是。通过第一小节的学习我们知道，要根据任务的“轻重缓急”来指定线程池的核心参数，包括线程数、回收策略和任务队列： 对于执行比较慢、数量不大的IO任务，或许要考虑更多的线程数，而不需要太大的队列。 而对于吞吐量较大的计算型任务，线程数量不宜过多，可以是CPU核数或核数*2（理由是，线程一定调度到某个CPU进行执行，如果任务本身是CPU绑定的任务，那么过多的线程只会增加线程切换的开销，并不能提升吞吐量），但可能需要较长的队列来做缓冲。 之前我也遇到过这么一个问题，业务代码使用了线程池异步处理一些内存中的数据，但通过监控发现处理得非常慢，整个处理过程都是内存中的计算不涉及IO操作，也需要数秒的处理时间，应用程序CPU占用也不是特别高，有点不可思议。 经排查发现，业务代码使用的线程池，还被一个后台的文件批处理任务用到了。 或许是够用就好的原则，这个线程池只有2个核心线程，最大线程也是2，使用了容量为100的ArrayBlockingQueue作为工作队列，使用了CallerRunsPolicy拒绝策略： private static ThreadPoolExecutor threadPool &#x3D; new ThreadPoolExecutor( 2, 2, 1, TimeUnit.HOURS, new ArrayBlockingQueue&lt;&gt;(100), new ThreadFactoryBuilder().setNameFormat(&quot;batchfileprocess-threadpool-%d&quot;).get(), new ThreadPoolExecutor.CallerRunsPolicy()); 这里，我们模拟一下文件批处理的代码，在程序启动后通过一个线程开启死循环逻辑，不断向线程池提交任务，任务的逻辑是向一个文件中写入大量的数据： @PostConstruct public void init() &#123; printStats(threadPool); new Thread(() -&gt; &#123; &#x2F;&#x2F;模拟需要写入的大量数据 String payload &#x3D; IntStream.rangeClosed(1, 1_000_000) .mapToObj(__ -&gt; &quot;a&quot;) .collect(Collectors.joining(&quot;&quot;)); while (true) &#123; threadPool.execute(() -&gt; &#123; try &#123; &#x2F;&#x2F;每次都是创建并写入相同的数据到相同的文件 Files.write(Paths.get(&quot;demo.txt&quot;), Collections.singletonList(LocalTime.now().toString() + &quot;:&quot; + payload), UTF_8, CREATE, TRUNCATE_EXISTING); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; log.info(&quot;batch file processing done&quot;); &#125;); &#125; &#125;).start(); &#125; 可以想象到，这个线程池中的2个线程任务是相当重的。通过printStats方法打印出的日志，我们观察下线程池的负担： 可以看到，线程池的2个线程始终处于活跃状态，队列也基本处于打满状态。因为开启了CallerRunsPolicy拒绝处理策略，所以当线程满载队列也满的情况下，任务会在提交任务的线程，或者说调用execute方法的线程执行，也就是说不能认为提交到线程池的任务就一定是异步处理的。如果使用了CallerRunsPolicy策略，那么有可能异步任务变为同步执行。从日志的第四行也可以看到这点。这也是这个拒绝策略比较特别的原因。 不知道写代码的同学为什么设置这个策略，或许是测试时发现线程池因为任务处理不过来出现了异常，而又不希望线程池丢弃任务，所以最终选择了这样的拒绝策略。不管怎样，这些日志足以说明线程池是饱和状态。 可以想象到，业务代码复用这样的线程池来做内存计算，命运一定是悲惨的。我们写一段代码测试下，向线程池提交一个简单的任务，这个任务只是休眠10毫秒没有其他逻辑： private Callable&lt;Integer&gt; calcTask() &#123; return () -&gt; &#123; TimeUnit.MILLISECONDS.sleep(10); return 1; &#125;; &#125; @GetMapping(&quot;wrong&quot;) public int wrong() throws ExecutionException, InterruptedException &#123; return threadPool.submit(calcTask()).get(); &#125; 我们使用wrk工具对这个接口进行一个简单的压测，可以看到TPS为75，性能的确非常差。 细想一下，问题其实没有这么简单。因为原来执行IO任务的线程池使用的是CallerRunsPolicy策略，所以直接使用这个线程池进行异步计算的话，当线程池饱和的时候，计算任务会在执行Web请求的Tomcat线程执行，这时就会进一步影响到其他同步处理的线程，甚至造成整个应用程序崩溃。 解决方案很简单，使用独立的线程池来做这样的“计算任务”即可。计算任务打了双引号，是因为我们的模拟代码执行的是休眠操作，并不属于CPU绑定的操作，更类似IO绑定的操作，如果线程池线程数设置太小会限制吞吐能力： private static ThreadPoolExecutor asyncCalcThreadPool &#x3D; new ThreadPoolExecutor( 200, 200, 1, TimeUnit.HOURS, new ArrayBlockingQueue&lt;&gt;(1000), new ThreadFactoryBuilder().setNameFormat(&quot;asynccalc-threadpool-%d&quot;).get()); @GetMapping(&quot;right&quot;) public int right() throws ExecutionException, InterruptedException &#123; return asyncCalcThreadPool.submit(calcTask()).get(); &#125; 使用单独的线程池改造代码后再来测试一下性能，TPS提高到了1727： 可以看到，盲目复用线程池混用线程的问题在于，别人定义的线程池属性不一定适合你的任务，而且混用会相互干扰。这就好比，我们往往会用虚拟化技术来实现资源的隔离，而不是让所有应用程序都直接使用物理机。 就线程池混用问题，我想再和你补充一个坑：Java 8的parallel stream功能，可以让我们很方便地并行处理集合中的元素，其背后是共享同一个ForkJoinPool，默认并行度是CPU核数-1。对于CPU绑定的任务来说，使用这样的配置比较合适，但如果集合操作涉及同步IO操作的话（比如数据库操作、外部服务调用等），建议自定义一个ForkJoinPool（或普通线程池）。你可以参考第一讲的相关Demo。 重点回顾线程池管理着线程，线程又属于宝贵的资源，有许多应用程序的性能问题都来自线程池的配置和使用不当。在今天的学习中，我通过三个和线程池相关的生产事故，和你分享了使用线程池的几个最佳实践。 第一，Executors类提供的一些快捷声明线程池的方法虽然简单，但隐藏了线程池的参数细节。因此，使用线程池时，我们一定要根据场景和需求配置合理的线程数、任务队列、拒绝策略、线程回收策略，并对线程进行明确的命名方便排查问题。 第二，既然使用了线程池就需要确保线程池是在复用的，每次new一个线程池出来可能比不用线程池还糟糕。如果你没有直接声明线程池而是使用其他同学提供的类库来获得一个线程池，请务必查看源码，以确认线程池的实例化方式和配置是符合预期的。 第三，复用线程池不代表应用程序始终使用同一个线程池，我们应该根据任务的性质来选用不同的线程池。特别注意IO绑定的任务和CPU绑定的任务对于线程池属性的偏好，如果希望减少任务间的相互干扰，考虑按需使用隔离的线程池。 最后我想强调的是，线程池作为应用程序内部的核心组件往往缺乏监控（如果你使用类似RabbitMQ这样的MQ中间件，运维同学一般会帮我们做好中间件监控），往往到程序崩溃后才发现线程池的问题，很被动。在设计篇中我们会重新谈及这个问题及其解决方案。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在第一节中我们提到，或许一个激进创建线程的弹性线程池更符合我们的需求，你能给出相关的实现吗？实现后再测试一下，是否所有的任务都可以正常处理完成呢？ 在第二节中，我们改进了ThreadPoolHelper使其能够返回复用的线程池。如果我们不小心每次都创建了这样一个自定义的线程池（10核心线程，50最大线程，2秒回收的），反复执行测试接口线程，最终可以被回收吗？会出现OOM问题吗？ 你还遇到过线程池相关的其他坑吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/04、连接池：别让连接池帮了倒忙","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/04、连接池：别让连接池帮了倒忙/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/04%E3%80%81%E8%BF%9E%E6%8E%A5%E6%B1%A0%EF%BC%9A%E5%88%AB%E8%AE%A9%E8%BF%9E%E6%8E%A5%E6%B1%A0%E5%B8%AE%E4%BA%86%E5%80%92%E5%BF%99/","excerpt":"","text":"04 | 连接池：别让连接池帮了倒忙作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我们来聊聊使用连接池需要注意的问题。 在上一讲，我们学习了使用线程池需要注意的问题。今天，我再与你说说另一种很重要的池化技术，即连接池。 我先和你说说连接池的结构。连接池一般对外提供获得连接、归还连接的接口给客户端使用，并暴露最小空闲连接数、最大连接数等可配置参数，在内部则实现连接建立、连接心跳保持、连接管理、空闲连接回收、连接可用性检测等功能。连接池的结构示意图，如下所示： 业务项目中经常会用到的连接池，主要是数据库连接池、Redis连接池和HTTP连接池。所以，今天我就以这三种连接池为例，和你聊聊使用和配置连接池容易出错的地方。 注意鉴别客户端SDK是否基于连接池在使用三方客户端进行网络通信时，我们首先要确定客户端SDK是否是基于连接池技术实现的。我们知道，TCP是面向连接的基于字节流的协议： 面向连接，意味着连接需要先创建再使用，创建连接的三次握手有一定开销； 基于字节流，意味着字节是发送数据的最小单元，TCP协议本身无法区分哪几个字节是完整的消息体，也无法感知是否有多个客户端在使用同一个TCP连接，TCP只是一个读写数据的管道。 如果客户端SDK没有使用连接池，而直接是TCP连接，那么就需要考虑每次建立TCP连接的开销，并且因为TCP基于字节流，在多线程的情况下对同一连接进行复用，可能会产生线程安全问题。 我们先看一下涉及TCP连接的客户端SDK，对外提供API的三种方式。在面对各种三方客户端的时候，只有先识别出其属于哪一种，才能理清楚使用方式。 连接池和连接分离的API：有一个XXXPool类负责连接池实现，先从其获得连接XXXConnection，然后用获得的连接进行服务端请求，完成后使用者需要归还连接。通常，XXXPool是线程安全的，可以并发获取和归还连接，而XXXConnection是非线程安全的。对应到连接池的结构示意图中，XXXPool就是右边连接池那个框，左边的客户端是我们自己的代码。 内部带有连接池的API：对外提供一个XXXClient类，通过这个类可以直接进行服务端请求；这个类内部维护了连接池，SDK使用者无需考虑连接的获取和归还问题。一般而言，XXXClient是线程安全的。对应到连接池的结构示意图中，整个API就是蓝色框包裹的部分。 非连接池的API：一般命名为XXXConnection，以区分其是基于连接池还是单连接的，而不建议命名为XXXClient或直接是XXX。直接连接方式的API基于单一连接，每次使用都需要创建和断开连接，性能一般，且通常不是线程安全的。对应到连接池的结构示意图中，这种形式相当于没有右边连接池那个框，客户端直接连接服务端创建连接。 虽然上面提到了SDK一般的命名习惯，但不排除有一些客户端特立独行，因此在使用三方SDK时，一定要先查看官方文档了解其最佳实践，或是在类似Stackoverflow的网站搜索XXX threadsafe&#x2F;singleton字样看看大家的回复，也可以一层一层往下看源码，直到定位到原始Socket来判断Socket和客户端API的对应关系。 明确了SDK连接池的实现方式后，我们就大概知道了使用SDK的最佳实践： 如果是分离方式，那么连接池本身一般是线程安全的，可以复用。每次使用需要从连接池获取连接，使用后归还，归还的工作由使用者负责。 如果是内置连接池，SDK会负责连接的获取和归还，使用的时候直接复用客户端。 如果SDK没有实现连接池（大多数中间件、数据库的客户端SDK都会支持连接池），那通常不是线程安全的，而且短连接的方式性能不会很高，使用的时候需要考虑是否自己封装一个连接池。 接下来，我就以Java中用于操作Redis最常见的库Jedis为例，从源码角度分析下Jedis类到底属于哪种类型的API，直接在多线程环境下复用一个连接会产生什么问题，以及如何用最佳实践来修复这个问题。 首先，向Redis初始化2组数据，Key&#x3D;a、Value&#x3D;1，Key&#x3D;b、Value&#x3D;2： @PostConstruct public void init() &#123; try (Jedis jedis &#x3D; new Jedis(&quot;127.0.0.1&quot;, 6379)) &#123; Assert.isTrue(&quot;OK&quot;.equals(jedis.set(&quot;a&quot;, &quot;1&quot;)), &quot;set a &#x3D; 1 return OK&quot;); Assert.isTrue(&quot;OK&quot;.equals(jedis.set(&quot;b&quot;, &quot;2&quot;)), &quot;set b &#x3D; 2 return OK&quot;); &#125; &#125; 然后，启动两个线程，共享操作同一个Jedis实例，每一个线程循环1000次，分别读取Key为a和b的Value，判断是否分别为1和2： Jedis jedis &#x3D; new Jedis(&quot;127.0.0.1&quot;, 6379); new Thread(() -&gt; &#123; for (int i &#x3D; 0; i &lt; 1000; i++) &#123; String result &#x3D; jedis.get(&quot;a&quot;); if (!result.equals(&quot;1&quot;)) &#123; log.warn(&quot;Expect a to be 1 but found &#123;&#125;&quot;, result); return; &#125; &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i &#x3D; 0; i &lt; 1000; i++) &#123; String result &#x3D; jedis.get(&quot;b&quot;); if (!result.equals(&quot;2&quot;)) &#123; log.warn(&quot;Expect b to be 2 but found &#123;&#125;&quot;, result); return; &#125; &#125; &#125;).start(); TimeUnit.SECONDS.sleep(5); 执行程序多次，可以看到日志中出现了各种奇怪的异常信息，有的是读取Key为b的Value读取到了1，有的是流非正常结束，还有的是连接关闭异常： &#x2F;&#x2F;错误1 [14:56:19.069] [Thread-28] [WARN ] [.t.c.c.redis.JedisMisreuseController:45 ] - Expect b to be 2 but found 1 &#x2F;&#x2F;错误2 redis.clients.jedis.exceptions.JedisConnectionException: Unexpected end of stream. at redis.clients.jedis.util.RedisInputStream.ensureFill(RedisInputStream.java:202) at redis.clients.jedis.util.RedisInputStream.readLine(RedisInputStream.java:50) at redis.clients.jedis.Protocol.processError(Protocol.java:114) at redis.clients.jedis.Protocol.process(Protocol.java:166) at redis.clients.jedis.Protocol.read(Protocol.java:220) at redis.clients.jedis.Connection.readProtocolWithCheckingBroken(Connection.java:318) at redis.clients.jedis.Connection.getBinaryBulkReply(Connection.java:255) at redis.clients.jedis.Connection.getBulkReply(Connection.java:245) at redis.clients.jedis.Jedis.get(Jedis.java:181) at org.geekbang.time.commonmistakes.connectionpool.redis.JedisMisreuseController.lambda$wrong$1(JedisMisreuseController.java:43) at java.lang.Thread.run(Thread.java:748) &#x2F;&#x2F;错误3 java.io.IOException: Socket Closed at java.net.AbstractPlainSocketImpl.getOutputStream(AbstractPlainSocketImpl.java:440) at java.net.Socket$3.run(Socket.java:954) at java.net.Socket$3.run(Socket.java:952) at java.security.AccessController.doPrivileged(Native Method) at java.net.Socket.getOutputStream(Socket.java:951) at redis.clients.jedis.Connection.connect(Connection.java:200) ... 7 more 让我们分析一下Jedis类的源码，搞清楚其中缘由吧。 public class Jedis extends BinaryJedis implements JedisCommands, MultiKeyCommands, AdvancedJedisCommands, ScriptingCommands, BasicCommands, ClusterCommands, SentinelCommands, ModuleCommands &#123; &#125; public class BinaryJedis implements BasicCommands, BinaryJedisCommands, MultiKeyBinaryCommands, AdvancedBinaryJedisCommands, BinaryScriptingCommands, Closeable &#123; protected Client client &#x3D; null; ... &#125; public class Client extends BinaryClient implements Commands &#123; &#125; public class BinaryClient extends Connection &#123; &#125; public class Connection implements Closeable &#123; private Socket socket; private RedisOutputStream outputStream; private RedisInputStream inputStream; &#125; 可以看到，Jedis继承了BinaryJedis，BinaryJedis中保存了单个Client的实例，Client最终继承了Connection，Connection中保存了单个Socket的实例，和Socket对应的两个读写流。因此，一个Jedis对应一个Socket连接。类图如下： BinaryClient封装了各种Redis命令，其最终会调用基类Connection的方法，使用Protocol类发送命令。看一下Protocol类的sendCommand方法的源码，可以发现其发送命令时是直接操作RedisOutputStream写入字节。 我们在多线程环境下复用Jedis对象，其实就是在复用RedisOutputStream。如果多个线程在执行操作，那么既无法确保整条命令以一个原子操作写入Socket，也无法确保写入后、读取前没有其他数据写到远端： private static void sendCommand(final RedisOutputStream os, final byte[] command, final byte[]... args) &#123; try &#123; os.write(ASTERISK_BYTE); os.writeIntCrLf(args.length + 1); os.write(DOLLAR_BYTE); os.writeIntCrLf(command.length); os.write(command); os.writeCrLf(); for (final byte[] arg : args) &#123; os.write(DOLLAR_BYTE); os.writeIntCrLf(arg.length); os.write(arg); os.writeCrLf(); &#125; &#125; catch (IOException e) &#123; throw new JedisConnectionException(e); &#125; &#125; 看到这里我们也可以理解了，为啥多线程情况下使用Jedis对象操作Redis会出现各种奇怪的问题。 比如，写操作互相干扰，多条命令相互穿插的话，必然不是合法的Redis命令，那么Redis会关闭客户端连接，导致连接断开；又比如，线程1和2先后写入了get a和get b操作的请求，Redis也返回了值1和2，但是线程2先读取了数据1就会出现数据错乱的问题。 修复方式是，使用Jedis提供的另一个线程安全的类JedisPool来获得Jedis的实例。JedisPool可以声明为static在多个线程之间共享，扮演连接池的角色。使用时，按需使用try-with-resources模式从JedisPool获得和归还Jedis实例。 private static JedisPool jedisPool &#x3D; new JedisPool(&quot;127.0.0.1&quot;, 6379); new Thread(() -&gt; &#123; try (Jedis jedis &#x3D; jedisPool.getResource()) &#123; for (int i &#x3D; 0; i &lt; 1000; i++) &#123; String result &#x3D; jedis.get(&quot;a&quot;); if (!result.equals(&quot;1&quot;)) &#123; log.warn(&quot;Expect a to be 1 but found &#123;&#125;&quot;, result); return; &#125; &#125; &#125; &#125;).start(); new Thread(() -&gt; &#123; try (Jedis jedis &#x3D; jedisPool.getResource()) &#123; for (int i &#x3D; 0; i &lt; 1000; i++) &#123; String result &#x3D; jedis.get(&quot;b&quot;); if (!result.equals(&quot;2&quot;)) &#123; log.warn(&quot;Expect b to be 2 but found &#123;&#125;&quot;, result); return; &#125; &#125; &#125; &#125;).start(); 这样修复后，代码不再有线程安全问题了。此外，我们最好通过shutdownhook，在程序退出之前关闭JedisPool： @PostConstruct public void init() &#123; Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; jedisPool.close(); &#125;)); &#125; 看一下Jedis类close方法的实现可以发现，如果Jedis是从连接池获取的话，那么close方法会调用连接池的return方法归还连接： public class Jedis extends BinaryJedis implements JedisCommands, MultiKeyCommands, AdvancedJedisCommands, ScriptingCommands, BasicCommands, ClusterCommands, SentinelCommands, ModuleCommands &#123; protected JedisPoolAbstract dataSource &#x3D; null; @Override public void close() &#123; if (dataSource !&#x3D; null) &#123; JedisPoolAbstract pool &#x3D; this.dataSource; this.dataSource &#x3D; null; if (client.isBroken()) &#123; pool.returnBrokenResource(this); &#125; else &#123; pool.returnResource(this); &#125; &#125; else &#123; super.close(); &#125; &#125; &#125; 如果不是，则直接关闭连接，其最终调用Connection类的disconnect方法来关闭TCP连接： public void disconnect() &#123; if (isConnected()) &#123; try &#123; outputStream.flush(); socket.close(); &#125; catch (IOException ex) &#123; broken &#x3D; true; throw new JedisConnectionException(ex); &#125; finally &#123; IOUtils.closeQuietly(socket); &#125; &#125; &#125; 可以看到，Jedis可以独立使用，也可以配合连接池使用，这个连接池就是JedisPool。我们再看看JedisPool的实现。 public class JedisPool extends JedisPoolAbstract &#123; @Override public Jedis getResource() &#123; Jedis jedis &#x3D; super.getResource(); jedis.setDataSource(this); return jedis; &#125; @Override protected void returnResource(final Jedis resource) &#123; if (resource !&#x3D; null) &#123; try &#123; resource.resetState(); returnResourceObject(resource); &#125; catch (Exception e) &#123; returnBrokenResource(resource); throw new JedisException(&quot;Resource is returned to the pool as broken&quot;, e); &#125; &#125; &#125; &#125; public class JedisPoolAbstract extends Pool&lt;Jedis&gt; &#123; &#125; public abstract class Pool&lt;T&gt; implements Closeable &#123; protected GenericObjectPool&lt;T&gt; internalPool; &#125; JedisPool的getResource方法在拿到Jedis对象后，将自己设置为了连接池。连接池JedisPool，继承了JedisPoolAbstract，而后者继承了抽象类Pool，Pool内部维护了Apache Common的通用池GenericObjectPool。JedisPool的连接池就是基于GenericObjectPool的。 看到这里我们了解了，Jedis的API实现是我们说的三种类型中的第一种，也就是连接池和连接分离的API，JedisPool是线程安全的连接池，Jedis是非线程安全的单一连接。知道了原理之后，我们再使用Jedis就胸有成竹了。 使用连接池务必确保复用在介绍线程池的时候我们强调过，池一定是用来复用的，否则其使用代价会比每次创建单一对象更大。对连接池来说更是如此，原因如下： 创建连接池的时候很可能一次性创建了多个连接，大多数连接池考虑到性能，会在初始化的时候维护一定数量的最小连接（毕竟初始化连接池的过程一般是一次性的），可以直接使用。如果每次使用连接池都按需创建连接池，那么很可能你只用到一个连接，但是创建了N个连接。 连接池一般会有一些管理模块，也就是连接池的结构示意图中的绿色部分。举个例子，大多数的连接池都有闲置超时的概念。连接池会检测连接的闲置时间，定期回收闲置的连接，把活跃连接数降到最低（闲置）连接的配置值，减轻服务端的压力。一般情况下，闲置连接由独立线程管理，启动了空闲检测的连接池相当于还会启动一个线程。此外，有些连接池还需要独立线程负责连接保活等功能。因此，启动一个连接池相当于启动了N个线程。 除了使用代价，连接池不释放，还可能会引起线程泄露。接下来，我就以Apache HttpClient为例，和你说说连接池不复用的问题。 首先，创建一个CloseableHttpClient，设置使用PoolingHttpClientConnectionManager连接池并启用空闲连接驱逐策略，最大空闲时间为60秒，然后使用这个连接来请求一个会返回OK字符串的服务端接口： @GetMapping(&quot;wrong1&quot;) public String wrong1() &#123; CloseableHttpClient client &#x3D; HttpClients.custom() .setConnectionManager(new PoolingHttpClientConnectionManager()) .evictIdleConnections(60, TimeUnit.SECONDS).build(); try (CloseableHttpResponse response &#x3D; client.execute(new HttpGet(&quot;http:&#x2F;&#x2F;127.0.0.1:45678&#x2F;httpclientnotreuse&#x2F;test&quot;))) &#123; return EntityUtils.toString(response.getEntity()); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; return null; &#125; 访问这个接口几次后查看应用线程情况，可以看到有大量叫作Connection evictor的线程，且这些线程不会销毁： 对这个接口进行几秒的压测（压测使用wrk，1个并发1个连接）可以看到，已经建立了三千多个TCP连接到45678端口（其中有1个是压测客户端到Tomcat的连接，大部分都是HttpClient到Tomcat的连接）： 好在有了空闲连接回收的策略，60秒之后连接处于CLOSE_WAIT状态，最终彻底关闭。 这2点证明，CloseableHttpClient属于第二种模式，即内部带有连接池的API，其背后是连接池，最佳实践一定是复用。 复用方式很简单，你可以把CloseableHttpClient声明为static，只创建一次，并且在JVM关闭之前通过addShutdownHook钩子关闭连接池，在使用的时候直接使用CloseableHttpClient即可，无需每次都创建。 首先，定义一个right接口来实现服务端接口调用： private static CloseableHttpClient httpClient &#x3D; null; static &#123; &#x2F;&#x2F;当然，也可以把CloseableHttpClient定义为Bean，然后在@PreDestroy标记的方法内close这个HttpClient httpClient &#x3D; HttpClients.custom().setMaxConnPerRoute(1).setMaxConnTotal(1).evictIdleConnections(60, TimeUnit.SECONDS).build(); Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; try &#123; httpClient.close(); &#125; catch (IOException ignored) &#123; &#125; &#125;)); &#125; @GetMapping(&quot;right&quot;) public String right() &#123; try (CloseableHttpResponse response &#x3D; httpClient.execute(new HttpGet(&quot;http:&#x2F;&#x2F;127.0.0.1:45678&#x2F;httpclientnotreuse&#x2F;test&quot;))) &#123; return EntityUtils.toString(response.getEntity()); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; return null; &#125; 然后，重新定义一个wrong2接口，修复之前按需创建CloseableHttpClient的代码，每次用完之后确保连接池可以关闭： @GetMapping(&quot;wrong2&quot;) public String wrong2() &#123; try (CloseableHttpClient client &#x3D; HttpClients.custom() .setConnectionManager(new PoolingHttpClientConnectionManager()) .evictIdleConnections(60, TimeUnit.SECONDS).build(); CloseableHttpResponse response &#x3D; client.execute(new HttpGet(&quot;http:&#x2F;&#x2F;127.0.0.1:45678&#x2F;httpclientnotreuse&#x2F;test&quot;))) &#123; return EntityUtils.toString(response.getEntity()); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; return null; &#125; 使用wrk对wrong2和right两个接口分别压测60秒，可以看到两种使用方式性能上的差异，每次创建连接池的QPS是337，而复用连接池的QPS是2022： 如此大的性能差异显然是因为TCP连接的复用。你可能注意到了，刚才定义连接池时，我将最大连接数设置为1。所以，复用连接池方式复用的始终应该是同一个连接，而新建连接池方式应该是每次都会创建新的TCP连接。 接下来，我们通过网络抓包工具Wireshark来证实这一点。 如果调用wrong2接口每次创建新的连接池来发起HTTP请求，从Wireshark可以看到，每次请求服务端45678的客户端端口都是新的。这里我发起了三次请求，程序通过HttpClient访问服务端45678的客户端端口号，分别是51677、51679和51681： 也就是说，每次都是新的TCP连接，放开HTTP这个过滤条件也可以看到完整的TCP握手、挥手的过程： 而复用连接池方式的接口right的表现就完全不同了。可以看到，第二次HTTP请求#41的客户端端口61468和第一次连接#23的端口是一样的，Wireshark也提示了整个TCP会话中，当前#41请求是第二次请求，前一次是#23，后面一次是#75： 只有TCP连接闲置超过60秒后才会断开，连接池会新建连接。你可以尝试通过Wireshark观察这一过程。 接下来，我们就继续聊聊连接池的配置问题。 连接池的配置不是一成不变的为方便根据容量规划设置连接处的属性，连接池提供了许多参数，包括最小（闲置）连接、最大连接、闲置连接生存时间、连接生存时间等。其中，最重要的参数是最大连接数，它决定了连接池能使用的连接数量上限，达到上限后，新来的请求需要等待其他请求释放连接。 但，最大连接数不是设置得越大越好。如果设置得太大，不仅仅是客户端需要耗费过多的资源维护连接，更重要的是由于服务端对应的是多个客户端，每一个客户端都保持大量的连接，会给服务端带来更大的压力。这个压力又不仅仅是内存压力，可以想一下如果服务端的网络模型是一个TCP连接一个线程，那么几千个连接意味着几千个线程，如此多的线程会造成大量的线程切换开销。 当然，连接池最大连接数设置得太小，很可能会因为获取连接的等待时间太长，导致吞吐量低下，甚至超时无法获取连接。 接下来，我们就模拟下压力增大导致数据库连接池打满的情况，来实践下如何确认连接池的使用情况，以及有针对性地进行参数优化。 首先，定义一个用户注册方法，通过@Transactional注解为方法开启事务。其中包含了500毫秒的休眠，一个数据库事务对应一个TCP连接，所以500多毫秒的时间都会占用数据库连接： @Transactional public User register()&#123; User user&#x3D;new User(); user.setName(&quot;new-user-&quot;+System.currentTimeMillis()); userRepository.save(user); try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return user; &#125; 随后，修改配置文件启用register-mbeans，使Hikari连接池能通过JMX MBean注册连接池相关统计信息，方便观察连接池： spring.datasource.hikari.register-mbeans&#x3D;true 启动程序并通过JConsole连接进程后，可以看到默认情况下最大连接数为10： 使用wrk对应用进行压测，可以看到连接数一下子从0到了10，有20个线程在等待获取连接： 不久就出现了无法获取数据库连接的异常，如下所示： [15:37:56.156] [http-nio-45678-exec-15] [ERROR] [.a.c.c.C.[.[.[&#x2F;].[dispatcherServlet]:175 ] - Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is org.springframework.dao.DataAccessResourceFailureException: unable to obtain isolated JDBC connection; nested exception is org.hibernate.exception.JDBCConnectionException: unable to obtain isolated JDBC connection] with root cause java.sql.SQLTransientConnectionException: HikariPool-1 - Connection is not available, request timed out after 30000ms. 从异常信息中可以看到，数据库连接池是HikariPool，解决方式很简单，修改一下配置文件，调整数据库连接池最大连接参数到50即可。 spring.datasource.hikari.maximum-pool-size&#x3D;50 然后，再观察一下这个参数是否适合当前压力，满足需求的同时也不占用过多资源。从监控来看这个调整是合理的，有一半的富余资源，再也没有线程需要等待连接了： 在这个Demo里，我知道压测大概能对应使用25左右的并发连接，所以直接把连接池最大连接设置为了50。在真实情况下，只要数据库可以承受，你可以选择在遇到连接超限的时候先设置一个足够大的连接数，然后观察最终应用的并发，再按照实际并发数留出一半的余量来设置最终的最大连接。 其实，看到错误日志后再调整已经有点儿晚了。更合适的做法是，对类似数据库连接池的重要资源进行持续检测，并设置一半的使用量作为报警阈值，出现预警后及时扩容。 在这里我是为了演示，才通过JConsole查看参数配置后的效果，生产上需要把相关数据对接到指标监控体系中持续监测。 这里要强调的是，修改配置参数务必验证是否生效，并且在监控系统中确认参数是否生效、是否合理。之所以要“强调”，是因为这里有坑。 我之前就遇到过这样一个事故。应用准备针对大促活动进行扩容，把数据库配置文件中Druid连接池最大连接数maxActive从50提高到了100，修改后并没有通过监控验证，结果大促当天应用因为连接池连接数不够爆了。 经排查发现，当时修改的连接数并没有生效。原因是，应用虽然一开始使用的是Druid连接池，但后来框架升级了，把连接池替换为了Hikari实现，原来的那些配置其实都是无效的，修改后的参数配置当然也不会生效。 所以说，对连接池进行调参，一定要眼见为实。 重点回顾今天，我以三种业务代码最常用的Redis连接池、HTTP连接池、数据库连接池为例，和你探讨了有关连接池实现方式、使用姿势和参数配置的三大问题。 客户端SDK实现连接池的方式，包括池和连接分离、内部带有连接池和非连接池三种。要正确使用连接池，就必须首先鉴别连接池的实现方式。比如，Jedis的API实现的是池和连接分离的方式，而Apache HttpClient是内置连接池的API。 对于使用姿势其实就是两点，一是确保连接池是复用的，二是尽可能在程序退出之前显式关闭连接池释放资源。连接池设计的初衷就是为了保持一定量的连接，这样连接可以随取随用。从连接池获取连接虽然很快，但连接池的初始化会比较慢，需要做一些管理模块的初始化以及初始最小闲置连接。一旦连接池不是复用的，那么其性能会比随时创建单一连接更差。 最后，连接池参数配置中，最重要的是最大连接数，许多高并发应用往往因为最大连接数不够导致性能问题。但，最大连接数不是设置得越大越好，够用就好。需要注意的是，针对数据库连接池、HTTP连接池、Redis连接池等重要连接池，务必建立完善的监控和报警机制，根据容量规划及时调整参数配置。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 有了连接池之后，获取连接是从连接池获取，没有足够连接时连接池会创建连接。这时，获取连接操作往往有两个超时时间：一个是从连接池获取连接的最长等待时间，通常叫作请求连接超时connectRequestTimeout或连接等待超时connectWaitTimeout；一个是连接池新建TCP连接三次握手的连接超时，通常叫作连接超时connectTimeout。针对JedisPool、Apache HttpClient和Hikari数据库连接池，你知道如何设置这2个参数吗？ 对于带有连接池的SDK的使用姿势，最主要的是鉴别其内部是否实现了连接池，如果实现了连接池要尽量复用Client。对于NoSQL中的MongoDB来说，使用MongoDB Java驱动时，MongoClient类应该是每次都创建还是复用呢？你能否在官方文档中找到答案呢？ 关于连接池，你还遇到过什么坑吗？我是朱晔，欢迎在评论区与我留言分享，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/05、HTTP调用：你考虑到超时、重试、并发了吗？","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/05、HTTP调用：你考虑到超时、重试、并发了吗？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/05%E3%80%81HTTP%E8%B0%83%E7%94%A8%EF%BC%9A%E4%BD%A0%E8%80%83%E8%99%91%E5%88%B0%E8%B6%85%E6%97%B6%E3%80%81%E9%87%8D%E8%AF%95%E3%80%81%E5%B9%B6%E5%8F%91%E4%BA%86%E5%90%97%EF%BC%9F/","excerpt":"","text":"05 | HTTP调用：你考虑到超时、重试、并发了吗？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我们一起聊聊进行HTTP调用需要注意的超时、重试、并发等问题。 与执行本地方法不同，进行HTTP调用本质上是通过HTTP协议进行一次网络请求。网络请求必然有超时的可能性，因此我们必须考虑到这三点： 首先，框架设置的默认超时是否合理； 其次，考虑到网络的不稳定，超时后的请求重试是一个不错的选择，但需要考虑服务端接口的幂等性设计是否允许我们重试； 最后，需要考虑框架是否会像浏览器那样限制并发连接数，以免在服务并发很大的情况下，HTTP调用的并发数限制成为瓶颈。 Spring Cloud是Java微服务架构的代表性框架。如果使用Spring Cloud进行微服务开发，就会使用Feign进行声明式的服务调用。如果不使用Spring Cloud，而直接使用Spring Boot进行微服务开发的话，可能会直接使用Java中最常用的HTTP客户端Apache HttpClient进行服务调用。 接下来，我们就看看使用Feign和Apache HttpClient进行HTTP接口调用时，可能会遇到的超时、重试和并发方面的坑。 配置连接超时和读取超时参数的学问对于HTTP调用，虽然应用层走的是HTTP协议，但网络层面始终是TCP&#x2F;IP协议。TCP&#x2F;IP是面向连接的协议，在传输数据之前需要建立连接。几乎所有的网络框架都会提供这么两个超时参数： 连接超时参数ConnectTimeout，让用户配置建连阶段的最长等待时间； 读取超时参数ReadTimeout，用来控制从Socket上读取数据的最长等待时间。 这两个参数看似是网络层偏底层的配置参数，不足以引起开发同学的重视。但，正确理解和配置这两个参数，对业务应用特别重要，毕竟超时不是单方面的事情，需要客户端和服务端对超时有一致的估计，协同配合方能平衡吞吐量和错误率。 连接超时参数和连接超时的误区有这么两个： 连接超时配置得特别长，比如60秒。一般来说，TCP三次握手建立连接需要的时间非常短，通常在毫秒级最多到秒级，不可能需要十几秒甚至几十秒。如果很久都无法建连，很可能是网络或防火墙配置的问题。这种情况下，如果几秒连接不上，那么可能永远也连接不上。因此，设置特别长的连接超时意义不大，将其配置得短一些（比如1~5秒）即可。如果是纯内网调用的话，这个参数可以设置得更短，在下游服务离线无法连接的时候，可以快速失败。 排查连接超时问题，却没理清连的是哪里。通常情况下，我们的服务会有多个节点，如果别的客户端通过客户端负载均衡技术来连接服务端，那么客户端和服务端会直接建立连接，此时出现连接超时大概率是服务端的问题；而如果服务端通过类似Nginx的反向代理来负载均衡，客户端连接的其实是Nginx，而不是服务端，此时出现连接超时应该排查Nginx。 读取超时参数和读取超时则会有更多的误区，我将其归纳为如下三个。 第一个误区：认为出现了读取超时，服务端的执行就会中断。 我们来简单测试下。定义一个client接口，内部通过HttpClient调用服务端接口server，客户端读取超时2秒，服务端接口执行耗时5秒。 @RestController @RequestMapping(&quot;clientreadtimeout&quot;) @Slf4j public class ClientReadTimeoutController &#123; private String getResponse(String url, int connectTimeout, int readTimeout) throws IOException &#123; return Request.Get(&quot;http:&#x2F;&#x2F;localhost:45678&#x2F;clientreadtimeout&quot; + url) .connectTimeout(connectTimeout) .socketTimeout(readTimeout) .execute() .returnContent() .asString(); &#125; @GetMapping(&quot;client&quot;) public String client() throws IOException &#123; log.info(&quot;client1 called&quot;); &#x2F;&#x2F;服务端5s超时，客户端读取超时2秒 return getResponse(&quot;&#x2F;server?timeout&#x3D;5000&quot;, 1000, 2000); &#125; @GetMapping(&quot;server&quot;) public void server(@RequestParam(&quot;timeout&quot;) int timeout) throws InterruptedException &#123; log.info(&quot;server called&quot;); TimeUnit.MILLISECONDS.sleep(timeout); log.info(&quot;Done&quot;); &#125; &#125; 调用client接口后，从日志中可以看到，客户端2秒后出现了SocketTimeoutException，原因是读取超时，服务端却丝毫没受影响在3秒后执行完成。 [11:35:11.943] [http-nio-45678-exec-1] [INFO ] [.t.c.c.d.ClientReadTimeoutController:29 ] - client1 called [11:35:12.032] [http-nio-45678-exec-2] [INFO ] [.t.c.c.d.ClientReadTimeoutController:36 ] - server called [11:35:14.042] [http-nio-45678-exec-1] [ERROR] [.a.c.c.C.[.[.[&#x2F;].[dispatcherServlet]:175 ] - Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) ... [11:35:17.036] [http-nio-45678-exec-2] [INFO ] [.t.c.c.d.ClientReadTimeoutController:38 ] - Done 我们知道，类似Tomcat的Web服务器都是把服务端请求提交到线程池处理的，只要服务端收到了请求，网络层面的超时和断开便不会影响服务端的执行。因此，出现读取超时不能随意假设服务端的处理情况，需要根据业务状态考虑如何进行后续处理。 第二个误区：认为读取超时只是Socket网络层面的概念，是数据传输的最长耗时，故将其配置得非常短，比如100毫秒。 其实，发生了读取超时，网络层面无法区分是服务端没有把数据返回给客户端，还是数据在网络上耗时较久或丢包。 但，因为TCP是先建立连接后传输数据，对于网络情况不是特别糟糕的服务调用，通常可以认为出现连接超时是网络问题或服务不在线，而出现读取超时是服务处理超时。确切地说，读取超时指的是，向Socket写入数据后，我们等到Socket返回数据的超时时间，其中包含的时间或者说绝大部分的时间，是服务端处理业务逻辑的时间。 第三个误区：认为超时时间越长任务接口成功率就越高，将读取超时参数配置得太长。 进行HTTP请求一般是需要获得结果的，属于同步调用。如果超时时间很长，在等待服务端返回数据的同时，客户端线程（通常是Tomcat线程）也在等待，当下游服务出现大量超时的时候，程序可能也会受到拖累创建大量线程，最终崩溃。 对定时任务或异步任务来说，读取超时配置得长些问题不大。但面向用户响应的请求或是微服务短平快的同步接口调用，并发量一般较大，我们应该设置一个较短的读取超时时间，以防止被下游服务拖慢，通常不会设置超过30秒的读取超时。 你可能会说，如果把读取超时设置为2秒，服务端接口需要3秒，岂不是永远都拿不到执行结果了？的确是这样，因此设置读取超时一定要根据实际情况，过长可能会让下游抖动影响到自己，过短又可能影响成功率。甚至，有些时候我们还要根据下游服务的SLA，为不同的服务端接口设置不同的客户端读取超时。 Feign和Ribbon配合使用，你知道怎么配置超时吗？刚才我强调了根据自己的需求配置连接超时和读取超时的重要性，你是否尝试过为Spring Cloud的Feign配置超时参数呢，有没有被网上的各种资料绕晕呢？ 在我看来，为Feign配置超时参数的复杂之处在于，Feign自己有两个超时参数，它使用的负载均衡组件Ribbon本身还有相关配置。那么，这些配置的优先级是怎样的，又哪些什么坑呢？接下来，我们做一些实验吧。 为测试服务端的超时，假设有这么一个服务端接口，什么都不干只休眠10分钟： @PostMapping(&quot;&#x2F;server&quot;) public void server() throws InterruptedException &#123; TimeUnit.MINUTES.sleep(10); &#125; 首先，定义一个Feign来调用这个接口： @FeignClient(name &#x3D; &quot;clientsdk&quot;) public interface Client &#123; @PostMapping(&quot;&#x2F;feignandribbon&#x2F;server&quot;) void server(); &#125; 然后，通过Feign Client进行接口调用： @GetMapping(&quot;client&quot;) public void timeout() &#123; long begin&#x3D;System.currentTimeMillis(); try&#123; client.server(); &#125;catch (Exception ex)&#123; log.warn(&quot;执行耗时：&#123;&#125;ms 错误：&#123;&#125;&quot;, System.currentTimeMillis() - begin, ex.getMessage()); &#125; &#125; 在配置文件仅指定服务端地址的情况下： clientsdk.ribbon.listOfServers&#x3D;localhost:45678 得到如下输出： [15:40:16.094] [http-nio-45678-exec-3] [WARN ] [o.g.t.c.h.f.FeignAndRibbonController :26 ] - 执行耗时：1007ms 错误：Read timed out executing POST http:&#x2F;&#x2F;clientsdk&#x2F;feignandribbon&#x2F;server 从这个输出中，我们可以得到结论一，默认情况下Feign的读取超时是1秒，如此短的读取超时算是坑点一。 我们来分析一下源码。打开RibbonClientConfiguration类后，会看到DefaultClientConfigImpl被创建出来之后，ReadTimeout和ConnectTimeout被设置为1s： &#x2F;** * Ribbon client default connect timeout. *&#x2F; public static final int DEFAULT_CONNECT_TIMEOUT &#x3D; 1000; &#x2F;** * Ribbon client default read timeout. *&#x2F; public static final int DEFAULT_READ_TIMEOUT &#x3D; 1000; @Bean @ConditionalOnMissingBean public IClientConfig ribbonClientConfig() &#123; DefaultClientConfigImpl config &#x3D; new DefaultClientConfigImpl(); config.loadProperties(this.name); config.set(CommonClientConfigKey.ConnectTimeout, DEFAULT_CONNECT_TIMEOUT); config.set(CommonClientConfigKey.ReadTimeout, DEFAULT_READ_TIMEOUT); config.set(CommonClientConfigKey.GZipPayload, DEFAULT_GZIP_PAYLOAD); return config; &#125; 如果要修改Feign客户端默认的两个全局超时时间，你可以设置feign.client.config.default.readTimeout和feign.client.config.default.connectTimeout参数： feign.client.config.default.readTimeout&#x3D;3000 feign.client.config.default.connectTimeout&#x3D;3000 修改配置后重试，得到如下日志： [15:43:39.955] [http-nio-45678-exec-3] [WARN ] [o.g.t.c.h.f.FeignAndRibbonController :26 ] - 执行耗时：3006ms 错误：Read timed out executing POST http:&#x2F;&#x2F;clientsdk&#x2F;feignandribbon&#x2F;server 可见，3秒读取超时生效了。注意：这里有一个大坑，如果你希望只修改读取超时，可能会只配置这么一行： feign.client.config.default.readTimeout&#x3D;3000 测试一下你就会发现，这样的配置是无法生效的！ 结论二，也是坑点二，如果要配置Feign的读取超时，就必须同时配置连接超时，才能生效。 打开FeignClientFactoryBean可以看到，只有同时设置ConnectTimeout和ReadTimeout，Request.Options才会被覆盖： if (config.getConnectTimeout() !&#x3D; null &amp;&amp; config.getReadTimeout() !&#x3D; null) &#123; builder.options(new Request.Options(config.getConnectTimeout(), config.getReadTimeout())); &#125; 更进一步，如果你希望针对单独的Feign Client设置超时时间，可以把default替换为Client的name： feign.client.config.default.readTimeout&#x3D;3000 feign.client.config.default.connectTimeout&#x3D;3000 feign.client.config.clientsdk.readTimeout&#x3D;2000 feign.client.config.clientsdk.connectTimeout&#x3D;2000 可以得出结论三，单独的超时可以覆盖全局超时，这符合预期，不算坑： [15:45:51.708] [http-nio-45678-exec-3] [WARN ] [o.g.t.c.h.f.FeignAndRibbonController :26 ] - 执行耗时：2006ms 错误：Read timed out executing POST http:&#x2F;&#x2F;clientsdk&#x2F;feignandribbon&#x2F;server 结论四，除了可以配置Feign，也可以配置Ribbon组件的参数来修改两个超时时间。这里的坑点三是，参数首字母要大写，和Feign的配置不同。 ribbon.ReadTimeout&#x3D;4000 ribbon.ConnectTimeout&#x3D;4000 可以通过日志证明参数生效： [15:55:18.019] [http-nio-45678-exec-3] [WARN ] [o.g.t.c.h.f.FeignAndRibbonController :26 ] - 执行耗时：4003ms 错误：Read timed out executing POST http:&#x2F;&#x2F;clientsdk&#x2F;feignandribbon&#x2F;server 最后，我们来看看同时配置Feign和Ribbon的参数，最终谁会生效？如下代码的参数配置： clientsdk.ribbon.listOfServers&#x3D;localhost:45678 feign.client.config.default.readTimeout&#x3D;3000 feign.client.config.default.connectTimeout&#x3D;3000 ribbon.ReadTimeout&#x3D;4000 ribbon.ConnectTimeout&#x3D;4000 日志输出证明，最终生效的是Feign的超时： [16:01:19.972] [http-nio-45678-exec-3] [WARN ] [o.g.t.c.h.f.FeignAndRibbonController :26 ] - 执行耗时：3006ms 错误：Read timed out executing POST http:&#x2F;&#x2F;clientsdk&#x2F;feignandribbon&#x2F;server 结论五，同时配置Feign和Ribbon的超时，以Feign为准。这有点反直觉，因为Ribbon更底层所以你会觉得后者的配置会生效，但其实不是这样的。 在LoadBalancerFeignClient源码中可以看到，如果Request.Options不是默认值，就会创建一个FeignOptionsClientConfig代替原来Ribbon的DefaultClientConfigImpl，导致Ribbon的配置被Feign覆盖： IClientConfig getClientConfig(Request.Options options, String clientName) &#123; IClientConfig requestConfig; if (options &#x3D;&#x3D; DEFAULT_OPTIONS) &#123; requestConfig &#x3D; this.clientFactory.getClientConfig(clientName); &#125; else &#123; requestConfig &#x3D; new FeignOptionsClientConfig(options); &#125; return requestConfig; &#125; 但如果这么配置最终生效的还是Ribbon的超时（4秒），这容易让人产生Ribbon覆盖了Feign的错觉，其实这还是因为坑二所致，单独配置Feign的读取超时并不能生效： clientsdk.ribbon.listOfServers&#x3D;localhost:45678 feign.client.config.default.readTimeout&#x3D;3000 feign.client.config.clientsdk.readTimeout&#x3D;2000 ribbon.ReadTimeout&#x3D;4000 你是否知道Ribbon会自动重试请求呢？一些HTTP客户端往往会内置一些重试策略，其初衷是好的，毕竟因为网络问题导致丢包虽然频繁但持续时间短，往往重试下第二次就能成功，但一定要小心这种自作主张是否符合我们的预期。 之前遇到过一个短信重复发送的问题，但短信服务的调用方用户服务，反复确认代码里没有重试逻辑。那问题究竟出在哪里了？我们来重现一下这个案例。 首先，定义一个Get请求的发送短信接口，里面没有任何逻辑，休眠2秒模拟耗时： @RestController @RequestMapping(&quot;ribbonretryissueserver&quot;) @Slf4j public class RibbonRetryIssueServerController &#123; @GetMapping(&quot;sms&quot;) public void sendSmsWrong(@RequestParam(&quot;mobile&quot;) String mobile, @RequestParam(&quot;message&quot;) String message, HttpServletRequest request) throws InterruptedException &#123; &#x2F;&#x2F;输出调用参数后休眠2秒 log.info(&quot;&#123;&#125; is called, &#123;&#125;&#x3D;&gt;&#123;&#125;&quot;, request.getRequestURL().toString(), mobile, message); TimeUnit.SECONDS.sleep(2); &#125; &#125; 配置一个Feign供客户端调用： @FeignClient(name &#x3D; &quot;SmsClient&quot;) public interface SmsClient &#123; @GetMapping(&quot;&#x2F;ribbonretryissueserver&#x2F;sms&quot;) void sendSmsWrong(@RequestParam(&quot;mobile&quot;) String mobile, @RequestParam(&quot;message&quot;) String message); &#125; Feign内部有一个Ribbon组件负责客户端负载均衡，通过配置文件设置其调用的服务端为两个节点： SmsClient.ribbon.listOfServers&#x3D;localhost:45679,localhost:45678 写一个客户端接口，通过Feign调用服务端： @RestController @RequestMapping(&quot;ribbonretryissueclient&quot;) @Slf4j public class RibbonRetryIssueClientController &#123; @Autowired private SmsClient smsClient; @GetMapping(&quot;wrong&quot;) public String wrong() &#123; log.info(&quot;client is called&quot;); try&#123; &#x2F;&#x2F;通过Feign调用发送短信接口 smsClient.sendSmsWrong(&quot;13600000000&quot;, UUID.randomUUID().toString()); &#125; catch (Exception ex) &#123; &#x2F;&#x2F;捕获可能出现的网络错误 log.error(&quot;send sms failed : &#123;&#125;&quot;, ex.getMessage()); &#125; return &quot;done&quot;; &#125; &#125; 在45678和45679两个端口上分别启动服务端，然后访问45678的客户端接口进行测试。因为客户端和服务端控制器在一个应用中，所以45678同时扮演了客户端和服务端的角色。 在45678日志中可以看到，29秒时客户端收到请求开始调用服务端接口发短信，同时服务端收到了请求，2秒后（注意对比第一条日志和第三条日志）客户端输出了读取超时的错误信息： [12:49:29.020] [http-nio-45678-exec-4] [INFO ] [c.d.RibbonRetryIssueClientController:23 ] - client is called [12:49:29.026] [http-nio-45678-exec-5] [INFO ] [c.d.RibbonRetryIssueServerController:16 ] - http:&#x2F;&#x2F;localhost:45678&#x2F;ribbonretryissueserver&#x2F;sms is called, 13600000000&#x3D;&gt;a2aa1b32-a044-40e9-8950-7f0189582418 [12:49:31.029] [http-nio-45678-exec-4] [ERROR] [c.d.RibbonRetryIssueClientController:27 ] - send sms failed : Read timed out executing GET http:&#x2F;&#x2F;SmsClient&#x2F;ribbonretryissueserver&#x2F;sms?mobile&#x3D;13600000000&amp;message&#x3D;a2aa1b32-a044-40e9-8950-7f0189582418 而在另一个服务端45679的日志中还可以看到一条请求，30秒时收到请求，也就是客户端接口调用后的1秒： [12:49:30.029] [http-nio-45679-exec-2] [INFO ] [c.d.RibbonRetryIssueServerController:16 ] - http:&#x2F;&#x2F;localhost:45679&#x2F;ribbonretryissueserver&#x2F;sms is called, 13600000000&#x3D;&gt;a2aa1b32-a044-40e9-8950-7f0189582418 客户端接口被调用的日志只输出了一次，而服务端的日志输出了两次。虽然Feign的默认读取超时时间是1秒，但客户端2秒后才出现超时错误。显然，这说明客户端自作主张进行了一次重试，导致短信重复发送。 翻看Ribbon的源码可以发现，MaxAutoRetriesNextServer参数默认为1，也就是Get请求在某个服务端节点出现问题（比如读取超时）时，Ribbon会自动重试一次： &#x2F;&#x2F; DefaultClientConfigImpl public static final int DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER &#x3D; 1; public static final int DEFAULT_MAX_AUTO_RETRIES &#x3D; 0; &#x2F;&#x2F; RibbonLoadBalancedRetryPolicy public boolean canRetry(LoadBalancedRetryContext context) &#123; HttpMethod method &#x3D; context.getRequest().getMethod(); return HttpMethod.GET &#x3D;&#x3D; method || lbContext.isOkToRetryOnAllOperations(); &#125; @Override public boolean canRetrySameServer(LoadBalancedRetryContext context) &#123; return sameServerCount &lt; lbContext.getRetryHandler().getMaxRetriesOnSameServer() &amp;&amp; canRetry(context); &#125; @Override public boolean canRetryNextServer(LoadBalancedRetryContext context) &#123; &#x2F;&#x2F; this will be called after a failure occurs and we increment the counter &#x2F;&#x2F; so we check that the count is less than or equals to too make sure &#x2F;&#x2F; we try the next server the right number of times return nextServerCount &lt;&#x3D; lbContext.getRetryHandler().getMaxRetriesOnNextServer() &amp;&amp; canRetry(context); &#125; 解决办法有两个： 一是，把发短信接口从Get改为Post。其实，这里还有一个API设计问题，有状态的API接口不应该定义为Get。根据HTTP协议的规范，Get请求用于数据查询，而Post才是把数据提交到服务端用于修改或新增。选择Get还是Post的依据，应该是API的行为，而不是参数大小。这里的一个误区是，Get请求的参数包含在Url QueryString中，会受浏览器长度限制，所以一些同学会选择使用JSON以Post提交大参数，使用Get提交小参数。 二是，将MaxAutoRetriesNextServer参数配置为0，禁用服务调用失败后在下一个服务端节点的自动重试。在配置文件中添加一行即可： ribbon.MaxAutoRetriesNextServer&#x3D;0 看到这里，你觉得问题出在用户服务还是短信服务呢？ 在我看来，双方都有问题。就像之前说的，Get请求应该是无状态或者幂等的，短信接口可以设计为支持幂等调用的；而用户服务的开发同学，如果对Ribbon的重试机制有所了解的话，或许就能在排查问题上少走些弯路。 并发限制了爬虫的抓取能力除了超时和重试的坑，进行HTTP请求调用还有一个常见的问题是，并发数的限制导致程序的处理能力上不去。 我之前遇到过一个爬虫项目，整体爬取数据的效率很低，增加线程池数量也无济于事，只能堆更多的机器做分布式的爬虫。现在，我们就来模拟下这个场景，看看问题出在了哪里。 假设要爬取的服务端是这样的一个简单实现，休眠1秒返回数字1： @GetMapping(&quot;server&quot;) public int server() throws InterruptedException &#123; TimeUnit.SECONDS.sleep(1); return 1; &#125; 爬虫需要多次调用这个接口进行数据抓取，为了确保线程池不是并发的瓶颈，我们使用一个没有线程上限的newCachedThreadPool作为爬取任务的线程池（再次强调，除非你非常清楚自己的需求，否则一般不要使用没有线程数量上限的线程池），然后使用HttpClient实现HTTP请求，把请求任务循环提交到线程池处理，最后等待所有任务执行完成后输出执行耗时： private int sendRequest(int count, Supplier&lt;CloseableHttpClient&gt; client) throws InterruptedException &#123; &#x2F;&#x2F;用于计数发送的请求个数 AtomicInteger atomicInteger &#x3D; new AtomicInteger(); &#x2F;&#x2F;使用HttpClient从server接口查询数据的任务提交到线程池并行处理 ExecutorService threadPool &#x3D; Executors.newCachedThreadPool(); long begin &#x3D; System.currentTimeMillis(); IntStream.rangeClosed(1, count).forEach(i -&gt; &#123; threadPool.execute(() -&gt; &#123; try (CloseableHttpResponse response &#x3D; client.get().execute(new HttpGet(&quot;http:&#x2F;&#x2F;127.0.0.1:45678&#x2F;routelimit&#x2F;server&quot;))) &#123; atomicInteger.addAndGet(Integer.parseInt(EntityUtils.toString(response.getEntity()))); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125;); &#125;); &#x2F;&#x2F;等到count个任务全部执行完毕 threadPool.shutdown(); threadPool.awaitTermination(1, TimeUnit.HOURS); log.info(&quot;发送 &#123;&#125; 次请求，耗时 &#123;&#125; ms&quot;, atomicInteger.get(), System.currentTimeMillis() - begin); return atomicInteger.get(); &#125; 首先，使用默认的PoolingHttpClientConnectionManager构造的CloseableHttpClient，测试一下爬取10次的耗时： static CloseableHttpClient httpClient1; static &#123; httpClient1 &#x3D; HttpClients.custom().setConnectionManager(new PoolingHttpClientConnectionManager()).build(); &#125; @GetMapping(&quot;wrong&quot;) public int wrong(@RequestParam(value &#x3D; &quot;count&quot;, defaultValue &#x3D; &quot;10&quot;) int count) throws InterruptedException &#123; return sendRequest(count, () -&gt; httpClient1); &#125; 虽然一个请求需要1秒执行完成，但我们的线程池是可以扩张使用任意数量线程的。按道理说，10个请求并发处理的时间基本相当于1个请求的处理时间，也就是1秒，但日志中显示实际耗时5秒： [12:48:48.122] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.h.r.RouteLimitController :54 ] - 发送 10 次请求，耗时 5265 ms 查看PoolingHttpClientConnectionManager源码，可以注意到有两个重要参数： defaultMaxPerRoute&#x3D;2，也就是同一个主机&#x2F;域名的最大并发请求数为2。我们的爬虫需要10个并发，显然是默认值太小限制了爬虫的效率。 maxTotal&#x3D;20，也就是所有主机整体最大并发为20，这也是HttpClient整体的并发度。目前，我们请求数是10最大并发是10，20不会成为瓶颈。举一个例子，使用同一个HttpClient访问10个域名，defaultMaxPerRoute设置为10，为确保每一个域名都能达到10并发，需要把maxTotal设置为100。 public PoolingHttpClientConnectionManager( final HttpClientConnectionOperator httpClientConnectionOperator, final HttpConnectionFactory&lt;HttpRoute, ManagedHttpClientConnection&gt; connFactory, final long timeToLive, final TimeUnit timeUnit) &#123; ... this.pool &#x3D; new CPool(new InternalConnectionFactory( this.configData, connFactory), 2, 20, timeToLive, timeUnit); ... &#125; public CPool( final ConnFactory&lt;HttpRoute, ManagedHttpClientConnection&gt; connFactory, final int defaultMaxPerRoute, final int maxTotal, final long timeToLive, final TimeUnit timeUnit) &#123; ... &#125;&#125; HttpClient是Java非常常用的HTTP客户端，这个问题经常出现。你可能会问，为什么默认值限制得这么小。 其实，这不能完全怪HttpClient，很多早期的浏览器也限制了同一个域名两个并发请求。对于同一个域名并发连接的限制，其实是HTTP 1.1协议要求的，这里有这么一段话： Clients that use persistent connections SHOULD limit the number of simultaneous connections that they maintain to a given server. A single-user client SHOULD NOT maintain more than 2 connections with any server or proxy. A proxy SHOULD use up to 2*N connections to another server or proxy, where N is the number of simultaneously active users. These guidelines are intended to improve HTTP response times and avoid congestion. HTTP 1.1协议是20年前制定的，现在HTTP服务器的能力强很多了，所以有些新的浏览器没有完全遵从2并发这个限制，放开并发数到了8甚至更大。如果需要通过HTTP客户端发起大量并发请求，不管使用什么客户端，请务必确认客户端的实现默认的并发度是否满足需求。 既然知道了问题所在，我们就尝试声明一个新的HttpClient放开相关限制，设置maxPerRoute为50、maxTotal为100，然后修改一下刚才的wrong方法，使用新的客户端进行测试： httpClient2 &#x3D; HttpClients.custom().setMaxConnPerRoute(10).setMaxConnTotal(20).build(); 输出如下，10次请求在1秒左右执行完成。可以看到，因为放开了一个Host 2个并发的默认限制，爬虫效率得到了大幅提升： [12:58:11.333] [http-nio-45678-exec-3] [INFO ] [o.g.t.c.h.r.RouteLimitController :54 ] - 发送 10 次请求，耗时 1023 ms 重点回顾今天，我和你分享了HTTP调用最常遇到的超时、重试和并发问题。 连接超时代表建立TCP连接的时间，读取超时代表了等待远端返回数据的时间，也包括远端程序处理的时间。在解决连接超时问题时，我们要搞清楚连的是谁；在遇到读取超时问题的时候，我们要综合考虑下游服务的服务标准和自己的服务标准，设置合适的读取超时时间。此外，在使用诸如Spring Cloud Feign等框架时务必确认，连接和读取超时参数的配置是否正确生效。 对于重试，因为HTTP协议认为Get请求是数据查询操作，是无状态的，又考虑到网络出现丢包是比较常见的事情，有些HTTP客户端或代理服务器会自动重试Get&#x2F;Head请求。如果你的接口设计不支持幂等，需要关闭自动重试。但，更好的解决方案是，遵从HTTP协议的建议来使用合适的HTTP方法。 最后我们看到，包括HttpClient在内的HTTP客户端以及浏览器，都会限制客户端调用的最大并发数。如果你的客户端有比较大的请求调用并发，比如做爬虫，或是扮演类似代理的角色，又或者是程序本身并发较高，如此小的默认值很容易成为吞吐量的瓶颈，需要及时调整。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 第一节中我们强调了要注意连接超时和读取超时参数的配置，大多数的HTTP客户端也都有这两个参数。有读就有写，但为什么我们很少看到“写入超时”的概念呢？ 除了Ribbon的AutoRetriesNextServer重试机制，Nginx也有类似的重试功能。你了解Nginx相关的配置吗？ 针对HTTP调用，你还遇到过什么坑吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/06、20%的业务代码的Spring声明式事务，可能都没处理正确","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/06、20%的业务代码的Spring声明式事务，可能都没处理正确/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/06%E3%80%8120%%E7%9A%84%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E7%9A%84Spring%E5%A3%B0%E6%98%8E%E5%BC%8F%E4%BA%8B%E5%8A%A1%EF%BC%8C%E5%8F%AF%E8%83%BD%E9%83%BD%E6%B2%A1%E5%A4%84%E7%90%86%E6%AD%A3%E7%A1%AE/","excerpt":"","text":"06 | 20%的业务代码的Spring声明式事务，可能都没处理正确作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来和你聊聊业务代码中与数据库事务相关的坑。 Spring针对Java Transaction API (JTA)、JDBC、Hibernate和Java Persistence API (JPA)等事务API，实现了一致的编程模型，而Spring的声明式事务功能更是提供了极其方便的事务配置方式，配合Spring Boot的自动配置，大多数Spring Boot项目只需要在方法上标记@Transactional注解，即可一键开启方法的事务性配置。 据我观察，大多数业务开发同学都有事务的概念，也知道如果整体考虑多个数据库操作要么成功要么失败时，需要通过数据库事务来实现多个操作的一致性和原子性。但，在使用上大多仅限于为方法标记@Transactional，不会去关注事务是否有效、出错后事务是否正确回滚，也不会考虑复杂的业务代码中涉及多个子业务逻辑时，怎么正确处理事务。 事务没有被正确处理，一般来说不会过于影响正常流程，也不容易在测试阶段被发现。但当系统越来越复杂、压力越来越大之后，就会带来大量的数据不一致问题，随后就是大量的人工介入查看和修复数据。 所以说，一个成熟的业务系统和一个基本可用能完成功能的业务系统，在事务处理细节上的差异非常大。要确保事务的配置符合业务功能的需求，往往不仅仅是技术问题，还涉及产品流程和架构设计的问题。今天这一讲的标题“20%的业务代码的Spring声明式事务，可能都没处理正确”中，20%这个数字在我看来还是比较保守的。 我今天要分享的内容，就是帮助你在技术问题上理清思路，避免因为事务处理不当让业务逻辑的实现产生大量偶发Bug。 小心Spring的事务可能没有生效在使用@Transactional注解开启声明式事务时， 第一个最容易忽略的问题是，很可能事务并没有生效。 实现下面的Demo需要一些基础类，首先定义一个具有ID和姓名属性的UserEntity，也就是一个包含两个字段的用户表： @Entity @Data public class UserEntity &#123; @Id @GeneratedValue(strategy &#x3D; AUTO) private Long id; private String name; public UserEntity() &#123; &#125; public UserEntity(String name) &#123; this.name &#x3D; name; &#125; &#125; 为了方便理解，我使用Spring JPA做数据库访问，实现这样一个Repository，新增一个根据用户名查询所有数据的方法： @Repository public interface UserRepository extends JpaRepository&lt;UserEntity, Long&gt; &#123; List&lt;UserEntity&gt; findByName(String name); &#125; 定义一个UserService类，负责业务逻辑处理。如果不清楚@Transactional的实现方式，只考虑代码逻辑的话，这段代码看起来没有问题。 定义一个入口方法createUserWrong1来调用另一个私有方法createUserPrivate，私有方法上标记了@Transactional注解。当传入的用户名包含test关键字时判断为用户名不合法，抛出异常，让用户创建操作失败，期望事务可以回滚： @Service @Slf4j public class UserService &#123; @Autowired private UserRepository userRepository; &#x2F;&#x2F;一个公共方法供Controller调用，内部调用事务性的私有方法 public int createUserWrong1(String name) &#123; try &#123; this.createUserPrivate(new UserEntity(name)); &#125; catch (Exception ex) &#123; log.error(&quot;create user failed because &#123;&#125;&quot;, ex.getMessage()); &#125; return userRepository.findByName(name).size(); &#125; &#x2F;&#x2F;标记了@Transactional的private方法 @Transactional private void createUserPrivate(UserEntity entity) &#123; userRepository.save(entity); if (entity.getName().contains(&quot;test&quot;)) throw new RuntimeException(&quot;invalid username!&quot;); &#125; &#x2F;&#x2F;根据用户名查询用户数 public int getUserCount(String name) &#123; return userRepository.findByName(name).size(); &#125; &#125; 下面是Controller的实现，只是调用一下刚才定义的UserService中的入口方法createUserWrong1。 @Autowired private UserService userService; @GetMapping(&quot;wrong1&quot;) public int wrong1(@RequestParam(&quot;name&quot;) String name) &#123; return userService.createUserWrong1(name); &#125; 调用接口后发现，即便用户名不合法，用户也能创建成功。刷新浏览器，多次发现有十几个的非法用户注册。 这里给出@Transactional生效原则1，除非特殊配置（比如使用AspectJ静态织入实现AOP），否则只有定义在public方法上的@Transactional才能生效。原因是，Spring默认通过动态代理的方式实现AOP，对目标方法进行增强，private方法无法代理到，Spring自然也无法动态增强事务处理逻辑。 你可能会说，修复方式很简单，把标记了事务注解的createUserPrivate方法改为public即可。在UserService中再建一个入口方法createUserWrong2，来调用这个public方法再次尝试： public int createUserWrong2(String name) &#123; try &#123; this.createUserPublic(new UserEntity(name)); &#125; catch (Exception ex) &#123; log.error(&quot;create user failed because &#123;&#125;&quot;, ex.getMessage()); &#125; return userRepository.findByName(name).size(); &#125; &#x2F;&#x2F;标记了@Transactional的public方法 @Transactional public void createUserPublic(UserEntity entity) &#123; userRepository.save(entity); if (entity.getName().contains(&quot;test&quot;)) throw new RuntimeException(&quot;invalid username!&quot;); &#125; 测试发现，调用新的createUserWrong2方法事务同样不生效。这里，我给出@Transactional生效原则2，必须通过代理过的类从外部调用目标方法才能生效。 Spring通过AOP技术对方法进行增强，要调用增强过的方法必然是调用代理后的对象。我们尝试修改下UserService的代码，注入一个self，然后再通过self实例调用标记有@Transactional注解的createUserPublic方法。设置断点可以看到，self是由Spring通过CGLIB方式增强过的类： CGLIB通过继承方式实现代理类，private方法在子类不可见，自然也就无法进行事务增强； this指针代表对象自己，Spring不可能注入this，所以通过this访问方法必然不是代理。 把this改为self后测试发现，在Controller中调用createUserRight方法可以验证事务是生效的，非法的用户注册操作可以回滚。 虽然在UserService内部注入自己调用自己的createUserPublic可以正确实现事务，但更合理的实现方式是，让Controller直接调用之前定义的UserService的createUserPublic方法，因为注入自己调用自己很奇怪，也不符合分层实现的规范： @GetMapping(&quot;right2&quot;) public int right2(@RequestParam(&quot;name&quot;) String name) &#123; try &#123; userService.createUserPublic(new UserEntity(name)); &#125; catch (Exception ex) &#123; log.error(&quot;create user failed because &#123;&#125;&quot;, ex.getMessage()); &#125; return userService.getUserCount(name); &#125; 我们再通过一张图来回顾下this自调用、通过self调用，以及在Controller中调用UserService三种实现的区别： 通过this自调用，没有机会走到Spring的代理类；后两种改进方案调用的是Spring注入的UserService，通过代理调用才有机会对createUserPublic方法进行动态增强。 这里，我还有一个小技巧，强烈建议你在开发时打开相关的Debug日志，以方便了解Spring事务实现的细节，并及时判断事务的执行情况。 我们的Demo代码使用JPA进行数据库访问，可以这么开启Debug日志： logging.level.org.springframework.orm.jpa&#x3D;DEBUG 开启日志后，我们再比较下在UserService中通过this调用和在Controller中通过注入的UserService Bean调用createUserPublic区别。很明显，this调用因为没有走代理，事务没有在createUserPublic方法上生效，只在Repository的save方法层面生效： &#x2F;&#x2F;在UserService中通过this调用public的createUserPublic [10:10:19.913] [http-nio-45678-exec-1] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :370 ] - Creating new transaction with name [org.springframework.data.jpa.repository.support.SimpleJpaRepository.save]: PROPAGATION_REQUIRED,ISOLATION_DEFAULT &#x2F;&#x2F;在Controller中通过注入的UserService Bean调用createUserPublic [10:10:47.750] [http-nio-45678-exec-6] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :370 ] - Creating new transaction with name [org.geekbang.time.commonmistakes.transaction.demo1.UserService.createUserPublic]: PROPAGATION_REQUIRED,ISOLATION_DEFAULT 你可能还会考虑一个问题，这种实现在Controller里处理了异常显得有点繁琐，还不如直接把createUserWrong2方法加上@Transactional注解，然后在Controller中直接调用这个方法。这样一来，既能从外部（Controller中）调用UserService中的方法，方法又是public的能够被动态代理AOP增强。 你可以试一下这种方法，但很容易就会踩第二个坑，即因为没有正确处理异常，导致事务即便生效也不一定能回滚。 事务即便生效也不一定能回滚通过AOP实现事务处理可以理解为，使用try…catch…来包裹标记了@Transactional注解的方法，当方法出现了异常并且满足一定条件的时候，在catch里面我们可以设置事务回滚，没有异常则直接提交事务。 这里的“一定条件”，主要包括两点。 第一，只有异常传播出了标记了@Transactional注解的方法，事务才能回滚。在Spring的TransactionAspectSupport里有个 invokeWithinTransaction方法，里面就是处理事务的逻辑。可以看到，只有捕获到异常才能进行后续事务处理： try &#123; &#x2F;&#x2F; This is an around advice: Invoke the next interceptor in the chain. &#x2F;&#x2F; This will normally result in a target object being invoked. retVal &#x3D; invocation.proceedWithInvocation(); &#125; catch (Throwable ex) &#123; &#x2F;&#x2F; target invocation exception completeTransactionAfterThrowing(txInfo, ex); throw ex; &#125; finally &#123; cleanupTransactionInfo(txInfo); &#125; 第二，默认情况下，出现RuntimeException（非受检异常）或Error的时候，Spring才会回滚事务。 打开Spring的DefaultTransactionAttribute类能看到如下代码块，可以发现相关证据，通过注释也能看到Spring这么做的原因，大概的意思是受检异常一般是业务异常，或者说是类似另一种方法的返回值，出现这样的异常可能业务还能完成，所以不会主动回滚；而Error或RuntimeException代表了非预期的结果，应该回滚： &#x2F;** * The default behavior is as with EJB: rollback on unchecked exception * (&#123;@link RuntimeException&#125;), assuming an unexpected outcome outside of any * business rules. Additionally, we also attempt to rollback on &#123;@link Error&#125; which * is clearly an unexpected outcome as well. By contrast, a checked exception is * considered a business exception and therefore a regular expected outcome of the * transactional business method, i.e. a kind of alternative return value which * still allows for regular completion of resource operations. * &lt;p&gt;This is largely consistent with TransactionTemplate&#39;s default behavior, * except that TransactionTemplate also rolls back on undeclared checked exceptions * (a corner case). For declarative transactions, we expect checked exceptions to be * intentionally declared as business exceptions, leading to a commit by default. * @see org.springframework.transaction.support.TransactionTemplate#execute *&#x2F; @Override public boolean rollbackOn(Throwable ex) &#123; return (ex instanceof RuntimeException || ex instanceof Error); &#125; 接下来，我和你分享2个反例。 重新实现一下UserService中的注册用户操作： 在createUserWrong1方法中会抛出一个RuntimeException，但由于方法内catch了所有异常，异常无法从方法传播出去，事务自然无法回滚。 在createUserWrong2方法中，注册用户的同时会有一次otherTask文件读取操作，如果文件读取失败，我们希望用户注册的数据库操作回滚。虽然这里没有捕获异常，但因为otherTask方法抛出的是受检异常，createUserWrong2传播出去的也是受检异常，事务同样不会回滚。 @Service @Slf4j public class UserService &#123; @Autowired private UserRepository userRepository; &#x2F;&#x2F;异常无法传播出方法，导致事务无法回滚 @Transactional public void createUserWrong1(String name) &#123; try &#123; userRepository.save(new UserEntity(name)); throw new RuntimeException(&quot;error&quot;); &#125; catch (Exception ex) &#123; log.error(&quot;create user failed&quot;, ex); &#125; &#125; &#x2F;&#x2F;即使出了受检异常也无法让事务回滚 @Transactional public void createUserWrong2(String name) throws IOException &#123; userRepository.save(new UserEntity(name)); otherTask(); &#125; &#x2F;&#x2F;因为文件不存在，一定会抛出一个IOException private void otherTask() throws IOException &#123; Files.readAllLines(Paths.get(&quot;file-that-not-exist&quot;)); &#125; &#125; Controller中的实现，仅仅是调用UserService的createUserWrong1和createUserWrong2方法，这里就贴出实现了。这2个方法的实现和调用，虽然完全避开了事务不生效的坑，但因为异常处理不当，导致程序没有如我们期望的文件操作出现异常时回滚事务。 现在，我们来看下修复方式，以及如何通过日志来验证是否修复成功。针对这2种情况，对应的修复方法如下。 第一，如果你希望自己捕获异常进行处理的话，也没关系，可以手动设置让当前事务处于回滚状态： @Transactional public void createUserRight1(String name) &#123; try &#123; userRepository.save(new UserEntity(name)); throw new RuntimeException(&quot;error&quot;); &#125; catch (Exception ex) &#123; log.error(&quot;create user failed&quot;, ex); TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); &#125; &#125; 运行后可以在日志中看到Rolling back字样，确认事务回滚了。同时，我们还注意到“Transactional code has requested rollback”的提示，表明手动请求回滚： [22:14:49.352] [http-nio-45678-exec-4] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :698 ] - Transactional code has requested rollback [22:14:49.353] [http-nio-45678-exec-4] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :834 ] - Initiating transaction rollback [22:14:49.353] [http-nio-45678-exec-4] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :555 ] - Rolling back JPA transaction on EntityManager [SessionImpl(1906719643&lt;open&gt;)] 第二，在注解中声明，期望遇到所有的Exception都回滚事务（来突破默认不回滚受检异常的限制）： @Transactional(rollbackFor &#x3D; Exception.class) public void createUserRight2(String name) throws IOException &#123; userRepository.save(new UserEntity(name)); otherTask(); &#125; 运行后，同样可以在日志中看到回滚的提示： [22:10:47.980] [http-nio-45678-exec-4] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :834 ] - Initiating transaction rollback [22:10:47.981] [http-nio-45678-exec-4] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :555 ] - Rolling back JPA transaction on EntityManager [SessionImpl(1419329213&lt;open&gt;)] 在这个例子中，我们展现的是一个复杂的业务逻辑，其中有数据库操作、IO操作，在IO操作出现问题时希望让数据库事务也回滚，以确保逻辑的一致性。在有些业务逻辑中，可能会包含多次数据库操作，我们不一定希望将两次操作作为一个事务来处理，这时候就需要仔细考虑事务传播的配置了，否则也可能踩坑。 请确认事务传播配置是否符合自己的业务逻辑有这么一个场景：一个用户注册的操作，会插入一个主用户到用户表，还会注册一个关联的子用户。我们希望将子用户注册的数据库操作作为一个独立事务来处理，即使失败也不会影响主流程，即不影响主用户的注册。 接下来，我们模拟一个实现类似业务逻辑的UserService： @Autowired private UserRepository userRepository; @Autowired private SubUserService subUserService; @Transactional public void createUserWrong(UserEntity entity) &#123; createMainUser(entity); subUserService.createSubUserWithExceptionWrong(entity); &#125; private void createMainUser(UserEntity entity) &#123; userRepository.save(entity); log.info(&quot;createMainUser finish&quot;); &#125; SubUserService的createSubUserWithExceptionWrong实现正如其名，因为最后我们抛出了一个运行时异常，错误原因是用户状态无效，所以子用户的注册肯定是失败的。我们期望子用户的注册作为一个事务单独回滚，不影响主用户的注册，这样的逻辑可以实现吗？ @Service @Slf4j public class SubUserService &#123; @Autowired private UserRepository userRepository; @Transactional public void createSubUserWithExceptionWrong(UserEntity entity) &#123; log.info(&quot;createSubUserWithExceptionWrong start&quot;); userRepository.save(entity); throw new RuntimeException(&quot;invalid status&quot;); &#125; &#125; 我们在Controller里实现一段测试代码，调用UserService： @GetMapping(&quot;wrong&quot;) public int wrong(@RequestParam(&quot;name&quot;) String name) &#123; try &#123; userService.createUserWrong(new UserEntity(name)); &#125; catch (Exception ex) &#123; log.error(&quot;createUserWrong failed, reason:&#123;&#125;&quot;, ex.getMessage()); &#125; return userService.getUserCount(name); &#125; 调用后可以在日志中发现如下信息，很明显事务回滚了，最后Controller打出了创建子用户抛出的运行时异常： [22:50:42.866] [http-nio-45678-exec-8] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :555 ] - Rolling back JPA transaction on EntityManager [SessionImpl(103972212&lt;open&gt;)] [22:50:42.869] [http-nio-45678-exec-8] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :620 ] - Closing JPA EntityManager [SessionImpl(103972212&lt;open&gt;)] after transaction [22:50:42.869] [http-nio-45678-exec-8] [ERROR] [t.d.TransactionPropagationController:23 ] - createUserWrong failed, reason:invalid status 你马上就会意识到，不对呀，因为运行时异常逃出了@Transactional注解标记的createUserWrong方法，Spring当然会回滚事务了。如果我们希望主方法不回滚，应该把子方法抛出的异常捕获了。 也就是这么改，把subUserService.createSubUserWithExceptionWrong包裹上catch，这样外层主方法就不会出现异常了： @Transactional public void createUserWrong2(UserEntity entity) &#123; createMainUser(entity); try&#123; subUserService.createSubUserWithExceptionWrong(entity); &#125; catch (Exception ex) &#123; &#x2F;&#x2F; 虽然捕获了异常，但是因为没有开启新事务，而当前事务因为异常已经被标记为rollback了，所以最终还是会回滚。 log.error(&quot;create sub user error:&#123;&#125;&quot;, ex.getMessage()); &#125; &#125; 运行程序后可以看到如下日志： [22:57:21.722] [http-nio-45678-exec-3] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :370 ] - Creating new transaction with name [org.geekbang.time.commonmistakes.transaction.demo3.UserService.createUserWrong2]: PROPAGATION_REQUIRED,ISOLATION_DEFAULT [22:57:21.739] [http-nio-45678-exec-3] [INFO ] [t.c.transaction.demo3.SubUserService:19 ] - createSubUserWithExceptionWrong start [22:57:21.739] [http-nio-45678-exec-3] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :356 ] - Found thread-bound EntityManager [SessionImpl(1794007607&lt;open&gt;)] for JPA transaction [22:57:21.739] [http-nio-45678-exec-3] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :471 ] - Participating in existing transaction [22:57:21.740] [http-nio-45678-exec-3] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :843 ] - Participating transaction failed - marking existing transaction as rollback-only [22:57:21.740] [http-nio-45678-exec-3] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :580 ] - Setting JPA transaction on EntityManager [SessionImpl(1794007607&lt;open&gt;)] rollback-only [22:57:21.740] [http-nio-45678-exec-3] [ERROR] [.g.t.c.transaction.demo3.UserService:37 ] - create sub user error:invalid status [22:57:21.740] [http-nio-45678-exec-3] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :741 ] - Initiating transaction commit [22:57:21.740] [http-nio-45678-exec-3] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :529 ] - Committing JPA transaction on EntityManager [SessionImpl(1794007607&lt;open&gt;)] [22:57:21.743] [http-nio-45678-exec-3] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :620 ] - Closing JPA EntityManager [SessionImpl(1794007607&lt;open&gt;)] after transaction [22:57:21.743] [http-nio-45678-exec-3] [ERROR] [t.d.TransactionPropagationController:33 ] - createUserWrong2 failed, reason:Transaction silently rolled back because it has been marked as rollback-only org.springframework.transaction.UnexpectedRollbackException: Transaction silently rolled back because it has been marked as rollback-only ... 需要注意以下几点： 如第1行所示，对createUserWrong2方法开启了异常处理； 如第5行所示，子方法因为出现了运行时异常，标记当前事务为回滚； 如第7行所示，主方法的确捕获了异常打印出了create sub user error字样； 如第9行所示，主方法提交了事务； 奇怪的是，如第11行和12行所示，Controller里出现了一个UnexpectedRollbackException，异常描述提示最终这个事务回滚了，而且是静默回滚的。之所以说是静默，是因为createUserWrong2方法本身并没有出异常，只不过提交后发现子方法已经把当前事务设置为了回滚，无法完成提交。 这挺反直觉的。我们之前说，出了异常事务不一定回滚，这里说的却是不出异常，事务也不一定可以提交。原因是，主方法注册主用户的逻辑和子方法注册子用户的逻辑是同一个事务，子逻辑标记了事务需要回滚，主逻辑自然也不能提交了。 看到这里，修复方式就很明确了，想办法让子逻辑在独立事务中运行，也就是改一下SubUserService注册子用户的方法，为注解加上propagation &#x3D; Propagation.REQUIRES_NEW来设置REQUIRES_NEW方式的事务传播策略，也就是执行到这个方法时需要开启新的事务，并挂起当前事务： @Transactional(propagation &#x3D; Propagation.REQUIRES_NEW) public void createSubUserWithExceptionRight(UserEntity entity) &#123; log.info(&quot;createSubUserWithExceptionRight start&quot;); userRepository.save(entity); throw new RuntimeException(&quot;invalid status&quot;); &#125; 主方法没什么变化，同样需要捕获异常，防止异常漏出去导致主事务回滚，重新命名为createUserRight： @Transactional public void createUserRight(UserEntity entity) &#123; createMainUser(entity); try&#123; subUserService.createSubUserWithExceptionRight(entity); &#125; catch (Exception ex) &#123; &#x2F;&#x2F; 捕获异常，防止主方法回滚 log.error(&quot;create sub user error:&#123;&#125;&quot;, ex.getMessage()); &#125; &#125; 改造后，重新运行程序可以看到如下的关键日志： 第1行日志提示我们针对createUserRight方法开启了主方法的事务； 第2行日志提示创建主用户完成； 第3行日志可以看到主事务挂起了，开启了一个新的事务，针对createSubUserWithExceptionRight方案，也就是我们的创建子用户的逻辑； 第4行日志提示子方法事务回滚； 第5行日志提示子方法事务完成，继续主方法之前挂起的事务； 第6行日志提示主方法捕获到了子方法的异常； 第8行日志提示主方法的事务提交了，随后我们在Controller里没看到静默回滚的异常。 [23:17:20.935] [http-nio-45678-exec-1] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :370 ] - Creating new transaction with name [org.geekbang.time.commonmistakes.transaction.demo3.UserService.createUserRight]: PROPAGATION_REQUIRED,ISOLATION_DEFAULT [23:17:21.079] [http-nio-45678-exec-1] [INFO ] [.g.t.c.transaction.demo3.UserService:55 ] - createMainUser finish [23:17:21.082] [http-nio-45678-exec-1] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :420 ] - Suspending current transaction, creating new transaction with name [org.geekbang.time.commonmistakes.transaction.demo3.SubUserService.createSubUserWithExceptionRight] [23:17:21.153] [http-nio-45678-exec-1] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :834 ] - Initiating transaction rollback [23:17:21.160] [http-nio-45678-exec-1] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :1009] - Resuming suspended transaction after completion of inner transaction [23:17:21.161] [http-nio-45678-exec-1] [ERROR] [.g.t.c.transaction.demo3.UserService:49 ] - create sub user error:invalid status [23:17:21.161] [http-nio-45678-exec-1] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :741 ] - Initiating transaction commit [23:17:21.161] [http-nio-45678-exec-1] [DEBUG] [o.s.orm.jpa.JpaTransactionManager :529 ] - Committing JPA transaction on EntityManager [SessionImpl(396441411&lt;open&gt;)] 运行测试程序看到如下结果，getUserCount得到的用户数量为1，代表只有一个用户也就是主用户注册完成了，符合预期： 重点回顾今天，我针对业务代码中最常见的使用数据库事务的方式，即Spring声明式事务，与你总结了使用上可能遇到的三类坑，包括： 第一，因为配置不正确，导致方法上的事务没生效。我们务必确认调用@Transactional注解标记的方法是public的，并且是通过Spring注入的Bean进行调用的。 第二，因为异常处理不正确，导致事务虽然生效但出现异常时没回滚。Spring默认只会对标记@Transactional注解的方法出现了RuntimeException和Error的时候回滚，如果我们的方法捕获了异常，那么需要通过手动编码处理事务回滚。如果希望Spring针对其他异常也可以回滚，那么可以相应配置@Transactional注解的rollbackFor和noRollbackFor属性来覆盖其默认设置。 第三，如果方法涉及多次数据库操作，并希望将它们作为独立的事务进行提交或回滚，那么我们需要考虑进一步细化配置事务传播方式，也就是@Transactional注解的Propagation属性。 可见，正确配置事务可以提高业务项目的健壮性。但，又因为健壮性问题往往体现在异常情况或一些细节处理上，很难在主流程的运行和测试中发现，导致业务代码的事务处理逻辑往往容易被忽略，因此我在代码审查环节一直很关注事务是否正确处理。 如果你无法确认事务是否真正生效，是否按照预期的逻辑进行，可以尝试打开Spring的部分Debug日志，通过事务的运作细节来验证。也建议你在单元测试时尽量覆盖多的异常场景，这样在重构时，也能及时发现因为方法的调用方式、异常处理逻辑的调整，导致的事务失效问题。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 考虑到Demo的简洁，文中所有数据访问使用的都是Spring Data JPA。国内大多数互联网业务项目是使用MyBatis进行数据访问的，使用MyBatis配合Spring的声明式事务也同样需要注意文中提到的这些点。你可以尝试把今天的Demo改为MyBatis做数据访问实现，看看日志中是否可以体现出这些坑。 在第一节中我们提到，如果要针对private方法启用事务，动态代理方式的AOP不可行，需要使用静态织入方式的AOP，也就是在编译期间织入事务增强代码，可以配置Spring框架使用AspectJ来实现AOP。你能否参阅Spring的文档“Using @Transactional with AspectJ”试试呢？注意：AspectJ配合lombok使用，还可能会踩一些坑。 有关数据库事务，你还遇到过其他坑吗？我是朱晔，欢迎在评论区与我留言分享，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/07、数据库索引：索引并不是万能药","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/07、数据库索引：索引并不是万能药/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/07%E3%80%81%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%EF%BC%9A%E7%B4%A2%E5%BC%95%E5%B9%B6%E4%B8%8D%E6%98%AF%E4%B8%87%E8%83%BD%E8%8D%AF/","excerpt":"","text":"07 | 数据库索引：索引并不是万能药作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我要和你分享的主题是，数据库的索引并不是万能药。 几乎所有的业务项目都会涉及数据存储，虽然当前各种NoSQL和文件系统大行其道，但MySQL等关系型数据库因为满足ACID、可靠性高、对开发友好等特点，仍然最常被用于存储重要数据。在关系型数据库中，索引是优化查询性能的重要手段。 为此，我经常看到一些同学一遇到查询性能问题，就盲目要求运维或DBA给数据表相关字段创建大量索引。显然，这种想法是错误的。今天，我们就以MySQL为例来深入理解下索引的原理，以及相关误区。 InnoDB是如何存储数据的？MySQL把数据存储和查询操作抽象成了存储引擎，不同的存储引擎，对数据的存储和读取方式各不相同。MySQL支持多种存储引擎，并且可以以表为粒度设置存储引擎。因为支持事务，我们最常使用的是InnoDB。为方便理解下面的内容，我先和你简单说说InnoDB是如何存储数据的。 虽然数据保存在磁盘中，但其处理是在内存中进行的。为了减少磁盘随机读取次数，InnoDB采用页而不是行的粒度来保存数据，即数据被分成若干页，以页为单位保存在磁盘中。InnoDB的页大小，一般是16KB。 各个数据页组成一个双向链表，每个数据页中的记录按照主键顺序组成单向链表；每一个数据页中有一个页目录，方便按照主键查询记录。数据页的结构如下： 页目录通过槽把记录分成不同的小组，每个小组有若干条记录。如图所示，记录中最前面的小方块中的数字，代表的是当前分组的记录条数，最小和最大的槽指向2个特殊的伪记录。有了槽之后，我们按照主键搜索页中记录时，就可以采用二分法快速搜索，无需从最小记录开始遍历整个页中的记录链表。 举一个例子，如果要搜索主键（PK）&#x3D;15的记录： 先二分得出槽中间位是(0+6)&#x2F;2&#x3D;3，看到其指向的记录是12＜15，所以需要从#3槽后继续搜索记录； 再使用二分搜索出#3槽和#6槽的中间位是(3+6)&#x2F;2&#x3D;4.5取整4，#4槽对应的记录是16＞15，所以记录一定在#4槽中； 再从#3槽指向的12号记录开始向下搜索3次，定位到15号记录。 理解了InnoDB存储数据的原理后，我们就可以继续学习MySQL索引相关的原理和坑了。 聚簇索引和二级索引说到索引，页目录就是最简单的索引，是通过对记录进行一级分组来降低搜索的时间复杂度。但，这样能够降低的时间复杂度数量级，非常有限。当有无数个数据页来存储表数据的时候，我们就需要考虑如何建立合适的索引，才能方便定位记录所在的页。 为了解决这个问题，InnoDB引入了B+树。如下图所示，B+树是一棵倒过来的树： B+树的特点包括： 最底层的节点叫作叶子节点，用来存放数据； 其他上层节点叫作非叶子节点，仅用来存放目录项，作为索引； 非叶子节点分为不同层次，通过分层来降低每一层的搜索量； 所有节点按照索引键大小排序，构成一个双向链表，加速范围查找。 因此，InnoDB使用B+树，既可以保存实际数据，也可以加速数据搜索，这就是聚簇索引。如果把上图叶子节点下面方块中的省略号看作实际数据的话，那么它就是聚簇索引的示意图。由于数据在物理上只会保存一份，所以包含实际数据的聚簇索引只能有一个。 InnoDB会自动使用主键（唯一定义一条记录的单个或多个字段）作为聚簇索引的索引键（如果没有主键，就选择第一个不包含NULL值的唯一列）。上图方框中的数字代表了索引键的值，对聚簇索引而言一般就是主键。 我们再看看B+树如何实现快速查找主键。比如，我们要搜索PK&#x3D;4的数据，通过根节点中的索引可以知道数据在第一个记录指向的2号页中，通过2号页的索引又可以知道数据在5号页，5号页就是实际的数据页，然后再通过二分法查找页目录马上可以找到记录的指针。 为了实现非主键字段的快速搜索，就引出了二级索引，也叫作非聚簇索引、辅助索引。二级索引，也是利用的B+树的数据结构，如下图所示： 这次二级索引的叶子节点中保存的不是实际数据，而是主键，获得主键值后去聚簇索引中获得数据行。这个过程就叫作回表。 举个例子，有个索引是针对用户名字段创建的，索引记录上面方块中的字母是用户名，按照顺序形成链表。如果我们要搜索用户名为b的数据，经过两次定位可以得出在#5数据页中，查出所有的主键为7和6，再拿着这两个主键继续使用聚簇索引进行两次回表得到完整数据。 考虑额外创建二级索引的代价创建二级索引的代价，主要表现在维护代价、空间代价和回表代价三个方面。接下来，我就与你仔细分析下吧。 首先是维护代价。创建N个二级索引，就需要再创建N棵B+树，新增数据时不仅要修改聚簇索引，还需要修改这N个二级索引。 我们通过实验测试一下创建索引的代价。假设有一个person表，有主键ID，以及name、score、create_time三个字段： CREATE TABLE &#96;person&#96; ( &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT, &#96;name&#96; varchar(255) NOT NULL, &#96;score&#96; int(11) NOT NULL, &#96;create_time&#96; timestamp NOT NULL, PRIMARY KEY (&#96;id&#96;) ) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4; 通过下面的存储过程循环创建10万条测试数据，我的机器的耗时是140秒（本文的例子均在MySQL 5.7.26中执行）： CREATE DEFINER&#x3D;&#96;root&#96;@&#96;%&#96; PROCEDURE &#96;insert_person&#96;() begin declare c_id integer default 1; while c_id&lt;&#x3D;100000 do insert into person values(c_id, concat(&#39;name&#39;,c_id), c_id+100, date_sub(NOW(), interval c_id second)); set c_id&#x3D;c_id+1; end while; end 如果再创建两个索引，一个是name和score构成的联合索引，另一个是单一列create_time的索引，那么创建10万条记录的耗时提高到154秒： KEY &#96;name_score&#96; (&#96;name&#96;,&#96;score&#96;) USING BTREE, KEY &#96;create_time&#96; (&#96;create_time&#96;) USING BTREE 这里，我再额外提一下，页中的记录都是按照索引值从小到大的顺序存放的，新增记录就需要往页中插入数据，现有的页满了就需要新创建一个页，把现有页的部分数据移过去，这就是页分裂；如果删除了许多数据使得页比较空闲，还需要进行页合并。页分裂和合并，都会有IO代价，并且可能在操作过程中产生死锁。 你可以查看这个文档，以进一步了解如何设置合理的合并阈值，来平衡页的空闲率和因为再次页分裂产生的代价。 其次是空间代价。虽然二级索引不保存原始数据，但要保存索引列的数据，所以会占用更多的空间。比如，person表创建了两个索引后，使用下面的SQL查看数据和索引占用的磁盘： SELECT DATA_LENGTH, INDEX_LENGTH FROM information_schema.TABLES WHERE TABLE_NAME&#x3D;&#39;person&#39; 结果显示，数据本身只占用了4.7M，而索引占用了8.4M。 最后是回表的代价。二级索引不保存原始数据，通过索引找到主键后需要再查询聚簇索引，才能得到我们要的数据。比如，使用SELECT * 按照name字段查询用户，使用EXPLAIN查看执行计划： EXPLAIN SELECT * FROM person WHERE NAME&#x3D;&#39;name1&#39; 执行计划如下，可以发现： key字段代表实际走的是哪个索引，其值是name_score，说明走的是name_score这个索引。 type字段代表了访问表的方式，其值ref说明是二级索引等值匹配，符合我们的查询。 把SQL中的*修改为NAME和SCORE，也就是SELECT name_score联合索引包含的两列： EXPLAIN SELECT NAME,SCORE FROM person WHERE NAME&#x3D;&#39;name1&#39; 再来看看执行计划： 可以看到，Extra列多了一行Using index的提示，证明这次查询直接查的是二级索引，免去了回表。 原因很简单，联合索引中其实保存了多个索引列的值，对于页中的记录先按照字段1排序，如果相同再按照字段2排序，如图所示： 图中，叶子节点每一条记录的第一和第二个方块是索引列的数据，第三个方块是记录的主键。如果我们需要查询的是索引列索引或联合索引能覆盖的数据，那么查询索引本身已经“覆盖”了需要的数据，不再需要回表查询。因此，这种情况也叫作索引覆盖。我会在最后一小节介绍如何查看不同查询的成本，和你一起看看索引覆盖和索引查询后回表的代价差异。 最后，我和你总结下关于索引开销的最佳实践吧。 第一，无需一开始就建立索引，可以等到业务场景明确后，或者是数据量超过1万、查询变慢后，再针对需要查询、排序或分组的字段创建索引。创建索引后可以使用EXPLAIN命令，确认查询是否可以使用索引。我会在下一小节展开说明。 第二，尽量索引轻量级的字段，比如能索引int字段就不要索引varchar字段。索引字段也可以是部分前缀，在创建的时候指定字段索引长度。针对长文本的搜索，可以考虑使用Elasticsearch等专门用于文本搜索的索引数据库。 第三，尽量不要在SQL语句中SELECT *，而是SELECT必要的字段，甚至可以考虑使用联合索引来包含我们要搜索的字段，既能实现索引加速，又可以避免回表的开销。 不是所有针对索引列的查询都能用上索引在上一个案例中，我创建了一个name+score的联合索引，仅搜索name时就能够用上这个联合索引。这就引出两个问题： 是不是建了索引一定可以用上？ 怎么选择创建联合索引还是多个独立索引？ 首先，我们通过几个案例来分析一下索引失效的情况。 第一，索引只能匹配列前缀。比如下面的LIKE语句，搜索name后缀为name123的用户无法走索引，执行计划的type&#x3D;ALL代表了全表扫描： EXPLAIN SELECT * FROM person WHERE NAME LIKE &#39;%name123&#39; LIMIT 100 把百分号放到后面走前缀匹配，type&#x3D;range表示走索引扫描，key&#x3D;name_score看到实际走了name_score索引： EXPLAIN SELECT * FROM person WHERE NAME LIKE &#39;name123%&#39; LIMIT 100 原因很简单，索引B+树中行数据按照索引值排序，只能根据前缀进行比较。如果要按照后缀搜索也希望走索引的话，并且永远只是按照后缀搜索的话，可以把数据反过来存，用的时候再倒过来。 第二，条件涉及函数操作无法走索引。比如搜索条件用到了LENGTH函数，肯定无法走索引： EXPLAIN SELECT * FROM person WHERE LENGTH(NAME)&#x3D;7 同样的原因，索引保存的是索引列的原始值，而不是经过函数计算后的值。如果需要针对函数调用走数据库索引的话，只能保存一份函数变换后的值，然后重新针对这个计算列做索引。 第三，联合索引只能匹配左边的列。也就是说，虽然对name和score建了联合索引，但是仅按照score列搜索无法走索引： EXPLAIN SELECT * FROM person WHERE SCORE&gt;45678 原因也很简单，在联合索引的情况下，数据是按照索引第一列排序，第一列数据相同时才会按照第二列排序。也就是说，如果我们想使用联合索引中尽可能多的列，查询条件中的各个列必须是联合索引中从最左边开始连续的列。如果我们仅仅按照第二列搜索，肯定无法走索引。尝试把搜索条件加入name列，可以看到走了name_score索引： EXPLAIN SELECT * FROM person WHERE SCORE&gt;45678 AND NAME LIKE &#39;NAME45%&#39; 需要注意的是，因为有查询优化器，所以name作为WHERE子句的第几个条件并不是很重要。 现在回到最开始的两个问题。 是不是建了索引一定可以用上？并不是，只有当查询能符合索引存储的实际结构时，才能用上。这里，我只给出了三个肯定用不上索引的反例。其实，有的时候即使可以走索引，MySQL也不一定会选择使用索引。我会在下一小节展开这一点。 怎么选择建联合索引还是多个独立索引？如果你的搜索条件经常会使用多个字段进行搜索，那么可以考虑针对这几个字段建联合索引；同时，针对多字段建立联合索引，使用索引覆盖的可能更大。如果只会查询单个字段，可以考虑建单独的索引，毕竟联合索引保存了不必要字段也有成本。 数据库基于成本决定是否走索引通过前面的案例，我们可以看到，查询数据可以直接在聚簇索引上进行全表扫描，也可以走二级索引扫描后到聚簇索引回表。看到这里，你不禁要问了，MySQL到底是怎么确定走哪种方案的呢。 其实，MySQL在查询数据之前，会先对可能的方案做执行计划，然后依据成本决定走哪个执行计划。 这里的成本，包括IO成本和CPU成本： IO成本，是从磁盘把数据加载到内存的成本。默认情况下，读取数据页的IO成本常数是1（也就是读取1个页成本是1）。 CPU成本，是检测数据是否满足条件和排序等CPU操作的成本。默认情况下，检测记录的成本是0.2。 基于此，我们分析下全表扫描的成本。 全表扫描，就是把聚簇索引中的记录依次和给定的搜索条件做比较，把符合搜索条件的记录加入结果集的过程。那么，要计算全表扫描的代价需要两个信息： 聚簇索引占用的页面数，用来计算读取数据的IO成本； 表中的记录数，用来计算搜索的CPU成本。 那么，MySQL是实时统计这些信息的吗？其实并不是，MySQL维护了表的统计信息，可以使用下面的命令查看： SHOW TABLE STATUS LIKE &#39;person&#39; 输出如下： 可以看到： 总行数是100086行（之前EXPLAIN时，也看到rows为100086）。你可能说，person表不是有10万行记录吗，为什么这里多了86行？其实，MySQL的统计信息是一个估算，其统计方式比较复杂我就不再展开了。但不妨碍我们根据这个值估算CPU成本，是100086*0.2&#x3D;20017左右。 数据长度是4734976字节。对于InnoDB来说，这就是聚簇索引占用的空间，等于聚簇索引的页面数量*每个页面的大小。InnoDB每个页面的大小是16KB，大概计算出页面数量是289，因此IO成本是289左右。 所以，全表扫描的总成本是20306左右。 接下来，我还是用person表这个例子，和你分析下MySQL如何基于成本来制定执行计划。现在，我要用下面的SQL查询name&gt;‘name84059’ AND create_time&gt;‘2020-01-24 05:00:00’ EXPLAIN SELECT * FROM person WHERE NAME &gt;&#39;name84059&#39; AND create_time&gt;&#39;2020-01-24 05:00:00&#39; 其执行计划是全表扫描： 只要把create_time条件中的5点改为6点就变为走索引了，并且走的是create_time索引而不是name_score联合索引： 我们可以得到两个结论： MySQL选择索引，并不是按照WHERE条件中列的顺序进行的； 即便列有索引，甚至有多个可能的索引方案，MySQL也可能不走索引。 其原因就是，MySQL并不是猜拳决定是否走索引的，而是根据成本来判断的。虽然表的统计信息不完全准确，但足够用于策略的判断了。 不过，有时会因为统计信息的不准确或成本估算的问题，实际开销会和MySQL统计出来的差距较大，导致MySQL选择错误的索引或是直接选择走全表扫描，这个时候就需要人工干预，使用强制索引了。比如，像这样强制走name_score索引： EXPLAIN SELECT * FROM person FORCE INDEX(name_score) WHERE NAME &gt;&#39;name84059&#39; AND create_time&gt;&#39;2020-01-24 05:00:00&#39; 我们介绍了MySQL会根据成本选择执行计划，也通过EXPLAIN知道了优化器最终会选择怎样的执行计划，但MySQL如何制定执行计划始终是一个黑盒。那么，有没有什么办法可以了解各种执行计划的成本，以及MySQL做出选择的依据呢？ 在MySQL 5.6及之后的版本中，我们可以使用optimizer trace功能查看优化器生成执行计划的整个过程。有了这个功能，我们不仅可以了解优化器的选择过程，更可以了解每一个执行环节的成本，然后依靠这些信息进一步优化查询。 如下代码所示，打开optimizer_trace后，再执行SQL就可以查询information_schema.OPTIMIZER_TRACE表查看执行计划了，最后可以关闭optimizer_trace功能： SET optimizer_trace&#x3D;&quot;enabled&#x3D;on&quot;; SELECT * FROM person WHERE NAME &gt;&#39;name84059&#39; AND create_time&gt;&#39;2020-01-24 05:00:00&#39;; SELECT * FROM information_schema.OPTIMIZER_TRACE; SET optimizer_trace&#x3D;&quot;enabled&#x3D;off&quot;; 对于按照create_time&gt;’2020-01-24 05:00:00’条件走全表扫描的SQL，我从OPTIMIZER_TRACE的执行结果中，摘出了几个重要片段来重点分析： 使用name_score对name84059&lt;name条件进行索引扫描需要扫描25362行，成本是30435，因此最终没有选择这个方案。这里的30435是查询二级索引的IO成本和CPU成本之和，再加上回表查询聚簇索引的IO成本和CPU成本之和，我就不再具体分析了： &#123; &quot;index&quot;: &quot;name_score&quot;, &quot;ranges&quot;: [ &quot;name84059 &lt; name&quot; ], &quot;rows&quot;: 25362, &quot;cost&quot;: 30435, &quot;chosen&quot;: false, &quot;cause&quot;: &quot;cost&quot; &#125;, 使用create_time进行索引扫描需要扫描23758行，成本是28511，同样因为成本原因没有选择这个方案： &#123; &quot;index&quot;: &quot;create_time&quot;, &quot;ranges&quot;: [ &quot;0x5e2a79d0 &lt; create_time&quot; ], &quot;rows&quot;: 23758, &quot;cost&quot;: 28511, &quot;chosen&quot;: false, &quot;cause&quot;: &quot;cost&quot; &#125; 最终选择了全表扫描方式作为执行计划。可以看到，全表扫描100086条记录的成本是20306，和我们之前计算的一致，显然是小于其他两个方案的28511和30435： &#123; &quot;considered_execution_plans&quot;: [&#123; &quot;table&quot;: &quot;&#96;person&#96;&quot;, &quot;best_access_path&quot;: &#123; &quot;considered_access_paths&quot;: [&#123; &quot;rows_to_scan&quot;: 100086, &quot;access_type&quot;: &quot;scan&quot;, &quot;resulting_rows&quot;: 100086, &quot;cost&quot;: 20306, &quot;chosen&quot;: true &#125;] &#125;, &quot;rows_for_plan&quot;: 100086, &quot;cost_for_plan&quot;: 20306, &quot;chosen&quot;: true &#125;] &#125;, 把SQL中的create_time条件从05:00改为06:00，再次分析OPTIMIZER_TRACE可以看到，这次执行计划选择的是走create_time索引。因为是查询更晚时间的数据，走create_time索引需要扫描的行数从23758减少到了16588。这次走这个索引的成本19907小于全表扫描的20306，更小于走name_score索引的30435： &#123; &quot;index&quot;: &quot;create_time&quot;, &quot;ranges&quot;: [ &quot;0x5e2a87e0 &lt; create_time&quot; ], &quot;rows&quot;: 16588, &quot;cost&quot;: 19907, &quot;chosen&quot;: true &#125; 有关optimizer trace的更多信息，你可以参考MySQL的文档。 重点回顾今天，我先和你分析了MySQL InnoDB存储引擎页、聚簇索引和二级索引的结构，然后分析了关于索引的两个误区。 第一个误区是，考虑到索引的维护代价、空间占用和查询时回表的代价，不能认为索引越多越好。索引一定是按需创建的，并且要尽可能确保足够轻量。一旦创建了多字段的联合索引，我们要考虑尽可能利用索引本身完成数据查询，减少回表的成本。 第二个误区是，不能认为建了索引就一定有效，对于后缀的匹配查询、查询中不包含联合索引的第一列、查询条件涉及函数计算等情况无法使用索引。此外，即使SQL本身符合索引的使用条件，MySQL也会通过评估各种查询方式的代价，来决定是否走索引，以及走哪个索引。 因此，在尝试通过索引进行SQL性能优化的时候，务必通过执行计划或实际的效果来确认索引是否能有效改善性能问题，否则增加了索引不但没解决性能问题，还增加了数据库增删改的负担。如果对EXPLAIN给出的执行计划有疑问的话，你还可以利用optimizer_trace查看详细的执行计划做进一步分析。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在介绍二级索引代价时，我们通过EXPLAIN命令看到了索引覆盖和回表的两种情况。你能用optimizer trace来分析一下这两种情况的成本差异吗？ 索引除了可以用于加速搜索外，还可以在排序时发挥作用，你能通过EXPLAIN来证明吗？你知道，在什么情况下针对排序索引会失效吗？ 针对数据库索引，你还有什么心得吗？我是朱晔，欢迎在评论区与我留言分享，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/08、判等问题：程序里如何确定你就是你？","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/08、判等问题：程序里如何确定你就是你？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/08%E3%80%81%E5%88%A4%E7%AD%89%E9%97%AE%E9%A2%98%EF%BC%9A%E7%A8%8B%E5%BA%8F%E9%87%8C%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9A%E4%BD%A0%E5%B0%B1%E6%98%AF%E4%BD%A0%EF%BC%9F/","excerpt":"","text":"08 | 判等问题：程序里如何确定你就是你？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来和你聊聊程序里的判等问题。 你可能会说，判等不就是一行代码的事情吗，有什么好说的。但，这一行代码如果处理不当，不仅会出现Bug，还可能会引起内存泄露等问题。涉及判等的Bug，即使是使用&#x3D;&#x3D;这种错误的判等方式，也不是所有时候都会出问题。所以类似的判等问题不太容易发现，可能会被隐藏很久。 今天，我就equals、compareTo和Java的数值缓存、字符串驻留等问题展开讨论，希望你可以理解其原理，彻底消除业务代码中的相关Bug。 注意equals和&#x3D;&#x3D;的区别在业务代码中，我们通常使用equals或&#x3D;&#x3D; 进行判等操作。equals是方法而&#x3D;&#x3D;是操作符，它们的使用是有区别的： 对基本类型，比如int、long，进行判等，只能使用&#x3D;&#x3D;，比较的是直接值。因为基本类型的值就是其数值。 对引用类型，比如Integer、Long和String，进行判等，需要使用equals进行内容判等。因为引用类型的直接值是指针，使用&#x3D;&#x3D;的话，比较的是指针，也就是两个对象在内存中的地址，即比较它们是不是同一个对象，而不是比较对象的内容。 这就引出了我们必须必须要知道的第一个结论：比较值的内容，除了基本类型只能使用&#x3D;&#x3D;外，其他类型都需要使用equals。 在开篇我提到了，即使使用&#x3D;&#x3D;对Integer或String进行判等，有些时候也能得到正确结果。这又是为什么呢？ 我们用下面的测试用例深入研究下： 使用&#x3D;&#x3D;对两个值为127的直接赋值的Integer对象判等； 使用&#x3D;&#x3D;对两个值为128的直接赋值的Integer对象判等； 使用&#x3D;&#x3D;对一个值为127的直接赋值的Integer和另一个通过new Integer声明的值为127的对象判等； 使用&#x3D;&#x3D;对两个通过new Integer声明的值为127的对象判等； 使用&#x3D;&#x3D;对一个值为128的直接赋值的Integer对象和另一个值为128的int基本类型判等。 Integer a &#x3D; 127; &#x2F;&#x2F;Integer.valueOf(127) Integer b &#x3D; 127; &#x2F;&#x2F;Integer.valueOf(127) log.info(&quot;\\nInteger a &#x3D; 127;\\n&quot; + &quot;Integer b &#x3D; 127;\\n&quot; + &quot;a &#x3D;&#x3D; b ? &#123;&#125;&quot;,a &#x3D;&#x3D; b); &#x2F;&#x2F; true Integer c &#x3D; 128; &#x2F;&#x2F;Integer.valueOf(128) Integer d &#x3D; 128; &#x2F;&#x2F;Integer.valueOf(128) log.info(&quot;\\nInteger c &#x3D; 128;\\n&quot; + &quot;Integer d &#x3D; 128;\\n&quot; + &quot;c &#x3D;&#x3D; d ? &#123;&#125;&quot;, c &#x3D;&#x3D; d); &#x2F;&#x2F;false Integer e &#x3D; 127; &#x2F;&#x2F;Integer.valueOf(127) Integer f &#x3D; new Integer(127); &#x2F;&#x2F;new instance log.info(&quot;\\nInteger e &#x3D; 127;\\n&quot; + &quot;Integer f &#x3D; new Integer(127);\\n&quot; + &quot;e &#x3D;&#x3D; f ? &#123;&#125;&quot;, e &#x3D;&#x3D; f); &#x2F;&#x2F;false Integer g &#x3D; new Integer(127); &#x2F;&#x2F;new instance Integer h &#x3D; new Integer(127); &#x2F;&#x2F;new instance log.info(&quot;\\nInteger g &#x3D; new Integer(127);\\n&quot; + &quot;Integer h &#x3D; new Integer(127);\\n&quot; + &quot;g &#x3D;&#x3D; h ? &#123;&#125;&quot;, g &#x3D;&#x3D; h); &#x2F;&#x2F;false Integer i &#x3D; 128; &#x2F;&#x2F;unbox int j &#x3D; 128; log.info(&quot;\\nInteger i &#x3D; 128;\\n&quot; + &quot;int j &#x3D; 128;\\n&quot; + &quot;i &#x3D;&#x3D; j ? &#123;&#125;&quot;, i &#x3D;&#x3D; j); &#x2F;&#x2F;true 通过运行结果可以看到，虽然看起来永远是在对127和127、128和128判等，但&#x3D;&#x3D;却没有永远给我们true的答复。原因是什么呢？ 第一个案例中，编译器会把Integer a &#x3D; 127转换为Integer.valueOf(127)。查看源码可以发现，这个转换在内部其实做了缓存，使得两个Integer指向同一个对象，所以&#x3D;&#x3D;返回true。 public static Integer valueOf(int i) &#123; if (i &gt;&#x3D; IntegerCache.low &amp;&amp; i &lt;&#x3D; IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 第二个案例中，之所以同样的代码128就返回false的原因是，默认情况下会缓存[-128, 127]的数值，而128处于这个区间之外。设置JVM参数加上-XX:AutoBoxCacheMax&#x3D;1000再试试，是不是就返回true了呢？ private static class IntegerCache &#123; static final int low &#x3D; -128; static final int high; static &#123; &#x2F;&#x2F; high value may be configured by property int h &#x3D; 127; String integerCacheHighPropValue &#x3D; sun.misc.VM.getSavedProperty(&quot;java.lang.Integer.IntegerCache.high&quot;); if (integerCacheHighPropValue !&#x3D; null) &#123; try &#123; int i &#x3D; parseInt(integerCacheHighPropValue); i &#x3D; Math.max(i, 127); &#x2F;&#x2F; Maximum array size is Integer.MAX_VALUE h &#x3D; Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; &#x2F;&#x2F; If the property cannot be parsed into an int, ignore it. &#125; &#125; high &#x3D; h; cache &#x3D; new Integer[(high - low) + 1]; int j &#x3D; low; for(int k &#x3D; 0; k &lt; cache.length; k++) cache[k] &#x3D; new Integer(j++); &#x2F;&#x2F; range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;&#x3D; 127; &#125; &#125; 第三和第四个案例中，New出来的Integer始终是不走缓存的新对象。比较两个新对象，或者比较一个新对象和一个来自缓存的对象，结果肯定不是相同的对象，因此返回false。 第五个案例中，我们把装箱的Integer和基本类型int比较，前者会先拆箱再比较，比较的肯定是数值而不是引用，因此返回true。 看到这里，对于Integer什么时候是相同对象什么时候是不同对象，就很清楚了吧。但知道这些其实意义不大，因为在大多数时候，我们并不关心Integer对象是否是同一个，**只需要记得比较Integer的值请使用equals，而不是&#x3D;&#x3D;**（对于基本类型int的比较当然只能使用&#x3D;&#x3D;）。 其实，我们应该都知道这个原则，只是有的时候特别容易忽略。以我之前遇到过的一个生产事故为例，有这么一个枚举定义了订单状态和对于状态的描述： enum StatusEnum &#123; CREATED(1000, &quot;已创建&quot;), PAID(1001, &quot;已支付&quot;), DELIVERED(1002, &quot;已送到&quot;), FINISHED(1003, &quot;已完成&quot;); private final Integer status; &#x2F;&#x2F;注意这里的Integer private final String desc; StatusEnum(Integer status, String desc) &#123; this.status &#x3D; status; this.desc &#x3D; desc; &#125; &#125; 在业务代码中，开发同学使用了&#x3D;&#x3D;对枚举和入参OrderQuery中的status属性进行判等： @Data public class OrderQuery &#123; private Integer status; private String name; &#125; @PostMapping(&quot;enumcompare&quot;) public void enumcompare(@RequestBody OrderQuery orderQuery)&#123; StatusEnum statusEnum &#x3D; StatusEnum.DELIVERED; log.info(&quot;orderQuery:&#123;&#125; statusEnum:&#123;&#125; result:&#123;&#125;&quot;, orderQuery, statusEnum, statusEnum.status &#x3D;&#x3D; orderQuery.getStatus()); &#125; 因为枚举和入参OrderQuery中的status都是包装类型，所以通过&#x3D;&#x3D;判等肯定是有问题的。只是这个问题比较隐晦，究其原因在于： 只看枚举的定义CREATED(1000, “已创建”)，容易让人误解status值是基本类型； 因为有Integer缓存机制的存在，所以使用&#x3D;&#x3D;判等并不是所有情况下都有问题。在这次事故中，订单状态的值从100开始增长，程序一开始不出问题，直到订单状态超过127后才出现Bug。 在了解清楚为什么Integer使用&#x3D;&#x3D;判等有时候也有效的原因之后，我们再来看看为什么String也有这个问题。我们使用几个用例来测试下： 对两个直接声明的值都为1的String使用&#x3D;&#x3D;判等； 对两个new出来的值都为2的String使用&#x3D;&#x3D;判等； 对两个new出来的值都为3的String先进行intern操作，再使用&#x3D;&#x3D;判等； 对两个new出来的值都为4的String通过equals判等。 String a &#x3D; &quot;1&quot;; String b &#x3D; &quot;1&quot;; log.info(&quot;\\nString a &#x3D; \\&quot;1\\&quot;;\\n&quot; + &quot;String b &#x3D; \\&quot;1\\&quot;;\\n&quot; + &quot;a &#x3D;&#x3D; b ? &#123;&#125;&quot;, a &#x3D;&#x3D; b); &#x2F;&#x2F;true String c &#x3D; new String(&quot;2&quot;); String d &#x3D; new String(&quot;2&quot;); log.info(&quot;\\nString c &#x3D; new String(\\&quot;2\\&quot;);\\n&quot; + &quot;String d &#x3D; new String(\\&quot;2\\&quot;);&quot; + &quot;c &#x3D;&#x3D; d ? &#123;&#125;&quot;, c &#x3D;&#x3D; d); &#x2F;&#x2F;false String e &#x3D; new String(&quot;3&quot;).intern(); String f &#x3D; new String(&quot;3&quot;).intern(); log.info(&quot;\\nString e &#x3D; new String(\\&quot;3\\&quot;).intern();\\n&quot; + &quot;String f &#x3D; new String(\\&quot;3\\&quot;).intern();\\n&quot; + &quot;e &#x3D;&#x3D; f ? &#123;&#125;&quot;, e &#x3D;&#x3D; f); &#x2F;&#x2F;true String g &#x3D; new String(&quot;4&quot;); String h &#x3D; new String(&quot;4&quot;); log.info(&quot;\\nString g &#x3D; new String(\\&quot;4\\&quot;);\\n&quot; + &quot;String h &#x3D; new String(\\&quot;4\\&quot;);\\n&quot; + &quot;g &#x3D;&#x3D; h ? &#123;&#125;&quot;, g.equals(h)); &#x2F;&#x2F;true 在分析这个结果之前，我先和你说说Java的字符串常量池机制。首先要明确的是其设计初衷是节省内存。当代码中出现双引号形式创建字符串对象时，JVM会先对这个字符串进行检查，如果字符串常量池中存在相同内容的字符串对象的引用，则将这个引用返回；否则，创建新的字符串对象，然后将这个引用放入字符串常量池，并返回该引用。这种机制，就是字符串驻留或池化。 再回到刚才的例子，再来分析一下运行结果： 第一个案例返回true，因为Java的字符串驻留机制，直接使用双引号声明出来的两个String对象指向常量池中的相同字符串。 第二个案例，new出来的两个String是不同对象，引用当然不同，所以得到false的结果。 第三个案例，使用String提供的intern方法也会走常量池机制，所以同样能得到true。 第四个案例，通过equals对值内容判等，是正确的处理方式，当然会得到true。 虽然使用new声明的字符串调用intern方法，也可以让字符串进行驻留，但在业务代码中滥用intern，可能会产生性能问题。 写代码测试一下，通过循环把1到1000万之间的数字以字符串形式intern后，存入一个List： List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); @GetMapping(&quot;internperformance&quot;) public int internperformance(@RequestParam(value &#x3D; &quot;size&quot;, defaultValue &#x3D; &quot;10000000&quot;)int size) &#123; &#x2F;&#x2F;-XX:+PrintStringTableStatistics &#x2F;&#x2F;-XX:StringTableSize&#x3D;10000000 long begin &#x3D; System.currentTimeMillis(); list &#x3D; IntStream.rangeClosed(1, size) .mapToObj(i-&gt; String.valueOf(i).intern()) .collect(Collectors.toList()); log.info(&quot;size:&#123;&#125; took:&#123;&#125;&quot;, size, System.currentTimeMillis() - begin); return list.size(); &#125; 在启动程序时设置JVM参数-XX:+PrintStringTableStatistic，程序退出时可以打印出字符串常量表的统计信息。调用接口后关闭程序，输出如下： [11:01:57.770] [http-nio-45678-exec-2] [INFO ] [.t.c.e.d.IntAndStringEqualController:54 ] - size:10000000 took:44907 StringTable statistics: Number of buckets : 60013 &#x3D; 480104 bytes, avg 8.000 Number of entries : 10030230 &#x3D; 240725520 bytes, avg 24.000 Number of literals : 10030230 &#x3D; 563005568 bytes, avg 56.131 Total footprint : &#x3D; 804211192 bytes Average bucket size : 167.134 Variance of bucket size : 55.808 Std. dev. of bucket size: 7.471 Maximum bucket size : 198 可以看到，1000万次intern操作耗时居然超过了44秒。 其实，原因在于字符串常量池是一个固定容量的Map。如果容量太小（Number of buckets&#x3D;60013）、字符串太多（1000万个字符串），那么每一个桶中的字符串数量会非常多，所以搜索起来就很慢。输出结果中的Average bucket size&#x3D;167，代表了Map中桶的平均长度是167。 解决方式是，设置JVM参数-XX:StringTableSize，指定更多的桶。设置-XX:StringTableSize&#x3D;10000000后，重启应用： [11:09:04.475] [http-nio-45678-exec-1] [INFO ] [.t.c.e.d.IntAndStringEqualController:54 ] - size:10000000 took:5557 StringTable statistics: Number of buckets : 10000000 &#x3D; 80000000 bytes, avg 8.000 Number of entries : 10030156 &#x3D; 240723744 bytes, avg 24.000 Number of literals : 10030156 &#x3D; 562999472 bytes, avg 56.131 Total footprint : &#x3D; 883723216 bytes Average bucket size : 1.003 Variance of bucket size : 1.587 Std. dev. of bucket size: 1.260 Maximum bucket size : 10 可以看到，1000万次调用耗时只有5.5秒，Average bucket size降到了1，效果明显。 好了，是时候给出第二原则了：没事别轻易用intern，如果要用一定要注意控制驻留的字符串的数量，并留意常量表的各项指标。 实现一个equals没有这么简单如果看过Object类源码，你可能就知道，equals的实现其实是比较对象引用： public boolean equals(Object obj) &#123; return (this &#x3D;&#x3D; obj); &#125; 之所以Integer或String能通过equals实现内容判等，是因为它们都重写了这个方法。比如，String的equals的实现： public boolean equals(Object anObject) &#123; if (this &#x3D;&#x3D; anObject) &#123; return true; &#125; if (anObject instanceof String) &#123; String anotherString &#x3D; (String)anObject; int n &#x3D; value.length; if (n &#x3D;&#x3D; anotherString.value.length) &#123; char v1[] &#x3D; value; char v2[] &#x3D; anotherString.value; int i &#x3D; 0; while (n-- !&#x3D; 0) &#123; if (v1[i] !&#x3D; v2[i]) return false; i++; &#125; return true; &#125; &#125; return false; &#125; 对于自定义类型，如果不重写equals的话，默认就是使用Object基类的按引用的比较方式。我们写一个自定义类测试一下。 假设有这样一个描述点的类Point，有x、y和描述三个属性： class Point &#123; private int x; private int y; private final String desc; public Point(int x, int y, String desc) &#123; this.x &#x3D; x; this.y &#x3D; y; this.desc &#x3D; desc; &#125; &#125; 定义三个点p1、p2和p3，其中p1和p2的描述属性不同，p1和p3的三个属性完全相同，并写一段代码测试一下默认行为： Point p1 &#x3D; new Point(1, 2, &quot;a&quot;); Point p2 &#x3D; new Point(1, 2, &quot;b&quot;); Point p3 &#x3D; new Point(1, 2, &quot;a&quot;); log.info(&quot;p1.equals(p2) ? &#123;&#125;&quot;, p1.equals(p2)); log.info(&quot;p1.equals(p3) ? &#123;&#125;&quot;, p1.equals(p3)); 通过equals方法比较p1和p2、p1和p3均得到false，原因正如刚才所说，我们并没有为Point类实现自定义的equals方法，Object超类中的equals默认使用&#x3D;&#x3D;判等，比较的是对象的引用。 我们期望的逻辑是，只要x和y这2个属性一致就代表是同一个点，所以写出了如下的改进代码，重写equals方法，把参数中的Object转换为Point比较其x和y属性： class PointWrong &#123; private int x; private int y; private final String desc; public PointWrong(int x, int y, String desc) &#123; this.x &#x3D; x; this.y &#x3D; y; this.desc &#x3D; desc; &#125; @Override public boolean equals(Object o) &#123; PointWrong that &#x3D; (PointWrong) o; return x &#x3D;&#x3D; that.x &amp;&amp; y &#x3D;&#x3D; that.y; &#125; &#125; 为测试改进后的Point是否可以满足需求，我们定义了三个用例： 比较一个Point对象和null； 比较一个Object对象和一个Point对象； 比较两个x和y属性值相同的Point对象。 PointWrong p1 &#x3D; new PointWrong(1, 2, &quot;a&quot;); try &#123; log.info(&quot;p1.equals(null) ? &#123;&#125;&quot;, p1.equals(null)); &#125; catch (Exception ex) &#123; log.error(ex.getMessage()); &#125; Object o &#x3D; new Object(); try &#123; log.info(&quot;p1.equals(expression) ? &#123;&#125;&quot;, p1.equals(o)); &#125; catch (Exception ex) &#123; log.error(ex.getMessage()); &#125; PointWrong p2 &#x3D; new PointWrong(1, 2, &quot;b&quot;); log.info(&quot;p1.equals(p2) ? &#123;&#125;&quot;, p1.equals(p2)); 通过日志中的结果可以看到，第一次比较出现了空指针异常，第二次比较出现了类型转换异常，第三次比较符合预期输出了true。 [17:54:39.120] [http-nio-45678-exec-1] [ERROR] [t.c.e.demo1.EqualityMethodController:32 ] - java.lang.NullPointerException [17:54:39.120] [http-nio-45678-exec-1] [ERROR] [t.c.e.demo1.EqualityMethodController:39 ] - java.lang.ClassCastException: java.lang.Object cannot be cast to org.geekbang.time.commonmistakes.equals.demo1.EqualityMethodController$PointWrong [17:54:39.120] [http-nio-45678-exec-1] [INFO ] [t.c.e.demo1.EqualityMethodController:43 ] - p1.equals(p2) ? true 通过这些失效的用例，我们大概可以总结出实现一个更好的equals应该注意的点： 考虑到性能，可以先进行指针判等，如果对象是同一个那么直接返回true； 需要对另一方进行判空，空对象和自身进行比较，结果一定是fasle； 需要判断两个对象的类型，如果类型都不同，那么直接返回false； 确保类型相同的情况下再进行类型强制转换，然后逐一判断所有字段。 修复和改进后的equals方法如下： @Override public boolean equals(Object o) &#123; if (this &#x3D;&#x3D; o) return true; if (o &#x3D;&#x3D; null || getClass() !&#x3D; o.getClass()) return false; PointRight that &#x3D; (PointRight) o; return x &#x3D;&#x3D; that.x &amp;&amp; y &#x3D;&#x3D; that.y; &#125; 改进后的equals看起来完美了，但还没完。我们继续往下看。 hashCode和equals要配对实现我们来试试下面这个用例，定义两个x和y属性值完全一致的Point对象p1和p2，把p1加入HashSet，然后判断这个Set中是否存在p2： PointWrong p1 &#x3D; new PointWrong(1, 2, &quot;a&quot;); PointWrong p2 &#x3D; new PointWrong(1, 2, &quot;b&quot;); HashSet&lt;PointWrong&gt; points &#x3D; new HashSet&lt;&gt;(); points.add(p1); log.info(&quot;points.contains(p2) ? &#123;&#125;&quot;, points.contains(p2)); 按照改进后的equals方法，这2个对象可以认为是同一个，Set中已经存在了p1就应该包含p2，但结果却是false。 出现这个Bug的原因是，散列表需要使用hashCode来定位元素放到哪个桶。如果自定义对象没有实现自定义的hashCode方法，就会使用Object超类的默认实现，得到的两个hashCode是不同的，导致无法满足需求。 要自定义hashCode，我们可以直接使用Objects.hash方法来实现，改进后的Point类如下： class PointRight &#123; private final int x; private final int y; private final String desc; ... @Override public boolean equals(Object o) &#123; ... &#125; @Override public int hashCode() &#123; return Objects.hash(x, y); &#125; &#125; 改进equals和hashCode后，再测试下之前的四个用例，结果全部符合预期。 [18:25:23.091] [http-nio-45678-exec-4] [INFO ] [t.c.e.demo1.EqualityMethodController:54 ] - p1.equals(null) ? false [18:25:23.093] [http-nio-45678-exec-4] [INFO ] [t.c.e.demo1.EqualityMethodController:61 ] - p1.equals(expression) ? false [18:25:23.094] [http-nio-45678-exec-4] [INFO ] [t.c.e.demo1.EqualityMethodController:67 ] - p1.equals(p2) ? true [18:25:23.094] [http-nio-45678-exec-4] [INFO ] [t.c.e.demo1.EqualityMethodController:71 ] - points.contains(p2) ? true 看到这里，你可能会觉得自己实现equals和hashCode很麻烦，实现equals有很多注意点而且代码量很大。不过，实现这两个方法也有简单的方式，一是后面要讲到的Lombok方法，二是使用IDE的代码生成功能。IDEA的类代码快捷生成菜单支持的功能如下： 注意compareTo和equals的逻辑一致性除了自定义类型需要确保equals和hashCode要逻辑一致外，还有一个更容易被忽略的问题，即compareTo同样需要和equals确保逻辑一致性。 我之前遇到过这么一个问题，代码里本来使用了ArrayList的indexOf方法进行元素搜索，但是一位好心的开发同学觉得逐一比较的时间复杂度是O(n)，效率太低了，于是改为了排序后通过Collections.binarySearch方法进行搜索，实现了O(log n)的时间复杂度。没想到，这么一改却出现了Bug。 我们来重现下这个问题。首先，定义一个Student类，有id和name两个属性，并实现了一个Comparable接口来返回两个id的值： @Data @AllArgsConstructor class Student implements Comparable&lt;Student&gt;&#123; private int id; private String name; @Override public int compareTo(Student other) &#123; int result &#x3D; Integer.compare(other.id, id); if (result&#x3D;&#x3D;0) log.info(&quot;this &#123;&#125; &#x3D;&#x3D; other &#123;&#125;&quot;, this, other); return result; &#125; &#125; 然后，写一段测试代码分别通过indexOf方法和Collections.binarySearch方法进行搜索。列表中我们存放了两个学生，第一个学生id是1叫zhang，第二个学生id是2叫wang，搜索这个列表是否存在一个id是2叫li的学生： @GetMapping(&quot;wrong&quot;) public void wrong()&#123; List&lt;Student&gt; list &#x3D; new ArrayList&lt;&gt;(); list.add(new Student(1, &quot;zhang&quot;)); list.add(new Student(2, &quot;wang&quot;)); Student student &#x3D; new Student(2, &quot;li&quot;); log.info(&quot;ArrayList.indexOf&quot;); int index1 &#x3D; list.indexOf(student); Collections.sort(list); log.info(&quot;Collections.binarySearch&quot;); int index2 &#x3D; Collections.binarySearch(list, student); log.info(&quot;index1 &#x3D; &quot; + index1); log.info(&quot;index2 &#x3D; &quot; + index2); &#125; 代码输出的日志如下： [18:46:50.226] [http-nio-45678-exec-1] [INFO ] [t.c.equals.demo2.CompareToController:28 ] - ArrayList.indexOf [18:46:50.226] [http-nio-45678-exec-1] [INFO ] [t.c.equals.demo2.CompareToController:31 ] - Collections.binarySearch [18:46:50.227] [http-nio-45678-exec-1] [INFO ] [t.c.equals.demo2.CompareToController:67 ] - this CompareToController.Student(id&#x3D;2, name&#x3D;wang) &#x3D;&#x3D; other CompareToController.Student(id&#x3D;2, name&#x3D;li) [18:46:50.227] [http-nio-45678-exec-1] [INFO ] [t.c.equals.demo2.CompareToController:34 ] - index1 &#x3D; -1 [18:46:50.227] [http-nio-45678-exec-1] [INFO ] [t.c.equals.demo2.CompareToController:35 ] - index2 &#x3D; 1 我们注意到如下几点： binarySearch方法内部调用了元素的compareTo方法进行比较； indexOf的结果没问题，列表中搜索不到id为2、name是li的学生； binarySearch返回了索引1，代表搜索到的结果是id为2，name是wang的学生。 修复方式很简单，确保compareTo的比较逻辑和equals的实现一致即可。重新实现一下Student类，通过Comparator.comparing这个便捷的方法来实现两个字段的比较： @Data @AllArgsConstructor class StudentRight implements Comparable&lt;StudentRight&gt;&#123; private int id; private String name; @Override public int compareTo(StudentRight other) &#123; return Comparator.comparing(StudentRight::getName) .thenComparingInt(StudentRight::getId) .compare(this, other); &#125; &#125; 其实，这个问题容易被忽略的原因在于两方面： 一是，我们使用了Lombok的@Data标记了Student，@Data注解（详见这里）其实包含了@EqualsAndHashCode注解（详见这里）的作用，也就是默认情况下使用类型所有的字段（不包括static和transient字段）参与到equals和hashCode方法的实现中。因为这两个方法的实现不是我们自己实现的，所以容易忽略其逻辑。 二是，compareTo方法需要返回数值，作为排序的依据，容易让人使用数值类型的字段随意实现。 我再强调下，对于自定义的类型，如果要实现Comparable，请记得equals、hashCode、compareTo三者逻辑一致。 小心Lombok生成代码的“坑”Lombok的@Data注解会帮我们实现equals和hashcode方法，但是有继承关系时，Lombok自动生成的方法可能就不是我们期望的了。 我们先来研究一下其实现：定义一个Person类型，包含姓名和身份证两个字段： @Data class Person &#123; private String name; private String identity; public Person(String name, String identity) &#123; this.name &#x3D; name; this.identity &#x3D; identity; &#125; &#125; 对于身份证相同、姓名不同的两个Person对象： Person person1 &#x3D; new Person(&quot;zhuye&quot;,&quot;001&quot;); Person person2 &#x3D; new Person(&quot;Joseph&quot;,&quot;001&quot;); log.info(&quot;person1.equals(person2) ? &#123;&#125;&quot;, person1.equals(person2)); 使用equals判等会得到false。如果你希望只要身份证一致就认为是同一个人的话，可以使用@EqualsAndHashCode.Exclude注解来修饰name字段，从equals和hashCode的实现中排除name字段： @EqualsAndHashCode.Exclude private String name; 修改后得到true。打开编译后的代码可以看到，Lombok为Person生成的equals方法的实现，确实只包含了identity属性： public boolean equals(final Object o) &#123; if (o &#x3D;&#x3D; this) &#123; return true; &#125; else if (!(o instanceof LombokEquealsController.Person)) &#123; return false; &#125; else &#123; LombokEquealsController.Person other &#x3D; (LombokEquealsController.Person)o; if (!other.canEqual(this)) &#123; return false; &#125; else &#123; Object this$identity &#x3D; this.getIdentity(); Object other$identity &#x3D; other.getIdentity(); if (this$identity &#x3D;&#x3D; null) &#123; if (other$identity !&#x3D; null) &#123; return false; &#125; &#125; else if (!this$identity.equals(other$identity)) &#123; return false; &#125; return true; &#125; &#125; &#125; 但到这里还没完，如果类型之间有继承，Lombok会怎么处理子类的equals和hashCode呢？我们来测试一下，写一个Employee类继承Person，并新定义一个公司属性： @Data class Employee extends Person &#123; private String company; public Employee(String name, String identity, String company) &#123; super(name, identity); this.company &#x3D; company; &#125; &#125; 在如下的测试代码中，声明两个Employee实例，它们具有相同的公司名称，但姓名和身份证均不同： Employee employee1 &#x3D; new Employee(&quot;zhuye&quot;,&quot;001&quot;, &quot;bkjk.com&quot;); Employee employee2 &#x3D; new Employee(&quot;Joseph&quot;,&quot;002&quot;, &quot;bkjk.com&quot;); log.info(&quot;employee1.equals(employee2) ? &#123;&#125;&quot;, employee1.equals(employee2)); 很遗憾，结果是true，显然是没有考虑父类的属性，而认为这两个员工是同一人，说明@EqualsAndHashCode默认实现没有使用父类属性。 为解决这个问题，我们可以手动设置callSuper开关为true，来覆盖这种默认行为： @Data @EqualsAndHashCode(callSuper &#x3D; true) class Employee extends Person &#123; 修改后的代码，实现了同时以子类的属性company加上父类中的属性identity，作为equals和hashCode方法的实现条件（实现上其实是调用了父类的equals和hashCode）。 重点回顾现在，我们来回顾下对象判等和比较的重点内容吧。 首先，我们要注意equals和&#x3D;&#x3D; 的区别。业务代码中进行内容的比较，针对基本类型只能使用&#x3D;&#x3D;，针对Integer、String在内的引用类型，需要使用equals。Integer和String的坑在于，使用&#x3D;&#x3D;判等有时也能获得正确结果。 其次，对于自定义类型，如果类型需要参与判等，那么务必同时实现equals和hashCode方法，并确保逻辑一致。如果希望快速实现equals、hashCode方法，我们可以借助IDE的代码生成功能，或使用Lombok来生成。如果类型也要参与比较，那么compareTo方法的逻辑同样需要和equals、hashCode方法一致。 最后，Lombok的@EqualsAndHashCode注解实现equals和hashCode的时候，默认使用类型所有非static、非transient的字段，且不考虑父类。如果希望改变这种默认行为，可以使用@EqualsAndHashCode.Exclude排除一些字段，并设置callSuper &#x3D; true来让子类的equals和hashCode调用父类的相应方法。 在比较枚举值和POJO参数值的例子中，我们还可以注意到，使用&#x3D;&#x3D;来判断两个包装类型的低级错误，确实容易被忽略。所以，我建议你在IDE中安装阿里巴巴的Java规约插件（详见这里），来及时提示我们这类低级错误： 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在实现equals时，我是先通过getClass方法判断两个对象的类型，你可能会想到还可以使用instanceof来判断。你能说说这两种实现方式的区别吗？ 在第三节的例子中，我演示了可以通过HashSet的contains方法判断元素是否在HashSet中，同样是Set的TreeSet其contains方法和HashSet有什么区别吗？ 有关对象判等、比较，你还遇到过其他坑吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/09、数值计算：注意精度、舍入和溢出问题","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/09、数值计算：注意精度、舍入和溢出问题/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/09%E3%80%81%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%EF%BC%9A%E6%B3%A8%E6%84%8F%E7%B2%BE%E5%BA%A6%E3%80%81%E8%88%8D%E5%85%A5%E5%92%8C%E6%BA%A2%E5%87%BA%E9%97%AE%E9%A2%98/","excerpt":"","text":"09 | 数值计算：注意精度、舍入和溢出问题作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我要和你说说数值计算的精度、舍入和溢出问题。 之所以要单独分享数值计算，是因为很多时候我们习惯的或者说认为理所当然的计算，在计算器或计算机看来并不是那么回事儿。就比如前段时间爆出的一条新闻，说是手机计算器把10%+10%算成了0.11而不是0.2。 出现这种问题的原因在于，国外的计算程序使用的是单步计算法。在单步计算法中，a+b%代表的是a*(1+b%)。所以，手机计算器计算10%+10%时，其实计算的是10%*（1+10%），所以得到的是0.11而不是0.2。 在我看来，计算器或计算机会得到反直觉的计算结果的原因，可以归结为： 在人看来，浮点数只是具有小数点的数字，0.1和1都是一样精确的数字。但，计算机其实无法精确保存浮点数，因此浮点数的计算结果也不可能精确。 在人看来，一个超大的数字只是位数多一点而已，多写几个1并不会让大脑死机。但，计算机是把数值保存在了变量中，不同类型的数值变量能保存的数值范围不同，当数值超过类型能表达的数值上限则会发生溢出问题。 接下来，我们就具体看看这些问题吧。 “危险”的Double我们先从简单的反直觉的四则运算看起。对几个简单的浮点数进行加减乘除运算： System.out.println(0.1+0.2); System.out.println(1.0-0.8); System.out.println(4.015*100); System.out.println(123.3&#x2F;100); double amount1 &#x3D; 2.15; double amount2 &#x3D; 1.10; if (amount1 - amount2 &#x3D;&#x3D; 1.05) System.out.println(&quot;OK&quot;); 输出结果如下： 0.30000000000000004 0.19999999999999996 401.49999999999994 1.2329999999999999 可以看到，输出结果和我们预期的很不一样。比如，0.1+0.2输出的不是0.3而是0.30000000000000004；再比如，对2.15-1.10和1.05判等，结果判等不成立。 出现这种问题的主要原因是，计算机是以二进制存储数值的，浮点数也不例外。Java采用了IEEE 754标准实现浮点数的表达和运算，你可以通过这里查看数值转化为二进制的结果。 比如，0.1的二进制表示为0.0 0011 0011 0011… （0011 无限循环)，再转换为十进制就是0.1000000000000000055511151231257827021181583404541015625。对于计算机而言，0.1无法精确表达，这是浮点数计算造成精度损失的根源。 你可能会说，以0.1为例，其十进制和二进制间转换后相差非常小，不会对计算产生什么影响。但，所谓积土成山，如果大量使用double来作大量的金钱计算，最终损失的精度就是大量的资金出入。比如，每天有一百万次交易，每次交易都差一分钱，一个月下来就差30万。这就不是小事儿了。那，如何解决这个问题呢？ 我们大都听说过BigDecimal类型，浮点数精确表达和运算的场景，一定要使用这个类型。不过，在使用BigDecimal时有几个坑需要避开。我们用BigDecimal把之前的四则运算改一下： System.out.println(new BigDecimal(0.1).add(new BigDecimal(0.2))); System.out.println(new BigDecimal(1.0).subtract(new BigDecimal(0.8))); System.out.println(new BigDecimal(4.015).multiply(new BigDecimal(100))); System.out.println(new BigDecimal(123.3).divide(new BigDecimal(100))); 输出如下： 0.3000000000000000166533453693773481063544750213623046875 0.1999999999999999555910790149937383830547332763671875 401.49999999999996802557689079549163579940795898437500 1.232999999999999971578290569595992565155029296875 可以看到，运算结果还是不精确，只不过是精度高了而已。这里给出浮点数运算避坑第一原则：使用BigDecimal表示和计算浮点数，且务必使用字符串的构造方法来初始化BigDecimal： System.out.println(new BigDecimal(&quot;0.1&quot;).add(new BigDecimal(&quot;0.2&quot;))); System.out.println(new BigDecimal(&quot;1.0&quot;).subtract(new BigDecimal(&quot;0.8&quot;))); System.out.println(new BigDecimal(&quot;4.015&quot;).multiply(new BigDecimal(&quot;100&quot;))); System.out.println(new BigDecimal(&quot;123.3&quot;).divide(new BigDecimal(&quot;100&quot;))); 改进后，就能得到我们想要的输出了： 0.3 0.2 401.500 1.233 到这里，你可能会继续问，不能调用BigDecimal传入Double的构造方法，但手头只有一个Double，如何转换为精确表达的BigDecimal呢？ 我们试试用Double.toString把double转换为字符串，看看行不行？ System.out.println(new BigDecimal(&quot;4.015&quot;).multiply(new BigDecimal(Double.toString(100)))); 输出为401.5000。与上面字符串初始化100和4.015相乘得到的结果401.500相比，这里为什么多了1个0呢？原因就是，BigDecimal有scale和precision的概念，scale表示小数点右边的位数，而precision表示精度，也就是有效数字的长度。 调试一下可以发现，new BigDecimal(Double.toString(100))得到的BigDecimal的scale&#x3D;1、precision&#x3D;4；而new BigDecimal(“100”)得到的BigDecimal的scale&#x3D;0、precision&#x3D;3。对于BigDecimal乘法操作，返回值的scale是两个数的scale相加。所以，初始化100的两种不同方式，导致最后结果的scale分别是4和3： private static void testScale() &#123; BigDecimal bigDecimal1 &#x3D; new BigDecimal(&quot;100&quot;); BigDecimal bigDecimal2 &#x3D; new BigDecimal(String.valueOf(100d)); BigDecimal bigDecimal3 &#x3D; new BigDecimal(String.valueOf(100)); BigDecimal bigDecimal4 &#x3D; BigDecimal.valueOf(100d); BigDecimal bigDecimal5 &#x3D; new BigDecimal(Double.toString(100)); print(bigDecimal1); &#x2F;&#x2F;scale 0 precision 3 result 401.500 print(bigDecimal2); &#x2F;&#x2F;scale 1 precision 4 result 401.5000 print(bigDecimal3); &#x2F;&#x2F;scale 0 precision 3 result 401.500 print(bigDecimal4); &#x2F;&#x2F;scale 1 precision 4 result 401.5000 print(bigDecimal5); &#x2F;&#x2F;scale 1 precision 4 result 401.5000 &#125; private static void print(BigDecimal bigDecimal) &#123; log.info(&quot;scale &#123;&#125; precision &#123;&#125; result &#123;&#125;&quot;, bigDecimal.scale(), bigDecimal.precision(), bigDecimal.multiply(new BigDecimal(&quot;4.015&quot;))); &#125; BigDecimal的toString方法得到的字符串和scale相关，又会引出了另一个问题：对于浮点数的字符串形式输出和格式化，我们应该考虑显式进行，通过格式化表达式或格式化工具来明确小数位数和舍入方式。接下来，我们就聊聊浮点数舍入和格式化。 考虑浮点数舍入和格式化的方式除了使用Double保存浮点数可能带来精度问题外，更匪夷所思的是这种精度问题，加上String.format的格式化舍入方式，可能得到让人摸不着头脑的结果。 我们看一个例子吧。首先用double和float初始化两个3.35的浮点数，然后通过String.format使用%.1f来格式化这2个数字： double num1 &#x3D; 3.35; float num2 &#x3D; 3.35f; System.out.println(String.format(&quot;%.1f&quot;, num1));&#x2F;&#x2F;四舍五入 System.out.println(String.format(&quot;%.1f&quot;, num2)); 得到的结果居然是3.4和3.3。 这就是由精度问题和舍入方式共同导致的，double和float的3.35其实相当于3.350xxx和3.349xxx： 3.350000000000000088817841970012523233890533447265625 3.349999904632568359375 String.format采用四舍五入的方式进行舍入，取1位小数，double的3.350四舍五入为3.4，而float的3.349四舍五入为3.3。 我们看一下Formatter类的相关源码，可以发现使用的舍入模式是HALF_UP（代码第11行）： else if (c &#x3D;&#x3D; Conversion.DECIMAL_FLOAT) &#123; &#x2F;&#x2F; Create a new BigDecimal with the desired precision. int prec &#x3D; (precision &#x3D;&#x3D; -1 ? 6 : precision); int scale &#x3D; value.scale(); if (scale &gt; prec) &#123; &#x2F;&#x2F; more &quot;scale&quot; digits than the requested &quot;precision&quot; int compPrec &#x3D; value.precision(); if (compPrec &lt;&#x3D; scale) &#123; &#x2F;&#x2F; case of 0.xxxxxx value &#x3D; value.setScale(prec, RoundingMode.HALF_UP); &#125; else &#123; compPrec -&#x3D; (scale - prec); value &#x3D; new BigDecimal(value.unscaledValue(), scale, new MathContext(compPrec)); &#125; &#125; 如果我们希望使用其他舍入方式来格式化字符串的话，可以设置DecimalFormat，如下代码所示： double num1 &#x3D; 3.35; float num2 &#x3D; 3.35f; DecimalFormat format &#x3D; new DecimalFormat(&quot;#.##&quot;); format.setRoundingMode(RoundingMode.DOWN); System.out.println(format.format(num1)); format.setRoundingMode(RoundingMode.DOWN); System.out.println(format.format(num2)); 当我们把这2个浮点数向下舍入取2位小数时，输出分别是3.35和3.34，还是我们之前说的浮点数无法精确存储的问题。 因此，即使通过DecimalFormat来精确控制舍入方式，double和float的问题也可能产生意想不到的结果，所以浮点数避坑第二原则：浮点数的字符串格式化也要通过BigDecimal进行。 比如下面这段代码，使用BigDecimal来格式化数字3.35，分别使用向下舍入和四舍五入方式取1位小数进行格式化： BigDecimal num1 &#x3D; new BigDecimal(&quot;3.35&quot;); BigDecimal num2 &#x3D; num1.setScale(1, BigDecimal.ROUND_DOWN); System.out.println(num2); BigDecimal num3 &#x3D; num1.setScale(1, BigDecimal.ROUND_HALF_UP); System.out.println(num3); 这次得到的结果是3.3和3.4，符合预期。 用equals做判等，就一定是对的吗？现在我们知道了，应该使用BigDecimal来进行浮点数的表示、计算、格式化。在上一讲介绍判等问题时，我提到一个原则：包装类的比较要通过equals进行，而不能使用&#x3D;&#x3D;。那么，使用equals方法对两个BigDecimal判等，一定能得到我们想要的结果吗？ 我们来看下面的例子。使用equals方法比较1.0和1这两个BigDecimal： System.out.println(new BigDecimal(&quot;1.0&quot;).equals(new BigDecimal(&quot;1&quot;))) 你可能已经猜到我要说什么了，结果当然是false。BigDecimal的equals方法的注释中说明了原因，equals比较的是BigDecimal的value和scale，1.0的scale是1，1的scale是0，所以结果一定是false： &#x2F;** * Compares this &#123;@code BigDecimal&#125; with the specified * &#123;@code Object&#125; for equality. Unlike &#123;@link * #compareTo(BigDecimal) compareTo&#125;, this method considers two * &#123;@code BigDecimal&#125; objects equal only if they are equal in * value and scale (thus 2.0 is not equal to 2.00 when compared by * this method). * * @param x &#123;@code Object&#125; to which this &#123;@code BigDecimal&#125; is * to be compared. * @return &#123;@code true&#125; if and only if the specified &#123;@code Object&#125; is a * &#123;@code BigDecimal&#125; whose value and scale are equal to this * &#123;@code BigDecimal&#125;&#39;s. * @see #compareTo(java.math.BigDecimal) * @see #hashCode *&#x2F; @Override public boolean equals(Object x) 如果我们希望只比较BigDecimal的value，可以使用compareTo方法，修改后代码如下： System.out.println(new BigDecimal(&quot;1.0&quot;).compareTo(new BigDecimal(&quot;1&quot;))&#x3D;&#x3D;0); 学过上一讲，你可能会意识到BigDecimal的equals和hashCode方法会同时考虑value和scale，如果结合HashSet或HashMap使用的话就可能会出现麻烦。比如，我们把值为1.0的BigDecimal加入HashSet，然后判断其是否存在值为1的BigDecimal，得到的结果是false： Set&lt;BigDecimal&gt; hashSet1 &#x3D; new HashSet&lt;&gt;(); hashSet1.add(new BigDecimal(&quot;1.0&quot;)); System.out.println(hashSet1.contains(new BigDecimal(&quot;1&quot;)));&#x2F;&#x2F;返回false 解决这个问题的办法有两个： 第一个方法是，使用TreeSet替换HashSet。TreeSet不使用hashCode方法，也不使用equals比较元素，而是使用compareTo方法，所以不会有问题。 Set&lt;BigDecimal&gt; treeSet &#x3D; new TreeSet&lt;&gt;(); treeSet.add(new BigDecimal(&quot;1.0&quot;)); System.out.println(treeSet.contains(new BigDecimal(&quot;1&quot;)));&#x2F;&#x2F;返回true 第二个方法是，把BigDecimal存入HashSet或HashMap前，先使用stripTrailingZeros方法去掉尾部的零，比较的时候也去掉尾部的0，确保value相同的BigDecimal，scale也是一致的： Set&lt;BigDecimal&gt; hashSet2 &#x3D; new HashSet&lt;&gt;(); hashSet2.add(new BigDecimal(&quot;1.0&quot;).stripTrailingZeros()); System.out.println(hashSet2.contains(new BigDecimal(&quot;1.000&quot;).stripTrailingZeros()));&#x2F;&#x2F;返回true 小心数值溢出问题数值计算还有一个要小心的点是溢出，不管是int还是long，所有的基本数值类型都有超出表达范围的可能性。 比如，对Long的最大值进行+1操作： long l &#x3D; Long.MAX_VALUE; System.out.println(l + 1); System.out.println(l + 1 &#x3D;&#x3D; Long.MIN_VALUE); 输出结果是一个负数，因为Long的最大值+1变为了Long的最小值： -9223372036854775808 true 显然这是发生了溢出，而且是默默地溢出，并没有任何异常。这类问题非常容易被忽略，改进方式有下面2种。 方法一是，考虑使用Math类的addExact、subtractExact等xxExact方法进行数值运算，这些方法可以在数值溢出时主动抛出异常。我们来测试一下，使用Math.addExact对Long最大值做+1操作： try &#123; long l &#x3D; Long.MAX_VALUE; System.out.println(Math.addExact(l, 1)); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; 执行后，可以得到ArithmeticException，这是一个RuntimeException： java.lang.ArithmeticException: long overflow at java.lang.Math.addExact(Math.java:809) at org.geekbang.time.commonmistakes.numeralcalculations.demo3.CommonMistakesApplication.right2(CommonMistakesApplication.java:25) at org.geekbang.time.commonmistakes.numeralcalculations.demo3.CommonMistakesApplication.main(CommonMistakesApplication.java:13) 方法二是，使用大数类BigInteger。BigDecimal是处理浮点数的专家，而BigInteger则是对大数进行科学计算的专家。 如下代码，使用BigInteger对Long最大值进行+1操作；如果希望把计算结果转换一个Long变量的话，可以使用BigInteger的longValueExact方法，在转换出现溢出时，同样会抛出ArithmeticException： BigInteger i &#x3D; new BigInteger(String.valueOf(Long.MAX_VALUE)); System.out.println(i.add(BigInteger.ONE).toString()); try &#123; long l &#x3D; i.add(BigInteger.ONE).longValueExact(); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; 输出结果如下： 9223372036854775808 java.lang.ArithmeticException: BigInteger out of long range at java.math.BigInteger.longValueExact(BigInteger.java:4632) at org.geekbang.time.commonmistakes.numeralcalculations.demo3.CommonMistakesApplication.right1(CommonMistakesApplication.java:37) at org.geekbang.time.commonmistakes.numeralcalculations.demo3.CommonMistakesApplication.main(CommonMistakesApplication.java:11) 可以看到，通过BigInteger对Long的最大值加1一点问题都没有，当尝试把结果转换为Long类型时，则会提示BigInteger out of long range。 重点回顾今天，我与你分享了浮点数的表示、计算、舍入和格式化、溢出等涉及的一些坑。 第一，切记，要精确表示浮点数应该使用BigDecimal。并且，使用BigDecimal的Double入参的构造方法同样存在精度丢失问题，应该使用String入参的构造方法或者BigDecimal.valueOf方法来初始化。 第二，对浮点数做精确计算，参与计算的各种数值应该始终使用BigDecimal，所有的计算都要通过BigDecimal的方法进行，切勿只是让BigDecimal来走过场。任何一个环节出现精度损失，最后的计算结果可能都会出现误差。 第三，对于浮点数的格式化，如果使用String.format的话，需要认识到它使用的是四舍五入，可以考虑使用DecimalFormat来明确指定舍入方式。但考虑到精度问题，我更建议使用BigDecimal来表示浮点数，并使用其setScale方法指定舍入的位数和方式。 第四，进行数值运算时要小心溢出问题，虽然溢出后不会出现异常，但得到的计算结果是完全错误的。我们考虑使用Math.xxxExact方法来进行运算，在溢出时能抛出异常，更建议对于可能会出现溢出的大数运算使用BigInteger类。 总之，对于金融、科学计算等场景，请尽可能使用BigDecimal和BigInteger，避免由精度和溢出问题引发难以发现，但影响重大的Bug。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 BigDecimal提供了8种舍入模式，你能通过一些例子说说它们的区别吗？ 数据库（比如MySQL）中的浮点数和整型数字，你知道应该怎样定义吗？又如何实现浮点数的准确计算呢？ 针对数值运算，你还遇到过什么坑吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/11、空值处理：分不清楚的null和恼人的空指针","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/11、空值处理：分不清楚的null和恼人的空指针/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/11%E3%80%81%E7%A9%BA%E5%80%BC%E5%A4%84%E7%90%86%EF%BC%9A%E5%88%86%E4%B8%8D%E6%B8%85%E6%A5%9A%E7%9A%84null%E5%92%8C%E6%81%BC%E4%BA%BA%E7%9A%84%E7%A9%BA%E6%8C%87%E9%92%88/","excerpt":"","text":"11 | 空值处理：分不清楚的null和恼人的空指针作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我要和你分享的主题是，空值处理：分不清楚的null和恼人的空指针。 有一天我收到一条短信，内容是“尊敬的null你好，XXX”。当时我就笑了，这是程序员都能Get的笑点，程序没有获取到我的姓名，然后把空格式化为了null。很明显，这是没处理好null。哪怕把null替换为贵宾、顾客，也不会引发这样的笑话。 程序中的变量是null，就意味着它没有引用指向或者说没有指针。这时，我们对这个变量进行任何操作，都必然会引发空指针异常，在Java中就是NullPointerException。那么，空指针异常容易在哪些情况下出现，又应该如何修复呢？ 空指针异常虽然恼人但好在容易定位，更麻烦的是要弄清楚null的含义。比如，客户端给服务端的一个数据是null，那么其意图到底是给一个空值，还是没提供值呢？再比如，数据库中字段的NULL值，是否有特殊的含义呢，针对数据库中的NULL值，写SQL需要特别注意什么呢？ 今天，就让我们带着这些问题开始null的踩坑之旅吧。 修复和定位恼人的空指针问题NullPointerException是Java代码中最常见的异常，我将其最可能出现的场景归为以下5种： 参数值是Integer等包装类型，使用时因为自动拆箱出现了空指针异常； 字符串比较出现空指针异常； 诸如ConcurrentHashMap这样的容器不支持Key和Value为null，强行put null的Key或Value会出现空指针异常； A对象包含了B，在通过A对象的字段获得B之后，没有对字段判空就级联调用B的方法出现空指针异常； 方法或远程服务返回的List不是空而是null，没有进行判空就直接调用List的方法出现空指针异常。 为模拟说明这5种场景，我写了一个wrongMethod方法，并用一个wrong方法来调用它。wrong方法的入参test是一个由0和1构成的、长度为4的字符串，第几位设置为1就代表第几个参数为null，用来控制wrongMethod方法的4个入参，以模拟各种空指针情况： private List&lt;String&gt; wrongMethod(FooService fooService, Integer i, String s, String t) &#123; log.info(&quot;result &#123;&#125; &#123;&#125; &#123;&#125; &#123;&#125;&quot;, i + 1, s.equals(&quot;OK&quot;), s.equals(t), new ConcurrentHashMap&lt;String, String&gt;().put(null, null)); if (fooService.getBarService().bar().equals(&quot;OK&quot;)) log.info(&quot;OK&quot;); return null; &#125; @GetMapping(&quot;wrong&quot;) public int wrong(@RequestParam(value &#x3D; &quot;test&quot;, defaultValue &#x3D; &quot;1111&quot;) String test) &#123; return wrongMethod(test.charAt(0) &#x3D;&#x3D; &#39;1&#39; ? null : new FooService(), test.charAt(1) &#x3D;&#x3D; &#39;1&#39; ? null : 1, test.charAt(2) &#x3D;&#x3D; &#39;1&#39; ? null : &quot;OK&quot;, test.charAt(3) &#x3D;&#x3D; &#39;1&#39; ? null : &quot;OK&quot;).size(); &#125; class FooService &#123; @Getter private BarService barService; &#125; class BarService &#123; String bar() &#123; return &quot;OK&quot;; &#125; &#125; 很明显，这个案例出现空指针异常是因为变量是一个空指针，尝试获得变量的值或访问变量的成员会获得空指针异常。但，这个异常的定位比较麻烦。 在测试方法wrongMethod中，我们通过一行日志记录的操作，在一行代码中模拟了4处空指针异常： 对入参Integer i进行+1操作； 对入参String s进行比较操作，判断内容是否等于”OK”； 对入参String s和入参String t进行比较操作，判断两者是否相等； 对new出来的ConcurrentHashMap进行put操作，Key和Value都设置为null。 输出的异常信息如下： java.lang.NullPointerException: null at org.geekbang.time.commonmistakes.nullvalue.demo2.AvoidNullPointerExceptionController.wrongMethod(AvoidNullPointerExceptionController.java:37) at org.geekbang.time.commonmistakes.nullvalue.demo2.AvoidNullPointerExceptionController.wrong(AvoidNullPointerExceptionController.java:20) 这段信息确实提示了这行代码出现了空指针异常，但我们很难定位出到底是哪里出现了空指针，可能是把入参Integer拆箱为int的时候出现的，也可能是入参的两个字符串任意一个为null，也可能是因为把null加入了ConcurrentHashMap。 你可能会想到，要排查这样的问题，只要设置一个断点看一下入参即可。但，在真实的业务场景中，空指针问题往往是在特定的入参和代码分支下才会出现，本地难以重现。如果要排查生产上出现的空指针问题，设置代码断点不现实，通常是要么把代码进行拆分，要么增加更多的日志，但都比较麻烦。 在这里，我推荐使用阿里开源的Java故障诊断神器Arthas。Arthas简单易用功能强大，可以定位出大多数的Java生产问题。 接下来，我就和你演示下如何在30秒内知道wrongMethod方法的入参，从而定位到空指针到底是哪个入参引起的。如下截图中有三个红框，我先和你分析第二和第三个红框： 第二个红框表示，Arthas启动后被附加到了JVM进程； 第三个红框表示，通过watch命令监控wrongMethod方法的入参。 watch命令的参数包括类名表达式、方法表达式和观察表达式。这里，我们设置观察类为AvoidNullPointerExceptionController，观察方法为wrongMethod，观察表达式为params表示观察入参： watch org.geekbang.time.commonmistakes.nullvalue.demo2.AvoidNullPointerExceptionController wrongMethod params 开启watch后，执行2次wrong方法分别设置test入参为1111和1101，也就是第一次传入wrongMethod的4个参数都为null，第二次传入的第1、2和4个参数为null。 配合图中第一和第四个红框可以看到，第二次调用时，第三个参数是字符串OK其他参数是null，Archas正确输出了方法的所有入参，这样我们很容易就能定位到空指针的问题了。 到这里，如果是简单的业务逻辑的话，你就可以定位到空指针异常了；如果是分支复杂的业务逻辑，你需要再借助stack命令来查看wrongMethod方法的调用栈，并配合watch命令查看各方法的入参，就可以很方便地定位到空指针的根源了。 下图演示了通过stack命令观察wrongMethod的调用路径： 如果你想了解Arthas各种命令的详细使用方法，可以点击这里查看。 接下来，我们看看如何修复上面出现的5种空指针异常。 其实，对于任何空指针异常的处理，最直白的方式是先判空后操作。不过，这只能让异常不再出现，我们还是要找到程序逻辑中出现的空指针究竟是来源于入参还是Bug： 如果是来源于入参，还要进一步分析入参是否合理等； 如果是来源于Bug，那空指针不一定是纯粹的程序Bug，可能还涉及业务属性和接口调用规范等。 在这里，因为是Demo，所以我们只考虑纯粹的空指针判空这种修复方式。如果要先判空后处理，大多数人会想到使用if-else代码块。但，这种方式既增加代码量又会降低易读性，我们可以尝试利用Java 8的Optional类来消除这样的if-else逻辑，使用一行代码进行判空和处理。 修复思路如下： 对于Integer的判空，可以使用Optional.ofNullable来构造一个Optional，然后使用orElse(0)把null替换为默认值再进行+1操作。 对于String和字面量的比较，可以把字面量放在前面，比如”OK”.equals(s)，这样即使s是null也不会出现空指针异常；而对于两个可能为null的字符串变量的equals比较，可以使用Objects.equals，它会做判空处理。 对于ConcurrentHashMap，既然其Key和Value都不支持null，修复方式就是不要把null存进去。HashMap的Key和Value可以存入null，而ConcurrentHashMap看似是HashMap的线程安全版本，却不支持null值的Key和Value，这是容易产生误区的一个地方。 对于类似fooService.getBarService().bar().equals(“OK”)的级联调用，需要判空的地方有很多，包括fooService、getBarService()方法的返回值，以及bar方法返回的字符串。如果使用if-else来判空的话可能需要好几行代码，但使用Optional的话一行代码就够了。 对于rightMethod返回的List，由于不能确认其是否为null，所以在调用size方法获得列表大小之前，同样可以使用Optional.ofNullable包装一下返回值，然后通过.orElse(Collections.emptyList())实现在List为null的时候获得一个空的List，最后再调用size方法。 private List&lt;String&gt; rightMethod(FooService fooService, Integer i, String s, String t) &#123; log.info(&quot;result &#123;&#125; &#123;&#125; &#123;&#125; &#123;&#125;&quot;, Optional.ofNullable(i).orElse(0) + 1, &quot;OK&quot;.equals(s), Objects.equals(s, t), new HashMap&lt;String, String&gt;().put(null, null)); Optional.ofNullable(fooService) .map(FooService::getBarService) .filter(barService -&gt; &quot;OK&quot;.equals(barService.bar())) .ifPresent(result -&gt; log.info(&quot;OK&quot;)); return new ArrayList&lt;&gt;(); &#125; @GetMapping(&quot;right&quot;) public int right(@RequestParam(value &#x3D; &quot;test&quot;, defaultValue &#x3D; &quot;1111&quot;) String test) &#123; return Optional.ofNullable(rightMethod(test.charAt(0) &#x3D;&#x3D; &#39;1&#39; ? null : new FooService(), test.charAt(1) &#x3D;&#x3D; &#39;1&#39; ? null : 1, test.charAt(2) &#x3D;&#x3D; &#39;1&#39; ? null : &quot;OK&quot;, test.charAt(3) &#x3D;&#x3D; &#39;1&#39; ? null : &quot;OK&quot;)) .orElse(Collections.emptyList()).size(); &#125; 经过修复后，调用right方法传入1111，也就是给rightMethod的4个参数都设置为null，日志中也看不到任何空指针异常了： [21:43:40.619] [http-nio-45678-exec-2] [INFO ] [.AvoidNullPointerExceptionController:45 ] - result 1 false true null 但是，如果我们修改right方法入参为0000，即传给rightMethod方法的4个参数都不可能是null，最后日志中也无法出现OK字样。这又是为什么呢，BarService的bar方法不是返回了OK字符串吗？ 我们还是用Arthas来定位问题，使用watch命令来观察方法rightMethod的入参，-x参数设置为2代表参数打印的深度为2层： 可以看到，FooService中的barService字段为null，这样也就可以理解为什么最终出现这个Bug了。 这又引申出一个问题，使用判空方式或Optional方式来避免出现空指针异常，不一定是解决问题的最好方式，空指针没出现可能隐藏了更深的Bug。因此，解决空指针异常，还是要真正case by case地定位分析案例，然后再去做判空处理，而处理时也并不只是判断非空然后进行正常业务流程这么简单，同样需要考虑为空的时候是应该出异常、设默认值还是记录日志等。 POJO中属性的null到底代表了什么？在我看来，相比判空避免空指针异常，更容易出错的是null的定位问题。对程序来说，null就是指针没有任何指向，而结合业务逻辑情况就复杂得多，我们需要考虑： DTO中字段的null到底意味着什么？是客户端没有传给我们这个信息吗？ 既然空指针问题很讨厌，那么DTO中的字段要设置默认值么？ 如果数据库实体中的字段有null，那么通过数据访问框架保存数据是否会覆盖数据库中的既有数据？ 如果不能明确地回答这些问题，那么写出的程序逻辑很可能会混乱不堪。接下来，我们看一个实际案例吧。 有一个User的POJO，同时扮演DTO和数据库Entity角色，包含用户ID、姓名、昵称、年龄、注册时间等属性： @Data @Entity public class User &#123; @Id @GeneratedValue(strategy &#x3D; IDENTITY) private Long id; private String name; private String nickname; private Integer age; private Date createDate &#x3D; new Date(); &#125; 有一个Post接口用于更新用户数据，更新逻辑非常简单，根据用户姓名自动设置一个昵称，昵称的规则是“用户类型+姓名”，然后直接把客户端在RequestBody中使用JSON传过来的User对象通过JPA更新到数据库中，最后返回保存到数据库的数据。 @Autowired private UserRepository userRepository; @PostMapping(&quot;wrong&quot;) public User wrong(@RequestBody User user) &#123; user.setNickname(String.format(&quot;guest%s&quot;, user.getName())); return userRepository.save(user); &#125; @Repository public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; &#125; 首先，在数据库中初始化一个用户，age&#x3D;36、name&#x3D;zhuye、create_date&#x3D;2020年1月4日、nickname是NULL： 然后，使用cURL测试一下用户信息更新接口Post，传入一个id&#x3D;1、name&#x3D;null的JSON字符串，期望把ID为1的用户姓名设置为空： curl -H &quot;Content-Type:application&#x2F;json&quot; -X POST -d &#39;&#123; &quot;id&quot;:1, &quot;name&quot;:null&#125;&#39; http:&#x2F;&#x2F;localhost:45678&#x2F;pojonull&#x2F;wrong &#123;&quot;id&quot;:1,&quot;name&quot;:null,&quot;nickname&quot;:&quot;guestnull&quot;,&quot;age&quot;:null,&quot;createDate&quot;:&quot;2020-01-05T02:01:03.784+0000&quot;&#125;% 接口返回的结果和数据库中记录一致： 可以看到，这里存在如下三个问题： 调用方只希望重置用户名，但age也被设置为了null； nickname是用户类型加姓名，name重置为null的话，访客用户的昵称应该是guest，而不是guestnull，重现了文首提到的那个笑点； 用户的创建时间原来是1月4日，更新了用户信息后变为了1月5日。 归根结底，这是如下5个方面的问题： 明确DTO中null的含义。对于JSON到DTO的反序列化过程，null的表达是有歧义的，客户端不传某个属性，或者传null，这个属性在DTO中都是null。但，对于用户信息更新操作，不传意味着客户端不需要更新这个属性，维持数据库原先的值；传了null，意味着客户端希望重置这个属性。因为Java中的null就是没有这个数据，无法区分这两种表达，所以本例中的age属性也被设置为了null，或许我们可以借助Optional来解决这个问题。 POJO中的字段有默认值。如果客户端不传值，就会赋值为默认值，导致创建时间也被更新到了数据库中。 注意字符串格式化时可能会把null值格式化为null字符串。比如昵称的设置，我们只是进行了简单的字符串格式化，存入数据库变为了guestnull。显然，这是不合理的，也是开头我们说的笑话的来源，还需要进行判断。 DTO和Entity共用了一个POJO。对于用户昵称的设置是程序控制的，我们不应该把它们暴露在DTO中，否则很容易把客户端随意设置的值更新到数据库中。此外，创建时间最好让数据库设置为当前时间，不用程序控制，可以通过在字段上设置columnDefinition来实现。 数据库字段允许保存null，会进一步增加出错的可能性和复杂度。因为如果数据真正落地的时候也支持NULL的话，可能就有NULL、空字符串和字符串null三种状态。这一点我会在下一小节展开。如果所有属性都有默认值，问题会简单一点。 按照这个思路，我们对DTO和Entity进行拆分，修改后代码如下所示： UserDto中只保留id、name和age三个属性，且name和age使用Optional来包装，以区分客户端不传数据还是故意传null。 在UserEntity的字段上使用@Column注解，把数据库字段name、nickname、age和createDate都设置为NOT NULL，并设置createDate的默认值为CURRENT_TIMESTAMP，由数据库来生成创建时间。 使用Hibernate的@DynamicUpdate注解实现更新SQL的动态生成，实现只更新修改后的字段，不过需要先查询一次实体，让Hibernate可以“跟踪”实体属性的当前状态，以确保有效。 @Data public class UserDto &#123; private Long id; private Optional&lt;String&gt; name; private Optional&lt;Integer&gt; age; ; @Data @Entity @DynamicUpdate public class UserEntity &#123; @Id @GeneratedValue(strategy &#x3D; IDENTITY) private Long id; @Column(nullable &#x3D; false) private String name; @Column(nullable &#x3D; false) private String nickname; @Column(nullable &#x3D; false) private Integer age; @Column(nullable &#x3D; false, columnDefinition &#x3D; &quot;TIMESTAMP DEFAULT CURRENT_TIMESTAMP&quot;) private Date createDate; &#125; 在重构了DTO和Entity后，我们重新定义一个right接口，以便对更新操作进行更精细化的处理。首先是参数校验： 对传入的UserDto和ID属性先判空，如果为空直接抛出IllegalArgumentException。 根据id从数据库中查询出实体后进行判空，如果为空直接抛出IllegalArgumentException。 然后，由于DTO中已经巧妙使用了Optional来区分客户端不传值和传null值，那么业务逻辑实现上就可以按照客户端的意图来分别实现逻辑。如果不传值，那么Optional本身为null，直接跳过Entity字段的更新即可，这样动态生成的SQL就不会包含这个列；如果传了值，那么进一步判断传的是不是null。 下面，我们根据业务需要分别对姓名、年龄和昵称进行更新： 对于姓名，我们认为客户端传null是希望把姓名重置为空，允许这样的操作，使用Optional的orElse方法一键把空转换为空字符串即可。 对于年龄，我们认为如果客户端希望更新年龄就必须传一个有效的年龄，年龄不存在重置操作，可以使用Optional的orElseThrow方法在值为空的时候抛出IllegalArgumentException。 对于昵称，因为数据库中姓名不可能为null，所以可以放心地把昵称设置为guest加上数据库取出来的姓名。 @PostMapping(&quot;right&quot;) public UserEntity right(@RequestBody UserDto user) &#123; if (user &#x3D;&#x3D; null || user.getId() &#x3D;&#x3D; null) throw new IllegalArgumentException(&quot;用户Id不能为空&quot;); UserEntity userEntity &#x3D; userEntityRepository.findById(user.getId()) .orElseThrow(() -&gt; new IllegalArgumentException(&quot;用户不存在&quot;)); if (user.getName() !&#x3D; null) &#123; userEntity.setName(user.getName().orElse(&quot;&quot;)); &#125; userEntity.setNickname(&quot;guest&quot; + userEntity.getName()); if (user.getAge() !&#x3D; null) &#123; userEntity.setAge(user.getAge().orElseThrow(() -&gt; new IllegalArgumentException(&quot;年龄不能为空&quot;))); &#125; return userEntityRepository.save(userEntity); &#125; 假设数据库中已经有这么一条记录，id&#x3D;1、age&#x3D;36、create_date&#x3D;2020年1月4日、name&#x3D;zhuye、nickname&#x3D;guestzhuye： 使用相同的参数调用right接口，再来试试是否解决了所有问题。传入一个id&#x3D;1、name&#x3D;null的JSON字符串，期望把id为1的用户姓名设置为空： curl -H &quot;Content-Type:application&#x2F;json&quot; -X POST -d &#39;&#123; &quot;id&quot;:1, &quot;name&quot;:null&#125;&#39; http:&#x2F;&#x2F;localhost:45678&#x2F;pojonull&#x2F;right &#123;&quot;id&quot;:1,&quot;name&quot;:&quot;&quot;,&quot;nickname&quot;:&quot;guest&quot;,&quot;age&quot;:36,&quot;createDate&quot;:&quot;2020-01-04T11:09:20.000+0000&quot;&#125;% 结果如下： 可以看到，right接口完美实现了仅重置name属性的操作，昵称也不再有null字符串，年龄和创建时间字段也没被修改。 通过日志可以看到，Hibernate生成的SQL语句只更新了name和nickname两个字段： Hibernate: update user_entity set name&#x3D;?, nickname&#x3D;? where id&#x3D;? 接下来，为了测试使用Optional是否可以有效区分JSON中没传属性还是传了null，我们在JSON中设置了一个null的age，结果是正确得到了年龄不能为空的错误提示： curl -H &quot;Content-Type:application&#x2F;json&quot; -X POST -d &#39;&#123; &quot;id&quot;:1, &quot;age&quot;:null&#125;&#39; http:&#x2F;&#x2F;localhost:45678&#x2F;pojonull&#x2F;right &#123;&quot;timestamp&quot;:&quot;2020-01-05T03:14:40.324+0000&quot;,&quot;status&quot;:500,&quot;error&quot;:&quot;Internal Server Error&quot;,&quot;message&quot;:&quot;年龄不能为空&quot;,&quot;path&quot;:&quot;&#x2F;pojonull&#x2F;right&quot;&#125;% 小心MySQL中有关NULL的三个坑前面提到，数据库表字段允许存NULL除了会让我们困惑外，还容易有坑。这里我会结合NULL字段，和你着重说明sum函数、count函数，以及NULL值条件可能踩的坑。 为方便演示，首先定义一个只有id和score两个字段的实体： @Entity @Data public class User &#123; @Id @GeneratedValue(strategy &#x3D; IDENTITY) private Long id; private Long score; &#125; 程序启动的时候，往实体初始化一条数据，其id是自增列自动设置的1，score是NULL： @Autowired private UserRepository userRepository; @PostConstruct public void init() &#123; userRepository.save(new User()); &#125; 然后，测试下面三个用例，来看看结合数据库中的null值可能会出现的坑： 通过sum函数统计一个只有NULL值的列的总和，比如SUM(score)； select记录数量，count使用一个允许NULL的字段，比如COUNT(score)； 使用&#x3D;NULL条件查询字段值为NULL的记录，比如score&#x3D;null条件。 @Repository public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Query(nativeQuery&#x3D;true,value &#x3D; &quot;SELECT SUM(score) FROM &#96;user&#96;&quot;) Long wrong1(); @Query(nativeQuery &#x3D; true, value &#x3D; &quot;SELECT COUNT(score) FROM &#96;user&#96;&quot;) Long wrong2(); @Query(nativeQuery &#x3D; true, value &#x3D; &quot;SELECT * FROM &#96;user&#96; WHERE score&#x3D;null&quot;) List&lt;User&gt; wrong3(); &#125; 得到的结果，分别是null、0和空List： [11:38:50.137] [http-nio-45678-exec-1] [INFO ] [t.c.nullvalue.demo3.DbNullController:26 ] - result: null 0 [] 显然，这三条SQL语句的执行结果和我们的期望不同： 虽然记录的score都是NULL，但sum的结果应该是0才对； 虽然这条记录的score是NULL，但记录总数应该是1才对； 使用&#x3D;NULL并没有查询到id&#x3D;1的记录，查询条件失效。 原因是： MySQL中sum函数没统计到任何记录时，会返回null而不是0，可以使用IFNULL函数把null转换为0； MySQL中count字段不统计null值，COUNT(*)才是统计所有记录数量的正确方式。 MySQL中使用诸如&#x3D;、&lt;、&gt;这样的算数比较操作符比较NULL的结果总是NULL，这种比较就显得没有任何意义，需要使用IS NULL、IS NOT NULL或 ISNULL()函数来比较。 修改一下SQL： @Query(nativeQuery &#x3D; true, value &#x3D; &quot;SELECT IFNULL(SUM(score),0) FROM &#96;user&#96;&quot;) Long right1(); @Query(nativeQuery &#x3D; true, value &#x3D; &quot;SELECT COUNT(*) FROM &#96;user&#96;&quot;) Long right2(); @Query(nativeQuery &#x3D; true, value &#x3D; &quot;SELECT * FROM &#96;user&#96; WHERE score IS NULL&quot;) List&lt;User&gt; right3(); 可以得到三个正确结果，分别为0、1、[User(id&#x3D;1, score&#x3D;null)] ： [14:50:35.768] [http-nio-45678-exec-1] [INFO ] [t.c.nullvalue.demo3.DbNullController:31 ] - result: 0 1 [User(id&#x3D;1, score&#x3D;null)] 重点回顾今天，我和你讨论了做好空值处理需要注意的几个问题。 我首先总结了业务代码中5种最容易出现空指针异常的写法，以及相应的修复方式。针对判空，通过Optional配合Stream可以避免大多数冗长的if-else判空逻辑，实现一行代码优雅判空。另外，要定位和修复空指针异常，除了可以通过增加日志进行排查外，在生产上使用Arthas来查看方法的调用栈和入参会更快捷。 在我看来，业务系统最基本的标准是不能出现未处理的空指针异常，因为它往往代表了业务逻辑的中断，所以我建议每天查询一次生产日志来排查空指针异常，有条件的话建议订阅空指针异常报警，以便及时发现及时处理。 POJO中字段的null定位，从服务端的角度往往很难分清楚，到底是客户端希望忽略这个字段还是有意传了null，因此我们尝试用Optional类来区分null的定位。同时，为避免把空值更新到数据库中，可以实现动态SQL，只更新必要的字段。 最后，我分享了数据库字段使用NULL可能会带来的三个坑（包括sum函数、count函数，以及NULL值条件），以及解决方式。 总结来讲，null的正确处理以及避免空指针异常，绝不是判空这么简单，还要根据业务属性从前到后仔细考虑，客户端传入的null代表了什么，出现了null是否允许使用默认值替代，入库的时候应该传入null还是空值，并确保整个逻辑处理的一致性，才能尽量避免Bug。 为处理好null，作为客户端的开发者，需要和服务端对齐字段null的含义以及降级逻辑；而作为服务端的开发者，需要对入参进行前置判断，提前挡掉服务端不可接受的空值，同时在整个业务逻辑过程中进行完善的空值处理。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 ConcurrentHashMap的Key和Value都不能为null，而HashMap却可以，你知道这么设计的原因是什么吗？TreeMap、Hashtable等Map的Key和Value是否支持null呢？ 对于Hibernate框架可以使用@DynamicUpdate注解实现字段的动态更新，对于MyBatis框架如何实现类似的动态SQL功能，实现插入和修改SQL只包含POJO中的非空字段？ 关于程序和数据库中的null、空指针问题，你还遇到过什么坑吗？我是朱晔，欢迎在评论区与我留言分享，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/12、异常处理：别让自己在出问题的时候变为瞎子","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/12、异常处理：别让自己在出问题的时候变为瞎子/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/12%E3%80%81%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%EF%BC%9A%E5%88%AB%E8%AE%A9%E8%87%AA%E5%B7%B1%E5%9C%A8%E5%87%BA%E9%97%AE%E9%A2%98%E7%9A%84%E6%97%B6%E5%80%99%E5%8F%98%E4%B8%BA%E7%9E%8E%E5%AD%90/","excerpt":"","text":"12 | 异常处理：别让自己在出问题的时候变为瞎子作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来和你聊聊异常处理容易踩的坑。 应用程序避免不了出异常，捕获和处理异常是考验编程功力的一个精细活。一些业务项目中，我曾看到开发同学在开发业务逻辑时不考虑任何异常处理，项目接近完成时再采用“流水线”的方式进行异常处理，也就是统一为所有方法打上try…catch…捕获所有异常记录日志，有些技巧的同学可能会使用AOP来进行类似的“统一异常处理”。 其实，这种处理异常的方式非常不可取。那么今天，我就和你分享下不可取的原因、与异常处理相关的坑和最佳实践。 捕获和处理异常容易犯的错“统一异常处理”方式正是我要说的第一个错：不在业务代码层面考虑异常处理，仅在框架层面粗犷捕获和处理异常。 为了理解错在何处，我们先来看看大多数业务应用都采用的三层架构： Controller层负责信息收集、参数校验、转换服务层处理的数据适配前端，轻业务逻辑； Service层负责核心业务逻辑，包括各种外部服务调用、访问数据库、缓存处理、消息处理等； Repository层负责数据访问实现，一般没有业务逻辑。 每层架构的工作性质不同，且从业务性质上异常可能分为业务异常和系统异常两大类，这就决定了很难进行统一的异常处理。我们从底向上看一下三层架构： Repository层出现异常或许可以忽略，或许可以降级，或许需要转化为一个友好的异常。如果一律捕获异常仅记录日志，很可能业务逻辑已经出错，而用户和程序本身完全感知不到。 Service层往往涉及数据库事务，出现异常同样不适合捕获，否则事务无法自动回滚。此外Service层涉及业务逻辑，有些业务逻辑执行中遇到业务异常，可能需要在异常后转入分支业务流程。如果业务异常都被框架捕获了，业务功能就会不正常。 如果下层异常上升到Controller层还是无法处理的话，Controller层往往会给予用户友好提示，或是根据每一个API的异常表返回指定的异常类型，同样无法对所有异常一视同仁。 因此，我不建议在框架层面进行异常的自动、统一处理，尤其不要随意捕获异常。但，框架可以做兜底工作。如果异常上升到最上层逻辑还是无法处理的话，可以以统一的方式进行异常转换，比如通过@RestControllerAdvice + @ExceptionHandler，来捕获这些“未处理”异常： 对于自定义的业务异常，以Warn级别的日志记录异常以及当前URL、执行方法等信息后，提取异常中的错误码和消息等信息，转换为合适的API包装体返回给API调用方； 对于无法处理的系统异常，以Error级别的日志记录异常和上下文信息（比如URL、参数、用户ID）后，转换为普适的“服务器忙，请稍后再试”异常信息，同样以API包装体返回给调用方。 比如，下面这段代码的做法： @RestControllerAdvice @Slf4j public class RestControllerExceptionHandler &#123; private static int GENERIC_SERVER_ERROR_CODE &#x3D; 2000; private static String GENERIC_SERVER_ERROR_MESSAGE &#x3D; &quot;服务器忙，请稍后再试&quot;; @ExceptionHandler public APIResponse handle(HttpServletRequest req, HandlerMethod method, Exception ex) &#123; if (ex instanceof BusinessException) &#123; BusinessException exception &#x3D; (BusinessException) ex; log.warn(String.format(&quot;访问 %s -&gt; %s 出现业务异常！&quot;, req.getRequestURI(), method.toString()), ex); return new APIResponse(false, null, exception.getCode(), exception.getMessage()); &#125; else &#123; log.error(String.format(&quot;访问 %s -&gt; %s 出现系统异常！&quot;, req.getRequestURI(), method.toString()), ex); return new APIResponse(false, null, GENERIC_SERVER_ERROR_CODE, GENERIC_SERVER_ERROR_MESSAGE); &#125; &#125; &#125; 出现运行时系统异常后，异常处理程序会直接把异常转换为JSON返回给调用方： 要做得更好，你可以把相关出入参、用户信息在脱敏后记录到日志中，方便出现问题时根据上下文进一步排查。 第二个错，捕获了异常后直接生吞。在任何时候，我们捕获了异常都不应该生吞，也就是直接丢弃异常不记录、不抛出。这样的处理方式还不如不捕获异常，因为被生吞掉的异常一旦导致Bug，就很难在程序中找到蛛丝马迹，使得Bug排查工作难上加难。 通常情况下，生吞异常的原因，可能是不希望自己的方法抛出受检异常，只是为了把异常“处理掉”而捕获并生吞异常，也可能是想当然地认为异常并不重要或不可能产生。但不管是什么原因，不管是你认为多么不重要的异常，都不应该生吞，哪怕是一个日志也好。 第三个错，丢弃异常的原始信息。我们来看两个不太合适的异常处理方式，虽然没有完全生吞异常，但也丢失了宝贵的异常信息。 比如有这么一个会抛出受检异常的方法readFile： private void readFile() throws IOException &#123; Files.readAllLines(Paths.get(&quot;a_file&quot;)); &#125; 像这样调用readFile方法，捕获异常后，完全不记录原始异常，直接抛出一个转换后异常，导致出了问题不知道IOException具体是哪里引起的： @GetMapping(&quot;wrong1&quot;) public void wrong1()&#123; try &#123; readFile(); &#125; catch (IOException e) &#123; &#x2F;&#x2F;原始异常信息丢失 throw new RuntimeException(&quot;系统忙请稍后再试&quot;); &#125; &#125; 或者是这样，只记录了异常消息，却丢失了异常的类型、栈等重要信息： catch (IOException e) &#123; &#x2F;&#x2F;只保留了异常消息，栈没有记录 log.error(&quot;文件读取错误, &#123;&#125;&quot;, e.getMessage()); throw new RuntimeException(&quot;系统忙请稍后再试&quot;); &#125; 留下的日志是这样的，看完一脸茫然，只知道文件读取错误的文件名，至于为什么读取错误、是不存在还是没权限，完全不知道。 [12:57:19.746] [http-nio-45678-exec-1] [ERROR] [.g.t.c.e.d.HandleExceptionController:35 ] - 文件读取错误, a_file 这两种处理方式都不太合理，可以改为如下方式： catch (IOException e) &#123; log.error(&quot;文件读取错误&quot;, e); throw new RuntimeException(&quot;系统忙请稍后再试&quot;); &#125; 或者，把原始异常作为转换后新异常的cause，原始异常信息同样不会丢： catch (IOException e) &#123; throw new RuntimeException(&quot;系统忙请稍后再试&quot;, e); &#125; 其实，JDK内部也会犯类似的错。之前我遇到一个使用JDK10的应用偶发启动失败的案例，日志中可以看到出现类似的错误信息： Caused by: java.lang.SecurityException: Couldn&#39;t parse jurisdiction policy files in: unlimited at java.base&#x2F;javax.crypto.JceSecurity.setupJurisdictionPolicies(JceSecurity.java:355) at java.base&#x2F;javax.crypto.JceSecurity.access$000(JceSecurity.java:73) at java.base&#x2F;javax.crypto.JceSecurity$1.run(JceSecurity.java:109) at java.base&#x2F;javax.crypto.JceSecurity$1.run(JceSecurity.java:106) at java.base&#x2F;java.security.AccessController.doPrivileged(Native Method) at java.base&#x2F;javax.crypto.JceSecurity.&lt;clinit&gt;(JceSecurity.java:105) ... 20 more 查看JDK JceSecurity类setupJurisdictionPolicies方法源码，发现异常e没有记录，也没有作为新抛出异常的cause，当时读取文件具体出现什么异常（权限问题又或是IO问题）可能永远都无法知道了，对问题定位造成了很大困扰： 第四个错，抛出异常时不指定任何消息。我见过一些代码中的偷懒做法，直接抛出没有message的异常： throw new RuntimeException(); 这么写的同学可能觉得永远不会走到这个逻辑，永远不会出现这样的异常。但，这样的异常却出现了，被ExceptionHandler拦截到后输出了下面的日志信息： [13:25:18.031] [http-nio-45678-exec-3] [ERROR] [c.e.d.RestControllerExceptionHandler:24 ] - 访问 &#x2F;handleexception&#x2F;wrong3 -&gt; org.geekbang.time.commonmistakes.exception.demo1.HandleExceptionController#wrong3(String) 出现系统异常！ java.lang.RuntimeException: null ... 这里的null非常容易引起误解。按照空指针问题排查半天才发现，其实是异常的message为空。 总之，如果你捕获了异常打算处理的话，除了通过日志正确记录异常原始信息外，通常还有三种处理模式： 转换，即转换新的异常抛出。对于新抛出的异常，最好具有特定的分类和明确的异常消息，而不是随便抛一个无关或没有任何信息的异常，并最好通过cause关联老异常。 重试，即重试之前的操作。比如远程调用服务端过载超时的情况，盲目重试会让问题更严重，需要考虑当前情况是否适合重试。 恢复，即尝试进行降级处理，或使用默认值来替代原始数据。 以上，就是通过catch捕获处理异常的一些最佳实践。 小心finally中的异常有些时候，我们希望不管是否遇到异常，逻辑完成后都要释放资源，这时可以使用finally代码块而跳过使用catch代码块。 但要千万小心finally代码块中的异常，因为资源释放处理等收尾操作同样也可能出现异常。比如下面这段代码，我们在finally中抛出一个异常： @GetMapping(&quot;wrong&quot;) public void wrong() &#123; try &#123; log.info(&quot;try&quot;); &#x2F;&#x2F;异常丢失 throw new RuntimeException(&quot;try&quot;); &#125; finally &#123; log.info(&quot;finally&quot;); throw new RuntimeException(&quot;finally&quot;); &#125; &#125; 最后在日志中只能看到finally中的异常，虽然try中的逻辑出现了异常，但却被finally中的异常覆盖了。这是非常危险的，特别是finally中出现的异常是偶发的，就会在部分时候覆盖try中的异常，让问题更不明显： [13:34:42.247] [http-nio-45678-exec-1] [ERROR] [.a.c.c.C.[.[.[&#x2F;].[dispatcherServlet]:175 ] - Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.RuntimeException: finally] with root cause java.lang.RuntimeException: finally 至于异常为什么被覆盖，原因也很简单，因为一个方法无法出现两个异常。修复方式是，finally代码块自己负责异常捕获和处理： @GetMapping(&quot;right&quot;) public void right() &#123; try &#123; log.info(&quot;try&quot;); throw new RuntimeException(&quot;try&quot;); &#125; finally &#123; log.info(&quot;finally&quot;); try &#123; throw new RuntimeException(&quot;finally&quot;); &#125; catch (Exception ex) &#123; log.error(&quot;finally&quot;, ex); &#125; &#125; &#125; 或者可以把try中的异常作为主异常抛出，使用addSuppressed方法把finally中的异常附加到主异常上： @GetMapping(&quot;right2&quot;) public void right2() throws Exception &#123; Exception e &#x3D; null; try &#123; log.info(&quot;try&quot;); throw new RuntimeException(&quot;try&quot;); &#125; catch (Exception ex) &#123; e &#x3D; ex; &#125; finally &#123; log.info(&quot;finally&quot;); try &#123; throw new RuntimeException(&quot;finally&quot;); &#125; catch (Exception ex) &#123; if (e!&#x3D; null) &#123; e.addSuppressed(ex); &#125; else &#123; e &#x3D; ex; &#125; &#125; &#125; throw e; &#125; 运行方法可以得到如下异常信息，其中同时包含了主异常和被屏蔽的异常： java.lang.RuntimeException: try at org.geekbang.time.commonmistakes.exception.finallyissue.FinallyIssueController.right2(FinallyIssueController.java:69) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ... Suppressed: java.lang.RuntimeException: finally at org.geekbang.time.commonmistakes.exception.finallyissue.FinallyIssueController.right2(FinallyIssueController.java:75) ... 54 common frames omitted 其实这正是try-with-resources语句的做法，对于实现了AutoCloseable接口的资源，建议使用try-with-resources来释放资源，否则也可能会产生刚才提到的，释放资源时出现的异常覆盖主异常的问题。比如如下我们定义一个测试资源，其read和close方法都会抛出异常： public class TestResource implements AutoCloseable &#123; public void read() throws Exception&#123; throw new Exception(&quot;read error&quot;); &#125; @Override public void close() throws Exception &#123; throw new Exception(&quot;close error&quot;); &#125; &#125; 使用传统的try-finally语句，在try中调用read方法，在finally中调用close方法： @GetMapping(&quot;useresourcewrong&quot;) public void useresourcewrong() throws Exception &#123; TestResource testResource &#x3D; new TestResource(); try &#123; testResource.read(); &#125; finally &#123; testResource.close(); &#125; &#125; 可以看到，同样出现了finally中的异常覆盖了try中异常的问题： java.lang.Exception: close error at org.geekbang.time.commonmistakes.exception.finallyissue.TestResource.close(TestResource.java:10) at org.geekbang.time.commonmistakes.exception.finallyissue.FinallyIssueController.useresourcewrong(FinallyIssueController.java:27) 而改为try-with-resources模式之后： @GetMapping(&quot;useresourceright&quot;) public void useresourceright() throws Exception &#123; try (TestResource testResource &#x3D; new TestResource())&#123; testResource.read(); &#125; &#125; try和finally中的异常信息都可以得到保留： java.lang.Exception: read error at org.geekbang.time.commonmistakes.exception.finallyissue.TestResource.read(TestResource.java:6) ... Suppressed: java.lang.Exception: close error at org.geekbang.time.commonmistakes.exception.finallyissue.TestResource.close(TestResource.java:10) at org.geekbang.time.commonmistakes.exception.finallyissue.FinallyIssueController.useresourceright(FinallyIssueController.java:35) ... 54 common frames omitted 千万别把异常定义为静态变量既然我们通常会自定义一个业务异常类型，来包含更多的异常信息，比如异常错误码、友好的错误提示等，那就需要在业务逻辑各处，手动抛出各种业务异常来返回指定的错误码描述（比如对于下单操作，用户不存在返回2001，商品缺货返回2002等）。 对于这些异常的错误代码和消息，我们期望能够统一管理，而不是散落在程序各处定义。这个想法很好，但稍有不慎就可能会出现把异常定义为静态变量的坑。 我在救火排查某项目生产问题时，遇到了一件非常诡异的事情：我发现异常堆信息显示的方法调用路径，在当前入参的情况下根本不可能产生，项目的业务逻辑又很复杂，就始终没往异常信息是错的这方面想，总觉得是因为某个分支流程导致业务没有按照期望的流程进行。 经过艰难的排查，最终定位到原因是把异常定义为了静态变量，导致异常栈信息错乱，类似于定义一个Exceptions类来汇总所有的异常，把异常存放在静态字段中： public class Exceptions &#123; public static BusinessException ORDEREXISTS &#x3D; new BusinessException(&quot;订单已经存在&quot;, 3001); ... &#125; 把异常定义为静态变量会导致异常信息固化，这就和异常的栈一定是需要根据当前调用来动态获取相矛盾。 我们写段代码来模拟下这个问题：定义两个方法createOrderWrong和cancelOrderWrong方法，它们内部都会通过Exceptions类来获得一个订单不存在的异常；先后调用两个方法，然后抛出。 @GetMapping(&quot;wrong&quot;) public void wrong() &#123; try &#123; createOrderWrong(); &#125; catch (Exception ex) &#123; log.error(&quot;createOrder got error&quot;, ex); &#125; try &#123; cancelOrderWrong(); &#125; catch (Exception ex) &#123; log.error(&quot;cancelOrder got error&quot;, ex); &#125; &#125; private void createOrderWrong() &#123; &#x2F;&#x2F;这里有问题 throw Exceptions.ORDEREXISTS; &#125; private void cancelOrderWrong() &#123; &#x2F;&#x2F;这里有问题 throw Exceptions.ORDEREXISTS; &#125; 运行程序后看到如下日志，cancelOrder got error的提示对应了createOrderWrong方法。显然，cancelOrderWrong方法在出错后抛出的异常，其实是createOrderWrong方法出错的异常： [14:05:25.782] [http-nio-45678-exec-1] [ERROR] [.c.e.d.PredefinedExceptionController:25 ] - cancelOrder got error org.geekbang.time.commonmistakes.exception.demo2.BusinessException: 订单已经存在 at org.geekbang.time.commonmistakes.exception.demo2.Exceptions.&lt;clinit&gt;(Exceptions.java:5) at org.geekbang.time.commonmistakes.exception.demo2.PredefinedExceptionController.createOrderWrong(PredefinedExceptionController.java:50) at org.geekbang.time.commonmistakes.exception.demo2.PredefinedExceptionController.wrong(PredefinedExceptionController.java:18) 修复方式很简单，改一下Exceptions类的实现，通过不同的方法把每一种异常都new出来抛出即可： public class Exceptions &#123; public static BusinessException orderExists()&#123; return new BusinessException(&quot;订单已经存在&quot;, 3001); &#125; &#125; 提交线程池的任务出了异常会怎么样？在第3讲介绍线程池时我提到，线程池常用作异步处理或并行处理。那么，把任务提交到线程池处理，任务本身出现异常时会怎样呢？ 我们来看一个例子：提交10个任务到线程池异步处理，第5个任务抛出一个RuntimeException，每个任务完成后都会输出一行日志： @GetMapping(&quot;execute&quot;) public void execute() throws InterruptedException &#123; String prefix &#x3D; &quot;test&quot;; ExecutorService threadPool &#x3D; Executors.newFixedThreadPool(1, new ThreadFactoryBuilder().setNameFormat(prefix+&quot;%d&quot;).get()); &#x2F;&#x2F;提交10个任务到线程池处理，第5个任务会抛出运行时异常 IntStream.rangeClosed(1, 10).forEach(i -&gt; threadPool.execute(() -&gt; &#123; if (i &#x3D;&#x3D; 5) throw new RuntimeException(&quot;error&quot;); log.info(&quot;I&#39;m done : &#123;&#125;&quot;, i); &#125;)); threadPool.shutdown(); threadPool.awaitTermination(1, TimeUnit.HOURS); &#125; 观察日志可以发现两点： ... [14:33:55.990] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:26 ] - I&#39;m done : 4 Exception in thread &quot;test0&quot; java.lang.RuntimeException: error at org.geekbang.time.commonmistakes.exception.demo3.ThreadPoolAndExceptionController.lambda$null$0(ThreadPoolAndExceptionController.java:25) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) [14:33:55.990] [test1] [INFO ] [e.d.ThreadPoolAndExceptionController:26 ] - I&#39;m done : 6 ... 任务1到4所在的线程是test0，任务6开始运行在线程test1。由于我的线程池通过线程工厂为线程使用统一的前缀test加上计数器进行命名，因此从线程名的改变可以知道因为异常的抛出老线程退出了，线程池只能重新创建一个线程。如果每个异步任务都以异常结束，那么线程池可能完全起不到线程重用的作用。 因为没有手动捕获异常进行处理，ThreadGroup帮我们进行了未捕获异常的默认处理，向标准错误输出打印了出现异常的线程名称和异常信息。显然，这种没有以统一的错误日志格式记录错误信息打印出来的形式，对生产级代码是不合适的，ThreadGroup的相关源码如下所示： public void uncaughtException(Thread t, Throwable e) &#123; if (parent !&#x3D; null) &#123; parent.uncaughtException(t, e); &#125; else &#123; Thread.UncaughtExceptionHandler ueh &#x3D; Thread.getDefaultUncaughtExceptionHandler(); if (ueh !&#x3D; null) &#123; ueh.uncaughtException(t, e); &#125; else if (!(e instanceof ThreadDeath)) &#123; System.err.print(&quot;Exception in thread \\&quot;&quot; + t.getName() + &quot;\\&quot; &quot;); e.printStackTrace(System.err); &#125; &#125; &#125; 修复方式有2步： 以execute方法提交到线程池的异步任务，最好在任务内部做好异常处理； 设置自定义的异常处理程序作为保底，比如在声明线程池时自定义线程池的未捕获异常处理程序： new ThreadFactoryBuilder() .setNameFormat(prefix+&quot;%d&quot;) .setUncaughtExceptionHandler((thread, throwable)-&gt; log.error(&quot;ThreadPool &#123;&#125; got exception&quot;, thread, throwable)) .get() 或者设置全局的默认未捕获异常处理程序： static &#123; Thread.setDefaultUncaughtExceptionHandler((thread, throwable)-&gt; log.error(&quot;Thread &#123;&#125; got exception&quot;, thread, throwable)); &#125; 通过线程池ExecutorService的execute方法提交任务到线程池处理，如果出现异常会导致线程退出，控制台输出中可以看到异常信息。那么，把execute方法改为submit，线程还会退出吗，异常还能被处理程序捕获到吗？ 修改代码后重新执行程序可以看到如下日志，说明线程没退出，异常也没记录被生吞了： [15:44:33.769] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:47 ] - I&#39;m done : 1 [15:44:33.770] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:47 ] - I&#39;m done : 2 [15:44:33.770] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:47 ] - I&#39;m done : 3 [15:44:33.770] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:47 ] - I&#39;m done : 4 [15:44:33.770] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:47 ] - I&#39;m done : 6 [15:44:33.770] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:47 ] - I&#39;m done : 7 [15:44:33.770] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:47 ] - I&#39;m done : 8 [15:44:33.771] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:47 ] - I&#39;m done : 9 [15:44:33.771] [test0] [INFO ] [e.d.ThreadPoolAndExceptionController:47 ] - I&#39;m done : 10 为什么会这样呢？ 查看FutureTask源码可以发现，在执行任务出现异常之后，异常存到了一个outcome字段中，只有在调用get方法获取FutureTask结果的时候，才会以ExecutionException的形式重新抛出异常： public void run() &#123; ... try &#123; Callable&lt;V&gt; c &#x3D; callable; if (c !&#x3D; null &amp;&amp; state &#x3D;&#x3D; NEW) &#123; V result; boolean ran; try &#123; result &#x3D; c.call(); ran &#x3D; true; &#125; catch (Throwable ex) &#123; result &#x3D; null; ran &#x3D; false; setException(ex); &#125; ... &#125; protected void setException(Throwable t) &#123; if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; outcome &#x3D; t; UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); &#x2F;&#x2F; final state finishCompletion(); &#125; &#125; public V get() throws InterruptedException, ExecutionException &#123; int s &#x3D; state; if (s &lt;&#x3D; COMPLETING) s &#x3D; awaitDone(false, 0L); return report(s); &#125; private V report(int s) throws ExecutionException &#123; Object x &#x3D; outcome; if (s &#x3D;&#x3D; NORMAL) return (V)x; if (s &gt;&#x3D; CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x); &#125; 修改后的代码如下所示，我们把submit返回的Future放到了List中，随后遍历List来捕获所有任务的异常。这么做确实合乎情理。既然是以submit方式来提交任务，那么我们应该关心任务的执行结果，否则应该以execute来提交任务： List&lt;Future&gt; tasks &#x3D; IntStream.rangeClosed(1, 10).mapToObj(i -&gt; threadPool.submit(() -&gt; &#123; if (i &#x3D;&#x3D; 5) throw new RuntimeException(&quot;error&quot;); log.info(&quot;I&#39;m done : &#123;&#125;&quot;, i); &#125;)).collect(Collectors.toList()); tasks.forEach(task-&gt; &#123; try &#123; task.get(); &#125; catch (Exception e) &#123; log.error(&quot;Got exception&quot;, e); &#125; &#125;); 执行这段程序可以看到如下的日志输出： [15:44:13.543] [http-nio-45678-exec-1] [ERROR] [e.d.ThreadPoolAndExceptionController:69 ] - Got exception java.util.concurrent.ExecutionException: java.lang.RuntimeException: error 重点回顾在今天的文章中，我介绍了处理异常容易犯的几个错和最佳实践。 第一，注意捕获和处理异常的最佳实践。首先，不应该用AOP对所有方法进行统一异常处理，异常要么不捕获不处理，要么根据不同的业务逻辑、不同的异常类型进行精细化、针对性处理；其次，处理异常应该杜绝生吞，并确保异常栈信息得到保留；最后，如果需要重新抛出异常的话，请使用具有意义的异常类型和异常消息。 第二，务必小心finally代码块中资源回收逻辑，确保finally代码块不出现异常，内部把异常处理完毕，避免finally中的异常覆盖try中的异常；或者考虑使用addSuppressed方法把finally中的异常附加到try中的异常上，确保主异常信息不丢失。此外，使用实现了AutoCloseable接口的资源，务必使用try-with-resources模式来使用资源，确保资源可以正确释放，也同时确保异常可以正确处理。 第三，虽然在统一的地方定义收口所有的业务异常是一个不错的实践，但务必确保异常是每次new出来的，而不能使用一个预先定义的static字段存放异常，否则可能会引起栈信息的错乱。 第四，确保正确处理了线程池中任务的异常，如果任务通过execute提交，那么出现异常会导致线程退出，大量的异常会导致线程重复创建引起性能问题，我们应该尽可能确保任务不出异常，同时设置默认的未捕获异常处理程序来兜底；如果任务通过submit提交意味着我们关心任务的执行结果，应该通过拿到的Future调用其get方法来获得任务运行结果和可能出现的异常，否则异常可能就被生吞了。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 关于在finally代码块中抛出异常的坑，如果在finally代码块中返回值，你觉得程序会以try或catch中返回值为准，还是以finally中的返回值为准呢？ 对于手动抛出的异常，不建议直接使用Exception或RuntimeException，通常建议复用JDK中的一些标准异常，比如IllegalArgumentException、IllegalStateException、UnsupportedOperationException，你能说说它们的适用场景，并列出更多常用异常吗？ 不知道针对异常处理，你还遇到过什么坑，还有什么最佳实践的心得吗？我是朱晔，欢迎在评论区与我留言分享，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/13、日志：日志记录真没你想象的那么简单","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/13、日志：日志记录真没你想象的那么简单/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/13%E3%80%81%E6%97%A5%E5%BF%97%EF%BC%9A%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95%E7%9C%9F%E6%B2%A1%E4%BD%A0%E6%83%B3%E8%B1%A1%E7%9A%84%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95/","excerpt":"","text":"13 | 日志：日志记录真没你想象的那么简单作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我和你分享的是，记录日志可能会踩的坑。 一些同学可能要说了，记录日志还不简单，无非是几个常用的API方法，比如debug、info、warn、error；但我就见过不少坑都是记录日志引起的，容易出错主要在于三个方面： 日志框架众多，不同的类库可能会使用不同的日志框架，如何兼容是一个问题。 配置复杂且容易出错。日志配置文件通常很复杂，因此有些开发同学会从其他项目或者网络上复制一份配置文件，但却不知道如何修改，甚至是胡乱修改，造成很多问题。比如，重复记录日志的问题、同步日志的性能问题、异步记录的错误配置问题。 日志记录本身就有些误区，比如没考虑到日志内容获取的代价、胡乱使用日志级别等。 Logback、Log4j、Log4j2、commons-logging、JDK自带的java.util.logging等，都是Java体系的日志框架，确实非常多。而不同的类库，还可能选择使用不同的日志框架。这样一来，日志的统一管理就变得非常困难。为了解决这个问题，就有了SLF4J（Simple Logging Facade For Java），如下图所示： SLF4J实现了三种功能： 一是提供了统一的日志门面API，即图中紫色部分，实现了中立的日志记录API。 二是桥接功能，即图中蓝色部分，用来把各种日志框架的API（图中绿色部分）桥接到SLF4J API。这样一来，即便你的程序中使用了各种日志API记录日志，最终都可以桥接到SLF4J门面API。 三是适配功能，即图中红色部分，可以实现SLF4J API和实际日志框架（图中灰色部分）的绑定。SLF4J只是日志标准，我们还是需要一个实际的日志框架。日志框架本身没有实现SLF4J API，所以需要有一个前置转换。Logback就是按照SLF4J API标准实现的，因此不需要绑定模块做转换。 需要理清楚的是，虽然我们可以使用log4j-over-slf4j来实现Log4j桥接到SLF4J，也可以使用slf4j-log4j12实现SLF4J适配到Log4j，也把它们画到了一列，但是它不能同时使用它们，否则就会产生死循环。jcl和jul也是同样的道理。 虽然图中有4个灰色的日志实现框架，但我看到的业务系统使用最广泛的是Logback和Log4j，它们是同一人开发的。Logback可以认为是Log4j的改进版本，我更推荐使用。所以，关于日志框架配置的案例，我都会围绕Logback展开。 Spring Boot是目前最流行的Java框架，它的日志框架也用的是Logback。那，为什么我们没有手动引入Logback的包，就可以直接使用Logback了呢？ 查看Spring Boot的Maven依赖树，可以发现spring-boot-starter模块依赖了spring-boot-starter-logging模块，而spring-boot-starter-logging模块又帮我们自动引入了logback-classic（包含了SLF4J和Logback日志框架）和SLF4J的一些适配器。其中，log4j-to-slf4j用于实现Log4j2 API到SLF4J的桥接，jul-to-slf4j则是实现java.util.logging API到SLF4J的桥接： 接下来，我就用几个实际的案例和你说说日志配置和记录这两大问题，顺便以Logback为例复习一下常见的日志配置。 为什么我的日志会重复记录？日志重复记录在业务上非常常见，不但给查看日志和统计工作带来不必要的麻烦，还会增加磁盘和日志收集系统的负担。接下来，我和你分享两个重复记录的案例，同时帮助你梳理Logback配置的基本结构。 第一个案例是，logger配置继承关系导致日志重复记录。首先，定义一个方法实现debug、info、warn和error四种日志的记录： @Log4j2 @RequestMapping(&quot;logging&quot;) @RestController public class LoggingController &#123; @GetMapping(&quot;log&quot;) public void log() &#123; log.debug(&quot;debug&quot;); log.info(&quot;info&quot;); log.warn(&quot;warn&quot;); log.error(&quot;error&quot;); &#125; &#125; 然后，使用下面的Logback配置： 第11和12行设置了全局的日志级别为INFO，日志输出使用CONSOLE Appender。 第3到7行，首先将CONSOLE Appender定义为ConsoleAppender，也就是把日志输出到控制台（System.out&#x2F;System.err）；然后通过PatternLayout定义了日志的输出格式。关于格式化字符串的各种使用方式，你可以进一步查阅官方文档。 第8到10行实现了一个Logger配置，将应用包的日志级别设置为DEBUG、日志输出同样使用CONSOLE Appender。 &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt; &lt;configuration&gt; &lt;appender name&#x3D;&quot;CONSOLE&quot; class&#x3D;&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class&#x3D;&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%thread] [%-5level] [%logger&#123;40&#125;:%line] - %msg%n&lt;&#x2F;pattern&gt; &lt;&#x2F;layout&gt; &lt;&#x2F;appender&gt; &lt;logger name&#x3D;&quot;org.geekbang.time.commonmistakes.logging&quot; level&#x3D;&quot;DEBUG&quot;&gt; &lt;appender-ref ref&#x3D;&quot;CONSOLE&quot;&#x2F;&gt; &lt;&#x2F;logger&gt; &lt;root level&#x3D;&quot;INFO&quot;&gt; &lt;appender-ref ref&#x3D;&quot;CONSOLE&quot;&#x2F;&gt; &lt;&#x2F;root&gt; &lt;&#x2F;configuration&gt; 这段配置看起来没啥问题，但执行方法后出现了日志重复记录的问题： 从配置文件的第9和12行可以看到，CONSOLE这个Appender同时挂载到了两个Logger上，一个是我们定义的，一个是，由于我们定义的继承自，所以同一条日志既会通过logger记录，也会发送到root记录，因此应用package下的日志出现了重复记录。 后来我了解到，这个同学如此配置的初衷是实现自定义的logger配置，让应用内的日志暂时开启DEBUG级别的日志记录。其实，他完全不需要重复挂载Appender，去掉下挂载的Appender即可： &lt;logger name&#x3D;&quot;org.geekbang.time.commonmistakes.logging&quot; level&#x3D;&quot;DEBUG&quot;&#x2F;&gt; 如果自定义的需要把日志输出到不同的Appender，比如将应用的日志输出到文件app.log、把其他框架的日志输出到控制台，可以设置的additivity属性为false，这样就不会继承的Appender了： &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt; &lt;configuration&gt; &lt;appender name&#x3D;&quot;FILE&quot; class&#x3D;&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;file&gt;app.log&lt;&#x2F;file&gt; &lt;encoder class&#x3D;&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%thread] [%-5level] [%logger&#123;40&#125;:%line] - %msg%n&lt;&#x2F;pattern&gt; &lt;&#x2F;encoder&gt; &lt;&#x2F;appender&gt; &lt;appender name&#x3D;&quot;CONSOLE&quot; class&#x3D;&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class&#x3D;&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%thread] [%-5level] [%logger&#123;40&#125;:%line] - %msg%n&lt;&#x2F;pattern&gt; &lt;&#x2F;layout&gt; &lt;&#x2F;appender&gt; &lt;logger name&#x3D;&quot;org.geekbang.time.commonmistakes.logging&quot; level&#x3D;&quot;DEBUG&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;appender-ref ref&#x3D;&quot;FILE&quot;&#x2F;&gt; &lt;&#x2F;logger&gt; &lt;root level&#x3D;&quot;INFO&quot;&gt; &lt;appender-ref ref&#x3D;&quot;CONSOLE&quot; &#x2F;&gt; &lt;&#x2F;root&gt; &lt;&#x2F;configuration&gt; 第二个案例是，错误配置LevelFilter造成日志重复记录。 一般互联网公司都会使用ELK三件套来统一收集日志，有一次我们发现Kibana上展示的日志有部分重复，一直怀疑是Logstash配置错误，但最后发现还是Logback的配置错误引起的。 这个项目的日志是这样配置的：在记录日志到控制台的同时，把日志记录按照不同的级别记录到两个文件中： &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt; &lt;configuration&gt; &lt;property name&#x3D;&quot;logDir&quot; value&#x3D;&quot;.&#x2F;logs&quot; &#x2F;&gt; &lt;property name&#x3D;&quot;app.name&quot; value&#x3D;&quot;common-mistakes&quot; &#x2F;&gt; &lt;appender name&#x3D;&quot;CONSOLE&quot; class&#x3D;&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class&#x3D;&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%thread] [%-5level] [%logger&#123;40&#125;:%line] - %msg%n&lt;&#x2F;pattern&gt; &lt;&#x2F;layout&gt; &lt;&#x2F;appender&gt; &lt;appender name&#x3D;&quot;INFO_FILE&quot; class&#x3D;&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;File&gt;$&#123;logDir&#125;&#x2F;$&#123;app.name&#125;_info.log&lt;&#x2F;File&gt; &lt;filter class&#x3D;&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;INFO&lt;&#x2F;level&gt; &lt;&#x2F;filter&gt; &lt;encoder class&#x3D;&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%thread] [%-5level] [%logger&#123;40&#125;:%line] - %msg%n&lt;&#x2F;pattern&gt; &lt;charset&gt;UTF-8&lt;&#x2F;charset&gt; &lt;&#x2F;encoder&gt; &lt;&#x2F;appender&gt; &lt;appender name&#x3D;&quot;ERROR_FILE&quot; class&#x3D;&quot;ch.qos.logback.core.FileAppender &quot;&gt; &lt;File&gt;$&#123;logDir&#125;&#x2F;$&#123;app.name&#125;_error.log&lt;&#x2F;File&gt; &lt;filter class&#x3D;&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;WARN&lt;&#x2F;level&gt; &lt;&#x2F;filter&gt; &lt;encoder class&#x3D;&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%thread] [%-5level] [%logger&#123;40&#125;:%line] - %msg%n&lt;&#x2F;pattern&gt; &lt;charset&gt;UTF-8&lt;&#x2F;charset&gt; &lt;&#x2F;encoder&gt; &lt;&#x2F;appender&gt; &lt;root level&#x3D;&quot;INFO&quot;&gt; &lt;appender-ref ref&#x3D;&quot;CONSOLE&quot; &#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;INFO_FILE&quot;&#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;ERROR_FILE&quot;&#x2F;&gt; &lt;&#x2F;root&gt; &lt;&#x2F;configuration&gt; 这个配置文件比较长，我带着你一段一段地看： 第31到35行定义的root引用了三个Appender。 第5到9行是第一个ConsoleAppender，用于把所有日志输出到控制台。 第10到19行定义了一个FileAppender，用于记录文件日志，并定义了文件名、记录日志的格式和编码等信息。最关键的是，第12到14行定义的LevelFilter过滤日志，将过滤级别设置为INFO，目的是希望_info.log文件中可以记录INFO级别的日志。 第20到30行定义了一个类似的FileAppender，并使用ThresholdFilter来过滤日志，过滤级别设置为WARN，目的是把WARN以上级别的日志记录到另一个_error.log文件中。 运行一下测试程序： 可以看到，_info.log中包含了INFO、WARN和ERROR三个级别的日志，不符合我们的预期；error.log包含了WARN和ERROR两个级别的日志。因此，造成了日志的重复收集。 你可能会问，这么明显的日志重复为什么没有及时发现？一些公司使用自动化的ELK方案收集日志，日志会同时输出到控制台和文件，开发人员在本机测试时不太会关心文件中记录的日志，而在测试和生产环境又因为开发人员没有服务器访问权限，所以原始日志文件中的重复问题并不容易发现。 为了分析日志重复的原因，我们来复习一下ThresholdFilter和LevelFilter的配置方式。 分析ThresholdFilter的源码发现，当日志级别大于等于配置的级别时返回NEUTRAL，继续调用过滤器链上的下一个过滤器；否则，返回DENY直接拒绝记录日志： public class ThresholdFilter extends Filter&lt;ILoggingEvent&gt; &#123; public FilterReply decide(ILoggingEvent event) &#123; if (!isStarted()) &#123; return FilterReply.NEUTRAL; &#125; if (event.getLevel().isGreaterOrEqual(level)) &#123; return FilterReply.NEUTRAL; &#125; else &#123; return FilterReply.DENY; &#125; &#125; &#125; 在这个案例中，把ThresholdFilter设置为WARN，可以记录WARN和ERROR级别的日志。 LevelFilter用来比较日志级别，然后进行相应处理：如果匹配就调用onMatch定义的处理方式，默认是交给下一个过滤器处理（AbstractMatcherFilter基类中定义的默认值）；否则，调用onMismatch定义的处理方式，默认也是交给下一个过滤器处理。 public class LevelFilter extends AbstractMatcherFilter&lt;ILoggingEvent&gt; &#123; public FilterReply decide(ILoggingEvent event) &#123; if (!isStarted()) &#123; return FilterReply.NEUTRAL; &#125; if (event.getLevel().equals(level)) &#123; return onMatch; &#125; else &#123; return onMismatch; &#125; &#125; &#125; public abstract class AbstractMatcherFilter&lt;E&gt; extends Filter&lt;E&gt; &#123; protected FilterReply onMatch &#x3D; FilterReply.NEUTRAL; protected FilterReply onMismatch &#x3D; FilterReply.NEUTRAL; &#125; 和ThresholdFilter不同的是，LevelFilter仅仅配置level是无法真正起作用的。由于没有配置onMatch和onMismatch属性，所以相当于这个过滤器是无用的，导致INFO以上级别的日志都记录了。 定位到问题后，修改方式就很明显了：配置LevelFilter的onMatch属性为ACCEPT，表示接收INFO级别的日志；配置onMismatch属性为DENY，表示除了INFO级别都不记录： &lt;appender name&#x3D;&quot;INFO_FILE&quot; class&#x3D;&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;File&gt;$&#123;logDir&#125;&#x2F;$&#123;app.name&#125;_info.log&lt;&#x2F;File&gt; &lt;filter class&#x3D;&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt; &lt;level&gt;INFO&lt;&#x2F;level&gt; &lt;onMatch&gt;ACCEPT&lt;&#x2F;onMatch&gt; &lt;onMismatch&gt;DENY&lt;&#x2F;onMismatch&gt; &lt;&#x2F;filter&gt; ... &lt;&#x2F;appender&gt; 这样修改后，_info.log文件中只会有INFO级别的日志，不会出现日志重复的问题了。 使用异步日志改善性能的坑掌握了把日志输出到文件中的方法后，我们接下来面临的问题是，如何避免日志记录成为应用的性能瓶颈。这可以帮助我们解决，磁盘（比如机械磁盘）IO性能较差、日志量又很大的情况下，如何记录日志的问题。 我们先来测试一下，记录日志的性能问题，定义如下的日志配置，一共有两个Appender： FILE是一个FileAppender，用于记录所有的日志； CONSOLE是一个ConsoleAppender，用于记录带有time标记的日志。 &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt; &lt;configuration&gt; &lt;appender name&#x3D;&quot;FILE&quot; class&#x3D;&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;file&gt;app.log&lt;&#x2F;file&gt; &lt;encoder class&#x3D;&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%thread] [%-5level] [%logger&#123;40&#125;:%line] - %msg%n&lt;&#x2F;pattern&gt; &lt;&#x2F;encoder&gt; &lt;&#x2F;appender&gt; &lt;appender name&#x3D;&quot;CONSOLE&quot; class&#x3D;&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class&#x3D;&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%thread] [%-5level] [%logger&#123;40&#125;:%line] - %msg%n&lt;&#x2F;pattern&gt; &lt;&#x2F;layout&gt; &lt;filter class&#x3D;&quot;ch.qos.logback.core.filter.EvaluatorFilter&quot;&gt; &lt;evaluator class&#x3D;&quot;ch.qos.logback.classic.boolex.OnMarkerEvaluator&quot;&gt; &lt;marker&gt;time&lt;&#x2F;marker&gt; &lt;&#x2F;evaluator&gt; &lt;onMismatch&gt;DENY&lt;&#x2F;onMismatch&gt; &lt;onMatch&gt;ACCEPT&lt;&#x2F;onMatch&gt; &lt;&#x2F;filter&gt; &lt;&#x2F;appender&gt; &lt;root level&#x3D;&quot;INFO&quot;&gt; &lt;appender-ref ref&#x3D;&quot;FILE&quot;&#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;CONSOLE&quot;&#x2F;&gt; &lt;&#x2F;root&gt; &lt;&#x2F;configuration&gt; 不知道你有没有注意到，这段代码中有个EvaluatorFilter（求值过滤器），用于判断日志是否符合某个条件。 在后续的测试代码中，我们会把大量日志输出到文件中，日志文件会非常大，如果性能测试结果也混在其中的话，就很难找到那条日志。所以，这里我们使用EvaluatorFilter对日志按照标记进行过滤，并将过滤出的日志单独输出到控制台上。在这个案例中，我们给输出测试结果的那条日志上做了time标记。 配合使用标记和EvaluatorFilter，实现日志的按标签过滤，是一个不错的小技巧。 如下测试代码中，实现了记录指定次数的大日志，每条日志包含1MB字节的模拟数据，最后记录一条以time为标记的方法执行耗时日志： @GetMapping(&quot;performance&quot;) public void performance(@RequestParam(name &#x3D; &quot;count&quot;, defaultValue &#x3D; &quot;1000&quot;) int count) &#123; long begin &#x3D; System.currentTimeMillis(); String payload &#x3D; IntStream.rangeClosed(1, 1000000) .mapToObj(__ -&gt; &quot;a&quot;) .collect(Collectors.joining(&quot;&quot;)) + UUID.randomUUID().toString(); IntStream.rangeClosed(1, count).forEach(i -&gt; log.info(&quot;&#123;&#125; &#123;&#125;&quot;, i, payload)); Marker timeMarker &#x3D; MarkerFactory.getMarker(&quot;time&quot;); log.info(timeMarker, &quot;took &#123;&#125; ms&quot;, System.currentTimeMillis() - begin); &#125; 执行程序后可以看到，记录1000次日志和10000次日志的调用耗时，分别是6.3秒和44.5秒： 对于只记录文件日志的代码了来说，这个耗时挺长的。为了分析其中原因，我们需要分析下FileAppender的源码。 FileAppender继承自OutputStreamAppender，查看OutputStreamAppender源码的第30到33行发现，在追加日志的时候，是直接把日志写入OutputStream中，属于同步记录日志： public class OutputStreamAppender&lt;E&gt; extends UnsynchronizedAppenderBase&lt;E&gt; &#123; private OutputStream outputStream; boolean immediateFlush &#x3D; true; @Override protected void append(E eventObject) &#123; if (!isStarted()) &#123; return; &#125; subAppend(eventObject); &#125; protected void subAppend(E event) &#123; if (!isStarted()) &#123; return; &#125; try &#123; &#x2F;&#x2F;编码LoggingEvent byte[] byteArray &#x3D; this.encoder.encode(event); &#x2F;&#x2F;写字节流 writeBytes(byteArray); &#125; catch (IOException ioe) &#123; ... &#125; &#125; private void writeBytes(byte[] byteArray) throws IOException &#123; if(byteArray &#x3D;&#x3D; null || byteArray.length &#x3D;&#x3D; 0) return; lock.lock(); try &#123; &#x2F;&#x2F;这个OutputStream其实是一个ResilientFileOutputStream，其内部使用的是带缓冲的BufferedOutputStream this.outputStream.write(byteArray); if (immediateFlush) &#123; this.outputStream.flush();&#x2F;&#x2F;刷入OS &#125; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 分析到这里，我们就明白为什么日志大量写入时会耗时这么久了。那，有没有办法实现大量日志写入时，不会过多影响业务逻辑执行耗时，影响吞吐量呢？ 办法当然有了，使用Logback提供的AsyncAppender即可实现异步的日志记录。AsyncAppende类似装饰模式，也就是在不改变类原有基本功能的情况下为其增添新功能。这样，我们就可以把AsyncAppender附加在其他的Appender上，将其变为异步的。 定义一个异步Appender ASYNCFILE，包装之前的同步文件日志记录的FileAppender，就可以实现异步记录日志到文件： &lt;appender name&#x3D;&quot;ASYNCFILE&quot; class&#x3D;&quot;ch.qos.logback.classic.AsyncAppender&quot;&gt; &lt;appender-ref ref&#x3D;&quot;FILE&quot;&#x2F;&gt; &lt;&#x2F;appender&gt; &lt;root level&#x3D;&quot;INFO&quot;&gt; &lt;appender-ref ref&#x3D;&quot;ASYNCFILE&quot;&#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;CONSOLE&quot;&#x2F;&gt; &lt;&#x2F;root&gt; 测试一下可以发现，记录1000次日志和10000次日志的调用耗时，分别是735毫秒和668毫秒： 性能居然这么好，你觉得其中有什么问题吗？异步日志真的如此神奇和万能吗？当然不是，因为这样并没有记录下所有日志。我之前就遇到过很多关于AsyncAppender异步日志的坑，这些坑可以归结为三类： 记录异步日志撑爆内存； 记录异步日志出现日志丢失； 记录异步日志出现阻塞。 为了解释这三种坑，我来模拟一个慢日志记录场景：首先，自定义一个继承自ConsoleAppender的MySlowAppender，作为记录到控制台的输出器，写入日志时休眠1秒。 public class MySlowAppender extends ConsoleAppender &#123; @Override protected void subAppend(Object event) &#123; try &#123; &#x2F;&#x2F; 模拟慢日志 TimeUnit.MILLISECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; super.subAppend(event); &#125; &#125; 然后，在配置文件中使用AsyncAppender，将MySlowAppender包装为异步日志记录： &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt; &lt;configuration&gt; &lt;appender name&#x3D;&quot;CONSOLE&quot; class&#x3D;&quot;org.geekbang.time.commonmistakes.logging.async.MySlowAppender&quot;&gt; &lt;layout class&#x3D;&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%thread] [%-5level] [%logger&#123;40&#125;:%line] - %msg%n&lt;&#x2F;pattern&gt; &lt;&#x2F;layout&gt; &lt;&#x2F;appender&gt; &lt;appender name&#x3D;&quot;ASYNC&quot; class&#x3D;&quot;ch.qos.logback.classic.AsyncAppender&quot;&gt; &lt;appender-ref ref&#x3D;&quot;CONSOLE&quot; &#x2F;&gt; &lt;&#x2F;appender&gt; &lt;root level&#x3D;&quot;INFO&quot;&gt; &lt;appender-ref ref&#x3D;&quot;ASYNC&quot; &#x2F;&gt; &lt;&#x2F;root&gt; &lt;&#x2F;configuration&gt; 定义一段测试代码，循环记录一定次数的日志，最后输出方法执行耗时： @GetMapping(&quot;manylog&quot;) public void manylog(@RequestParam(name &#x3D; &quot;count&quot;, defaultValue &#x3D; &quot;1000&quot;) int count) &#123; long begin &#x3D; System.currentTimeMillis(); IntStream.rangeClosed(1, count).forEach(i -&gt; log.info(&quot;log-&#123;&#125;&quot;, i)); System.out.println(&quot;took &quot; + (System.currentTimeMillis() - begin) + &quot; ms&quot;); &#125; 执行方法后发现，耗时很短但出现了日志丢失：我们要记录1000条日志，最终控制台只能搜索到215条日志，而且日志的行号变为了一个问号。 出现这个问题的原因在于，AsyncAppender提供了一些配置参数，而我们没用对。我们结合相关源码分析一下： includeCallerData用于控制是否收集调用方数据，默认是false，此时方法行号、方法名等信息将不能显示（源码第2行以及7到11行）。 queueSize用于控制阻塞队列大小，使用的ArrayBlockingQueue阻塞队列（源码第15到17行），默认大小是256，即内存中最多保存256条日志。 discardingThreshold是控制丢弃日志的阈值，主要是防止队列满后阻塞。默认情况下，队列剩余量低于队列长度的20%，就会丢弃TRACE、DEBUG和INFO级别的日志。（参见源码第3到6行、18到19行、26到27行、33到34行、40到42行） neverBlock用于控制队列满的时候，加入的数据是否直接丢弃，不会阻塞等待，默认是false（源码第44到68行）。这里需要注意一下offer方法和put方法的区别，当队列满的时候offer方法不阻塞，而put方法会阻塞；neverBlock为true时，使用offer方法。 public class AsyncAppender extends AsyncAppenderBase&lt;ILoggingEvent&gt; &#123; boolean includeCallerData &#x3D; false;&#x2F;&#x2F;是否收集调用方数据 protected boolean isDiscardable(ILoggingEvent event) &#123; Level level &#x3D; event.getLevel(); return level.toInt() &lt;&#x3D; Level.INFO_INT;&#x2F;&#x2F;丢弃&lt;&#x3D;INFO级别的日志 &#125; protected void preprocess(ILoggingEvent eventObject) &#123; eventObject.prepareForDeferredProcessing(); if (includeCallerData) eventObject.getCallerData(); &#125; &#125; public class AsyncAppenderBase&lt;E&gt; extends UnsynchronizedAppenderBase&lt;E&gt; implements AppenderAttachable&lt;E&gt; &#123; BlockingQueue&lt;E&gt; blockingQueue;&#x2F;&#x2F;异步日志的关键，阻塞队列 public static final int DEFAULT_QUEUE_SIZE &#x3D; 256;&#x2F;&#x2F;默认队列大小 int queueSize &#x3D; DEFAULT_QUEUE_SIZE; static final int UNDEFINED &#x3D; -1; int discardingThreshold &#x3D; UNDEFINED; boolean neverBlock &#x3D; false;&#x2F;&#x2F;控制队列满的时候加入数据时是否直接丢弃，不会阻塞等待 @Override public void start() &#123; ... blockingQueue &#x3D; new ArrayBlockingQueue&lt;E&gt;(queueSize); if (discardingThreshold &#x3D;&#x3D; UNDEFINED) discardingThreshold &#x3D; queueSize &#x2F; 5;&#x2F;&#x2F;默认丢弃阈值是队列剩余量低于队列长度的20%，参见isQueueBelowDiscardingThreshold方法 ... &#125; @Override protected void append(E eventObject) &#123; if (isQueueBelowDiscardingThreshold() &amp;&amp; isDiscardable(eventObject)) &#123; &#x2F;&#x2F;判断是否可以丢数据 return; &#125; preprocess(eventObject); put(eventObject); &#125; private boolean isQueueBelowDiscardingThreshold() &#123; return (blockingQueue.remainingCapacity() &lt; discardingThreshold); &#125; private void put(E eventObject) &#123; if (neverBlock) &#123; &#x2F;&#x2F;根据neverBlock决定使用不阻塞的offer还是阻塞的put方法 blockingQueue.offer(eventObject); &#125; else &#123; putUninterruptibly(eventObject); &#125; &#125; &#x2F;&#x2F;以阻塞方式添加数据到队列 private void putUninterruptibly(E eventObject) &#123; boolean interrupted &#x3D; false; try &#123; while (true) &#123; try &#123; blockingQueue.put(eventObject); break; &#125; catch (InterruptedException e) &#123; interrupted &#x3D; true; &#125; &#125; &#125; finally &#123; if (interrupted) &#123; Thread.currentThread().interrupt(); &#125; &#125; &#125; &#125; 看到默认队列大小为256，达到80%容量后开始丢弃&lt;&#x3D;INFO级别的日志后，我们就可以理解日志中为什么只有215条INFO日志了。 我们可以继续分析下异步记录日志出现坑的原因。 queueSize设置得特别大，就可能会导致OOM。 queueSize设置得比较小（默认值就非常小），且discardingThreshold设置为大于0的值（或者为默认值），队列剩余容量少于discardingThreshold的配置就会丢弃&lt;&#x3D;INFO的日志。这里的坑点有两个。一是，因为discardingThreshold的存在，设置queueSize时容易踩坑。比如，本例中最大日志并发是1000，即便设置queueSize为1000同样会导致日志丢失。二是，discardingThreshold参数容易有歧义，它不是百分比，而是日志条数。对于总容量10000的队列，如果希望队列剩余容量少于1000条的时候丢弃，需要配置为1000。 neverBlock默认为false，意味着总可能会出现阻塞。如果discardingThreshold为0，那么队列满时再有日志写入就会阻塞；如果discardingThreshold不为0，也只会丢弃&lt;&#x3D;INFO级别的日志，那么出现大量错误日志时，还是会阻塞程序。 可以看出queueSize、discardingThreshold和neverBlock这三个参数息息相关，务必按需进行设置和取舍，到底是性能为先，还是数据不丢为先： 如果考虑绝对性能为先，那就设置neverBlock为true，永不阻塞。 如果考虑绝对不丢数据为先，那就设置discardingThreshold为0，即使是&lt;&#x3D;INFO的级别日志也不会丢，但最好把queueSize设置大一点，毕竟默认的queueSize显然太小，太容易阻塞。 如果希望兼顾两者，可以丢弃不重要的日志，把queueSize设置大一点，再设置一个合理的discardingThreshold。 以上就是日志配置最常见的两个误区了。接下来，我们再看一个日志记录本身的误区。 使用日志占位符就不需要进行日志级别判断了？不知道你有没有听人说过：SLF4J的{}占位符语法，到真正记录日志时才会获取实际参数，因此解决了日志数据获取的性能问题。你觉得，这种说法对吗？ 为了验证这个问题，我们写一段测试代码：有一个slowString方法，返回结果耗时1秒： private String slowString(String s) &#123; System.out.println(&quot;slowString called via &quot; + s); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; return &quot;OK&quot;; &#125; 如果我们记录DEBUG日志，并设置只记录&gt;&#x3D;INFO级别的日志，程序是否也会耗时1秒呢？我们使用三种方法来测试： 拼接字符串方式记录slowString； 使用占位符方式记录slowString； 先判断日志级别是否启用DEBUG。 StopWatch stopWatch &#x3D; new StopWatch(); stopWatch.start(&quot;debug1&quot;); log.debug(&quot;debug1:&quot; + slowString(&quot;debug1&quot;)); stopWatch.stop(); stopWatch.start(&quot;debug2&quot;); log.debug(&quot;debug2:&#123;&#125;&quot;, slowString(&quot;debug2&quot;)); stopWatch.stop(); stopWatch.start(&quot;debug3&quot;); if (log.isDebugEnabled()) log.debug(&quot;debug3:&#123;&#125;&quot;, slowString(&quot;debug3&quot;)); stopWatch.stop(); 可以看到，前两种方式都调用了slowString方法，所以耗时都是1秒： 使用占位符方式记录slowString的方式，同样需要耗时1秒，是因为这种方式虽然允许我们传入Object，不用拼接字符串，但也只是延迟（如果日志不记录那么就是省去）了日志参数对象.toString()和字符串拼接的耗时。 在这个案例中，除非事先判断日志级别，否则必然会调用slowString方法。回到之前提的问题，使用{}占位符语法不能通过延迟参数值获取，来解决日志数据获取的性能问题。 除了事先判断日志级别，我们还可以通过lambda表达式进行延迟参数内容获取。但，SLF4J的API还不支持lambda，因此需要使用Log4j2日志API，把Lombok的@Slf4j注解替换为@Log4j2注解，这样就可以提供一个lambda表达式作为提供参数数据的方法： @Log4j2 public class LoggingController &#123; ... log.debug(&quot;debug4:&#123;&#125;&quot;, ()-&gt;slowString(&quot;debug4&quot;)); 像这样调用debug方法，签名是Supplier&lt;?&gt;，参数会延迟到真正需要记录日志时再获取： void debug(String message, Supplier&lt;?&gt;... paramSuppliers); public void logIfEnabled(final String fqcn, final Level level, final Marker marker, final String message, final Supplier&lt;?&gt;... paramSuppliers) &#123; if (isEnabled(level, marker, message)) &#123; logMessage(fqcn, level, marker, message, paramSuppliers); &#125; &#125; protected void logMessage(final String fqcn, final Level level, final Marker marker, final String message, final Supplier&lt;?&gt;... paramSuppliers) &#123; final Message msg &#x3D; messageFactory.newMessage(message, LambdaUtil.getAll(paramSuppliers)); logMessageSafely(fqcn, level, marker, msg, msg.getThrowable()); &#125; 修改后再次运行测试，可以看到这次debug4并不会调用slowString方法： 其实，我们只是换成了Log4j2 API，真正的日志记录还是走的Logback框架。没错，这就是SLF4J适配的一个好处。 重点回顾我将记录日志的坑，总结为框架使用配置和记录本身两个方面。 Java的日志框架众多，SLF4J实现了这些框架记录日志的统一。在使用SLF4J时，我们需要理清楚其桥接API和绑定这两个模块。如果程序启动时出现SLF4J的错误提示，那很可能是配置出现了问题，可以使用Maven的dependency:tree命令梳理依赖关系。 Logback是Java最常用的日志框架，其配置比较复杂，你可以参考官方文档中关于Appender、Layout、Filter的配置，切记不要随意从其他地方复制别人的配置，避免出现错误或与当前需求不符。 使用异步日志解决性能问题，是用空间换时间。但空间毕竟有限，当空间满了之后，我们要考虑是阻塞等待，还是丢弃日志。如果更希望不丢弃重要日志，那么选择阻塞等待；如果更希望程序不要因为日志记录而阻塞，那么就需要丢弃日志。 最后，我强调的是，日志框架提供的参数化日志记录方式不能完全取代日志级别的判断。如果你的日志量很大，获取日志参数代价也很大，就要进行相应日志级别的判断，避免不记录日志也要花费时间获取日志参数的问题。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在第一小节的案例中，我们把INFO级别的日志存放到_info.log中，把WARN和ERROR级别的日志存放到_error.log中。如果现在要把INFO和WARN级别的日志存放到_info.log中，把ERROR日志存放到_error.log中，应该如何配置Logback呢？ 生产级项目的文件日志肯定需要按时间和日期进行分割和归档处理，以避免单个文件太大，同时保留一定天数的历史日志，你知道如何配置吗？可以在官方文档找到答案。 针对日志记录和配置，你还遇到过其他坑吗？我是朱晔，欢迎在评论区与我留言分享，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/15、序列化：一来一回你还是原来的你吗？","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/15、序列化：一来一回你还是原来的你吗？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/15%E3%80%81%E5%BA%8F%E5%88%97%E5%8C%96%EF%BC%9A%E4%B8%80%E6%9D%A5%E4%B8%80%E5%9B%9E%E4%BD%A0%E8%BF%98%E6%98%AF%E5%8E%9F%E6%9D%A5%E7%9A%84%E4%BD%A0%E5%90%97%EF%BC%9F/","excerpt":"","text":"15 | 序列化：一来一回你还是原来的你吗？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来和你聊聊序列化相关的坑和最佳实践。 序列化是把对象转换为字节流的过程，以方便传输或存储。反序列化，则是反过来把字节流转换为对象的过程。在介绍文件IO的时候，我提到字符编码是把字符转换为二进制的过程，至于怎么转换需要由字符集制定规则。同样地，对象的序列化和反序列化，也需要由序列化算法制定规则。 关于序列化算法，几年前常用的有JDK（Java）序列化、XML序列化等，但前者不能跨语言，后者性能较差（时间空间开销大）；现在RESTful应用最常用的是JSON序列化，追求性能的RPC框架（比如gRPC）使用protobuf序列化，这2种方法都是跨语言的，而且性能不错，应用广泛。 在架构设计阶段，我们可能会重点关注算法选型，在性能、易用性和跨平台性等中权衡，不过这里的坑比较少。通常情况下，序列化问题常见的坑会集中在业务场景中，比如Redis、参数和响应序列化反序列化。 今天，我们就一起聊聊开发中序列化常见的一些坑吧。 序列化和反序列化需要确保算法一致业务代码中涉及序列化时，很重要的一点是要确保序列化和反序列化的算法一致性。有一次我要排查缓存命中率问题，需要运维同学帮忙拉取Redis中的Key，结果他反馈Redis中存的都是乱码，怀疑Redis被攻击了。其实呢，这个问题就是序列化算法导致的，我们来看下吧。 在这个案例中，开发同学使用RedisTemplate来操作Redis进行数据缓存。因为相比于Jedis，使用Spring提供的RedisTemplate操作Redis，除了无需考虑连接池、更方便外，还可以与Spring Cache等其他组件无缝整合。如果使用Spring Boot的话，无需任何配置就可以直接使用。 数据（包含Key和Value）要保存到Redis，需要经过序列化算法来序列化成字符串。虽然Redis支持多种数据结构，比如Hash，但其每一个field的Value还是字符串。如果Value本身也是字符串的话，能否有便捷的方式来使用RedisTemplate，而无需考虑序列化呢？ 其实是有的，那就是StringRedisTemplate。 那StringRedisTemplate和RedisTemplate的区别是什么呢？开头提到的乱码又是怎么回事呢？带着这些问题让我们来研究一下吧。 写一段测试代码，在应用初始化完成后向Redis设置两组数据，第一次使用RedisTemplate设置Key为redisTemplate、Value为User对象，第二次使用StringRedisTemplate设置Key为stringRedisTemplate、Value为JSON序列化后的User对象： @Autowired private RedisTemplate redisTemplate; @Autowired private StringRedisTemplate stringRedisTemplate; @Autowired private ObjectMapper objectMapper; @PostConstruct public void init() throws JsonProcessingException &#123; redisTemplate.opsForValue().set(&quot;redisTemplate&quot;, new User(&quot;zhuye&quot;, 36)); stringRedisTemplate.opsForValue().set(&quot;stringRedisTemplate&quot;, objectMapper.writeValueAsString(new User(&quot;zhuye&quot;, 36))); &#125; 如果你认为，StringRedisTemplate和RedisTemplate的区别，无非是读取的Value是String和Object，那就大错特错了，因为使用这两种方式存取的数据完全无法通用。 我们做个小实验，通过RedisTemplate读取Key为stringRedisTemplate的Value，使用StringRedisTemplate读取Key为redisTemplate的Value： log.info(&quot;redisTemplate get &#123;&#125;&quot;, redisTemplate.opsForValue().get(&quot;stringRedisTemplate&quot;)); log.info(&quot;stringRedisTemplate get &#123;&#125;&quot;, stringRedisTemplate.opsForValue().get(&quot;redisTemplate&quot;)); 结果是，两次都无法读取到Value： [11:49:38.478] [http-nio-45678-exec-1] [INFO ] [.t.c.s.demo1.RedisTemplateController:38 ] - redisTemplate get null [11:49:38.481] [http-nio-45678-exec-1] [INFO ] [.t.c.s.demo1.RedisTemplateController:39 ] - stringRedisTemplate get null 通过redis-cli客户端工具连接到Redis，你会发现根本就没有叫作redisTemplate的Key，所以StringRedisTemplate无法查到数据： 查看RedisTemplate的源码发现，默认情况下RedisTemplate针对Key和Value使用了JDK序列化： public void afterPropertiesSet() &#123; ... if (defaultSerializer &#x3D;&#x3D; null) &#123; defaultSerializer &#x3D; new JdkSerializationRedisSerializer( classLoader !&#x3D; null ? classLoader : this.getClass().getClassLoader()); &#125; if (enableDefaultSerializer) &#123; if (keySerializer &#x3D;&#x3D; null) &#123; keySerializer &#x3D; defaultSerializer; defaultUsed &#x3D; true; &#125; if (valueSerializer &#x3D;&#x3D; null) &#123; valueSerializer &#x3D; defaultSerializer; defaultUsed &#x3D; true; &#125; if (hashKeySerializer &#x3D;&#x3D; null) &#123; hashKeySerializer &#x3D; defaultSerializer; defaultUsed &#x3D; true; &#125; if (hashValueSerializer &#x3D;&#x3D; null) &#123; hashValueSerializer &#x3D; defaultSerializer; defaultUsed &#x3D; true; &#125; &#125; ... &#125; redis-cli看到的类似一串乱码的”\\xac\\xed\\x00\\x05t\\x00\\rredisTemplate”字符串，其实就是字符串redisTemplate经过JDK序列化后的结果。这就回答了之前提到的乱码问题。而RedisTemplate尝试读取Key为stringRedisTemplate数据时，也会对这个字符串进行JDK序列化处理，所以同样无法读取到数据。 而StringRedisTemplate对于Key和Value，使用的是String序列化方式，Key和Value只能是String： public class StringRedisTemplate extends RedisTemplate&lt;String, String&gt; &#123; public StringRedisTemplate() &#123; setKeySerializer(RedisSerializer.string()); setValueSerializer(RedisSerializer.string()); setHashKeySerializer(RedisSerializer.string()); setHashValueSerializer(RedisSerializer.string()); &#125; &#125; public class StringRedisSerializer implements RedisSerializer&lt;String&gt; &#123; @Override public String deserialize(@Nullable byte[] bytes) &#123; return (bytes &#x3D;&#x3D; null ? null : new String(bytes, charset)); &#125; @Override public byte[] serialize(@Nullable String string) &#123; return (string &#x3D;&#x3D; null ? null : string.getBytes(charset)); &#125; &#125; 看到这里，我们应该知道RedisTemplate和StringRedisTemplate保存的数据无法通用。修复方式就是，让它们读取自己存的数据： 使用RedisTemplate读出的数据，由于是Object类型的，使用时可以先强制转换为User类型； 使用StringRedisTemplate读取出的字符串，需要手动将JSON反序列化为User类型。 &#x2F;&#x2F;使用RedisTemplate获取Value，无需反序列化就可以拿到实际对象，虽然方便，但是Redis中保存的Key和Value不易读 User userFromRedisTemplate &#x3D; (User) redisTemplate.opsForValue().get(&quot;redisTemplate&quot;); log.info(&quot;redisTemplate get &#123;&#125;&quot;, userFromRedisTemplate); &#x2F;&#x2F;使用StringRedisTemplate，虽然Key正常，但是Value存取需要手动序列化成字符串 User userFromStringRedisTemplate &#x3D; objectMapper.readValue(stringRedisTemplate.opsForValue().get(&quot;stringRedisTemplate&quot;), User.class); log.info(&quot;stringRedisTemplate get &#123;&#125;&quot;, userFromStringRedisTemplate); 这样就可以得到正确输出： [13:32:09.087] [http-nio-45678-exec-6] [INFO ] [.t.c.s.demo1.RedisTemplateController:45 ] - redisTemplate get User(name&#x3D;zhuye, age&#x3D;36) [13:32:09.092] [http-nio-45678-exec-6] [INFO ] [.t.c.s.demo1.RedisTemplateController:47 ] - stringRedisTemplate get User(name&#x3D;zhuye, age&#x3D;36) 看到这里你可能会说，使用RedisTemplate获取Value虽然方便，但是Key和Value不易读；而使用StringRedisTemplate虽然Key是普通字符串，但是Value存取需要手动序列化成字符串，有没有两全其美的方式呢？ 当然有，自己定义RedisTemplate的Key和Value的序列化方式即可：Key的序列化使用RedisSerializer.string()（也就是StringRedisSerializer方式）实现字符串序列化，而Value的序列化使用Jackson2JsonRedisSerializer： @Bean public &lt;T&gt; RedisTemplate&lt;String, T&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate&lt;String, T&gt; redisTemplate &#x3D; new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer &#x3D; new Jackson2JsonRedisSerializer(Object.class); redisTemplate.setKeySerializer(RedisSerializer.string()); redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); redisTemplate.setHashKeySerializer(RedisSerializer.string()); redisTemplate.setHashValueSerializer(jackson2JsonRedisSerializer); redisTemplate.afterPropertiesSet(); return redisTemplate; &#125; 写代码测试一下存取，直接注入类型为RedisTemplate&lt;String, User&gt;的userRedisTemplate字段，然后在right2方法中，使用注入的userRedisTemplate存入一个User对象，再分别使用userRedisTemplate和StringRedisTemplate取出这个对象： @Autowired private RedisTemplate&lt;String, User&gt; userRedisTemplate; @GetMapping(&quot;right2&quot;) public void right2() &#123; User user &#x3D; new User(&quot;zhuye&quot;, 36); userRedisTemplate.opsForValue().set(user.getName(), user); Object userFromRedis &#x3D; userRedisTemplate.opsForValue().get(user.getName()); log.info(&quot;userRedisTemplate get &#123;&#125; &#123;&#125;&quot;, userFromRedis, userFromRedis.getClass()); log.info(&quot;stringRedisTemplate get &#123;&#125;&quot;, stringRedisTemplate.opsForValue().get(user.getName())); &#125; 乍一看没啥问题，StringRedisTemplate成功查出了我们存入的数据： [14:07:41.315] [http-nio-45678-exec-1] [INFO ] [.t.c.s.demo1.RedisTemplateController:55 ] - userRedisTemplate get &#123;name&#x3D;zhuye, age&#x3D;36&#125; class java.util.LinkedHashMap [14:07:41.318] [http-nio-45678-exec-1] [INFO ] [.t.c.s.demo1.RedisTemplateController:56 ] - stringRedisTemplate get &#123;&quot;name&quot;:&quot;zhuye&quot;,&quot;age&quot;:36&#125; Redis里也可以查到Key是纯字符串，Value是JSON序列化后的User对象： 但值得注意的是，这里有一个坑。第一行的日志输出显示，userRedisTemplate获取到的Value，是LinkedHashMap类型的，完全不是泛型的RedisTemplate设置的User类型。 如果我们把代码里从Redis中获取到的Value变量类型由Object改为User，编译不会出现问题，但会出现ClassCastException： java.lang.ClassCastException: java.util.LinkedHashMap cannot be cast to org.geekbang.time.commonmistakes.serialization.demo1.User 修复方式是，修改自定义RestTemplate的代码，把new出来的Jackson2JsonRedisSerializer设置一个自定义的ObjectMapper，启用activateDefaultTyping方法把类型信息作为属性写入序列化后的数据中（当然了，你也可以调整JsonTypeInfo.As枚举以其他形式保存类型信息）： ... Jackson2JsonRedisSerializer jackson2JsonRedisSerializer &#x3D; new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper &#x3D; new ObjectMapper(); &#x2F;&#x2F;把类型信息作为属性写入Value objectMapper.activateDefaultTyping(objectMapper.getPolymorphicTypeValidator(), ObjectMapper.DefaultTyping.NON_FINAL, JsonTypeInfo.As.PROPERTY); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); ... 或者，直接使用RedisSerializer.json()快捷方法，它内部使用的GenericJackson2JsonRedisSerializer直接设置了把类型作为属性保存到Value中： redisTemplate.setKeySerializer(RedisSerializer.string()); redisTemplate.setValueSerializer(RedisSerializer.json()); redisTemplate.setHashKeySerializer(RedisSerializer.string()); redisTemplate.setHashValueSerializer(RedisSerializer.json()); 重启程序调用right2方法进行测试，可以看到，从自定义的RedisTemplate中获取到的Value是User类型的（第一行日志），而且Redis中实际保存的Value包含了类型完全限定名（第二行日志）： [15:10:50.396] [http-nio-45678-exec-1] [INFO ] [.t.c.s.demo1.RedisTemplateController:55 ] - userRedisTemplate get User(name&#x3D;zhuye, age&#x3D;36) class org.geekbang.time.commonmistakes.serialization.demo1.User [15:10:50.399] [http-nio-45678-exec-1] [INFO ] [.t.c.s.demo1.RedisTemplateController:56 ] - stringRedisTemplate get [&quot;org.geekbang.time.commonmistakes.serialization.demo1.User&quot;,&#123;&quot;name&quot;:&quot;zhuye&quot;,&quot;age&quot;:36&#125;] 因此，反序列化时可以直接得到User类型的Value。 通过对RedisTemplate组件的分析，可以看到，当数据需要序列化后保存时，读写数据使用一致的序列化算法的必要性，否则就像对牛弹琴。 这里，我再总结下Spring提供的4种RedisSerializer（Redis序列化器）： 默认情况下，RedisTemplate使用JdkSerializationRedisSerializer，也就是JDK序列化，容易产生Redis中保存了乱码的错觉。 通常考虑到易读性，可以设置Key的序列化器为StringRedisSerializer。但直接使用RedisSerializer.string()，相当于使用了UTF_8编码的StringRedisSerializer，需要注意字符集问题。 如果希望Value也是使用JSON序列化的话，可以把Value序列化器设置为Jackson2JsonRedisSerializer。默认情况下，不会把类型信息保存在Value中，即使我们定义RedisTemplate的Value泛型为实际类型，查询出的Value也只能是LinkedHashMap类型。如果希望直接获取真实的数据类型，你可以启用Jackson ObjectMapper的activateDefaultTyping方法，把类型信息一起序列化保存在Value中。 如果希望Value以JSON保存并带上类型信息，更简单的方式是，直接使用RedisSerializer.json()快捷方法来获取序列化器。 注意Jackson JSON反序列化对额外字段的处理前面我提到，通过设置JSON序列化工具Jackson的activateDefaultTyping方法，可以在序列化数据时写入对象类型。其实，Jackson还有很多参数可以控制序列化和反序列化，是一个功能强大而完善的序列化工具。因此，很多框架都将Jackson作为JDK序列化工具，比如Spring Web。但也正是这个原因，我们使用时要小心各个参数的配置。 比如，在开发Spring Web应用程序时，如果自定义了ObjectMapper，并把它注册成了Bean，那很可能会导致Spring Web使用的ObjectMapper也被替换，导致Bug。 我们来看一个案例。程序一开始是正常的，某一天开发同学希望修改一下ObjectMapper的行为，让枚举序列化为索引值而不是字符串值，比如默认情况下序列化一个Color枚举中的Color.BLUE会得到字符串BLUE： @Autowired private ObjectMapper objectMapper; @GetMapping(&quot;test&quot;) public void test() throws JsonProcessingException &#123; log.info(&quot;color:&#123;&#125;&quot;, objectMapper.writeValueAsString(Color.BLUE)); &#125; enum Color &#123; RED, BLUE &#125; 于是，这位同学就重新定义了一个ObjectMapper Bean，开启了WRITE_ENUMS_USING_INDEX功能特性： @Bean public ObjectMapper objectMapper()&#123; ObjectMapper objectMapper&#x3D;new ObjectMapper(); objectMapper.configure(SerializationFeature.WRITE_ENUMS_USING_INDEX,true); return objectMapper; &#125; 开启这个特性后，Color.BLUE枚举序列化成索引值1： [16:11:37.382] [http-nio-45678-exec-1] [INFO ] [c.s.d.JsonIgnorePropertiesController:19 ] - color:1 修改后处理枚举序列化的逻辑是满足了要求，但线上爆出了大量400错误，日志中也出现了很多UnrecognizedPropertyException： JSON parse error: Unrecognized field \\&quot;ver\\&quot; (class org.geekbang.time.commonmistakes.serialization.demo4.UserWrong), not marked as ignorable; nested exception is com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field \\&quot;version\\&quot; (class org.geekbang.time.commonmistakes.serialization.demo4.UserWrong), not marked as ignorable (one known property: \\&quot;name\\&quot;])\\n at [Source: (PushbackInputStream); line: 1, column: 22] (through reference chain: org.geekbang.time.commonmistakes.serialization.demo4.UserWrong[\\&quot;ver\\&quot;]) 从异常信息中可以看到，这是因为反序列化的时候，原始数据多了一个version属性。进一步分析发现，我们使用了UserWrong类型作为Web控制器wrong方法的入参，其中只有一个name属性： @Data public class UserWrong &#123; private String name; &#125; @PostMapping(&quot;wrong&quot;) public UserWrong wrong(@RequestBody UserWrong user) &#123; return user; &#125; 而客户端实际传过来的数据多了一个version属性。那，为什么之前没这个问题呢？ 问题就出在，自定义ObjectMapper启用WRITE_ENUMS_USING_INDEX序列化功能特性时，覆盖了Spring Boot自动创建的ObjectMapper；而这个自动创建的ObjectMapper设置过FAIL_ON_UNKNOWN_PROPERTIES反序列化特性为false，以确保出现未知字段时不要抛出异常。源码如下： public MappingJackson2HttpMessageConverter() &#123; this(Jackson2ObjectMapperBuilder.json().build()); &#125; public class Jackson2ObjectMapperBuilder &#123; ... private void customizeDefaultFeatures(ObjectMapper objectMapper) &#123; if (!this.features.containsKey(MapperFeature.DEFAULT_VIEW_INCLUSION)) &#123; configureFeature(objectMapper, MapperFeature.DEFAULT_VIEW_INCLUSION, false); &#125; if (!this.features.containsKey(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES)) &#123; configureFeature(objectMapper, DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); &#125; &#125; &#125; 要修复这个问题，有三种方式： 第一种，同样禁用自定义的ObjectMapper的FAIL_ON_UNKNOWN_PROPERTIES： @Bean public ObjectMapper objectMapper()&#123; ObjectMapper objectMapper&#x3D;new ObjectMapper(); objectMapper.configure(SerializationFeature.WRITE_ENUMS_USING_INDEX,true); objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES,false); return objectMapper; &#125; 第二种，设置自定义类型，加上@JsonIgnoreProperties注解，开启ignoreUnknown属性，以实现反序列化时忽略额外的数据： @Data @JsonIgnoreProperties(ignoreUnknown &#x3D; true) public class UserRight &#123; private String name; &#125; 第三种，不要自定义ObjectMapper，而是直接在配置文件设置相关参数，来修改Spring默认的ObjectMapper的功能。比如，直接在配置文件启用把枚举序列化为索引号： spring.jackson.serialization.write_enums_using_index&#x3D;true 或者可以直接定义Jackson2ObjectMapperBuilderCustomizer Bean来启用新特性： @Bean public Jackson2ObjectMapperBuilderCustomizer customizer()&#123; return builder -&gt; builder.featuresToEnable(SerializationFeature.WRITE_ENUMS_USING_INDEX); &#125; 这个案例告诉我们两点： Jackson针对序列化和反序列化有大量的细节功能特性，我们可以参考Jackson官方文档来了解这些特性，详见SerializationFeature、DeserializationFeature和MapperFeature。 忽略多余字段，是我们写业务代码时最容易遇到的一个配置项。Spring Boot在自动配置时贴心地做了全局设置。如果需要设置更多的特性，可以直接修改配置文件spring.jackson.**或设置Jackson2ObjectMapperBuilderCustomizer回调接口，来启用更多设置，无需重新定义ObjectMapper Bean。 反序列化时要小心类的构造方法使用Jackson反序列化时，除了要注意忽略额外字段的问题外，还要小心类的构造方法。我们看一个实际的踩坑案例吧。 有一个APIResult类包装了REST接口的返回体（作为Web控制器的出参），其中boolean类型的success字段代表是否处理成功、int类型的code字段代表处理状态码。 开始时，在返回APIResult的时候每次都根据code来设置success。如果code是2000，那么success是true，否则是false。后来为了减少重复代码，把这个逻辑放到了APIResult类的构造方法中处理： @Data public class APIResultWrong &#123; private boolean success; private int code; public APIResultWrong() &#123; &#125; public APIResultWrong(int code) &#123; this.code &#x3D; code; if (code &#x3D;&#x3D; 2000) success &#x3D; true; else success &#x3D; false; &#125; &#125; 经过改动后发现，即使code为2000，返回APIResult的success也是false。比如，我们反序列化两次APIResult，一次使用code&#x3D;&#x3D;1234，一次使用code&#x3D;&#x3D;2000： @Autowired ObjectMapper objectMapper; @GetMapping(&quot;wrong&quot;) public void wrong() throws JsonProcessingException &#123; log.info(&quot;result :&#123;&#125;&quot;, objectMapper.readValue(&quot;&#123;\\&quot;code\\&quot;:1234&#125;&quot;, APIResultWrong.class)); log.info(&quot;result :&#123;&#125;&quot;, objectMapper.readValue(&quot;&#123;\\&quot;code\\&quot;:2000&#125;&quot;, APIResultWrong.class)); &#125; 日志输出如下： [17:36:14.591] [http-nio-45678-exec-1] [INFO ] [DeserializationConstructorController:20 ] - result :APIResultWrong(success&#x3D;false, code&#x3D;1234) [17:36:14.591] [http-nio-45678-exec-1] [INFO ] [DeserializationConstructorController:21 ] - result :APIResultWrong(success&#x3D;false, code&#x3D;2000) 可以看到，两次的APIResult的success字段都是false。 出现这个问题的原因是，默认情况下，在反序列化的时候，Jackson框架只会调用无参构造方法创建对象。如果走自定义的构造方法创建对象，需要通过@JsonCreator来指定构造方法，并通过@JsonProperty设置构造方法中参数对应的JSON属性名： @Data public class APIResultRight &#123; ... @JsonCreator public APIResultRight(@JsonProperty(&quot;code&quot;) int code) &#123; this.code &#x3D; code; if (code &#x3D;&#x3D; 2000) success &#x3D; true; else success &#x3D; false; &#125; &#125; 重新运行程序，可以得到正确输出： [17:41:23.188] [http-nio-45678-exec-1] [INFO ] [DeserializationConstructorController:26 ] - result :APIResultRight(success&#x3D;false, code&#x3D;1234) [17:41:23.188] [http-nio-45678-exec-1] [INFO ] [DeserializationConstructorController:27 ] - result :APIResultRight(success&#x3D;true, code&#x3D;2000) 可以看到，这次传入code&#x3D;&#x3D;2000时，success可以设置为true。 枚举作为API接口参数或返回值的两个大坑在前面的例子中，我演示了如何把枚举序列化为索引值。但对于枚举，我建议尽量在程序内部使用，而不是作为API接口的参数或返回值，原因是枚举涉及序列化和反序列化时会有两个大坑。 第一个坑是，客户端和服务端的枚举定义不一致时，会出异常。比如，客户端版本的枚举定义了4个枚举值： @Getter enum StatusEnumClient &#123; CREATED(1, &quot;已创建&quot;), PAID(2, &quot;已支付&quot;), DELIVERED(3, &quot;已送到&quot;), FINISHED(4, &quot;已完成&quot;); private final int status; private final String desc; StatusEnumClient(Integer status, String desc) &#123; this.status &#x3D; status; this.desc &#x3D; desc; &#125; &#125; 服务端定义了5个枚举值： @Getter enum StatusEnumServer &#123; ... CANCELED(5, &quot;已取消&quot;); private final int status; private final String desc; StatusEnumServer(Integer status, String desc) &#123; this.status &#x3D; status; this.desc &#x3D; desc; &#125; &#125; 写代码测试一下，使用RestTemplate来发起请求，让服务端返回客户端不存在的枚举值： @GetMapping(&quot;getOrderStatusClient&quot;) public void getOrderStatusClient() &#123; StatusEnumClient result &#x3D; restTemplate.getForObject(&quot;http:&#x2F;&#x2F;localhost:45678&#x2F;enumusedinapi&#x2F;getOrderStatus&quot;, StatusEnumClient.class); log.info(&quot;result &#123;&#125;&quot;, result); &#125; @GetMapping(&quot;getOrderStatus&quot;) public StatusEnumServer getOrderStatus() &#123; return StatusEnumServer.CANCELED; &#125; 访问接口会出现如下异常信息，提示在枚举StatusEnumClient中找不到CANCELED： JSON parse error: Cannot deserialize value of type &#96;org.geekbang.time.commonmistakes.enums.enumusedinapi.StatusEnumClient&#96; from String &quot;CANCELED&quot;: not one of the values accepted for Enum class: [CREATED, FINISHED, DELIVERED, PAID]; 要解决这个问题，可以开启Jackson的read_unknown_enum_values_using_default_value反序列化特性，也就是在枚举值未知的时候使用默认值： spring.jackson.deserialization.read_unknown_enum_values_using_default_value&#x3D;true 并为枚举添加一个默认值，使用@JsonEnumDefaultValue注解注释： @JsonEnumDefaultValue UNKNOWN(-1, &quot;未知&quot;); 需要注意的是，这个枚举值一定是添加在客户端StatusEnumClient中的，因为反序列化使用的是客户端枚举。 这里还有一个小坑是，仅仅这样配置还不能让RestTemplate生效这个反序列化特性，还需要配置RestTemplate，来使用Spring Boot的MappingJackson2HttpMessageConverter才行： @Bean public RestTemplate restTemplate(MappingJackson2HttpMessageConverter mappingJackson2HttpMessageConverter) &#123; return new RestTemplateBuilder() .additionalMessageConverters(mappingJackson2HttpMessageConverter) .build(); &#125; 现在，请求接口可以返回默认值了： [21:49:03.887] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.e.e.EnumUsedInAPIController:25 ] - result UNKNOWN 第二个坑，也是更大的坑，枚举序列化反序列化实现自定义的字段非常麻烦，会涉及Jackson的Bug。比如，下面这个接口，传入枚举List，为List增加一个CENCELED枚举值然后返回： @PostMapping(&quot;queryOrdersByStatusList&quot;) public List&lt;StatusEnumServer&gt; queryOrdersByStatus(@RequestBody List&lt;StatusEnumServer&gt; enumServers) &#123; enumServers.add(StatusEnumServer.CANCELED); return enumServers; &#125; 如果我们希望根据枚举的Desc字段来序列化，传入“已送到”作为入参： 会得到异常，提示“已送到”不是正确的枚举值： JSON parse error: Cannot deserialize value of type &#96;org.geekbang.time.commonmistakes.enums.enumusedinapi.StatusEnumServer&#96; from String &quot;已送到&quot;: not one of the values accepted for Enum class: [CREATED, CANCELED, FINISHED, DELIVERED, PAID] 显然，这里反序列化使用的是枚举的name，序列化也是一样： 你可能也知道，要让枚举的序列化和反序列化走desc字段，可以在字段上加@JsonValue注解，修改StatusEnumServer和StatusEnumClient： @JsonValue private final String desc; 然后再尝试下，果然可以用desc作为入参了，而且出参也使用了枚举的desc： 但是，如果你认为这样就完美解决问题了，那就大错特错了。你可以再尝试把@JsonValue注解加在int类型的status字段上，也就是希望序列化反序列化走status字段： @JsonValue private final int status; 写一个客户端测试一下，传入CREATED和PAID两个枚举值： @GetMapping(&quot;queryOrdersByStatusListClient&quot;) public void queryOrdersByStatusListClient() &#123; List&lt;StatusEnumClient&gt; request &#x3D; Arrays.asList(StatusEnumClient.CREATED, StatusEnumClient.PAID); HttpEntity&lt;List&lt;StatusEnumClient&gt;&gt; entity &#x3D; new HttpEntity&lt;&gt;(request, new HttpHeaders()); List&lt;StatusEnumClient&gt; response &#x3D; restTemplate.exchange(&quot;http:&#x2F;&#x2F;localhost:45678&#x2F;enumusedinapi&#x2F;queryOrdersByStatusList&quot;, HttpMethod.POST, entity, new ParameterizedTypeReference&lt;List&lt;StatusEnumClient&gt;&gt;() &#123;&#125;).getBody(); log.info(&quot;result &#123;&#125;&quot;, response); &#125; 请求接口可以看到，传入的是CREATED和PAID，返回的居然是DELIVERED和FINISHED。果然如标题所说，一来一回你已不是原来的你： [22:03:03.579] [http-nio-45678-exec-4] [INFO ] [o.g.t.c.e.e.EnumUsedInAPIController:34 ] - result [DELIVERED, FINISHED, UNKNOWN] 出现这个问题的原因是，序列化走了status的值，而反序列化并没有根据status来，还是使用了枚举的ordinal()索引值。这是Jackson至今（2.10）没有解决的Bug，应该会在2.11解决。 如下图所示，我们调用服务端接口，传入一个不存在的status值0，也能反序列化成功，最后服务端的返回是1： 有一个解决办法是，设置@JsonCreator来强制反序列化时使用自定义的工厂方法，可以实现使用枚举的status字段来取值。我们把这段代码加在StatusEnumServer枚举类中： @JsonCreator public static StatusEnumServer parse(Object o) &#123; return Arrays.stream(StatusEnumServer.values()).filter(value-&gt;o.equals(value.status)).findFirst().orElse(null); &#125; 要特别注意的是，我们同样要为StatusEnumClient也添加相应的方法。因为除了服务端接口接收StatusEnumServer参数涉及一次反序列化外，从服务端返回值转换为List还会有一次反序列化： @JsonCreator public static StatusEnumClient parse(Object o) &#123; return Arrays.stream(StatusEnumClient.values()).filter(value-&gt;o.equals(value.status)).findFirst().orElse(null); &#125; 重新调用接口发现，虽然结果正确了，但是服务端不存在的枚举值CANCELED被设置为了null，而不是@JsonEnumDefaultValue设置的UNKNOWN。 这个问题，我们之前已经通过设置@JsonEnumDefaultValue注解解决了，但现在又出现了： [22:20:13.727] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.e.e.EnumUsedInAPIController:34 ] - result [CREATED, PAID, null] 原因也很简单，我们自定义的parse方法实现的是找不到枚举值时返回null。 为彻底解决这个问题，并避免通过@JsonCreator在枚举中自定义一个非常复杂的工厂方法，我们可以实现一个自定义的反序列化器。这段代码比较复杂，我特意加了详细的注释： class EnumDeserializer extends JsonDeserializer&lt;Enum&gt; implements ContextualDeserializer &#123; private Class&lt;Enum&gt; targetClass; public EnumDeserializer() &#123; &#125; public EnumDeserializer(Class&lt;Enum&gt; targetClass) &#123; this.targetClass &#x3D; targetClass; &#125; @Override public Enum deserialize(JsonParser p, DeserializationContext ctxt) &#123; &#x2F;&#x2F;找枚举中带有@JsonValue注解的字段，这是我们反序列化的基准字段 Optional&lt;Field&gt; valueFieldOpt &#x3D; Arrays.asList(targetClass.getDeclaredFields()).stream() .filter(m -&gt; m.isAnnotationPresent(JsonValue.class)) .findFirst(); if (valueFieldOpt.isPresent()) &#123; Field valueField &#x3D; valueFieldOpt.get(); if (!valueField.isAccessible()) &#123; valueField.setAccessible(true); &#125; &#x2F;&#x2F;遍历枚举项，查找字段的值等于反序列化的字符串的那个枚举项 return Arrays.stream(targetClass.getEnumConstants()).filter(e -&gt; &#123; try &#123; return valueField.get(e).toString().equals(p.getValueAsString()); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; return false; &#125;).findFirst().orElseGet(() -&gt; Arrays.stream(targetClass.getEnumConstants()).filter(e -&gt; &#123; &#x2F;&#x2F;如果找不到，就需要寻找默认枚举值来替代，同样遍历所有枚举项，查找@JsonEnumDefaultValue注解标识的枚举项 try &#123; return targetClass.getField(e.name()).isAnnotationPresent(JsonEnumDefaultValue.class); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; return false; &#125;).findFirst().orElse(null)); &#125; return null; &#125; @Override public JsonDeserializer&lt;?&gt; createContextual(DeserializationContext ctxt, BeanProperty property) throws JsonMappingException &#123; targetClass &#x3D; (Class&lt;Enum&gt;) ctxt.getContextualType().getRawClass(); return new EnumDeserializer(targetClass); &#125; &#125; 然后，把这个自定义反序列化器注册到Jackson中： @Bean public Module enumModule() &#123; SimpleModule module &#x3D; new SimpleModule(); module.addDeserializer(Enum.class, new EnumDeserializer()); return module; &#125; 第二个大坑终于被完美地解决了： [22:32:28.327] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.e.e.EnumUsedInAPIController:34 ] - result [CREATED, PAID, UNKNOWN] 这样做，虽然解决了序列化反序列化使用枚举中自定义字段的问题，也解决了找不到枚举值时使用默认值的问题，但解决方案很复杂。因此，我还是建议在DTO中直接使用int或String等简单的数据类型，而不是使用枚举再配合各种复杂的序列化配置，来实现枚举到枚举中字段的映射，会更加清晰明了。 重点回顾今天，我基于Redis和Web API的入参和出参两个场景，和你介绍了序列化和反序列化时需要避开的几个坑。 第一，要确保序列化和反序列化算法的一致性。因为，不同序列化算法输出必定不同，要正确处理序列化后的数据就要使用相同的反序列化算法。 第二，Jackson有大量的序列化和反序列化特性，可以用来微调序列化和反序列化的细节。需要注意的是，如果自定义ObjectMapper的Bean，小心不要和Spring Boot自动配置的Bean冲突。 第三，在调试序列化反序列化问题时，我们一定要捋清楚三点：是哪个组件在做序列化反序列化、整个过程有几次序列化反序列化，以及目前到底是序列化还是反序列化。 第四，对于反序列化默认情况下，框架调用的是无参构造方法，如果要调用自定义的有参构造方法，那么需要告知框架如何调用。更合理的方式是，对于需要序列化的POJO考虑尽量不要自定义构造方法。 第五，枚举不建议定义在DTO中跨服务传输，因为会有版本问题，并且涉及序列化反序列化时会很复杂，容易出错。因此，我只建议在程序内部使用枚举。 最后还有一点需要注意，如果需要跨平台使用序列化的数据，那么除了两端使用的算法要一致外，还可能会遇到不同语言对数据类型的兼容问题。这，也是经常踩坑的一个地方。如果你有相关需求，可以多做实验、多测试。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 在讨论Redis序列化方式的时候，我们自定义了RedisTemplate，让Key使用String序列化、让Value使用JSON序列化，从而使Redis获得的Value可以直接转换为需要的对象类型。那么，使用RedisTemplate&lt;String, Long&gt;能否存取Value是Long的数据呢？这其中有什么坑吗？ 你可以看一下Jackson2ObjectMapperBuilder类源码的实现（注意configure方法），分析一下其除了关闭FAIL_ON_UNKNOWN_PROPERTIES外，还做了什么吗？ 关于序列化和反序列化，你还遇到过什么坑吗？我是朱晔，欢迎在评论区与我留言分享，也欢迎你把这篇文章分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/16、用好Java 8的日期时间类，少踩一些“老三样”的坑","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/16、用好Java 8的日期时间类，少踩一些“老三样”的坑/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/16%E3%80%81%E7%94%A8%E5%A5%BDJava%208%E7%9A%84%E6%97%A5%E6%9C%9F%E6%97%B6%E9%97%B4%E7%B1%BB%EF%BC%8C%E5%B0%91%E8%B8%A9%E4%B8%80%E4%BA%9B%E2%80%9C%E8%80%81%E4%B8%89%E6%A0%B7%E2%80%9D%E7%9A%84%E5%9D%91/","excerpt":"","text":"16 | 用好Java 8的日期时间类，少踩一些“老三样”的坑作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我来和你说说恼人的时间错乱问题。 在Java 8之前，我们处理日期时间需求时，使用Date、Calender和SimpleDateFormat，来声明时间戳、使用日历处理日期和格式化解析日期时间。但是，这些类的API的缺点比较明显，比如可读性差、易用性差、使用起来冗余繁琐，还有线程安全问题。 因此，Java 8推出了新的日期时间类。每一个类功能明确清晰、类之间协作简单、API定义清晰不踩坑，API功能强大无需借助外部工具类即可完成操作，并且线程安全。 但是，Java 8刚推出的时候，诸如序列化、数据访问等类库都还不支持Java 8的日期时间类型，需要在新老类中来回转换。比如，在业务逻辑层使用LocalDateTime，存入数据库或者返回前端的时候还要切换回Date。因此，很多同学还是选择使用老的日期时间类。 现在几年时间过去了，几乎所有的类库都支持了新日期时间类型，使用起来也不会有来回切换等问题了。但，很多代码中因为还是用的遗留的日期时间类，因此出现了很多时间错乱的错误实践。比如，试图通过随意修改时区，使读取到的数据匹配当前时钟；再比如，试图直接对读取到的数据做加、减几个小时的操作，来“修正数据”。 今天，我就重点与你分析下时间错乱问题背后的原因，看看使用遗留的日期时间类，来处理日期时间初始化、格式化、解析、计算等可能会遇到的问题，以及如何使用新日期时间类来解决。 初始化日期时间我们先从日期时间的初始化看起。如果要初始化一个2019年12月31日11点12分13秒这样的时间，可以使用下面的两行代码吗？ Date date &#x3D; new Date(2019, 12, 31, 11, 12, 13); System.out.println(date); 可以看到，输出的时间是3029年1月31日11点12分13秒： Sat Jan 31 11:12:13 CST 3920 相信看到这里，你会说这是新手才会犯的低级错误：年应该是和1900的差值，月应该是从0到11而不是从1到12。 Date date &#x3D; new Date(2019 - 1900, 11, 31, 11, 12, 13); 你说的没错，但更重要的问题是，当有国际化需求时，需要使用Calendar类来初始化时间。 使用Calendar改造之后，初始化时年参数直接使用当前年即可，不过月需要注意是从0到11。当然，你也可以直接使用Calendar.DECEMBER来初始化月份，更不容易犯错。为了说明时区的问题，我分别使用当前时区和纽约时区初始化了两次相同的日期： Calendar calendar &#x3D; Calendar.getInstance(); calendar.set(2019, 11, 31, 11, 12, 13); System.out.println(calendar.getTime()); Calendar calendar2 &#x3D; Calendar.getInstance(TimeZone.getTimeZone(&quot;America&#x2F;New_York&quot;)); calendar2.set(2019, Calendar.DECEMBER, 31, 11, 12, 13); System.out.println(calendar2.getTime()); 输出显示了两个时间，说明时区产生了作用。但，我们更习惯年&#x2F;月&#x2F;日 时:分:秒这样的日期时间格式，对现在输出的日期格式还不满意： Tue Dec 31 11:12:13 CST 2019 Wed Jan 01 00:12:13 CST 2020 那，时区的问题是怎么回事，又怎么格式化需要输出的日期时间呢？接下来，我就与你逐一分析下这两个问题。 “恼人”的时区问题我们知道，全球有24个时区，同一个时刻不同时区（比如中国上海和美国纽约）的时间是不一样的。对于需要全球化的项目，如果初始化时间时没有提供时区，那就不是一个真正意义上的时间，只能认为是我看到的当前时间的一个表示。 关于Date类，我们要有两点认识： 一是，Date并无时区问题，世界上任何一台计算机使用new Date()初始化得到的时间都一样。因为，Date中保存的是UTC时间，UTC是以原子钟为基础的统一时间，不以太阳参照计时，并无时区划分。 二是，Date中保存的是一个时间戳，代表的是从1970年1月1日0点（Epoch时间）到现在的毫秒数。尝试输出Date(0)： System.out.println(new Date(0)); System.out.println(TimeZone.getDefault().getID() + &quot;:&quot; + TimeZone.getDefault().getRawOffset()&#x2F;3600000); 我得到的是1970年1月1日8点。因为我机器当前的时区是中国上海，相比UTC时差+8小时： Thu Jan 01 08:00:00 CST 1970 Asia&#x2F;Shanghai:8 对于国际化（世界各国的人都在使用）的项目，处理好时间和时区问题首先就是要正确保存日期时间。这里有两种保存方式： 方式一，以UTC保存，保存的时间没有时区属性，是不涉及时区时间差问题的世界统一时间。我们通常说的时间戳，或Java中的Date类就是用的这种方式，这也是推荐的方式。 方式二，以字面量保存，比如年&#x2F;月&#x2F;日 时:分:秒，一定要同时保存时区信息。只有有了时区信息，我们才能知道这个字面量时间真正的时间点，否则它只是一个给人看的时间表示，只在当前时区有意义。Calendar是有时区概念的，所以我们通过不同的时区初始化Calendar，得到了不同的时间。 正确保存日期时间之后，就是正确展示，即我们要使用正确的时区，把时间点展示为符合当前时区的时间表示。到这里，我们就能理解为什么会有所谓的“时间错乱”问题了。接下来，我再通过实际案例分析一下，从字面量解析成时间和从时间格式化为字面量这两类问题。 第一类是，对于同一个时间表示，比如2020-01-02 22:00:00，不同时区的人转换成Date会得到不同的时间（时间戳）： String stringDate &#x3D; &quot;2020-01-02 22:00:00&quot;; SimpleDateFormat inputFormat &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#x2F;&#x2F;默认时区解析时间表示 Date date1 &#x3D; inputFormat.parse(stringDate); System.out.println(date1 + &quot;:&quot; + date1.getTime()); &#x2F;&#x2F;纽约时区解析时间表示 inputFormat.setTimeZone(TimeZone.getTimeZone(&quot;America&#x2F;New_York&quot;)); Date date2 &#x3D; inputFormat.parse(stringDate); System.out.println(date2 + &quot;:&quot; + date2.getTime()); 可以看到，把2020-01-02 22:00:00这样的时间表示，对于当前的上海时区和纽约时区，转化为UTC时间戳是不同的时间： Thu Jan 02 22:00:00 CST 2020:1577973600000 Fri Jan 03 11:00:00 CST 2020:1578020400000 这正是UTC的意义，并不是时间错乱。对于同一个本地时间的表示，不同时区的人解析得到的UTC时间一定是不同的，反过来不同的本地时间可能对应同一个UTC。 第二类问题是，格式化后出现的错乱，即同一个Date，在不同的时区下格式化得到不同的时间表示。比如，在我的当前时区和纽约时区格式化2020-01-02 22:00:00： String stringDate &#x3D; &quot;2020-01-02 22:00:00&quot;; SimpleDateFormat inputFormat &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#x2F;&#x2F;同一Date Date date &#x3D; inputFormat.parse(stringDate); &#x2F;&#x2F;默认时区格式化输出： System.out.println(new SimpleDateFormat(&quot;[yyyy-MM-dd HH:mm:ss Z]&quot;).format(date)); &#x2F;&#x2F;纽约时区格式化输出 TimeZone.setDefault(TimeZone.getTimeZone(&quot;America&#x2F;New_York&quot;)); System.out.println(new SimpleDateFormat(&quot;[yyyy-MM-dd HH:mm:ss Z]&quot;).format(date)); 输出如下，我当前时区的Offset（时差）是+8小时，对于-5小时的纽约，晚上10点对应早上9点： [2020-01-02 22:00:00 +0800] [2020-01-02 09:00:00 -0500] 因此，有些时候数据库中相同的时间，由于服务器的时区设置不同，读取到的时间表示不同。这，不是时间错乱，正是时区发挥了作用，因为UTC时间需要根据当前时区解析为正确的本地时间。 所以，要正确处理时区，在于存进去和读出来两方面：存的时候，需要使用正确的当前时区来保存，这样UTC时间才会正确；读的时候，也只有正确设置本地时区，才能把UTC时间转换为正确的当地时间。 Java 8推出了新的时间日期类ZoneId、ZoneOffset、LocalDateTime、ZonedDateTime和DateTimeFormatter，处理时区问题更简单清晰。我们再用这些类配合一个完整的例子，来理解一下时间的解析和展示： 首先初始化上海、纽约和东京三个时区。我们可以使用ZoneId.of来初始化一个标准的时区，也可以使用ZoneOffset.ofHours通过一个offset，来初始化一个具有指定时间差的自定义时区。 对于日期时间表示，LocalDateTime不带有时区属性，所以命名为本地时区的日期时间；而ZonedDateTime&#x3D;LocalDateTime+ZoneId，具有时区属性。因此，LocalDateTime只能认为是一个时间表示，ZonedDateTime才是一个有效的时间。在这里我们把2020-01-02 22:00:00这个时间表示，使用东京时区来解析得到一个ZonedDateTime。 使用DateTimeFormatter格式化时间的时候，可以直接通过withZone方法直接设置格式化使用的时区。最后，分别以上海、纽约和东京三个时区来格式化这个时间输出： &#x2F;&#x2F;一个时间表示 String stringDate &#x3D; &quot;2020-01-02 22:00:00&quot;; &#x2F;&#x2F;初始化三个时区 ZoneId timeZoneSH &#x3D; ZoneId.of(&quot;Asia&#x2F;Shanghai&quot;); ZoneId timeZoneNY &#x3D; ZoneId.of(&quot;America&#x2F;New_York&quot;); ZoneId timeZoneJST &#x3D; ZoneOffset.ofHours(9); &#x2F;&#x2F;格式化器 DateTimeFormatter dateTimeFormatter &#x3D; DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;); ZonedDateTime date &#x3D; ZonedDateTime.of(LocalDateTime.parse(stringDate, dateTimeFormatter), timeZoneJST); &#x2F;&#x2F;使用DateTimeFormatter格式化时间，可以通过withZone方法直接设置格式化使用的时区 DateTimeFormatter outputFormat &#x3D; DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss Z&quot;); System.out.println(timeZoneSH.getId() + outputFormat.withZone(timeZoneSH).format(date)); System.out.println(timeZoneNY.getId() + outputFormat.withZone(timeZoneNY).format(date)); System.out.println(timeZoneJST.getId() + outputFormat.withZone(timeZoneJST).format(date)); 可以看到，相同的时区，经过解析存进去和读出来的时间表示是一样的（比如最后一行）；而对于不同的时区，比如上海和纽约，最后输出的本地时间不同。+9小时时区的晚上10点，对于上海是+8小时，所以上海本地时间是晚上9点；而对于纽约是-5小时，差14小时，所以是早上8点： Asia&#x2F;Shanghai2020-01-02 21:00:00 +0800 America&#x2F;New_York2020-01-02 08:00:00 -0500 +09:002020-01-02 22:00:00 +0900 到这里，我来小结下。要正确处理国际化时间问题，我推荐使用Java 8的日期时间类，即使用ZonedDateTime保存时间，然后使用设置了ZoneId的DateTimeFormatter配合ZonedDateTime进行时间格式化得到本地时间表示。这样的划分十分清晰、细化，也不容易出错。 接下来，我们继续看看对于日期时间的格式化和解析，使用遗留的SimpleDateFormat，会遇到哪些问题。 日期时间格式化和解析每到年底，就有很多开发同学踩时间格式化的坑，比如“这明明是一个2019年的日期，怎么使用SimpleDateFormat格式化后就提前跨年了”。我们来重现一下这个问题。 初始化一个Calendar，设置日期时间为2019年12月29日，使用大写的YYYY来初始化SimpleDateFormat： Locale.setDefault(Locale.SIMPLIFIED_CHINESE); System.out.println(&quot;defaultLocale:&quot; + Locale.getDefault()); Calendar calendar &#x3D; Calendar.getInstance(); calendar.set(2019, Calendar.DECEMBER, 29,0,0,0); SimpleDateFormat YYYY &#x3D; new SimpleDateFormat(&quot;YYYY-MM-dd&quot;); System.out.println(&quot;格式化: &quot; + YYYY.format(calendar.getTime())); System.out.println(&quot;weekYear:&quot; + calendar.getWeekYear()); System.out.println(&quot;firstDayOfWeek:&quot; + calendar.getFirstDayOfWeek()); System.out.println(&quot;minimalDaysInFirstWeek:&quot; + calendar.getMinimalDaysInFirstWeek()); 得到的输出却是2020年12月29日： defaultLocale:zh_CN 格式化: 2020-12-29 weekYear:2020 firstDayOfWeek:1 minimalDaysInFirstWeek:1 出现这个问题的原因在于，这位同学混淆了SimpleDateFormat的各种格式化模式。JDK的文档中有说明：小写y是年，而大写Y是week year，也就是所在的周属于哪一年。 一年第一周的判断方式是，从getFirstDayOfWeek()开始，完整的7天，并且包含那一年至少getMinimalDaysInFirstWeek()天。这个计算方式和区域相关，对于当前zh_CN区域来说，2020年第一周的条件是，从周日开始的完整7天，2020年包含1天即可。显然，2019年12月29日周日到2020年1月4日周六是2020年第一周，得出的week year就是2020年。 如果把区域改为法国： Locale.setDefault(Locale.FRANCE); 那么week yeay就还是2019年，因为一周的第一天从周一开始算，2020年的第一周是2019年12月30日周一开始，29日还是属于去年： defaultLocale:fr_FR 格式化: 2019-12-29 weekYear:2019 firstDayOfWeek:2 minimalDaysInFirstWeek:4 这个案例告诉我们，没有特殊需求，针对年份的日期格式化，应该一律使用 “y” 而非 “Y”。 除了格式化表达式容易踩坑外，SimpleDateFormat还有两个著名的坑。 第一个坑是，定义的static的SimpleDateFormat可能会出现线程安全问题。比如像这样，使用一个100线程的线程池，循环20次把时间格式化任务提交到线程池处理，每个任务中又循环10次解析2020-01-01 11:12:13这样一个时间表示： ExecutorService threadPool &#x3D; Executors.newFixedThreadPool(100); for (int i &#x3D; 0; i &lt; 20; i++) &#123; &#x2F;&#x2F;提交20个并发解析时间的任务到线程池，模拟并发环境 threadPool.execute(() -&gt; &#123; for (int j &#x3D; 0; j &lt; 10; j++) &#123; try &#123; System.out.println(simpleDateFormat.parse(&quot;2020-01-01 11:12:13&quot;)); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; threadPool.shutdown(); threadPool.awaitTermination(1, TimeUnit.HOURS); 运行程序后大量报错，且没有报错的输出结果也不正常，比如2020年解析成了1212年： SimpleDateFormat的作用是定义解析和格式化日期时间的模式。这，看起来这是一次性的工作，应该复用，但它的解析和格式化操作是非线程安全的。我们来分析一下相关源码： SimpleDateFormat继承了DateFormat，DateFormat有一个字段Calendar； SimpleDateFormat的parse方法调用CalendarBuilder的establish方法，来构建Calendar； establish方法内部先清空Calendar再构建Calendar，整个操作没有加锁。 显然，如果多线程池调用parse方法，也就意味着多线程在并发操作一个Calendar，可能会产生一个线程还没来得及处理Calendar就被另一个线程清空了的情况： public abstract class DateFormat extends Format &#123; protected Calendar calendar; &#125; public class SimpleDateFormat extends DateFormat &#123; @Override public Date parse(String text, ParsePosition pos) &#123; CalendarBuilder calb &#x3D; new CalendarBuilder(); parsedDate &#x3D; calb.establish(calendar).getTime(); return parsedDate; &#125; &#125; class CalendarBuilder &#123; Calendar establish(Calendar cal) &#123; ... cal.clear();&#x2F;&#x2F;清空 for (int stamp &#x3D; MINIMUM_USER_STAMP; stamp &lt; nextStamp; stamp++) &#123; for (int index &#x3D; 0; index &lt;&#x3D; maxFieldIndex; index++) &#123; if (field[index] &#x3D;&#x3D; stamp) &#123; cal.set(index, field[MAX_FIELD + index]);&#x2F;&#x2F;构建 break; &#125; &#125; &#125; return cal; &#125; &#125; format方法也类似，你可以自己分析。因此只能在同一个线程复用SimpleDateFormat，比较好的解决方式是，通过ThreadLocal来存放SimpleDateFormat： private static ThreadLocal&lt;SimpleDateFormat&gt; threadSafeSimpleDateFormat &#x3D; ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)); 第二个坑是，当需要解析的字符串和格式不匹配的时候，SimpleDateFormat表现得很宽容，还是能得到结果。比如，我们期望使用yyyyMM来解析20160901字符串： String dateString &#x3D; &quot;20160901&quot;; SimpleDateFormat dateFormat &#x3D; new SimpleDateFormat(&quot;yyyyMM&quot;); System.out.println(&quot;result:&quot; + dateFormat.parse(dateString)); 居然输出了2091年1月1日，原因是把0901当成了月份，相当于75年： result:Mon Jan 01 00:00:00 CST 2091 对于SimpleDateFormat的这三个坑，我们使用Java 8中的DateTimeFormatter就可以避过去。首先，使用DateTimeFormatterBuilder来定义格式化字符串，不用去记忆使用大写的Y还是小写的Y，大写的M还是小写的m： private static DateTimeFormatter dateTimeFormatter &#x3D; new DateTimeFormatterBuilder() .appendValue(ChronoField.YEAR) &#x2F;&#x2F;年 .appendLiteral(&quot;&#x2F;&quot;) .appendValue(ChronoField.MONTH_OF_YEAR) &#x2F;&#x2F;月 .appendLiteral(&quot;&#x2F;&quot;) .appendValue(ChronoField.DAY_OF_MONTH) &#x2F;&#x2F;日 .appendLiteral(&quot; &quot;) .appendValue(ChronoField.HOUR_OF_DAY) &#x2F;&#x2F;时 .appendLiteral(&quot;:&quot;) .appendValue(ChronoField.MINUTE_OF_HOUR) &#x2F;&#x2F;分 .appendLiteral(&quot;:&quot;) .appendValue(ChronoField.SECOND_OF_MINUTE) &#x2F;&#x2F;秒 .appendLiteral(&quot;.&quot;) .appendValue(ChronoField.MILLI_OF_SECOND) &#x2F;&#x2F;毫秒 .toFormatter(); 其次，DateTimeFormatter是线程安全的，可以定义为static使用；最后，DateTimeFormatter的解析比较严格，需要解析的字符串和格式不匹配时，会直接报错，而不会把0901解析为月份。我们测试一下： &#x2F;&#x2F;使用刚才定义的DateTimeFormatterBuilder构建的DateTimeFormatter来解析这个时间 LocalDateTime localDateTime &#x3D; LocalDateTime.parse(&quot;2020&#x2F;1&#x2F;2 12:34:56.789&quot;, dateTimeFormatter); &#x2F;&#x2F;解析成功 System.out.println(localDateTime.format(dateTimeFormatter)); &#x2F;&#x2F;使用yyyyMM格式解析20160901是否可以成功呢？ String dt &#x3D; &quot;20160901&quot;; DateTimeFormatter dateTimeFormatter &#x3D; DateTimeFormatter.ofPattern(&quot;yyyyMM&quot;); System.out.println(&quot;result:&quot; + dateTimeFormatter.parse(dt)); 输出日志如下： 2020&#x2F;1&#x2F;2 12:34:56.789 Exception in thread &quot;main&quot; java.time.format.DateTimeParseException: Text &#39;20160901&#39; could not be parsed at index 0 at java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949) at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777) at org.geekbang.time.commonmistakes.datetime.dateformat.CommonMistakesApplication.better(CommonMistakesApplication.java:80) at org.geekbang.time.commonmistakes.datetime.dateformat.CommonMistakesApplication.main(CommonMistakesApplication.java:41) 到这里我们可以发现，使用Java 8中的DateTimeFormatter进行日期时间的格式化和解析，显然更让人放心。那么，对于日期时间的运算，使用Java 8中的日期时间类会不会更简单呢？ 日期时间的计算关于日期时间的计算，我先和你说一个常踩的坑。有些同学喜欢直接使用时间戳进行时间计算，比如希望得到当前时间之后30天的时间，会这么写代码：直接把new Date().getTime方法得到的时间戳加30天对应的毫秒数，也就是30天*1000毫秒*3600秒*24小时： Date today &#x3D; new Date(); Date nextMonth &#x3D; new Date(today.getTime() + 30 * 1000 * 60 * 60 * 24); System.out.println(today); System.out.println(nextMonth); 得到的日期居然比当前日期还要早，根本不是晚30天的时间： Sat Feb 01 14:17:41 CST 2020 Sun Jan 12 21:14:54 CST 2020 出现这个问题，其实是因为int发生了溢出。修复方式就是把30改为30L，让其成为一个long： Date today &#x3D; new Date(); Date nextMonth &#x3D; new Date(today.getTime() + 30L * 1000 * 60 * 60 * 24); System.out.println(today); System.out.println(nextMonth); 这样就可以得到正确结果了： Sat Feb 01 14:17:41 CST 2020 Mon Mar 02 14:17:41 CST 2020 不难发现，手动在时间戳上进行计算操作的方式非常容易出错。对于Java 8之前的代码，我更建议使用Calendar： Calendar c &#x3D; Calendar.getInstance(); c.setTime(new Date()); c.add(Calendar.DAY_OF_MONTH, 30); System.out.println(c.getTime()); 使用Java 8的日期时间类型，可以直接进行各种计算，更加简洁和方便： LocalDateTime localDateTime &#x3D; LocalDateTime.now(); System.out.println(localDateTime.plusDays(30)); 并且，对日期时间做计算操作，Java 8日期时间API会比Calendar功能强大很多。 第一，可以使用各种minus和plus方法直接对日期进行加减操作，比如如下代码实现了减一天和加一天，以及减一个月和加一个月： System.out.println(&quot;&#x2F;&#x2F;测试操作日期&quot;); System.out.println(LocalDate.now() .minus(Period.ofDays(1)) .plus(1, ChronoUnit.DAYS) .minusMonths(1) .plus(Period.ofMonths(1))); 可以得到： &#x2F;&#x2F;测试操作日期 2020-02-01 第二，还可以通过with方法进行快捷时间调节，比如： 使用TemporalAdjusters.firstDayOfMonth得到当前月的第一天； 使用TemporalAdjusters.firstDayOfYear()得到当前年的第一天； 使用TemporalAdjusters.previous(DayOfWeek.SATURDAY)得到上一个周六； 使用TemporalAdjusters.lastInMonth(DayOfWeek.FRIDAY)得到本月最后一个周五。 System.out.println(&quot;&#x2F;&#x2F;本月的第一天&quot;); System.out.println(LocalDate.now().with(TemporalAdjusters.firstDayOfMonth())); System.out.println(&quot;&#x2F;&#x2F;今年的程序员日&quot;); System.out.println(LocalDate.now().with(TemporalAdjusters.firstDayOfYear()).plusDays(255)); System.out.println(&quot;&#x2F;&#x2F;今天之前的一个周六&quot;); System.out.println(LocalDate.now().with(TemporalAdjusters.previous(DayOfWeek.SATURDAY))); System.out.println(&quot;&#x2F;&#x2F;本月最后一个工作日&quot;); System.out.println(LocalDate.now().with(TemporalAdjusters.lastInMonth(DayOfWeek.FRIDAY))); 输出如下： &#x2F;&#x2F;本月的第一天 2020-02-01 &#x2F;&#x2F;今年的程序员日 2020-09-12 &#x2F;&#x2F;今天之前的一个周六 2020-01-25 &#x2F;&#x2F;本月最后一个工作日 2020-02-28 第三，可以直接使用lambda表达式进行自定义的时间调整。比如，为当前时间增加100天以内的随机天数： System.out.println(LocalDate.now().with(temporal -&gt; temporal.plus(ThreadLocalRandom.current().nextInt(100), ChronoUnit.DAYS))); 得到： 2020-03-15 除了计算外，还可以判断日期是否符合某个条件。比如，自定义函数，判断指定日期是否是家庭成员的生日： public static Boolean isFamilyBirthday(TemporalAccessor date) &#123; int month &#x3D; date.get(MONTH_OF_YEAR); int day &#x3D; date.get(DAY_OF_MONTH); if (month &#x3D;&#x3D; Month.FEBRUARY.getValue() &amp;&amp; day &#x3D;&#x3D; 17) return Boolean.TRUE; if (month &#x3D;&#x3D; Month.SEPTEMBER.getValue() &amp;&amp; day &#x3D;&#x3D; 21) return Boolean.TRUE; if (month &#x3D;&#x3D; Month.MAY.getValue() &amp;&amp; day &#x3D;&#x3D; 22) return Boolean.TRUE; return Boolean.FALSE; &#125; 然后，使用query方法查询是否匹配条件： System.out.println(&quot;&#x2F;&#x2F;查询是否是今天要举办生日&quot;); System.out.println(LocalDate.now().query(CommonMistakesApplication::isFamilyBirthday)); 使用Java 8操作和计算日期时间虽然方便，但计算两个日期差时可能会踩坑：Java 8中有一个专门的类Period定义了日期间隔，通过Period.between得到了两个LocalDate的差，返回的是两个日期差几年零几月零几天。如果希望得知两个日期之间差几天，直接调用Period的getDays()方法得到的只是最后的“零几天”，而不是算总的间隔天数。 比如，计算2019年12月12日和2019年10月1日的日期间隔，很明显日期差是2个月零11天，但获取getDays方法得到的结果只是11天，而不是72天： System.out.println(&quot;&#x2F;&#x2F;计算日期差&quot;); LocalDate today &#x3D; LocalDate.of(2019, 12, 12); LocalDate specifyDate &#x3D; LocalDate.of(2019, 10, 1); System.out.println(Period.between(specifyDate, today).getDays()); System.out.println(Period.between(specifyDate, today)); System.out.println(ChronoUnit.DAYS.between(specifyDate, today)); 可以使用ChronoUnit.DAYS.between解决这个问题： &#x2F;&#x2F;计算日期差 11 P2M11D 72 从日期时间的时区到格式化再到计算，你是不是体会到Java 8日期时间类的强大了呢？ 重点回顾今天，我和你一起看了日期时间的初始化、时区、格式化、解析和计算的问题。我们看到，使用Java 8中的日期时间包Java.time的类进行各种操作，会比使用遗留的Date、Calender和SimpleDateFormat更简单、清晰，功能也更丰富、坑也比较少。 如果有条件的话，我还是建议全面改为使用Java 8的日期时间类型。我把Java 8前后的日期时间类型，汇总到了一张思维导图上，图中箭头代表的是新老类型在概念上等价的类型： 这里有个误区是，认为java.util.Date类似于新API中的LocalDateTime。其实不是，虽然它们都没有时区概念，但java.util.Date类是因为使用UTC表示，所以没有时区概念，其本质是时间戳；而LocalDateTime，严格上可以认为是一个日期时间的表示，而不是一个时间点。 因此，在把Date转换为LocalDateTime的时候，需要通过Date的toInstant方法得到一个UTC时间戳进行转换，并需要提供当前的时区，这样才能把UTC时间转换为本地日期时间（的表示）。反过来，把LocalDateTime的时间表示转换为Date时，也需要提供时区，用于指定是哪个时区的时间表示，也就是先通过atZone方法把LocalDateTime转换为ZonedDateTime，然后才能获得UTC时间戳： Date in &#x3D; new Date(); LocalDateTime ldt &#x3D; LocalDateTime.ofInstant(in.toInstant(), ZoneId.systemDefault()); Date out &#x3D; Date.from(ldt.atZone(ZoneId.systemDefault()).toInstant()); 很多同学说使用新API很麻烦，还需要考虑时区的概念，一点都不简洁。但我通过这篇文章要和你说的是，并不是因为API需要设计得这么繁琐，而是UTC时间要变为当地时间，必须考虑时区。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 我今天多次强调Date是一个时间戳，是UTC时间、没有时区概念，为什么调用其toString方法会输出类似CST之类的时区字样呢？ 日期时间数据始终要保存到数据库中，MySQL中有两种数据类型datetime和timestamp可以用来保存日期时间。你能说说它们的区别吗，它们是否包含时区信息呢？ 对于日期和时间，你还遇到过什么坑吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/17、别以为“自动挡”就不可能出现OOM","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/17、别以为“自动挡”就不可能出现OOM/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/17%E3%80%81%E5%88%AB%E4%BB%A5%E4%B8%BA%E2%80%9C%E8%87%AA%E5%8A%A8%E6%8C%A1%E2%80%9D%E5%B0%B1%E4%B8%8D%E5%8F%AF%E8%83%BD%E5%87%BA%E7%8E%B0OOM/","excerpt":"","text":"17 | 别以为“自动挡”就不可能出现OOM作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我要和你分享的主题是，别以为“自动挡”就不可能出现OOM。 这里的“自动挡”，是我对Java自动垃圾收集器的戏称。的确，经过这么多年的发展，Java的垃圾收集器已经非常成熟了。有了自动垃圾收集器，绝大多数情况下我们写程序时可以专注于业务逻辑，无需过多考虑对象的分配和释放，一般也不会出现OOM。 但，内存空间始终是有限的，Java的几大内存区域始终都有OOM的可能。相应地，Java程序的常见OOM类型，可以分为堆内存的OOM、栈OOM、元空间OOM、直接内存OOM等。几乎每一种OOM都可以使用几行代码模拟，市面上也有很多资料在堆、元空间、直接内存中分配超大对象或是无限分配对象，尝试创建无限个线程或是进行方法无限递归调用来模拟。 但值得注意的是，我们的业务代码并不会这么干。所以今天，我会从内存分配意识的角度通过一些案例，展示业务代码中可能导致OOM的一些坑。这些坑，或是因为我们意识不到对象的分配，或是因为不合理的资源使用，或是没有控制缓存的数据量等。 在第3讲介绍线程时，我们已经看到了两种OOM的情况，一是因为使用无界队列导致的堆OOM，二是因为使用没有最大线程数量限制的线程池导致无限创建线程的OOM。接下来，我们再一起看看，在写业务代码的过程中，还有哪些意识上的疏忽可能会导致OOM。 太多份相同的对象导致OOM我要分享的第一个案例是这样的。有一个项目在内存中缓存了全量用户数据，在搜索用户时可以直接从缓存中返回用户信息。现在为了改善用户体验，需要实现输入部分用户名自动在下拉框提示补全用户名的功能（也就是所谓的自动完成功能）。 在第10讲介绍集合时，我提到对于这种快速检索的需求，最好使用Map来实现，会比直接从List搜索快得多。 为实现这个功能，我们需要一个HashMap来存放这些用户数据，Key是用户姓名索引，Value是索引下对应的用户列表。举一个例子，如果有两个用户aa和ab，那么Key就有三个，分别是a、aa和ab。用户输入字母a时，就能从Value这个List中拿到所有字母a开头的用户，即aa和ab。 在代码中，在数据库中存入1万个测试用户，用户名由a~j这6个字母随机构成，然后把每一个用户名的前1个字母、前2个字母以此类推直到完整用户名作为Key存入缓存中，缓存的Value是一个UserDTO的List，存放的是所有相同的用户名索引，以及对应的用户信息： &#x2F;&#x2F;自动完成的索引，Key是用户输入的部分用户名，Value是对应的用户数据 private ConcurrentHashMap&lt;String, List&lt;UserDTO&gt;&gt; autoCompleteIndex &#x3D; new ConcurrentHashMap&lt;&gt;(); @Autowired private UserRepository userRepository; @PostConstruct public void wrong() &#123; &#x2F;&#x2F;先保存10000个用户名随机的用户到数据库中 userRepository.saveAll(LongStream.rangeClosed(1, 10000).mapToObj(i -&gt; new UserEntity(i, randomName())).collect(Collectors.toList())); &#x2F;&#x2F;从数据库加载所有用户 userRepository.findAll().forEach(userEntity -&gt; &#123; int len &#x3D; userEntity.getName().length(); &#x2F;&#x2F;对于每一个用户，对其用户名的前N位进行索引，N可能是1~6六种长度类型 for (int i &#x3D; 0; i &lt; len; i++) &#123; String key &#x3D; userEntity.getName().substring(0, i + 1); autoCompleteIndex.computeIfAbsent(key, s -&gt; new ArrayList&lt;&gt;()) .add(new UserDTO(userEntity.getName())); &#125; &#125;); log.info(&quot;autoCompleteIndex size:&#123;&#125; count:&#123;&#125;&quot;, autoCompleteIndex.size(), autoCompleteIndex.entrySet().stream().map(item -&gt; item.getValue().size()).reduce(0, Integer::sum)); &#125; 对于每一个用户对象UserDTO，除了有用户名，我们还加入了10K左右的数据模拟其用户信息： @Data public class UserDTO &#123; private String name; @EqualsAndHashCode.Exclude private String payload; public UserDTO(String name) &#123; this.name &#x3D; name; this.payload &#x3D; IntStream.rangeClosed(1, 10_000) .mapToObj(__ -&gt; &quot;a&quot;) .collect(Collectors.joining(&quot;&quot;)); &#125; &#125; 运行程序后，日志输出如下： [11:11:22.982] [main] [INFO ] [.t.c.o.d.UsernameAutoCompleteService:37 ] - autoCompleteIndex size:26838 count:60000 可以看到，一共有26838个索引（也就是所有用户名的1位、2位一直到6位有26838个组合），HashMap的Value，也就是List一共有1万个用户*6&#x3D;6万个UserDTO对象。 使用内存分析工具MAT打开堆dump发现，6万个UserDTO占用了约1.2GB的内存： 看到这里发现，虽然真正的用户只有1万个，但因为使用部分用户名作为索引的Key，导致缓存的Key有26838个，缓存的用户信息多达6万个。如果我们的用户名不是6位而是10位、20位，那么缓存的用户信息可能就是10万、20万个，必然会产生堆OOM。 尝试调大用户名的最大长度，重启程序可以看到类似如下的错误： [17:30:29.858] [main] [ERROR] [ringframework.boot.SpringApplication:826 ] - Application run failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#39;usernameAutoCompleteService&#39;: Invocation of init method failed; nested exception is java.lang.OutOfMemoryError: Java heap space 我们可能会想当然地认为，数据库中有1万个用户，内存中也应该只有1万个UserDTO对象，但实现的时候每次都会new出来UserDTO加入缓存，当然在内存中都是新对象。在实际的项目中，用户信息的缓存可能是随着用户输入增量缓存的，而不是像这个案例一样在程序初始化的时候全量缓存，所以问题暴露得不会这么早。 知道原因后，解决起来就比较简单了。把所有UserDTO先加入HashSet中，因为UserDTO以name来标识唯一性，所以重复用户名会被过滤掉，最终加入HashSet的UserDTO就不足1万个。 有了HashSet来缓存所有可能的UserDTO信息，我们再构建自动完成索引autoCompleteIndex这个HashMap时，就可以直接从HashSet获取所有用户信息来构建了。这样一来，同一个用户名前缀的不同组合（比如用户名为abc的用户，a、ab和abc三个Key）关联到UserDTO是同一份： @PostConstruct public void right() &#123; ... HashSet&lt;UserDTO&gt; cache &#x3D; userRepository.findAll().stream() .map(item -&gt; new UserDTO(item.getName())) .collect(Collectors.toCollection(HashSet::new)); cache.stream().forEach(userDTO -&gt; &#123; int len &#x3D; userDTO.getName().length(); for (int i &#x3D; 0; i &lt; len; i++) &#123; String key &#x3D; userDTO.getName().substring(0, i + 1); autoCompleteIndex.computeIfAbsent(key, s -&gt; new ArrayList&lt;&gt;()) .add(userDTO); &#125; &#125;); ... &#125; 再次分析堆内存，可以看到UserDTO只有9945份，总共占用的内存不到200M。这才是我们真正想要的结果。 修复后的程序，不仅相同的UserDTO只有一份，总副本数变为了原来的六分之一；而且因为HashSet的去重特性，双重节约了内存。 值得注意的是，我们虽然清楚数据总量，但却忽略了每一份数据在内存中可能有多份。我之前还遇到一个案例，一个后台程序需要从数据库加载大量信息用于数据导出，这些数据在数据库中占用100M内存，但是1GB的JVM堆却无法完成导出操作。 我来和你分析下原因吧。100M的数据加载到程序内存中，变为Java的数据结构就已经占用了200M堆内存；这些数据经过JDBC、MyBatis等框架其实是加载了2份，然后领域模型、DTO再进行转换可能又加载了2次；最终，占用的内存达到了200M*4&#x3D;800M。 所以，在进行容量评估时，我们不能认为一份数据在程序内存中也是一份。 使用WeakHashMap不等于不会OOM对于上一节实现快速检索的案例，为了防止缓存中堆积大量数据导致OOM，一些同学可能会想到使用WeakHashMap作为缓存容器。 WeakHashMap的特点是Key在哈希表内部是弱引用的，当没有强引用指向这个Key之后，Entry会被GC，即使我们无限往WeakHashMap加入数据，只要Key不再使用，也就不会OOM。 说到了强引用和弱引用，我先和你回顾下Java中引用类型和垃圾回收的关系： 垃圾回收器不会回收有强引用的对象； 在内存充足时，垃圾回收器不会回收具有软引用的对象； 垃圾回收器只要扫描到了具有弱引用的对象就会回收，WeakHashMap就是利用了这个特点。 不过，我要和你分享的第二个案例，恰巧就是不久前我遇到的一个使用WeakHashMap却最终OOM的案例。我们暂且不论使用WeakHashMap作为缓存是否合适，先分析一下这个OOM问题。 声明一个Key是User类型、Value是UserProfile类型的WeakHashMap，作为用户数据缓存，往其中添加200万个Entry，然后使用ScheduledThreadPoolExecutor发起一个定时任务，每隔1秒输出缓存中的Entry个数： private Map&lt;User, UserProfile&gt; cache &#x3D; new WeakHashMap&lt;&gt;(); @GetMapping(&quot;wrong&quot;) public void wrong() &#123; String userName &#x3D; &quot;zhuye&quot;; &#x2F;&#x2F;间隔1秒定时输出缓存中的条目数 Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate( () -&gt; log.info(&quot;cache size:&#123;&#125;&quot;, cache.size()), 1, 1, TimeUnit.SECONDS); LongStream.rangeClosed(1, 2000000).forEach(i -&gt; &#123; User user &#x3D; new User(userName + i); cache.put(user, new UserProfile(user, &quot;location&quot; + i)); &#125;); &#125; 执行程序后日志如下： [10:30:28.509] [pool-3-thread-1] [INFO ] [t.c.o.demo3.WeakHashMapOOMController:29 ] - cache size:2000000 [10:30:29.507] [pool-3-thread-1] [INFO ] [t.c.o.demo3.WeakHashMapOOMController:29 ] - cache size:2000000 [10:30:30.509] [pool-3-thread-1] [INFO ] [t.c.o.demo3.WeakHashMapOOMController:29 ] - cache size:2000000 可以看到，输出的cache size始终是200万，即使我们通过jvisualvm进行手动GC还是这样。这就说明，这些Entry无法通过GC回收。如果你把200万改为1000万，就可以在日志中看到如下的OOM错误： Exception in thread &quot;http-nio-45678-exec-1&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded Exception in thread &quot;Catalina-utility-2&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded 我们来分析一下这个问题。进行堆转储后可以看到，堆内存中有200万个UserProfie和User： 如下是User和UserProfile类的定义，需要注意的是，WeakHashMap的Key是User对象，而其Value是UserProfile对象，持有了User的引用： @Data @AllArgsConstructor @NoArgsConstructor class User &#123; private String name; &#125; @Data @AllArgsConstructor @NoArgsConstructor public class UserProfile &#123; private User user; private String location; &#125; 没错，这就是问题的所在。分析一下WeakHashMap的源码，你会发现WeakHashMap和HashMap的最大区别，是Entry对象的实现。接下来，我们暂且忽略HashMap的实现，来看下Entry对象： private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; ... &#x2F;** * Creates new entry. *&#x2F; Entry(Object key, V value, ReferenceQueue&lt;Object&gt; queue, int hash, Entry&lt;K,V&gt; next) &#123; super(key, queue); this.value &#x3D; value; this.hash &#x3D; hash; this.next &#x3D; next; &#125; Entry对象继承了WeakReference，Entry的构造函数调用了super (key,queue)，这是父类的构造函数。其中，key是我们执行put方法时的key；queue是一个ReferenceQueue。如果你了解Java的引用就会知道，被GC的对象会被丢进这个queue里面。 再来看看对象被丢进queue后是如何被销毁的： public V get(Object key) &#123; Object k &#x3D; maskNull(key); int h &#x3D; hash(k); Entry&lt;K,V&gt;[] tab &#x3D; getTable(); int index &#x3D; indexFor(h, tab.length); Entry&lt;K,V&gt; e &#x3D; tab[index]; while (e !&#x3D; null) &#123; if (e.hash &#x3D;&#x3D; h &amp;&amp; eq(k, e.get())) return e.value; e &#x3D; e.next; &#125; return null; &#125; private Entry&lt;K,V&gt;[] getTable() &#123; expungeStaleEntries(); return table; &#125; &#x2F;** * Expunges stale entries from the table. *&#x2F; private void expungeStaleEntries() &#123; for (Object x; (x &#x3D; queue.poll()) !&#x3D; null; ) &#123; synchronized (queue) &#123; @SuppressWarnings(&quot;unchecked&quot;) Entry&lt;K,V&gt; e &#x3D; (Entry&lt;K,V&gt;) x; int i &#x3D; indexFor(e.hash, table.length); Entry&lt;K,V&gt; prev &#x3D; table[i]; Entry&lt;K,V&gt; p &#x3D; prev; while (p !&#x3D; null) &#123; Entry&lt;K,V&gt; next &#x3D; p.next; if (p &#x3D;&#x3D; e) &#123; if (prev &#x3D;&#x3D; e) table[i] &#x3D; next; else prev.next &#x3D; next; &#x2F;&#x2F; Must not null out e.next; &#x2F;&#x2F; stale entries may be in use by a HashIterator e.value &#x3D; null; &#x2F;&#x2F; Help GC size--; break; &#125; prev &#x3D; p; &#96;&#96; p &#x3D; next; &#125; &#125; &#125; &#125; 从源码中可以看到，每次调用get、put、size等方法时，都会从queue里拿出所有已经被GC掉的key并删除对应的Entry对象。我们再来回顾下这个逻辑： put一个对象进Map时，它的key会被封装成弱引用对象； 发生GC时，弱引用的key被发现并放入queue； 调用get等方法时，扫描queue删除key，以及包含key和value的Entry对象。 WeakHashMap的Key虽然是弱引用，但是其Value却持有Key中对象的强引用，Value被Entry引用，Entry被WeakHashMap引用，最终导致Key无法回收。解决方案就是让Value变为弱引用，使用WeakReference来包装UserProfile即可： private Map&lt;User, WeakReference&lt;UserProfile&gt;&gt; cache2 &#x3D; new WeakHashMap&lt;&gt;(); @GetMapping(&quot;right&quot;) public void right() &#123; String userName &#x3D; &quot;zhuye&quot;; &#x2F;&#x2F;间隔1秒定时输出缓存中的条目数 Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate( () -&gt; log.info(&quot;cache size:&#123;&#125;&quot;, cache2.size()), 1, 1, TimeUnit.SECONDS); LongStream.rangeClosed(1, 2000000).forEach(i -&gt; &#123; User user &#x3D; new User(userName + i); &#x2F;&#x2F;这次，我们使用弱引用来包装UserProfile cache2.put(user, new WeakReference(new UserProfile(user, &quot;location&quot; + i))); &#125;); &#125; 重新运行程序，从日志中观察到cache size不再是固定的200万，而是在不断减少，甚至在手动GC后所有的Entry都被回收了： [10:40:05.792] [pool-3-thread-1] [INFO ] [t.c.o.demo3.WeakHashMapOOMController:40 ] - cache size:1367402 [10:40:05.795] [pool-3-thread-1] [INFO ] [t.c.o.demo3.WeakHashMapOOMController:40 ] - cache size:1367846 [10:40:06.773] [pool-3-thread-1] [INFO ] [t.c.o.demo3.WeakHashMapOOMController:40 ] - cache size:549551 ... [10:40:20.742] [pool-3-thread-1] [INFO ] [t.c.o.demo3.WeakHashMapOOMController:40 ] - cache size:549551 [10:40:22.862] [pool-3-thread-1] [INFO ] [t.c.o.demo3.WeakHashMapOOMController:40 ] - cache size:547937 [10:40:22.865] [pool-3-thread-1] [INFO ] [t.c.o.demo3.WeakHashMapOOMController:40 ] - cache size:542134 [10:40:23.779] [pool-3-thread-1] [INFO ] &#x2F;&#x2F;手动进行GC [t.c.o.demo3.WeakHashMapOOMController:40 ] - cache size:0 当然，还有一种办法就是，让Value也就是UserProfile不再引用Key，而是重新new出一个新的User对象赋值给UserProfile： @GetMapping(&quot;right2&quot;) public void right2() &#123; String userName &#x3D; &quot;zhuye&quot;; ... User user &#x3D; new User(userName + i); cache.put(user, new UserProfile(new User(user.getName()), &quot;location&quot; + i)); &#125;); &#125; 此外，Spring提供的ConcurrentReferenceHashMap类可以使用弱引用、软引用做缓存，Key和Value同时被软引用或弱引用包装，也能解决相互引用导致的数据不能释放问题。与WeakHashMap相比，ConcurrentReferenceHashMap不但性能更好，还可以确保线程安全。你可以自己做实验测试下。 Tomcat参数配置不合理导致OOM我们再来看看第三个案例。有一次运维同学反馈，有个应用在业务量大的情况下会出现假死，日志中也有大量OOM异常： [13:18:17.597] [http-nio-45678-exec-70] [ERROR] [ache.coyote.http11.Http11NioProtocol:175 ] - Failed to complete processing of a request java.lang.OutOfMemoryError: Java heap space 于是，我让运维同学进行生产堆Dump。通过MAT打开dump文件后，我们一眼就看到OOM的原因是，有接近1.7GB的byte数组分配，而JVM进程的最大堆内存我们只配置了2GB： 通过查看引用可以发现，大量引用都是Tomcat的工作线程。大部分工作线程都分配了两个10M左右的数组，100个左右工作线程吃满了内存。第一个红框是Http11InputBuffer，其buffer大小是10008192字节；而第二个红框的Http11OutputBuffer的buffer，正好占用10000000字节： 我们先来看看第一个Http11InputBuffer为什么会占用这么多内存。查看Http11InputBuffer类的init方法注意到，其中一个初始化方法会分配headerBufferSize+readBuffer大小的内存： void init(SocketWrapperBase&lt;?&gt; socketWrapper) &#123; wrapper &#x3D; socketWrapper; wrapper.setAppReadBufHandler(this); int bufLength &#x3D; headerBufferSize + wrapper.getSocketBufferHandler().getReadBuffer().capacity(); if (byteBuffer &#x3D;&#x3D; null || byteBuffer.capacity() &lt; bufLength) &#123; byteBuffer &#x3D; ByteBuffer.allocate(bufLength); byteBuffer.position(0).limit(0); &#125; &#125; 在Tomcat文档中有提到，这个Socket的读缓冲，也就是readBuffer默认是8192字节。显然，问题出在了headerBufferSize上： 向上追溯初始化Http11InputBuffer的Http11Processor类，可以看到，传入的headerBufferSize配置的是MaxHttpHeaderSize： inputBuffer &#x3D; new Http11InputBuffer(request, protocol.getMaxHttpHeaderSize(), protocol.getRejectIllegalHeaderName(), httpParser); Http11OutputBuffer中的buffer正好占用了10000000字节，这又是为什么？通过Http11OutputBuffer的构造方法，可以看到它是直接根据headerBufferSize分配了固定大小的headerBuffer： protected Http11OutputBuffer(Response response, int headerBufferSize)&#123; ... headerBuffer &#x3D; ByteBuffer.allocate(headerBufferSize); &#125; 那么我们就可以想到，一定是应用把Tomcat头相关的参数配置为10000000了，使得每一个请求对于Request和Response都占用了20M内存，最终在并发较多的情况下引起了OOM。 果不其然，查看项目代码发现配置文件中有这样的配置项： server.max-http-header-size&#x3D;10000000 翻看源码提交记录可以看到，当时开发同学遇到了这样的异常： java.lang.IllegalArgumentException: Request header is too large 于是他就到网上搜索了一下解决方案，随意将server.max-http-header-size修改为了一个超大值，期望永远不会再出现类似问题。但，没想到这个修改却引起了这么大的问题。把这个参数改为比较合适的20000再进行压测，我们就可以发现应用的各项指标都比较稳定。 这个案例告诉我们，一定要根据实际需求来修改参数配置，可以考虑预留2到5倍的量。容量类的参数背后往往代表了资源，设置超大的参数就有可能占用不必要的资源，在并发量大的时候因为资源大量分配导致OOM。 重点回顾今天，我从内存分配意识的角度和你分享了OOM的问题。通常而言，Java程序的OOM有如下几种可能。 一是，我们的程序确实需要超出JVM配置的内存上限的内存。不管是程序实现的不合理，还是因为各种框架对数据的重复处理、加工和转换，相同的数据在内存中不一定只占用一份空间。针对内存量使用超大的业务逻辑，比如缓存逻辑、文件上传下载和导出逻辑，我们在做容量评估时，可能还需要实际做一下Dump，而不是进行简单的假设。 二是，出现内存泄露，其实就是我们认为没有用的对象最终会被GC，但却没有。GC并不会回收强引用对象，我们可能经常在程序中定义一些容器作为缓存，但如果容器中的数据无限增长，要特别小心最终会导致OOM。使用WeakHashMap是解决这个问题的好办法，但值得注意的是，如果强引用的Value有引用Key，也无法回收Entry。 三是，不合理的资源需求配置，在业务量小的时候可能不会出现问题，但业务量一大可能很快就会撑爆内存。比如，随意配置Tomcat的max-http-header-size参数，会导致一个请求使用过多的内存，请求量大的时候出现OOM。在进行参数配置的时候，我们要认识到，很多限制类参数限制的是背后资源的使用，资源始终是有限的，需要根据实际需求来合理设置参数。 最后我想说的是，在出现OOM之后，也不用过于紧张。我们可以根据错误日志中的异常信息，再结合jstat等命令行工具观察内存使用情况，以及程序的GC日志，来大致定位出现OOM的内存区块和类型。其实，我们遇到的90%的OOM都是堆OOM，对JVM进程进行堆内存Dump，或使用jmap命令分析对象内存占用排行，一般都可以很容易定位到问题。 这里，我建议你为生产系统的程序配置JVM参数启用详细的GC日志，方便观察垃圾收集器的行为，并开启HeapDumpOnOutOfMemoryError，以便在出现OOM时能自动Dump留下第一问题现场。对于JDK8，你可以这么设置： XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath&#x3D;. -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles&#x3D;10 -XX:GCLogFileSize&#x3D;100M 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 Spring的ConcurrentReferenceHashMap，针对Key和Value支持软引用和弱引用两种方式。你觉得哪种方式更适合做缓存呢？ 当我们需要动态执行一些表达式时，可以使用Groovy动态语言实现：new出一个GroovyShell类，然后调用evaluate方法动态执行脚本。这种方式的问题是，会重复产生大量的类，增加Metaspace区的GC负担，有可能会引起OOM。你知道如何避免这个问题吗？ 针对OOM或内存泄露，你还遇到过什么案例吗？我是朱晔，欢迎在评论区与我留言分享，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/18、当反射、注解和泛型遇到OOP时，会有哪些坑？","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/18、当反射、注解和泛型遇到OOP时，会有哪些坑？/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/18%E3%80%81%E5%BD%93%E5%8F%8D%E5%B0%84%E3%80%81%E6%B3%A8%E8%A7%A3%E5%92%8C%E6%B3%9B%E5%9E%8B%E9%81%87%E5%88%B0OOP%E6%97%B6%EF%BC%8C%E4%BC%9A%E6%9C%89%E5%93%AA%E4%BA%9B%E5%9D%91%EF%BC%9F/","excerpt":"","text":"18 | 当反射、注解和泛型遇到OOP时，会有哪些坑？作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我们聊聊Java高级特性的话题，看看反射、注解和泛型遇到重载和继承时可能会产生的坑。 你可能说，业务项目中几乎都是增删改查，用到反射、注解和泛型这些高级特性的机会少之又少，没啥好学的。但我要说的是，只有学好、用好这些高级特性，才能开发出更简洁易读的代码，而且几乎所有的框架都使用了这三大高级特性。比如，要减少重复代码，就得用到反射和注解（详见第21讲）。 如果你从来没用过反射、注解和泛型，可以先通过官网有一个大概了解： Java Reflection API &amp; Reflection Tutorials； Annotations &amp; Lesson: Annotations； Generics &amp; Lesson: Generics。 接下来，我们就通过几个案例，看看这三大特性结合OOP使用时会有哪些坑吧。 反射调用方法不是以传参决定重载反射的功能包括，在运行时动态获取类和类成员定义，以及动态读取属性调用方法。也就是说，针对类动态调用方法，不管类中字段和方法怎么变动，我们都可以用相同的规则来读取信息和执行方法。因此，几乎所有的ORM（对象关系映射）、对象映射、MVC框架都使用了反射。 反射的起点是Class类，Class类提供了各种方法帮我们查询它的信息。你可以通过这个文档，了解每一个方法的作用。 接下来，我们先看一个反射调用方法遇到重载的坑：有两个叫age的方法，入参分别是基本类型int和包装类型Integer。 @Slf4j public class ReflectionIssueApplication &#123; private void age(int age) &#123; log.info(&quot;int age &#x3D; &#123;&#125;&quot;, age); &#125; private void age(Integer age) &#123; log.info(&quot;Integer age &#x3D; &#123;&#125;&quot;, age); &#125; &#125; 如果不通过反射调用，走哪个重载方法很清晰，比如传入36走int参数的重载方法，传入Integer.valueOf(“36”)走Integer重载： ReflectionIssueApplication application &#x3D; new ReflectionIssueApplication(); application.age(36); application.age(Integer.valueOf(&quot;36&quot;)); 但使用反射时的误区是，认为反射调用方法还是根据入参确定方法重载。比如，使用getDeclaredMethod来获取age方法，然后传入Integer.valueOf(“36”)： getClass().getDeclaredMethod(&quot;age&quot;, Integer.TYPE).invoke(this, Integer.valueOf(&quot;36&quot;)); 输出的日志证明，走的是int重载方法： 14:23:09.801 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo1.ReflectionIssueApplication - int age &#x3D; 36 其实，要通过反射进行方法调用，第一步就是通过方法签名来确定方法。具体到这个案例，getDeclaredMethod传入的参数类型Integer.TYPE代表的是int，所以实际执行方法时无论传的是包装类型还是基本类型，都会调用int入参的age方法。 把Integer.TYPE改为Integer.class，执行的参数类型就是包装类型的Integer。这时，无论传入的是Integer.valueOf(“36”)还是基本类型的36： getClass().getDeclaredMethod(&quot;age&quot;, Integer.class).invoke(this, Integer.valueOf(&quot;36&quot;)); getClass().getDeclaredMethod(&quot;age&quot;, Integer.class).invoke(this, 36); 都会调用Integer为入参的age方法： 14:25:18.028 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo1.ReflectionIssueApplication - Integer age &#x3D; 36 14:25:18.029 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo1.ReflectionIssueApplication - Integer age &#x3D; 36 现在我们非常清楚了，反射调用方法，是以反射获取方法时传入的方法名称和参数类型来确定调用方法的。接下来，我们再来看一下反射、泛型擦除和继承结合在一起会碰撞出什么坑。 泛型经过类型擦除多出桥接方法的坑泛型是一种风格或范式，一般用于强类型程序设计语言，允许开发者使用类型参数替代明确的类型，实例化时再指明具体的类型。它是代码重用的有效手段，允许把一套代码应用到多种数据类型上，避免针对每一种数据类型实现重复的代码。 Java 编译器对泛型应用了强大的类型检测，如果代码违反了类型安全就会报错，可以在编译时暴露大多数泛型的编码错误。但总有一部分编码错误，比如泛型类型擦除的坑，在运行时才会暴露。接下来，我就和你分享一个案例吧。 有一个项目希望在类字段内容变动时记录日志，于是开发同学就想到定义一个泛型父类，并在父类中定义一个统一的日志记录方法，子类可以通过继承重用这个方法。代码上线后业务没啥问题，但总是出现日志重复记录的问题。开始时，我们怀疑是日志框架的问题，排查到最后才发现是泛型的问题，反复修改多次才解决了这个问题。 父类是这样的：有一个泛型占位符T；有一个AtomicInteger计数器，用来记录value字段更新的次数，其中value字段是泛型T类型的，setValue方法每次为value赋值时对计数器进行+1操作。我重写了toString方法，输出value字段的值和计数器的值： class Parent&lt;T&gt; &#123; &#x2F;&#x2F;用于记录value更新的次数，模拟日志记录的逻辑 AtomicInteger updateCount &#x3D; new AtomicInteger(); private T value; &#x2F;&#x2F;重写toString，输出值和值更新次数 @Override public String toString() &#123; return String.format(&quot;value: %s updateCount: %d&quot;, value, updateCount.get()); &#125; &#x2F;&#x2F;设置值 public void setValue(T value) &#123; this.value &#x3D; value; updateCount.incrementAndGet(); &#125; &#125; 子类Child1的实现是这样的：继承父类，但没有提供父类泛型参数；定义了一个参数为String的setValue方法，通过super.setValue调用父类方法实现日志记录。我们也能明白，开发同学这么设计是希望覆盖父类的setValue实现： class Child1 extends Parent &#123; public void setValue(String value) &#123; System.out.println(&quot;Child1.setValue called&quot;); super.setValue(value); &#125; &#125; 在实现的时候，子类方法的调用是通过反射进行的。实例化Child1类型后，通过getClass().getMethods方法获得所有的方法；然后按照方法名过滤出setValue方法进行调用，传入字符串test作为参数： Child1 child1 &#x3D; new Child1(); Arrays.stream(child1.getClass().getMethods()) .filter(method -&gt; method.getName().equals(&quot;setValue&quot;)) .forEach(method -&gt; &#123; try &#123; method.invoke(child1, &quot;test&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); System.out.println(child1.toString()); 运行代码后可以看到，虽然Parent的value字段正确设置了test，但父类的setValue方法调用了两次，计数器也显示2而不是1： Child1.setValue called Parent.setValue called Parent.setValue called value: test updateCount: 2 显然，两次Parent的setValue方法调用，是因为getMethods方法找到了两个名为setValue的方法，分别是父类和子类的setValue方法。 这个案例中，子类方法重写父类方法失败的原因，包括两方面： 一是，子类没有指定String泛型参数，父类的泛型方法setValue(T value)在泛型擦除后是setValue(Object value)，子类中入参是String的setValue方法被当作了新方法； 二是，子类的setValue方法没有增加@Override注解，因此编译器没能检测到重写失败的问题。这就说明，重写子类方法时，标记@Override是一个好习惯。 但是，开发同学认为问题出在反射API使用不当，却没意识到重写失败。他查文档后发现，getMethods方法能获得当前类和父类的所有public方法，而getDeclaredMethods只能获得当前类所有的public、protected、package和private方法。 于是，他就用getDeclaredMethods替代了getMethods： Arrays.stream(child1.getClass().getDeclaredMethods()) .filter(method -&gt; method.getName().equals(&quot;setValue&quot;)) .forEach(method -&gt; &#123; try &#123; method.invoke(child1, &quot;test&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); 这样虽然能解决重复记录日志的问题，但没有解决子类方法重写父类方法失败的问题，得到如下输出： Child1.setValue called Parent.setValue called value: test updateCount: 1 其实这治标不治本，其他人使用Child1时还是会发现有两个setValue方法，非常容易让人困惑。 幸好，架构师在修复上线前发现了这个问题，让开发同学重新实现了Child2，继承Parent的时候提供了String作为泛型T类型，并使用@Override关键字注释了setValue方法，实现了真正有效的方法重写： class Child2 extends Parent&lt;String&gt; &#123; @Override public void setValue(String value) &#123; System.out.println(&quot;Child2.setValue called&quot;); super.setValue(value); &#125; &#125; 但很可惜，修复代码上线后，还是出现了日志重复记录： Child2.setValue called Parent.setValue called Child2.setValue called Parent.setValue called value: test updateCount: 2 可以看到，这次是Child2类的setValue方法被调用了两次。开发同学惊讶地说，肯定是反射出Bug了，通过getDeclaredMethods查找到的方法一定是来自Child2类本身；而且，怎么看Child2类中也只有一个setValue方法，为什么还会重复呢？ 调试一下可以发现，Child2类其实有2个setValue方法，入参分别是String和Object。 如果不通过反射来调用方法，我们确实很难发现这个问题。其实，这就是泛型类型擦除导致的问题。我们来分析一下。 我们知道，Java的泛型类型在编译后擦除为Object。虽然子类指定了父类泛型T类型是String，但编译后T会被擦除成为Object，所以父类setValue方法的入参是Object，value也是Object。如果子类Child2的setValue方法要覆盖父类的setValue方法，那入参也必须是Object。所以，编译器会为我们生成一个所谓的bridge桥接方法，你可以使用javap命令来反编译编译后的Child2类的class字节码： javap -c &#x2F;Users&#x2F;zhuye&#x2F;Documents&#x2F;common-mistakes&#x2F;target&#x2F;classes&#x2F;org&#x2F;geekbang&#x2F;time&#x2F;commonmistakes&#x2F;advancedfeatures&#x2F;demo3&#x2F;Child2.class Compiled from &quot;GenericAndInheritanceApplication.java&quot; class org.geekbang.time.commonmistakes.advancedfeatures.demo3.Child2 extends org.geekbang.time.commonmistakes.advancedfeatures.demo3.Parent&lt;java.lang.String&gt; &#123; org.geekbang.time.commonmistakes.advancedfeatures.demo3.Child2(); Code: 0: aload_0 1: invokespecial #1 &#x2F;&#x2F; Method org&#x2F;geekbang&#x2F;time&#x2F;commonmistakes&#x2F;advancedfeatures&#x2F;demo3&#x2F;Parent.&quot;&lt;init&gt;&quot;:()V 4: return public void setValue(java.lang.String); Code: 0: getstatic #2 &#x2F;&#x2F; Field java&#x2F;lang&#x2F;System.out:Ljava&#x2F;io&#x2F;PrintStream; 3: ldc #3 &#x2F;&#x2F; String Child2.setValue called 5: invokevirtual #4 &#x2F;&#x2F; Method java&#x2F;io&#x2F;PrintStream.println:(Ljava&#x2F;lang&#x2F;String;)V 8: aload_0 9: aload_1 10: invokespecial #5 &#x2F;&#x2F; Method org&#x2F;geekbang&#x2F;time&#x2F;commonmistakes&#x2F;advancedfeatures&#x2F;demo3&#x2F;Parent.setValue:(Ljava&#x2F;lang&#x2F;Object;)V 13: return public void setValue(java.lang.Object); Code: 0: aload_0 1: aload_1 2: checkcast #6 &#x2F;&#x2F; class java&#x2F;lang&#x2F;String 5: invokevirtual #7 &#x2F;&#x2F; Method setValue:(Ljava&#x2F;lang&#x2F;String;)V 8: return &#125; 可以看到，入参为Object的setValue方法在内部调用了入参为String的setValue方法（第27行），也就是代码里实现的那个方法。如果编译器没有帮我们实现这个桥接方法，那么Child2子类重写的是父类经过泛型类型擦除后、入参是Object的setValue方法。这两个方法的参数，一个是String一个是Object，明显不符合Java的语义： class Parent &#123; AtomicInteger updateCount &#x3D; new AtomicInteger(); private Object value; public void setValue(Object value) &#123; System.out.println(&quot;Parent.setValue called&quot;); this.value &#x3D; value; updateCount.incrementAndGet(); &#125; &#125; class Child2 extends Parent &#123; @Override public void setValue(String value) &#123; System.out.println(&quot;Child2.setValue called&quot;); super.setValue(value); &#125; &#125; 使用jclasslib工具打开Child2类，同样可以看到入参为Object的桥接方法上标记了public + synthetic + bridge三个属性。synthetic代表由编译器生成的不可见代码，bridge代表这是泛型类型擦除后生成的桥接代码： 知道这个问题之后，修改方式就明朗了，可以使用method的isBridge方法，来判断方法是不是桥接方法： 通过getDeclaredMethods方法获取到所有方法后，必须同时根据方法名setValue和非isBridge两个条件过滤，才能实现唯一过滤； 使用Stream时，如果希望只匹配0或1项的话，可以考虑配合ifPresent来使用findFirst方法。 修复代码如下： Arrays.stream(child2.getClass().getDeclaredMethods()) .filter(method -&gt; method.getName().equals(&quot;setValue&quot;) &amp;&amp; !method.isBridge()) .findFirst().ifPresent(method -&gt; &#123; try &#123; method.invoke(chi2, &quot;test&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); 这样就可以得到正确输出了： Child2.setValue called Parent.setValue called value: test updateCount: 1 最后小结下，使用反射查询类方法清单时，我们要注意两点： getMethods和getDeclaredMethods是有区别的，前者可以查询到父类方法，后者只能查询到当前类。 反射进行方法调用要注意过滤桥接方法。 注解可以继承吗？注解可以为Java代码提供元数据，各种框架也都会利用注解来暴露功能，比如Spring框架中的@Service、@Controller、@Bean注解，Spring Boot的@SpringBootApplication注解。 框架可以通过类或方法等元素上标记的注解，来了解它们的功能或特性，并以此来启用或执行相应的功能。通过注解而不是API调用来配置框架，属于声明式交互，可以简化框架的配置工作，也可以和框架解耦。 开发同学可能会认为，类继承后，类的注解也可以继承，子类重写父类方法后，父类方法上的注解也能作用于子类，但这些观点其实是错误或者说是不全面的。我们来验证下吧。 首先，定义一个包含value属性的MyAnnotation注解，可以标记在方法或类上： @Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;) @Retention(RetentionPolicy.RUNTIME) public @interface MyAnnotation &#123; String value(); &#125; 然后，定义一个标记了@MyAnnotation注解的父类Parent，设置value为Class字符串；同时这个类的foo方法也标记了@MyAnnotation注解，设置value为Method字符串。接下来，定义一个子类Child继承Parent父类，并重写父类的foo方法，子类的foo方法和类上都没有@MyAnnotation注解。 @MyAnnotation(value &#x3D; &quot;Class&quot;) @Slf4j static class Parent &#123; @MyAnnotation(value &#x3D; &quot;Method&quot;) public void foo() &#123; &#125; &#125; @Slf4j static class Child extends Parent &#123; @Override public void foo() &#123; &#125; &#125; 再接下来，通过反射分别获取Parent和Child的类和方法的注解信息，并输出注解的value属性的值（如果注解不存在则输出空字符串）： private static String getAnnotationValue(MyAnnotation annotation) &#123; if (annotation &#x3D;&#x3D; null) return &quot;&quot;; return annotation.value(); &#125; public static void wrong() throws NoSuchMethodException &#123; &#x2F;&#x2F;获取父类的类和方法上的注解 Parent parent &#x3D; new Parent(); log.info(&quot;ParentClass:&#123;&#125;&quot;, getAnnotationValue(parent.getClass().getAnnotation(MyAnnotation.class))); log.info(&quot;ParentMethod:&#123;&#125;&quot;, getAnnotationValue(parent.getClass().getMethod(&quot;foo&quot;).getAnnotation(MyAnnotation.class))); &#x2F;&#x2F;获取子类的类和方法上的注解 Child child &#x3D; new Child(); log.info(&quot;ChildClass:&#123;&#125;&quot;, getAnnotationValue(child.getClass().getAnnotation(MyAnnotation.class))); log.info(&quot;ChildMethod:&#123;&#125;&quot;, getAnnotationValue(child.getClass().getMethod(&quot;foo&quot;).getAnnotation(MyAnnotation.class))); &#125; 输出如下： 17:34:25.495 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ParentClass:Class 17:34:25.501 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ParentMethod:Method 17:34:25.504 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ChildClass: 17:34:25.504 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ChildMethod: 可以看到，父类的类和方法上的注解都可以正确获得，但是子类的类和方法却不能。这说明，子类以及子类的方法，无法自动继承父类和父类方法上的注解。 如果你详细了解过注解应该知道，在注解上标记@Inherited元注解可以实现注解的继承。那么，把@MyAnnotation注解标记了@Inherited，就可以一键解决问题了吗？ @Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;) @Retention(RetentionPolicy.RUNTIME) @Inherited public @interface MyAnnotation &#123; String value(); &#125; 重新运行代码输出如下： 17:44:54.831 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ParentClass:Class 17:44:54.837 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ParentMethod:Method 17:44:54.838 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ChildClass:Class 17:44:54.838 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ChildMethod: 可以看到，子类可以获得父类上的注解；子类foo方法虽然是重写父类方法，并且注解本身也支持继承，但还是无法获得方法上的注解。 如果你再仔细阅读一下@Inherited的文档就会发现，@Inherited只能实现类上的注解继承。要想实现方法上注解的继承，你可以通过反射在继承链上找到方法上的注解。但，这样实现起来很繁琐，而且需要考虑桥接方法。 好在Spring提供了AnnotatedElementUtils类，来方便我们处理注解的继承问题。这个类的findMergedAnnotation工具方法，可以帮助我们找出父类和接口、父类方法和接口方法上的注解，并可以处理桥接方法，实现一键找到继承链的注解： Child child &#x3D; new Child(); log.info(&quot;ChildClass:&#123;&#125;&quot;, getAnnotationValue(AnnotatedElementUtils.findMergedAnnotation(child.getClass(), MyAnnotation.class))); log.info(&quot;ChildMethod:&#123;&#125;&quot;, getAnnotationValue(AnnotatedElementUtils.findMergedAnnotation(child.getClass().getMethod(&quot;foo&quot;), MyAnnotation.class))); 修改后，可以得到如下输出： 17:47:30.058 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ChildClass:Class 17:47:30.059 [main] INFO org.geekbang.time.commonmistakes.advancedfeatures.demo2.AnnotationInheritanceApplication - ChildMethod:Method 可以看到，子类foo方法也获得了父类方法上的注解。 重点回顾今天，我和你分享了使用Java反射、注解和泛型高级特性配合OOP时，可能会遇到的一些坑。 第一，反射调用方法并不是通过调用时的传参确定方法重载，而是在获取方法的时候通过方法名和参数类型来确定的。遇到方法有包装类型和基本类型重载的时候，你需要特别注意这一点。 第二，反射获取类成员，需要注意getXXX和getDeclaredXXX方法的区别，其中XXX包括Methods、Fields、Constructors、Annotations。这两类方法，针对不同的成员类型XXX和对象，在实现上都有一些细节差异，详情请查看官方文档。今天提到的getDeclaredMethods方法无法获得父类定义的方法，而getMethods方法可以，只是差异之一，不能适用于所有的XXX。 第三，泛型因为类型擦除会导致泛型方法T占位符被替换为Object，子类如果使用具体类型覆盖父类实现，编译器会生成桥接方法。这样既满足子类方法重写父类方法的定义，又满足子类实现的方法有具体的类型。使用反射来获取方法清单时，你需要特别注意这一点。 第四，自定义注解可以通过标记元注解@Inherited实现注解的继承，不过这只适用于类。如果要继承定义在接口或方法上的注解，可以使用Spring的工具类AnnotatedElementUtils，并注意各种getXXX方法和findXXX方法的区别，详情查看Spring的文档。 最后，我要说的是。编译后的代码和原始代码并不完全一致，编译器可能会做一些优化，加上还有诸如AspectJ等编译时增强框架，使用反射动态获取类型的元数据可能会和我们编写的源码有差异，这点需要特别注意。你可以在反射中多写断言，遇到非预期的情况直接抛异常，避免通过反射实现的业务逻辑不符合预期。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 泛型类型擦除后会生成一个bridge方法，这个方法同时又是synthetic方法。除了泛型类型擦除，你知道还有什么情况编译器会生成synthetic方法吗？ 关于注解继承问题，你觉得Spring的常用注解@Service、@Controller是否支持继承呢？ 你还遇到过与Java高级特性相关的其他坑吗？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"","slug":"2024/Java 业务开发常见错误 100 例/19、Spring框架：IoC和AOP是扩展的核心","date":"2024-06-17T01:04:53.995Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/06/17/2024/Java 业务开发常见错误 100 例/19、Spring框架：IoC和AOP是扩展的核心/","link":"","permalink":"https://blog.ehzyil.xyz/2024/06/17/2024/Java%20%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%20100%20%E4%BE%8B/19%E3%80%81Spring%E6%A1%86%E6%9E%B6%EF%BC%9AIoC%E5%92%8CAOP%E6%98%AF%E6%89%A9%E5%B1%95%E7%9A%84%E6%A0%B8%E5%BF%83/","excerpt":"","text":"19 | Spring框架：IoC和AOP是扩展的核心作者: 朱晔 完成时间: 总结时间: 你好，我是朱晔。今天，我们来聊聊Spring框架中的IoC和AOP，及其容易出错的地方。 熟悉Java的同学都知道，Spring的家族庞大，常用的模块就有Spring Data、Spring Security、Spring Boot、Spring Cloud等。其实呢，Spring体系虽然庞大，但都是围绕Spring Core展开的，而Spring Core中最核心的就是IoC（控制反转）和AOP（面向切面编程）。 概括地说，IoC和AOP的初衷是解耦和扩展。理解这两个核心技术，就可以让你的代码变得更灵活、可随时替换，以及业务组件间更解耦。在接下来的两讲中，我会与你深入剖析几个案例，带你绕过业务中通过Spring实现IoC和AOP相关的坑。 为了便于理解这两讲中的案例，我们先回顾下IoC和AOP的基础知识。 IoC，其实就是一种设计思想。使用Spring来实现IoC，意味着将你设计好的对象交给Spring容器控制，而不是直接在对象内部控制。那，为什么要让容器来管理对象呢？或许你能想到的是，使用IoC方便、可以实现解耦。但在我看来，相比于这两个原因，更重要的是IoC带来了更多的可能性。 如果以容器为依托来管理所有的框架、业务对象，我们不仅可以无侵入地调整对象的关系，还可以无侵入地随时调整对象的属性，甚至是实现对象的替换。这就使得框架开发者在程序背后实现一些扩展不再是问题，带来的可能性是无限的。比如我们要监控的对象如果是Bean，实现就会非常简单。所以，这套容器体系，不仅被Spring Core和Spring Boot大量依赖，还实现了一些外部框架和Spring的无缝整合。 AOP，体现了松耦合、高内聚的精髓，在切面集中实现横切关注点（缓存、权限、日志等），然后通过切点配置把代码注入合适的地方。切面、切点、增强、连接点，是AOP中非常重要的概念，也是我们这两讲会大量提及的。 为方便理解，我们把Spring AOP技术看作为蛋糕做奶油夹层的工序。如果我们希望找到一个合适的地方把奶油注入蛋糕胚子中，那应该如何指导工人完成操作呢？ 首先，我们要提醒他，只能往蛋糕胚子里面加奶油，而不能上面或下面加奶油。这就是连接点（Join point），对于Spring AOP来说，连接点就是方法执行。 然后，我们要告诉他，在什么点切开蛋糕加奶油。比如，可以在蛋糕坯子中间加入一层奶油，在中间切一次；也可以在中间加两层奶油，在1&#x2F;3和2&#x2F;3的地方切两次。这就是切点（Pointcut），Spring AOP中默认使用AspectJ查询表达式，通过在连接点运行查询表达式来匹配切入点。 接下来也是最重要的，我们要告诉他，切开蛋糕后要做什么，也就是加入奶油。这就是增强（Advice），也叫作通知，定义了切入切点后增强的方式，包括前、后、环绕等。Spring AOP中，把增强定义为拦截器。 最后，我们要告诉他，找到蛋糕胚子中要加奶油的地方并加入奶油。为蛋糕做奶油夹层的操作，对Spring AOP来说就是切面（Aspect），也叫作方面。切面&#x3D;切点+增强。 好了，理解了这几个核心概念，我们就可以继续分析案例了。 我要首先说明的是，Spring相关问题的问题比较复杂，一方面是Spring提供的IoC和AOP本就灵活，另一方面Spring Boot的自动装配、Spring Cloud复杂的模块会让问题排查变得更复杂。因此，今天这一讲，我会带你先打好基础，通过两个案例来重点聊聊IoC和AOP；然后，我会在下一讲中与你分享Spring相关的坑。 单例的Bean如何注入Prototype的Bean？我们虽然知道Spring创建的Bean默认是单例的，但当Bean遇到继承的时候，可能会忽略这一点。为什么呢？忽略这一点又会造成什么影响呢？接下来，我就和你分享一个由单例引起内存泄露的案例。 架构师一开始定义了这么一个SayService抽象类，其中维护了一个类型是ArrayList的字段data，用于保存方法处理的中间数据。每次调用say方法都会往data加入新数据，可以认为SayService是有状态，如果SayService是单例的话必然会OOM： @Slf4j public abstract class SayService &#123; List&lt;String&gt; data &#x3D; new ArrayList&lt;&gt;(); public void say() &#123; data.add(IntStream.rangeClosed(1, 1000000) .mapToObj(__ -&gt; &quot;a&quot;) .collect(Collectors.joining(&quot;&quot;)) + UUID.randomUUID().toString()); log.info(&quot;I&#39;m &#123;&#125; size:&#123;&#125;&quot;, this, data.size()); &#125; &#125; 但实际开发的时候，开发同学没有过多思考就把SayHello和SayBye类加上了@Service注解，让它们成为了Bean，也没有考虑到父类是有状态的： @Service @Slf4j public class SayHello extends SayService &#123; @Override public void say() &#123; super.say(); log.info(&quot;hello&quot;); &#125; &#125; @Service @Slf4j public class SayBye extends SayService &#123; @Override public void say() &#123; super.say(); log.info(&quot;bye&quot;); &#125; &#125; 许多开发同学认为，@Service注解的意义在于，能通过@Autowired注解让Spring自动注入对象，就比如可以直接使用注入的List获取到SayHello和SayBye，而没想过类的生命周期： @Autowired List&lt;SayService&gt; sayServiceList; @GetMapping(&quot;test&quot;) public void test() &#123; log.info(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;); sayServiceList.forEach(SayService::say); &#125; 这一个点非常容易忽略。开发基类的架构师将基类设计为有状态的，但并不知道子类是怎么使用基类的；而开发子类的同学，没多想就直接标记了@Service，让类成为了Bean，通过@Autowired注解来注入这个服务。但这样设置后，有状态的基类就可能产生内存泄露或线程安全问题。 正确的方式是，在为类标记上@Service注解把类型交由容器管理前，首先评估一下类是否有状态，然后为Bean设置合适的Scope。好在上线前，架构师发现了这个内存泄露问题，开发同学也做了修改，为SayHello和SayBye两个类都标记了@Scope注解，设置了PROTOTYPE的生命周期，也就是多例： @Scope(value &#x3D; ConfigurableBeanFactory.SCOPE_PROTOTYPE) 但，上线后还是出现了内存泄漏，证明修改是无效的。 从日志可以看到，第一次调用和第二次调用的时候，SayBye对象都是4c0bfe9e，SayHello也是一样的问题。从日志第7到10行还可以看到，第二次调用后List的元素个数变为了2，说明父类SayService维护的List在不断增长，不断调用必然出现OOM： [15:01:09.349] [http-nio-45678-exec-1] [INFO ] [.s.d.BeanSingletonAndOrderController:22 ] - &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [15:01:09.401] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo1.SayService :19 ] - I&#39;m org.geekbang.time.commonmistakes.spring.demo1.SayBye@4c0bfe9e size:1 [15:01:09.402] [http-nio-45678-exec-1] [INFO ] [t.commonmistakes.spring.demo1.SayBye:16 ] - bye [15:01:09.469] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo1.SayService :19 ] - I&#39;m org.geekbang.time.commonmistakes.spring.demo1.SayHello@490fbeaa size:1 [15:01:09.469] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo1.SayHello :17 ] - hello [15:01:15.167] [http-nio-45678-exec-2] [INFO ] [.s.d.BeanSingletonAndOrderController:22 ] - &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [15:01:15.197] [http-nio-45678-exec-2] [INFO ] [o.g.t.c.spring.demo1.SayService :19 ] - I&#39;m org.geekbang.time.commonmistakes.spring.demo1.SayBye@4c0bfe9e size:2 [15:01:15.198] [http-nio-45678-exec-2] [INFO ] [t.commonmistakes.spring.demo1.SayBye:16 ] - bye [15:01:15.224] [http-nio-45678-exec-2] [INFO ] [o.g.t.c.spring.demo1.SayService :19 ] - I&#39;m org.geekbang.time.commonmistakes.spring.demo1.SayHello@490fbeaa size:2 [15:01:15.224] [http-nio-45678-exec-2] [INFO ] [o.g.t.c.spring.demo1.SayHello :17 ] - hello 这就引出了单例的Bean如何注入Prototype的Bean这个问题。Controller标记了@RestController注解，而@RestController注解&#x3D;@Controller注解+@ResponseBody注解，又因为@Controller标记了@Component元注解，所以@RestController注解其实也是一个Spring Bean： &#x2F;&#x2F;@RestController注解&#x3D;@Controller注解+@ResponseBody注解@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Controller @ResponseBody public @interface RestController &#123;&#125; &#x2F;&#x2F;@Controller又标记了@Component元注解 @Target(&#123;ElementType.TYPE&#125;) @Retention(RetentionPolicy.RUNTIME) @Documented @Component public @interface Controller &#123;&#125; Bean默认是单例的，所以单例的Controller注入的Service也是一次性创建的，即使Service本身标识了prototype的范围也没用。 修复方式是，让Service以代理方式注入。这样虽然Controller本身是单例的，但每次都能从代理获取Service。这样一来，prototype范围的配置才能真正生效： @Scope(value &#x3D; ConfigurableBeanFactory.SCOPE_PROTOTYPE, proxyMode &#x3D; ScopedProxyMode.TARGET_CLASS) 通过日志可以确认这种修复方式有效： [15:08:42.649] [http-nio-45678-exec-1] [INFO ] [.s.d.BeanSingletonAndOrderController:22 ] - &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [15:08:42.747] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo1.SayService :19 ] - I&#39;m org.geekbang.time.commonmistakes.spring.demo1.SayBye@3fa64743 size:1 [15:08:42.747] [http-nio-45678-exec-1] [INFO ] [t.commonmistakes.spring.demo1.SayBye:17 ] - bye [15:08:42.871] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo1.SayService :19 ] - I&#39;m org.geekbang.time.commonmistakes.spring.demo1.SayHello@2f0b779 size:1 [15:08:42.872] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo1.SayHello :17 ] - hello [15:08:42.932] [http-nio-45678-exec-2] [INFO ] [.s.d.BeanSingletonAndOrderController:22 ] - &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; [15:08:42.991] [http-nio-45678-exec-2] [INFO ] [o.g.t.c.spring.demo1.SayService :19 ] - I&#39;m org.geekbang.time.commonmistakes.spring.demo1.SayBye@7319b18e size:1 [15:08:42.992] [http-nio-45678-exec-2] [INFO ] [t.commonmistakes.spring.demo1.SayBye:17 ] - bye [15:08:43.046] [http-nio-45678-exec-2] [INFO ] [o.g.t.c.spring.demo1.SayService :19 ] - I&#39;m org.geekbang.time.commonmistakes.spring.demo1.SayHello@77262b35 size:1 [15:08:43.046] [http-nio-45678-exec-2] [INFO ] [o.g.t.c.spring.demo1.SayHello :17 ] - hello 调试一下也可以发现，注入的Service都是Spring生成的代理类： 当然，如果不希望走代理的话还有一种方式是，每次直接从ApplicationContext中获取Bean： @Autowired private ApplicationContext applicationContext; @GetMapping(&quot;test2&quot;) public void test2() &#123; applicationContext.getBeansOfType(SayService.class).values().forEach(SayService::say); &#125; 如果细心的话，你可以发现另一个潜在的问题。这里Spring注入的SayService的List，第一个元素是SayBye，第二个元素是SayHello。但，我们更希望的是先执行Hello再执行Bye，所以注入一个List Bean时，需要进一步考虑Bean的顺序或者说优先级。 大多数情况下顺序并不是那么重要，但对于AOP，顺序可能会引发致命问题。我们继续往下看这个问题吧。 监控切面因为顺序问题导致Spring事务失效实现横切关注点，是AOP非常常见的一个应用。我曾看到过一个不错的AOP实践，通过AOP实现了一个整合日志记录、异常处理和方法耗时打点为一体的统一切面。但后来发现，使用了AOP切面后，这个应用的声明式事务处理居然都是无效的。你可以先回顾下第6讲中提到的，Spring事务失效的几种可能性。 现在我们来看下这个案例，分析下AOP实现的监控组件和事务失效有什么关系，以及通过AOP实现监控组件是否还有其他坑。 首先，定义一个自定义注解Metrics，打上了该注解的方法可以实现各种监控功能： @Retention(RetentionPolicy.RUNTIME) @Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;) public @interface Metrics &#123; &#x2F;** * 在方法成功执行后打点，记录方法的执行时间发送到指标系统，默认开启 * * @return *&#x2F; boolean recordSuccessMetrics() default true; &#x2F;** * 在方法成功失败后打点，记录方法的执行时间发送到指标系统，默认开启 * * @return *&#x2F; boolean recordFailMetrics() default true; &#x2F;** * 通过日志记录请求参数，默认开启 * * @return *&#x2F; boolean logParameters() default true; &#x2F;** * 通过日志记录方法返回值，默认开启 * * @return *&#x2F; boolean logReturn() default true; &#x2F;** * 出现异常后通过日志记录异常信息，默认开启 * * @return *&#x2F; boolean logException() default true; &#x2F;** * 出现异常后忽略异常返回默认值，默认关闭 * * @return *&#x2F; boolean ignoreException() default false; &#125; 然后，实现一个切面完成Metrics注解提供的功能。这个切面可以实现标记了@RestController注解的Web控制器的自动切入，如果还需要对更多Bean进行切入的话，再自行标记@Metrics注解。 备注：这段代码有些长，里面还用到了一些小技巧，你需要仔细阅读代码中的注释。 @Aspect @Component @Slf4j public class MetricsAspect &#123; &#x2F;&#x2F;让Spring帮我们注入ObjectMapper，以方便通过JSON序列化来记录方法入参和出参 @Autowired private ObjectMapper objectMapper; &#x2F;&#x2F;实现一个返回Java基本类型默认值的工具。其实，你也可以逐一写很多if-else判断类型，然后手动设置其默认值。这里为了减少代码量用了一个小技巧，即通过初始化一个具有1个元素的数组，然后通过获取这个数组的值来获取基本类型默认值 private static final Map&lt;Class&lt;?&gt;, Object&gt; DEFAULT_VALUES &#x3D; Stream .of(boolean.class, byte.class, char.class, double.class, float.class, int.class, long.class, short.class) .collect(toMap(clazz -&gt; (Class&lt;?&gt;) clazz, clazz -&gt; Array.get(Array.newInstance(clazz, 1), 0))); public static &lt;T&gt; T getDefaultValue(Class&lt;T&gt; clazz) &#123; return (T) DEFAULT_VALUES.get(clazz); &#125; &#x2F;&#x2F;@annotation指示器实现对标记了Metrics注解的方法进行匹配 @Pointcut(&quot;within(@org.geekbang.time.commonmistakes.springpart1.aopmetrics.Metrics *)&quot;) public void withMetricsAnnotation() &#123; &#125; &#x2F;&#x2F;within指示器实现了匹配那些类型上标记了@RestController注解的方法 @Pointcut(&quot;within(@org.springframework.web.bind.annotation.RestController *)&quot;) public void controllerBean() &#123; &#125; @Around(&quot;controllerBean() || withMetricsAnnotation())&quot;) public Object metrics(ProceedingJoinPoint pjp) throws Throwable &#123; &#x2F;&#x2F;通过连接点获取方法签名和方法上Metrics注解，并根据方法签名生成日志中要输出的方法定义描述 MethodSignature signature &#x3D; (MethodSignature) pjp.getSignature(); Metrics metrics &#x3D; signature.getMethod().getAnnotation(Metrics.class); String name &#x3D; String.format(&quot;【%s】【%s】&quot;, signature.getDeclaringType().toString(), signature.toLongString()); &#x2F;&#x2F;因为需要默认对所有@RestController标记的Web控制器实现@Metrics注解的功能，在这种情况下方法上必然是没有@Metrics注解的，我们需要获取一个默认注解。虽然可以手动实例化一个@Metrics注解的实例出来，但为了节省代码行数，我们通过在一个内部类上定义@Metrics注解方式，然后通过反射获取注解的小技巧，来获得一个默认的@Metrics注解的实例 if (metrics &#x3D;&#x3D; null) &#123; @Metrics final class c &#123;&#125; metrics &#x3D; c.class.getAnnotation(Metrics.class); &#125; &#x2F;&#x2F;尝试从请求上下文（如果有的话）获得请求URL，以方便定位问题 RequestAttributes requestAttributes &#x3D; RequestContextHolder.getRequestAttributes(); if (requestAttributes !&#x3D; null) &#123; HttpServletRequest request &#x3D; ((ServletRequestAttributes) requestAttributes).getRequest(); if (request !&#x3D; null) name +&#x3D; String.format(&quot;【%s】&quot;, request.getRequestURL().toString()); &#125; &#x2F;&#x2F;实现的是入参的日志输出 if (metrics.logParameters()) log.info(String.format(&quot;【入参日志】调用 %s 的参数是：【%s】&quot;, name, objectMapper.writeValueAsString(pjp.getArgs()))); &#x2F;&#x2F;实现连接点方法的执行，以及成功失败的打点，出现异常的时候还会记录日志 Object returnValue; Instant start &#x3D; Instant.now(); try &#123; returnValue &#x3D; pjp.proceed(); if (metrics.recordSuccessMetrics()) &#x2F;&#x2F;在生产级代码中，我们应考虑使用类似Micrometer的指标框架，把打点信息记录到时间序列数据库中，实现通过图表来查看方法的调用次数和执行时间，在设计篇我们会重点介绍 log.info(String.format(&quot;【成功打点】调用 %s 成功，耗时：%d ms&quot;, name, Duration.between(start, Instant.now()).toMillis())); &#125; catch (Exception ex) &#123; if (metrics.recordFailMetrics()) log.info(String.format(&quot;【失败打点】调用 %s 失败，耗时：%d ms&quot;, name, Duration.between(start, Instant.now()).toMillis())); if (metrics.logException()) log.error(String.format(&quot;【异常日志】调用 %s 出现异常！&quot;, name), ex); &#x2F;&#x2F;忽略异常的时候，使用一开始定义的getDefaultValue方法，来获取基本类型的默认值 if (metrics.ignoreException()) returnValue &#x3D; getDefaultValue(signature.getReturnType()); else throw ex; &#125; &#x2F;&#x2F;实现了返回值的日志输出 if (metrics.logReturn()) log.info(String.format(&quot;【出参日志】调用 %s 的返回是：【%s】&quot;, name, returnValue)); return returnValue; &#125; &#125; 接下来，分别定义最简单的Controller、Service和Repository，来测试MetricsAspect的功能。 其中，Service中实现创建用户的时候做了事务处理，当用户名包含test字样时会抛出异常，导致事务回滚。同时，我们为Service中的createUser标记了@Metrics注解。这样一来，我们还可以手动为类或方法标记@Metrics注解，实现Controller之外的其他组件的自动监控。 @Slf4j @RestController &#x2F;&#x2F;自动进行监控 @RequestMapping(&quot;metricstest&quot;) public class MetricsController &#123; @Autowired private UserService userService; @GetMapping(&quot;transaction&quot;) public int transaction(@RequestParam(&quot;name&quot;) String name) &#123; try &#123; userService.createUser(new UserEntity(name)); &#125; catch (Exception ex) &#123; log.error(&quot;create user failed because &#123;&#125;&quot;, ex.getMessage()); &#125; return userService.getUserCount(name); &#125; &#125; @Service @Slf4j public class UserService &#123; @Autowired private UserRepository userRepository; @Transactional @Metrics &#x2F;&#x2F;启用方法监控 public void createUser(UserEntity entity) &#123; userRepository.save(entity); if (entity.getName().contains(&quot;test&quot;)) throw new RuntimeException(&quot;invalid username!&quot;); &#125; public int getUserCount(String name) &#123; return userRepository.findByName(name).size(); &#125; &#125; @Repository public interface UserRepository extends JpaRepository&lt;UserEntity, Long&gt; &#123; List&lt;UserEntity&gt; findByName(String name); &#125; 使用用户名“test”测试一下注册功能： [16:27:52.586] [http-nio-45678-exec-3] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :85 ] - 【入参日志】调用 【class org.geekbang.time.commonmistakes.spring.demo2.MetricsController】【public int org.geekbang.time.commonmistakes.spring.demo2.MetricsController.transaction(java.lang.String)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 的参数是：【[&quot;test&quot;]】 [16:27:52.590] [http-nio-45678-exec-3] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :85 ] - 【入参日志】调用 【class org.geekbang.time.commonmistakes.spring.demo2.UserService】【public void org.geekbang.time.commonmistakes.spring.demo2.UserService.createUser(org.geekbang.time.commonmistakes.spring.demo2.UserEntity)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 的参数是：【[&#123;&quot;id&quot;:null,&quot;name&quot;:&quot;test&quot;&#125;]】 [16:27:52.609] [http-nio-45678-exec-3] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :96 ] - 【失败打点】调用 【class org.geekbang.time.commonmistakes.spring.demo2.UserService】【public void org.geekbang.time.commonmistakes.spring.demo2.UserService.createUser(org.geekbang.time.commonmistakes.spring.demo2.UserEntity)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 失败，耗时：19 ms [16:27:52.610] [http-nio-45678-exec-3] [ERROR] [o.g.t.c.spring.demo2.MetricsAspect :98 ] - 【异常日志】调用 【class org.geekbang.time.commonmistakes.spring.demo2.UserService】【public void org.geekbang.time.commonmistakes.spring.demo2.UserService.createUser(org.geekbang.time.commonmistakes.spring.demo2.UserEntity)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 出现异常！ java.lang.RuntimeException: invalid username! at org.geekbang.time.commonmistakes.spring.demo2.UserService.createUser(UserService.java:18) at org.geekbang.time.commonmistakes.spring.demo2.UserService$$FastClassBySpringCGLIB$$9eec91f.invoke(&lt;generated&gt;) [16:27:52.614] [http-nio-45678-exec-3] [ERROR] [g.t.c.spring.demo2.MetricsController:21 ] - create user failed because invalid username! [16:27:52.617] [http-nio-45678-exec-3] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :93 ] - 【成功打点】调用 【class org.geekbang.time.commonmistakes.spring.demo2.MetricsController】【public int org.geekbang.time.commonmistakes.spring.demo2.MetricsController.transaction(java.lang.String)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 成功，耗时：31 ms [16:27:52.618] [http-nio-45678-exec-3] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :108 ] - 【出参日志】调用 【class org.geekbang.time.commonmistakes.spring.demo2.MetricsController】【public int org.geekbang.time.commonmistakes.spring.demo2.MetricsController.transaction(java.lang.String)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 的返回是：【0】 看起来这个切面很不错，日志中打出了整个调用的出入参、方法耗时： 第1、8、9和10行分别是Controller方法的入参日志、调用Service方法出错后记录的错误信息、成功执行的打点和出参日志。因为Controller方法内部进行了try-catch处理，所以其方法最终是成功执行的。出参日志中显示最后查询到的用户数量是0，表示用户创建实际是失败的。 第2、3和4~7行分别是Service方法的入参日志、失败打点和异常日志。正是因为Service方法的异常抛到了Controller，所以整个方法才能被@Transactional声明式事务回滚。在这里，MetricsAspect捕获了异常又重新抛出，记录了异常的同时又不影响事务回滚。 一段时间后，开发同学觉得默认的@Metrics配置有点不合适，希望进行两个调整： 对于Controller的自动打点，不要自动记录入参和出参日志，否则日志量太大； 对于Service中的方法，最好可以自动捕获异常。 于是，他就为MetricsController手动加上了@Metrics注解，设置logParameters和logReturn为false；然后为Service中的createUser方法的@Metrics注解，设置了ignoreException属性为true： @Metrics(logParameters &#x3D; false, logReturn &#x3D; false) &#x2F;&#x2F;改动点1 public class MetricsController &#123; @Service @Slf4j public class UserService &#123; @Transactional @Metrics(ignoreException &#x3D; true) &#x2F;&#x2F;改动点2 public void createUser(UserEntity entity) &#123; ... 代码上线后发现日志量并没有减少，更要命的是事务回滚失效了，从输出看到最后查询到了名为test的用户： [17:01:16.549] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :75 ] - 【入参日志】调用 【class org.geekbang.time.commonmistakes.spring.demo2.MetricsController】【public int org.geekbang.time.commonmistakes.spring.demo2.MetricsController.transaction(java.lang.String)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 的参数是：【[&quot;test&quot;]】 [17:01:16.670] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :75 ] - 【入参日志】调用 【class org.geekbang.time.commonmistakes.spring.demo2.UserService】【public void org.geekbang.time.commonmistakes.spring.demo2.UserService.createUser(org.geekbang.time.commonmistakes.spring.demo2.UserEntity)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 的参数是：【[&#123;&quot;id&quot;:null,&quot;name&quot;:&quot;test&quot;&#125;]】 [17:01:16.885] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :86 ] - 【失败打点】调用 【class org.geekbang.time.commonmistakes.spring.demo2.UserService】【public void org.geekbang.time.commonmistakes.spring.demo2.UserService.createUser(org.geekbang.time.commonmistakes.spring.demo2.UserEntity)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 失败，耗时：211 ms [17:01:16.899] [http-nio-45678-exec-1] [ERROR] [o.g.t.c.spring.demo2.MetricsAspect :88 ] - 【异常日志】调用 【class org.geekbang.time.commonmistakes.spring.demo2.UserService】【public void org.geekbang.time.commonmistakes.spring.demo2.UserService.createUser(org.geekbang.time.commonmistakes.spring.demo2.UserEntity)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 出现异常！ java.lang.RuntimeException: invalid username! at org.geekbang.time.commonmistakes.spring.demo2.UserService.createUser(UserService.java:18) at org.geekbang.time.commonmistakes.spring.demo2.UserService$$FastClassBySpringCGLIB$$9eec91f.invoke(&lt;generated&gt;) [17:01:16.902] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :98 ] - 【出参日志】调用 【class org.geekbang.time.commonmistakes.spring.demo2.UserService】【public void org.geekbang.time.commonmistakes.spring.demo2.UserService.createUser(org.geekbang.time.commonmistakes.spring.demo2.UserEntity)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 的返回是：【null】 [17:01:17.466] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :83 ] - 【成功打点】调用 【class org.geekbang.time.commonmistakes.spring.demo2.MetricsController】【public int org.geekbang.time.commonmistakes.spring.demo2.MetricsController.transaction(java.lang.String)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 成功，耗时：915 ms [17:01:17.467] [http-nio-45678-exec-1] [INFO ] [o.g.t.c.spring.demo2.MetricsAspect :98 ] - 【出参日志】调用 【class org.geekbang.time.commonmistakes.spring.demo2.MetricsController】【public int org.geekbang.time.commonmistakes.spring.demo2.MetricsController.transaction(java.lang.String)】【http:&#x2F;&#x2F;localhost:45678&#x2F;metricstest&#x2F;transaction】 的返回是：【1】 在介绍数据库事务时，我们分析了Spring通过TransactionAspectSupport类实现事务。在invokeWithinTransaction方法中设置断点可以发现，在执行Service的createUser方法时，TransactionAspectSupport并没有捕获到异常，所以自然无法回滚事务。原因就是，异常被MetricsAspect吃掉了。 我们知道，切面本身是一个Bean，Spring对不同切面增强的执行顺序是由Bean优先级决定的，具体规则是： 入操作（Around（连接点执行前）、Before），切面优先级越高，越先执行。一个切面的入操作执行完，才轮到下一切面，所有切面入操作执行完，才开始执行连接点（方法）。 出操作（Around（连接点执行后）、After、AfterReturning、AfterThrowing），切面优先级越低，越先执行。一个切面的出操作执行完，才轮到下一切面，直到返回到调用点。 同一切面的Around比After、Before先执行。 对于Bean可以通过@Order注解来设置优先级，查看@Order注解和Ordered接口源码可以发现，默认情况下Bean的优先级为最低优先级，其值是Integer的最大值。其实，值越大优先级反而越低，这点比较反直觉： @Retention(RetentionPolicy.RUNTIME) @Target(&#123;ElementType.TYPE, ElementType.METHOD, ElementType.FIELD&#125;) @Documented public @interface Order &#123; int value() default Ordered.LOWEST_PRECEDENCE; &#125; public interface Ordered &#123; int HIGHEST_PRECEDENCE &#x3D; Integer.MIN_VALUE; int LOWEST_PRECEDENCE &#x3D; Integer.MAX_VALUE; int getOrder(); &#125; 我们再通过一个例子，来理解下增强的执行顺序。新建一个TestAspectWithOrder10切面，通过@Order注解设置优先级为10，在内部定义@Before、@After、@Around三类增强，三个增强的逻辑只是简单的日志输出，切点是TestController所有方法；然后再定义一个类似的TestAspectWithOrder20切面，设置优先级为20： @Aspect @Component @Order(10) @Slf4j public class TestAspectWithOrder10 &#123; @Before(&quot;execution(* org.geekbang.time.commonmistakes.springpart1.aopmetrics.TestController.*(..))&quot;) public void before(JoinPoint joinPoint) throws Throwable &#123; log.info(&quot;TestAspectWithOrder10 @Before&quot;); &#125; @After(&quot;execution(* org.geekbang.time.commonmistakes.springpart1.aopmetrics.TestController.*(..))&quot;) public void after(JoinPoint joinPoint) throws Throwable &#123; log.info(&quot;TestAspectWithOrder10 @After&quot;); &#125; @Around(&quot;execution(* org.geekbang.time.commonmistakes.springpart1.aopmetrics.TestController.*(..))&quot;) public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; log.info(&quot;TestAspectWithOrder10 @Around before&quot;); Object o &#x3D; pjp.proceed(); log.info(&quot;TestAspectWithOrder10 @Around after&quot;); return o; &#125; &#125; @Aspect @Component @Order(20) @Slf4j public class TestAspectWithOrder20 &#123; ... &#125; 调用TestController的方法后，通过日志输出可以看到，增强执行顺序符合切面执行顺序的三个规则： 因为Spring的事务管理也是基于AOP的，默认情况下优先级最低也就是会先执行出操作，但是自定义切面MetricsAspect也同样是最低优先级，这个时候就可能出现问题：如果出操作先执行捕获了异常，那么Spring的事务处理就会因为无法捕获到异常导致无法回滚事务。 解决方式是，明确MetricsAspect的优先级，可以设置为最高优先级，也就是最先执行入操作最后执行出操作： &#x2F;&#x2F;将MetricsAspect这个Bean的优先级设置为最高 @Order(Ordered.HIGHEST_PRECEDENCE) public class MetricsAspect &#123; ... &#125; 此外，我们要知道切入的连接点是方法，注解定义在类上是无法直接从方法上获取到注解的。修复方式是，改为优先从方法获取，如果获取不到再从类获取，如果还是获取不到再使用默认的注解： Metrics metrics &#x3D; signature.getMethod().getAnnotation(Metrics.class); if (metrics &#x3D;&#x3D; null) &#123; metrics &#x3D; signature.getMethod().getDeclaringClass().getAnnotation(Metrics.class); &#125; 经过这2处修改，事务终于又可以回滚了，并且Controller的监控日志也不再出现入参、出参信息。 我再总结下这个案例。利用反射+注解+Spring AOP实现统一的横切日志关注点时，我们遇到的Spring事务失效问题，是由自定义的切面执行顺序引起的。这也让我们认识到，因为Spring内部大量利用IoC和AOP实现了各种组件，当使用IoC和AOP时，一定要考虑是否会影响其他内部组件。 重点回顾今天，我通过2个案例和你分享了Spring IoC和AOP的基本概念，以及三个比较容易出错的点。 第一，让Spring容器管理对象，要考虑对象默认的Scope单例是否适合，对于有状态的类型，单例可能产生内存泄露问题。 第二，如果要为单例的Bean注入Prototype的Bean，绝不是仅仅修改Scope属性这么简单。由于单例的Bean在容器启动时就会完成一次性初始化。最简单的解决方案是，把Prototype的Bean设置为通过代理注入，也就是设置proxyMode属性为TARGET_CLASS。 第三，如果一组相同类型的Bean是有顺序的，需要明确使用@Order注解来设置顺序。你可以再回顾下，两个不同优先级切面中@Before、@After和@Around三种增强的执行顺序，是什么样的。 最后我要说的是，文内第二个案例是一个完整的统一日志监控案例，继续修改就可以实现一个完善的、生产级的方法调用监控平台。这些修改主要是两方面：把日志打点，改为对接Metrics监控系统；把各种功能的监控开关，从注解属性获取改为通过配置系统实时获取。 今天用到的代码，我都放在了GitHub上，你可以点击这个链接查看。 思考与讨论 除了通过@Autowired注入Bean外，还可以使用@Inject或@Resource来注入Bean。你知道这三种方式的区别是什么吗？ 当Bean产生循环依赖时，比如BeanA的构造方法依赖BeanB作为成员需要注入，BeanB也依赖BeanA，你觉得会出现什么问题呢？又有哪些解决方式呢？ 在下一讲中，我会继续与你探讨Spring核心的其他问题。我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。","categories":[],"tags":[]},{"title":"修改某个人在Git中所有提交记录的名字","slug":"2024/修改某个人在Git中所有提交记录的名字","date":"2024-04-17T00:00:00.000Z","updated":"2024-06-17T01:04:54.007Z","comments":true,"path":"2024/04/17/2024/修改某个人在Git中所有提交记录的名字/","link":"","permalink":"https://blog.ehzyil.xyz/2024/04/17/2024/%E4%BF%AE%E6%94%B9%E6%9F%90%E4%B8%AA%E4%BA%BA%E5%9C%A8Git%E4%B8%AD%E6%89%80%E6%9C%89%E6%8F%90%E4%BA%A4%E8%AE%B0%E5%BD%95%E7%9A%84%E5%90%8D%E5%AD%97/","excerpt":"","text":"确认本地全局邮箱&#x2F;用户名： 使用命令git config user.name和git config user.email查看当前设置的用户名和邮箱。如果需要修改，可以使用git config --global user.name &quot;新用户名&quot;和git config --global user.email &quot;新邮箱地址&quot;进行设置【3】。 使用git filter-branch命令： git filter-branch是一个强大的工具，可以用来修改历史记录中的信息。你可以通过编写一个shell脚本来执行这个命令，脚本中包含如下内容： #!&#x2F;bin&#x2F;sh git filter-branch --env-filter &#39; OLD_EMAIL&#x3D;&quot;旧邮箱地址&quot; CORRECT_NAME&#x3D;&quot;新名字&quot; CORRECT_EMAIL&#x3D;&quot;新邮箱地址&quot; if [ &quot;$GIT_COMMITTER_EMAIL&quot; &#x3D; &quot;$OLD_EMAIL&quot; ] then export GIT_COMMITTER_NAME&#x3D;&quot;$CORRECT_NAME&quot; export GIT_COMMITTER_EMAIL&#x3D;&quot;$CORRECT_EMAIL&quot; fi if [ &quot;$GIT_AUTHOR_EMAIL&quot; &#x3D; &quot;$OLD_EMAIL&quot; ] then export GIT_AUTHOR_NAME&#x3D;&quot;$CORRECT_NAME&quot; export GIT_AUTHOR_EMAIL&#x3D;&quot;$CORRECT_EMAIL&quot; fi &#39; --tag-name-filter cat -- --branches --tags 这个脚本会遍历所有的提交记录，查找提交者邮箱为旧邮箱地址的记录，并将其中的用户名和邮箱修改为新名字和新邮箱地址【3】【2】。 执行脚本： 保存上述脚本为一个.sh文件，并在Git Bash中执行它。执行时间可能会较长，特别是当提交记录很多时【3】。 推送修改到远程仓库： references: 检索 如何修改Git提交历史中的author，email和name等信息 - 知乎(https://zhuanlan.zhihu.com/p/455741996) … 检索 修改Git Commit提交记录的用户名Name和邮箱Email …(https://www.cnblogs.com/schips/p/change_git_commit_user_info.html) … 检索 Git 修改历史 commits 中的用户名和邮箱 - lelelong - 博客园(https://www.cnblogs.com/longjunhao/p/15005916.html) ……","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"git","slug":"git","permalink":"https://blog.ehzyil.xyz/tags/git/"},{"name":"踩坑","slug":"踩坑","permalink":"https://blog.ehzyil.xyz/tags/%E8%B8%A9%E5%9D%91/"}],"author":"ehzyil"},{"title":"git同时连接gitlab和github","slug":"2024/git同时连接gitlab和github","date":"2024-04-01T00:00:00.000Z","updated":"2024-06-17T01:04:54.007Z","comments":true,"path":"2024/04/01/2024/git同时连接gitlab和github/","link":"","permalink":"https://blog.ehzyil.xyz/2024/04/01/2024/git%E5%90%8C%E6%97%B6%E8%BF%9E%E6%8E%A5gitlab%E5%92%8Cgithub/","excerpt":"","text":"GIT同时连接gitlab和github1、生成秘钥公司的 Gitlab 生成一个 SSH-Key # 在~&#x2F;.ssh&#x2F;目录会生成id-rsa_lab和id-rsa_lab.pub私钥和公钥。 $ ssh-keygen -t rsa -C &quot;注册的gitlab邮箱&quot; -f ~&#x2F;.ssh&#x2F;id_rsa_lab 公网 Github 生成一个 SSH-Key # 在~&#x2F;.ssh&#x2F;目录会生成id_rsa_hub和id_rsa_hub.pub私钥和公钥。 $ ssh-keygen -t rsa -C &quot;注册的github邮箱&quot; -f ~&#x2F;.ssh&#x2F;id_rsa_hub 2、添加 config在~&#x2F;.ssh 下添加 config 配置文件, 内容如下： Host github.com User git Hostname ssh.github.com PreferredAuthentications publickey IdentityFile ~&#x2F;.ssh&#x2F;id_rsa_hub Port 443 Host git.abc.com.cn Hostname git.abc.com.cn User git Port 443 PreferredAuthentications publickey IdentityFile ~&#x2F;.ssh&#x2F;id_rsa_lab Host行指定了主机配置的开始。它定义了 SSH 客户端连接到特定远程服务器时的设置。 Port 选项指定了连接到远程服务器的端口号。默认情况下，SSH 使用端口 22，可不配置。 User 选项指定了在远程服务器上使用的用户名，可不配置。 HostName 选项指定了远程服务器的主机名或 IP 地址。 PreferredAuthentications 选项指定了 SSH 客户端连接到远程服务器时首选的身份验证方法。publickey 表示使用公钥身份验证。 IdentityFile 选项指定了包含用于公钥身份验证的私钥文件的路径。 若不确定config内的内容可以去找一个project复制clone的信息，如下@后的即为Host，一开始配置的是gitlab git@git.abc.com.cn:cpb&#x2F;cpb7&#x2F;eps-rhc.git 3、将公钥添加到 gitlab 服务器和 github 服务器进入生成的 ssh 目录 : C:\\Users\\你的用户名\\.ssh中, 使用Git执行 cat ~/.ssh/id_rsa_lab.pub命令如下 ehZyiL@DESKTOP-1H567GC MINGW64 ~&#x2F;.ssh $ cat ~&#x2F;.ssh&#x2F;id_rsa_lab.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9iuMY1dBZ1TqhjVQHDnf94dAwZqvFKAF2oLSxNuwKwuW6BmVycTKwYrYTWthfLL0HfBJ1LFiMIFpMBxPQzVW6HaH8rjC33N+uwRQPO7xZWtxs39sXHPAthBt3kFRIevnu1cBv85+SnUXydAtzjnNPWIhkB&#x2F;i3&#x2F;t7G6cy6PzPyLdQxDT2YaCUpUQ2H9+mgifGzd0rW3UQ8&#x2F;lhhW35kLM+J17yIq&#x2F;VNYUY1OEK1t2SMXXcTDe0bKwx8D8hLJdy6XtK4xypg71IMhIWpMpKYp6iwyWyEpZAkctuHPry4bIlnmdnxG3ssGJccZGFvwggSw&#x2F;LGOIPvyDezidaV1KYz6BYp2oQj1iNYui085lF2qbmHwqR0X8QKkdmKB2jMZKrTPEZG93d3gfeXjXTxUG&#x2F;w1OVUN+BRt+8kV+27RUMi5xmJIRJjfcOnPCynULfV+ujd4p3HBeAwnglQUjdLVGrY7k&#x2F;mq+VJXLEap8k6MPj&#x2F;28QsoCXFwYIcbck6aBCo2FQdz80&#x3D; xxxx@qq.com 登录 GitLab 或 GitHub，选择 Settings找到 SSH Keys，将上面输出的内容填写到文本域内并保存 github的流程相似。 4.测试是否配置成功ssh -T git@config配置中HostName的名称 在第一次测试时会显示如下内容 输入yes即可 C:\\Users\\ehZyiL\\.ssh&gt;ssh -T git@gitlab The authenticity of host &#39;git.abc.com.cn (193.193.193.206)&#39; can&#39;t be established. RSA key fingerprint is SHA256:LNrmtcJm70MXzUGdwS9&#x2F;Rvzoc7ki8PXqk2YTakkDQOc. Are you sure you want to continue connecting (yes&#x2F;no&#x2F;[fingerprint])? yes Warning: Permanently added &#39;git.abc.com.cn,193.193.193.206&#39; (RSA) to the list of known hosts. 测试gitlab C:\\Users\\ehZyiL\\.ssh&gt;ssh -T git@git.abc.com.cn Enter passphrase for key &#39;C:\\Users\\ehZyiL&#x2F;.ssh&#x2F;id_rsa_lab&#39;: Welcome to GitLab, @liyuanzhe! 测试github C:\\Users\\ehZyiL\\.ssh&gt;ssh -T git@github.com Hi ehZyiL! You&#39;ve successfully authenticated, but GitHub does not provide shell access. references: - GIT同时连接gitlab和github - 简书 - Generating a new SSH key and adding it to the ssh-agent - GitHub Docs","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"git","slug":"git","permalink":"https://blog.ehzyil.xyz/tags/git/"}],"author":"ehzyil"},{"title":"Linux下如何查看CPU使用率","slug":"2024/Linux下如何查看CPU使用率","date":"2024-03-22T00:00:00.000Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/03/22/2024/Linux下如何查看CPU使用率/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/22/2024/Linux%E4%B8%8B%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8BCPU%E4%BD%BF%E7%94%A8%E7%8E%87/","excerpt":"","text":"Linux下如何查看CPU使用率在Linux环境下，我们有多种命令和工具可以用于查看CPU使用率和系统资源使用情况。以下是几种常用方法： 一、使用top命令查看Linux资源占用情况top命令可以实时动态地查看运行中的系统状态，包括系统任务、系统负载、运行队列、活动进程等信息。它是Linux下常用的查看cpu使用情况的命令。使用时，在终端里输入top，然后按回车，就可以看到CPU、内存、任务等资源占用情况。 列名 含义 PID 进程标识号 USER 进程所有者 PR 进程优先级 NI 进程优先级别数值 VIRT 进程占用的虚拟内存值 RES 进程占用的物理内存值 SHR 进程使用的共享内存值 S 进程的状态，其中S表示休眠，R表示正在运行，Z表示僵死状态 %CPU 进程占用的CPU使用率 %MEM 进程占用的物理内存百分比 TIME+ 进程启动后占用的总的CPU时间 Command 进程启动的启动命令名称 二、使用free命令查看内存使用情况 free命令可以查看和监控系统的物理内存、交换空间以及内核缓冲的使用情况。在终端输入free命令，可以看到总内存、已用内存、空闲内存等情况。 三、使用ps命令查看各进程CPU使用情况ps命令是Linux下最常用的查看进程状态的命令，它可以查看系统上所有的进程信息，包括进程的PID、TTY、时间、命令等。 ps ux ps -H -eo user,pid,ppid,tid,time,%cpu,cmd --sort&#x3D;%cpu 上述命令：第一条按默认方式查看状态，第二条命令指定显示列和排序方式，使用时任选其一。 使用ps命令查看各进程内存使用情况，你可以使用以下的命令： ps -eo pid,user,%mem,vsz,rss,cmd --sort=-%mem 这个命令的各个部分的含义是： ps：ps命令用于报告当前系统的进程状态。 -eo：选择输出的格式，后面跟着的是你想要输出的选项。 pid,user,%mem,vsz,rss,cmd：这里定义了你要输出的列。其中，“pid”显示进程ID，“user”显示用户名，“%mem”显示进程使用的物理内存的百分比，“vsz”显示虚拟内存的大小， “rss”显示驻留内存的大小，”cmd”则显示了执行命令的完整路径。 --sort=-%mem：这个选项让ps命令按照内存使用量的百分比降序排序输出的进程。 四、定位CPU高占用问题的方法如果系统的CPU使用率过高，会导致系统响应变慢，这时就需要找出并分析是哪些进程在占用过多的CPU资源。首先，我们可以用top命令或ps命令查看各进程的CPU使用情况，找出CPU使用率过高的那个进程，获取到该进程的PID。然后，我们可以使用以下命令查看该进程正在使用的文件： ll &#x2F;proc&#x2F;PID&#x2F;fd 其中，PID应该替换为实际查询到的高占用CPU的进程的编号。通过以上命令能找到高占用CPU的进程与其调用的文件之间的关系，接下来就是分析这些文件，找出问题的所在。 以上就是在Linux下查看CPU使用率以及处理CPU占用问题的一些基本方法，对于Linux用户和开发者来说，这些命令和工具的使用技巧是需要掌握的基本技能。希望本文能对你有所帮助。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ehzyil.xyz/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ehzyil.xyz/tags/Linux/"}],"author":"ehzyil"},{"title":"Oracle数据库使用记录","slug":"2024/Oracle数据库使用记录","date":"2024-03-22T00:00:00.000Z","updated":"2024-06-17T01:04:54.003Z","comments":true,"path":"2024/03/22/2024/Oracle数据库使用记录/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/22/2024/Oracle%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/","excerpt":"","text":"Oracle数据库使用记录一、Oracle忘记用户名和密码 1、打开命令提示符，输入命令sqlplus ,进入oracle控制台 2、用户名输入 sqlplus &#x2F; as sysdba，口令：空（回车即可） 3、连接成功后，输入“select username from dba_users”查看用户列表 4、若修改某一个用户密码， 修改用户口令 格式为（注意后面的分号；）： alter user 用户名 identified by 新密码； 以system 为例，密码修改为 123456. 可输入 alter user system identified by 123456; 例如 sqlplus / as sysdba ---------sysdba为超级用户 alter user lxemr account unlock; --------- 解除锁定(必须带“;”号，注意用英文字符) alter user lxemr identified by lxemr12345; -------------修改密码 alter user 'WF_TRAIN' identified by WF_TRAIN; 二、Oracle 用户解锁 SQL&gt; ALTER USER 用户名 ACCOUNT UNLOCK; 三、账号解锁 Oracle 账号被锁定，显示错误 “ORA-28000: the account is locked”。 解决方案： 1. 登录数据库 sqlplus / as sysdba 使用 sysdba 权限登录数据库。 2. 查看用户列表 select username from dba_users; 这将显示数据库中所有用户的列表。 3. 解锁用户 ALTER USER USER_NAME ACCOUNT UNLOCK; 将 “USER_NAME” 替换为要解锁的用户的名称。 4. 修改登录尝试限制 alter profile default limit failed_login_attempts unlimited; 修改默认配置文件，允许无限次登录尝试。 四、IDEA&#x2F;Datagtrip连接Oracle先决条件： 已安装 IDEA 或 DataGrip 已安装 Oracle 数据库并配置了监听器(默认有配置) 已创建 Oracle 数据库用户和密码 步骤： 1. 创建数据源（IDEA） 打开 IDEA，转到 “Database” &gt; “Data Sources”。 单击 “加号” 图标 (+)。 选择 “Oracle” 作为数据库类型。 2. 配置连接参数 URL：输入 Oracle 数据库的 JDBC URL。格式为： jdbc:oracle:thin:@&lt;主机名&gt;:&lt;端口号&gt;&#x2F;&lt;服务名&gt; 例如： jdbc:oracle:thin:@localhost:1521&#x2F;XE 用户名：输入 Oracle 数据库用户的用户名。 密码：输入 Oracle 数据库用户的密码。 3. 获取 Oracle 数据库实例的 SID 使用 SQLPlus SQL> SELECT INSTANCE_NAME FROM V$INSTANCE; INSTANCE_NAME ---------------- orcl 解释： SQLPlus 是 Oracle 提供的命令行工具，用于与 Oracle 数据库交互。 SELECT INSTANCE_NAME FROM V$INSTANCE; 查询 V$INSTANCE 视图以获取当前数据库实例的名称。 INSTANCE_NAME 列显示数据库实例的名称，在本例中为 “orcl”。 4. 填写用户名密码和端口 填充后的 URL 显示如下： jdbc:oracle:thin:@localhost:1521:orcl 解释： jdbc:oracle:thin: 是 Oracle JDBC 驱动程序的 URL 前缀。 @localhost 是数据库服务器的主机名或 IP 地址。 1521 是数据库服务器的监听器端口。 orcl 是数据库实例的 SID，在前面的步骤中已获取。 5. 测试连接 单击 “Test Connection” 按钮。 如果连接成功，您将看到 “Connection successful” 消息。 五.oracle导入.dmp文件1.使用 Oracle imp 实用程序导入 .dmp 文件的步骤： 确保您具有导入到目标数据库的适当权限（DBA）。 打开命令提示符或终端窗口。 导航到包含 .dmp 文件的目录。 使用以下语法运行 imp 命令： imp username&#x2F;password@database_name file&#x3D;path&#x2F;to&#x2F;dump_file.dmp [options] 选项： FULL&#x3D;Y：导入所有对象，包括用户、表、索引和约束。 FROMUSER&#x2F;TOUSER：指定要导入对象的源用户和目标用户。 TABLES：指定要导入的特定表列表。 IGNORE&#x3D;Y：忽略导入期间遇到的错误。 LOG&#x3D;path&#x2F;to&#x2F;log_file.log：将导入日志写入指定的文件。 2.示例：假设您要将名为 my_dump.dmp 的文件导入到名为 new_db 的数据库中，并且您希望将对象导入到名为 scott 的用户中，可以使用以下命令： imp scott&#x2F;tiger@new_db file&#x3D;my_dump.dmp fromuser&#x3D;scott touser&#x3D;scott 注意事项： 如果导出的文件包含由其他用户创建的对象，则您需要使用 FROMUSER/TOUSER 选项指定目标用户。 如果您使用 FULL=Y 选项，则导入将覆盖目标数据库中的现有对象。 导入过程可能需要一段时间，具体取决于导出的文件大小和目标数据库的性能。 导入完成后，您可以使用以下查询检查导入的对象： SELECT * FROM dba_objects WHERE owner &#x3D; &#39;目标用户&#39;; 3.导入过程第一句是从网上复制的没修改账号密码因此又输入了一遍，17行错误表明在使用 imp 实用程序导入 .dmp 文件时未指定必需的参数。 imp prophet&#x2F;prophet@orcl file&#x3D;C:\\Users\\ehZyiL\\Desktop\\train20201130.dmp Import: Release 11.2.0.4.0 - Production on 星期二 3月 12 20:24:17 2024 Copyright (c) 1982, 2011, Oracle and&#x2F;or its affiliates. All rights reserved. IMP-00058: 遇到 ORACLE 错误 1017 ORA-01017: invalid username&#x2F;password; logon denied用户名: scott 口令: 连接到: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit Production With the Partitioning, OLAP, Data Mining and Real Application Testing options 经由常规路径由 EXPORT:V11.02.00 创建的导出文件 警告: 这些对象由 TRAIN 导出, 而不是当前用户 已经完成 ZHS16GBK 字符集和 AL16UTF16 NCHAR 字符集中的导入 IMP-00031: 必须指定 FULL&#x3D;Y 或提供 FROMUSER&#x2F;TOUSER 或 TABLES 参数 IMP-00000: 未成功终止导入 在指定导入参数后正常导入了 C:\\Users\\ehZyiL&gt;imp prophet&#x2F;prophet@orcl file&#x3D;C:\\Users\\ehZyiL\\Desktop\\train20201130.dmp full&#x3D;y Import: Release 11.2.0.4.0 - Production on 星期二 3月 12 20:25:52 2024 Copyright (c) 1982, 2011, Oracle and&#x2F;or its affiliates. All rights reserved. IMP-00058: 遇到 ORACLE 错误 1017 ORA-01017: invalid username&#x2F;password; logon denied用户名: scott 口令: 连接到: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit Production With the Partitioning, OLAP, Data Mining and Real Application Testing options 经由常规路径由 EXPORT:V11.02.00 创建的导出文件 警告: 这些对象由 TRAIN 导出, 而不是当前用户 已经完成 ZHS16GBK 字符集和 AL16UTF16 NCHAR 字符集中的导入 . 正在将 TRAIN 的对象导入到 SCOTT https://blog.csdn.net/weixin_45740811/article/details/125841798 六、新建数据库使用 Oracle 命令行界面 (CLI) 创建 WF-TRAIN 数据库 打开命令提示符或终端窗口。 连接到数据库服务器。 运行以下命令： CREATE DATABASE WF-TRAIN GLOBAL UNDO ON &lt;disk_group&gt; DATAFILE &#39;&lt;datafile_location&gt;&#39; SIZE &lt;size_in_MB&gt; LOGFILE GROUP &lt;log_group_name&gt; (&#39;&lt;log_file_location&gt;&#39; SIZE &lt;size_in_MB&gt;) CHARACTER SET &lt;character_set&gt; TEMPLATE &lt;template_name&gt; 示例： CREATE DATABASE WF-TRAIN GLOBAL UNDO ON DATA DATAFILE &#39;&#x2F;data&#x2F;wf_train_datafile.dbf&#39; SIZE 100M LOGFILE GROUP wf_train_log_group (&#39;&#x2F;data&#x2F;wf_train_log_file.dbf&#39; SIZE 20M) CHARACTER SET UTF8 TEMPLATE basic 按 Enter 键。 注意： 替换 &lt;disk_group&gt;、&lt;datafile_location&gt;、&lt;log_group_name&gt;、&lt;log_file_location&gt;、&lt;character_set&gt; 和 &lt;template_name&gt; 处的占位符。 确保您具有创建数据库所需的权限。 六点五、从建库到导入数据库 查询远程数据库的表空间：使用以下查询语句来查询远程数据库中的表空间：select * from dba_users where username='XXX' 创建本地表空间：使用以下命令来创建一个名为”data”的本地表空间，并指定数据文件路径和大小：create tablespace data datafile 'D:\\ORCALE\\ORADATA\\ORCL\\data.DBF' size 200M autoextend on next 5M maxsize unlimited; 然后，使用下面的命令将表空间的数据文件设置为自动扩展：alter database datafile 'D:\\ORCALE\\ORADATA\\ORCL\\data.DBF' autoextend on next 5m maxsize unlimited; 最后，使用下面的命令为”USERS”表空间添加一个数据文件，大小设置为50M：alter tablespace USERS add datafile 'D:\\ORCALE\\ORADATA\\ORCL\\data.DBF' size 50m 创建本地用户并关联表空间：使用以下命令创建一个名为”USER”的本地用户，并将其默认表空间设置为”data”：create user USER identified by PWD default tablespace data; 对用户进行赋予权限：使用以下命令为”USER”用户授予DBA、CONNECT和RESOURCE权限：grant dba, connect, resource to USER; 将数据导入本地磁盘：使用以下命令将数据从远程数据库导出到本地磁盘：exp USER/PWD@193.193.193.41:1521/orcl file=d:\\XXX.dmp owner=GGZYXHX 将数据导入数据库中：使用以下命令将数据从导出文件中导入到本地数据库中：imp USER/PWD@orcl file=D:\\XXX.dmp fromuser=GGZYXBASE touser=GGZYXBASE imp USER/PWD@192.168.200.30/orcl file=D:\\XXX.dmp full=y 七、处理’ORA-01950: 对表空间 ‘USERS’ 无权限 当我们使用任何形式的数据库时，遇到错误是不可避免的。Oracle数据库常见的一个错误是ORA-01950: 对表空间 &#39;USERS&#39; 无权限。 理解ORA-01950：造成ORA-01950的原因 ORA-01950错误是一个标志，表示Oracle模式没有足够的权限对表空间执行特定的操作。Oracle通过特定的安全层次结构指定权限，用户配额管理是其中的一个组成部分。 以SYSDBA权限登录通过以SYSDBA权限登录Oracle，你可以获得允许你执行高级操作的管理权限。 sqlplus / as sysdba “&#x2F; as sysdba”命令不需要任何密码，允许你直接以SYSDBA权限访问Oracle。 更改用户配额一旦登录，你可以使用以下命令更改用户配额： SQL> alter user \"WF-TRAIN\" quota unlimited on \"USERS\"; 这意味着你正在给WF-TRAIN用户在USERS表空间上授予无限的空间。 更改成功正确运行命令后，Oracle将返回： 用户已更改。 这个响应表示配额分配成功。 总结 什么是ORA-01950错误？ 当用户在表空间没有足够的权限时，将触发ORA-01950错误。 什么导致了’ORA-01950: 对表空间 ‘USERS’ 无权限’错误？ 当用户在表空间没有足够的空间时，会出现此错误。 八、如何关闭本地启动的 Oraclesqlplus &#x2F; as sysdba SHUTDOWN IMMEDIATE 九、设置 Oracle 数据库字符集为 UTF-8以下是在 Oracle 数据库中设置字符集为 UTF-8 的步骤： 查看当前字符集 首先，你需要查看 Oracle 数据库当前的字符集。通过以下 SQL 语句可以查询： SELECT parameter, value FROM nls_database_parameters WHERE parameter = 'NLS_CHARACTERSET'; 停止数据库 在修改字符集之前，需要先停止 Oracle 数据库： sqlplus / as sysdba SHUTDOWN IMMEDIATE; 启动数据库到挂起状态 启动数据库到挂起状态（MOUNT）： STARTUP MOUNT; 修改字符集 使用以下命令将字符集更改为 UTF-8（AL32UTF8）： ALTER SYSTEM ENABLE RESTRICTED SESSION; ALTER SYSTEM SET job_queue_processes=0; ALTER SYSTEM SET aq_tm_processes=0; ALTER DATABASE OPEN; ALTER DATABASE CHARACTER SET AL32UTF8; 注意: 上述命令中的 ALTER DATABASE CHARACTER SET AL32UTF8 通常只允许从相容的字符集更改。如果你需要强制更改字符集，需要使用以下命令： ALTER DATABASE CHARACTER SET INTERNAL_USE AL32UTF8; 但请谨慎使用 INTERNAL_USE，因为它会绕过字符集的兼容性检查，可能导致数据损坏。因此，在操作前，请务必备份数据库。5. 重启数据库 字符集更改后，关闭数据库并重新启动。 SHUTDOWN IMMEDIATE; STARTUP; 验证字符集 再次查询字符集，以确保更改已经生效： SELECT parameter, value FROM nls_database_parameters WHERE parameter = 'NLS_CHARACTERSET'; 通过以上步骤，你可以将 Oracle 数据库的字符集设置为 UTF-8（AL32UTF8）。","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ehzyil.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Oracle","slug":"Oracle","permalink":"https://blog.ehzyil.xyz/tags/Oracle/"}],"author":"ehzyil"},{"title":"PostgreSQL 使用记录","slug":"2024/PostgreSQL 使用记录","date":"2024-03-22T00:00:00.000Z","updated":"2024-06-17T01:04:54.003Z","comments":true,"path":"2024/03/22/2024/PostgreSQL 使用记录/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/22/2024/PostgreSQL%20%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/","excerpt":"","text":"PostgreSQL 使用记录Docker安装PostgreSQL安装 Docker 访问 Docker 官方网站：https://docs.docker.com/desktop/ 拉取 PostgreSQL 镜像 打开终端或命令提示符。 运行以下命令拉取 PostgreSQL 镜像： docker pull postgres:13.0 创建 PostgreSQL 容器 运行以下命令创建 PostgreSQL 容器： docker run --name postgres_db -e POSTGRES_PASSWORD&#x3D;postgresql -e POSTGRES_USER&#x3D;ehzyil -e POSTGRES_DB&#x3D;freshrss -v &#x2F;data&#x2F;postgresql:&#x2F;var&#x2F;lib&#x2F;postgresql&#x2F;data -p 5433:5432 -d postgres --name postgres_db：将创建的容器命名为 postgres_db。 -e POSTGRES_PASSWORD=postgresql：设置环境变量 POSTGRES_PASSWORD，该环境变量定义了PostgreSQL数据库的超级用户密码，这里设置为 postgresql。 -e POSTGRES_USER=ehzyil：设置环境变量 POSTGRES_USER，以创建具有超级用户权限的新用户，这里用户名设为 ehzyil。 -e POSTGRES_DB=freshrss：设置环境变量 POSTGRES_DB，以创建一个名为 freshrss 的新数据库。 -v /data/postgresql:/var/lib/postgresql/data：将宿主机的 /data/postgresql 目录挂载到容器内的 /var/lib/postgresql/data 目录。这样做可以保证数据库的数据即使在容器停止后也能够持久化。 -p 5432:5432：将宿主机的 5432 端口映射到容器内的 5432 端口，允许您从宿主机的该端口连接到PostgreSQL服务。 -d postgres：以守护进程模式在后台启动 postgres 镜像的容器。 连接到容器 运行以下命令连接到容器： docker exec -it postgres_db psql -U ehzyil -d freshrss docker exec -it some-postgres：docker exec 命令允许您在运行中的容器里执行命令，-it 参数让您可以交互式地使用容器的命令行接口。 psql：是PostgreSQL的命令行工具。 -U ehzyil：-U 参数用于指定要以哪个用户身份登录，这里您应使用您创建容器时设置的用户ehzyil。 -d freshrss：-d 参数用于指定要连接的数据库, 这里是您创建的数据库freshrss。 如果您的宿主机上安装有psql客户端或其他数据库管理工具，您也可以直接使用本机的管理工具连接到容器的数据库。假设您的容器正运行在同一台宿主机上，您可以使用如下命令： 已连接到容器，可以使用 psql 命令连接到 PostgreSQL 数据库： psql -U postgres 执行 SQL 查询，例如： select now(); 创建新的数据库用户 进入数据库命令行 首先，以 PostgreSQL 用户身份登录，然后使用 psql 命令进入数据库命令行： sudo su postgres psql 创建新用户 使用以下命令创建新的数据库用户 dbuser，并设置密码 &lt;CUSTOM PASSWORD&gt;： CREATE USER dbuser WITH PASSWORD &#39;&lt;CUSTOM PASSWORD&gt;&#39;; 创建数据库 创建名为 exampledb 的新数据库，并将其所有者设置为 dbuser： CREATE DATABASE exampledb OWNER dbuser; 授予数据库权限 将 exampledb 数据库的所有权限授予 dbuser： GRANT ALL PRIVILEGES ON DATABASE exampledb TO dbuser; 授予表权限（可选） 如果需要授予 dbuser 对特定表的读写权限，请使用以下命令： GRANT ALL PRIVILEGES ON TABLE mytable TO dbuser; 授予当前库所有表的权限 GRANT ALL PRIVILEGES ON all tables in schema public TO dbuser; 注意： 授予权限的命令必须在要操作的数据库中执行。 授予所有权限时，请谨慎操作，因为它会授予用户对数据库的完全控制权。 对于生产环境，建议使用更细粒度的权限授予策略。 6.删除角色 drop role memos; # 删除角色 删除数据库 查找正在使用数据库的会话：首先，找出哪些会话正在使用您想要删除的数据库。 SELECT pid, usename, datname, query, state FROM pg_stat_activity WHERE datname = &#39;memos&#39;; 这将列出所有正在使用 memos 数据库的会话及其详细信息。 终止会话：知道了这些会话的 pid（即进程 ID），可以使用 pg_terminate_backend 函数来终止它们： SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = &#39;memos&#39;; 这将终止所有正在使用 memos 数据库的会话。 删除数据库：在所有相关会话被终止后，能够成功删除数据库： DROP DATABASE memos; 如何切换用户 在PostgreSQL数据库中，切换用户通常指的是以不同的用户身份登录数据库。这可以通过以下几种方式实现： 使用命令行或终端： 如果你已经以某个用户身份登录了PostgreSQL，并且想要切换到另一个用户，你可以退出当前会话，然后使用新用户的凭据重新登录。例如： psql -U 新用户名 -d 数据库名 其中，新用户名是你想要切换到的用户名称，数据库名是你想要连接的数据库。 在psql会话中： 如果你已经通过psql命令行工具登录到PostgreSQL，你可以使用\\c命令来切换数据库和用户。例如： \\c 数据库名 新用户名 这将切换到指定的数据库，并尝试以新用户名登录。如果该用户没有访问该数据库的权限，切换将失败。","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.ehzyil.xyz/tags/Docker/"},{"name":"数据库","slug":"数据库","permalink":"https://blog.ehzyil.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://blog.ehzyil.xyz/tags/PostgreSQL/"}],"author":"ehzyil"},{"title":"Prompt Summarize","slug":"2024/Prompt  Summarize","date":"2024-03-22T00:00:00.000Z","updated":"2024-06-17T01:04:54.003Z","comments":true,"path":"2024/03/22/2024/Prompt  Summarize/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/22/2024/Prompt%20%20Summarize/","excerpt":"","text":"Prompt SummarizeB站视频分章节You are a helpful assistant that summarize key points of video subtitle. Summarize 3 to 8 brief key points in language &#39;中文简体&#39;. Answer in markdown json format. The emoji should be related to the key point and 1 char length. example output format: &#96;json [ &#123; &quot;time&quot;: &quot;03:00&quot;, &quot;emoji&quot;: &quot;👍&quot;, &quot;key&quot;: &quot;key point 1&quot; &#125;, &#123; &quot;time&quot;: &quot;10:05&quot;, &quot;emoji&quot;: &quot;😊&quot;, &quot;key&quot;: &quot;key point 2&quot; &#125; ] &#96; 对视频进行摘要总结You are a helpful assistant that summarize video subtitle. Summarize in language &#39;中文简体&#39;. Answer in markdown json format. example output format: &#96;json &#123; &quot;summary&quot;: &quot;brief summary&quot; &#125; &#96; 视频的主要观点You are a helpful assistant that summarize key points of video subtitle. Summarize brief key points in language &#39;中文简体&#39;. Answer in markdown json format. example output format: &#96;json [ &quot;key point 1&quot;, &quot;key point 2&quot; ] &#96;","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"Prompt","slug":"Prompt","permalink":"https://blog.ehzyil.xyz/tags/Prompt/"}],"author":"ehzyil"},{"title":"Git连接超时","slug":"2024/Git问题：解决“sshconnect to host github.com port 22 Connection timed out”","date":"2024-03-11T20:22:46.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/03/11/2024/Git问题：解决“sshconnect to host github.com port 22 Connection timed out”/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/11/2024/Git%E9%97%AE%E9%A2%98%EF%BC%9A%E8%A7%A3%E5%86%B3%E2%80%9Csshconnect%20to%20host%20github.com%20port%2022%20Connection%20timed%20out%E2%80%9D/","excerpt":"","text":"Git问题：解决“sshconnect to host github.com port 22 Connection timed out”在写毕设的时候提交代码发现报错，查了查以为公司网禁不能提交代码，后来发现用自己热点也提交不了，于是继续在网上寻找解决方案。 Push failed ssh: connect to host github.com port 22: Connection timed out Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 问题原因可能是由于电脑的防火墙或者其他网络原因导致ssh连接方式端口22被封锁。 解决方案因为端口被封锁，就换成443端口 操作方法： 1.进入~&#x2F;.ssh下 cd ~/.ssh 2.创建一个config文件 3.编辑文件内容： Host github.com User git Hostname ssh.github.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa Port 443 Host gitlab.com Hostname altssh.gitlab.com User git Port 443 PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa 4.保存退出 5.检查是否成功ssh -T git@github.com ehZyiL@DESKTOP-1H567GC MINGW64 ~/.ssh $ ssh -T git@github.com Hi ehZyiL! You've successfully authenticated, but GitHub does not provide shell access.","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://blog.ehzyil.xyz/tags/Git/"}],"author":"ehzyil"},{"title":"MySQL开启安全模式产生的错误","slug":"2024/MySQL开启安全模式产生的错误","date":"2024-03-11T20:22:46.000Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/03/11/2024/MySQL开启安全模式产生的错误/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/11/2024/MySQL%E5%BC%80%E5%90%AF%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%E4%BA%A7%E7%94%9F%E7%9A%84%E9%94%99%E8%AF%AF/","excerpt":"","text":"MySQL错误：ERROR 1175: You are using safe update mode解决方法Error updating database. Cause: java.sql.SQLException: You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column ### The error may involve UserMapper.unCollectStaffTeam-Inline ### The error occurred while setting parameters ### SQL: delete from t_qm_staff_collect WHERE STAFF_ID &#x3D; ? AND ORGA_ID in ( ? ) ### Cause: java.sql.SQLException: You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column ; uncategorized SQLException for SQL []; SQL state [HY000]; error code [1175]; You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column; nested exception is java.sql.SQLException: You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column 错误信息： 你正在使用安全更新模式，你试图更新一个没有使用 KEY 列的 WHERE 子句的表 错误详情： 错误信息： 你正在使用安全更新模式，你试图更新一个没有使用 KEY 列的 WHERE 子句的表 SQL 状态： HY000 错误代码： 1175 原因： mysql有个叫SQL_SAFE_UPDATES的变量，为了数据库更新操作的安全性，此值默认为1或ON，所以才会出现更新失败的情况。 当你尝试在 MySQL 数据库上执行一个没有使用 WHERE 子句指定要更新哪些行的更新语句时，就会发生此错误。 查看设置： mysql&gt; show variables like &#39;sql_safe%&#39;; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | sql_safe_updates | ON | +------------------+-------+ 下面是SQL_SAFE_UPDATES变量为0和1时的取值说明： SQL_SAFE_UPDATES有两个取值0和1， 或ON 和OFF; SQL_SAFE_UPDATES &#x3D; 1，ON时，不带where和limit条件的update和delete操作语句是无法执行的，即使是有where和limit条件但不带key column的update和delete也不能执行。 SQL_SAFE_UPDATES &#x3D;0，OFF时，update和delete操作将会顺利执行。那么很显然，此变量的默认值是1。 所以，出现1175错误的时候，可以先设置SQL_SAFE_UPDATES的值为0 OFF，然后再执行更新; 以下2条命令都可以修改配置； mysql&gt; set sql_safe_updates&#x3D;0; mysql&gt; set sql_safe_updates&#x3D;off; 修改后发现并没有用。 或者修改的配置并没有生效。 解决方案： 原来的逻辑是查询符合条件的直接删除，sql语句： &lt;delete id&#x3D;&quot;unCollectstaffTeam&quot; parameterType&#x3D;&quot;java.util.Map&quot;&gt; &lt;where&gt; AND STAFF_ID &#x3D; #&#123;staffId&#125; AND ID IS NOT NULL AND ORGA_ID IN &lt;foreach item&#x3D;&quot;item&quot; index&#x3D;&quot;index&quot; collection&#x3D;&quot;orgIds&quot; open&#x3D;&quot;(&quot; separator&#x3D;&quot; &quot; close&#x3D;&quot;)&quot;&gt; #&#123;item&#125; &lt;&#x2F;foreach&gt; &lt;&#x2F;where&gt; &lt;&#x2F;delete&gt; 修改后的逻辑先查询符合条件的id，再根据id进行删除，sql语句： &lt;delete id&#x3D;&quot;unCollectStaffTeam&quot; parameterType&#x3D;&quot;java.util.Map&quot;&gt; &lt;where&gt; ID in &lt;foreach item&#x3D;&quot;item&quot; index&#x3D;&quot;index&quot; collection&#x3D;&quot;queryCollectStaffTeamIds&quot; open&#x3D;&quot;(&quot; separator&#x3D;&quot;,&quot; close&#x3D;&quot;)&quot;&gt; #&#123;item&#125; &lt;&#x2F;foreach&gt; &lt;&#x2F;where&gt; &lt;&#x2F;delete&gt; &lt;select id&#x3D;&quot;queryCollectStaffTeamIds&quot; parameterType&#x3D;&quot;java.util.Map&quot; resultType&#x3D;&quot;java.util.Map&quot;&gt; &lt;where&gt; AND STAFF_ID &#x3D; #&#123;staffId&#125; AND ORGA_ID in &lt;foreach item&#x3D;&quot;item&quot; index&#x3D;&quot;index&quot; collection&#x3D;&quot;orgIds&quot; open&#x3D;&quot;(&quot; separator&#x3D;&quot;,&quot; close&#x3D;&quot;)&quot;&gt; #&#123;item&#125; &lt;&#x2F;foreach&gt; &lt;&#x2F;where&gt; &lt;&#x2F;select&gt;","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"}],"author":"ehzyil"},{"title":"MyBatis test表达式中判断相等无效","slug":"2024/MyBatis test表达式中判断相等无效","date":"2024-03-11T20:22:46.000Z","updated":"2024-06-17T01:04:53.999Z","comments":true,"path":"2024/03/11/2024/MyBatis test表达式中判断相等无效/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/11/2024/MyBatis%20test%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD%E5%88%A4%E6%96%AD%E7%9B%B8%E7%AD%89%E6%97%A0%E6%95%88/","excerpt":"","text":"现象 想通过传入一个remarkflag标志，当remarkflag为”0”时，查询数据库中remark列为空字符串的行，当remarkflag为”1”时，查询数据库中某字段为非空字符串的行。代码如下： &lt;if test&#x3D;&quot;remarkflag !&#x3D; &#39;&#39; and remarkflag &#x3D;&#x3D; &#39;0&#39;&quot;&gt; AND remark &#x3D;&#39;&#39; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;remarkflag !&#x3D; &#39;&#39; and remarkflag &#x3D;&#x3D; &#39;1&#39;&quot;&gt; AND remark !&#x3D;&#39;&#39; &lt;&#x2F;if&gt; 结果不论remarkflag设为”0”还是”1”，上面两个判断均不成立。 问题处理 查了许多文档，发现当判断的单引号内只有一个字符时，会被识别为Java语言中的char类型，上述remarkflag&#x3D;&#x3D;‘0’，左边是字符串，右边是字符型，所以不成立，同理remarkflag &#x3D;&#x3D; ‘1’也不成立。 解决 强制转换下类型后，问题解决： &lt;if test&#x3D;&quot;remarkflag !&#x3D; &#39;&#39; and remarkflag &#x3D;&#x3D; &#39;0&#39;.toString()&quot;&gt; AND remark &#x3D;&#39;&#39; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;remarkflag !&#x3D; &#39;&#39; and remarkflag &#x3D;&#x3D; &#39;1&#39;.toString()&quot;&gt; AND remark !&#x3D;&#39;&#39; &lt;&#x2F;if&gt;","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://blog.ehzyil.xyz/tags/Mybatis/"}],"author":"ehzyil"},{"title":"Spring、SpringBoot常用扩展特性之事件驱动","slug":"2024/Spring、SpringBoot 常用扩展特性 之 事件驱动","date":"2024-03-11T20:22:46.000Z","updated":"2024-06-17T01:04:54.003Z","comments":true,"path":"2024/03/11/2024/Spring、SpringBoot 常用扩展特性 之 事件驱动/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/11/2024/Spring%E3%80%81SpringBoot%20%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E7%89%B9%E6%80%A7%20%E4%B9%8B%20%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8/","excerpt":"","text":"Spring、SpringBoot常用扩展特性之事件驱动 事件驱动:即跟随当前时间点上出现的事件，调动可用资源，执行相关任务，使不断出现的问题得以解决，防止事务堆积。 如:注册账户时会收到短信验证码，火车发车前收到提醒、预定酒店后收到短信通知等。 如:浏览器中点击按钮请求后台，鼠标点击变化内容，键盘输入显示数据、服务接收请求后分发请求等在解决上述问题时，应用程序是由“事件”驱动运行的，这类程序在编写时往往可以采用相同的模型实现，我们可以将这种编程模型称为事件驱动模型。 (PS:事件驱动模型其实是一种抽象模型，用于对由外部事件驱动系统业务逻辑这类应用程序进行建模。) 一.事件驱动模型1.实现的思路 事件驱动模型有很多种体现形式，如简单的事件触发机制、单线程异步任务、多线程异步任务等，但是各种技术中实现事件驱动模型的基本思路相同。 事件驱动模型包括四个(三个)基本要寨:事件、事件消费方、事件生产方、[事件管理器] 事件: 描述发生的事情。比如说浏览器页面点击事件，鼠标、键盘输入事件，spring 请求处理完成、Spring容器刷新完毕等。 事件生产方(事件源):事件的产生者，任何一个事件都必须有一个事件源。比如input、button，Spring中请求处理完成的事件源就是 DispatcherServlet、Spring容器刷新完毕的事件源就是 AppicaionContext。 事件管理器(事件广播器): 派发事件。事件和事件监听器的桥梁、负责把事件通知给事件监听器(可在事件源中实现)。 事件消费方(事件监听器): 处理事件。监听事件的发生、可以在监听器中做一些处理。 2.解决的问题 基于事件驱动的应用程序可以实时响应所关心的事件，实现实时检测、响应外部动作，这是事件驱动模型的基本功能和作用。在一些复杂的系统中，事件驱动模型还可很好地发挥以下作用 实现组件之间的松耦合、解耦 实现异步任务 跟踪状态变化 限流、消峰等 二.观察者模式、发布订阅模式1.观察者模式 观察者模式是一种对象行为模式。它定义对象间的一种一对多的依赖关系(被观察者维护观察者列表) 当一个对象的状态发生改变时，列表中所有观察者都会接收到状态改变的通知 观察者把自己注册到被观察者持有的列表中 当被观察者发布通知，也就是有事件触发时，由被观察者轮询调用观察者的处理代码 (PS:1.目标需要知道观察者存在;2.目标和观察者之间是依赖关系 2.发布、订阅模式 发布订阅模式其实是对象间的一对多的依赖关系(利用消息管理器) 当一个对象的状态发生改变时，所有依赖于它的对象都得到状态改变的通知 订阅者通过调度中心订阅自己关注的事件 当发布者发布事件到调度中心，也就是该事件触发时，由调度中统一调度订阅者的处理代码 (PS:1.发布者不知道订阅者的存在;2.存在消息管理器,彼此之间不知道对方的存在) 3.使用场景区别 观察者模式 发布、订阅模式 目标和观察者之间存在依赖关系 发布者和订阅者之间无依赖关系 关注的是事件发生后观察者能够获取到足够的数据 关注的是事件发生后能够准确地触发相应的动作。 vent种类单一，但是Event会被多个EventListener关注，Event中携带的信息更多的是观察者需要的数据 Event种类多，每类Event只有单个EventListener关注，Event中携带的多是事件自身的一些信息(例如EventlD之类)，而很少携带其他数据。 要求事件发生后EventSource能够通知所有EventListener，此时调度Event的逻辑很简单，不必设置独立的调度器执行调度任务。 Event更像一个触发信号，不同的Event会触发不同的EventListener执行动作，此时调度Event的逻辑复杂，需要设置独立的调度器执行调度任务。 三、Spring中事件驱动应用1.事件驱动最基础的使用@Slf4j @SpringBootApplication public class Demo1App implements ApplicationRunner &#123; &#x2F;** * Spring事件驱动最基础的使用 ApplicationEventPublisher、ApplicationEvent、ApplicationListener * ApplicationEventPublisher 子类ApplicationContext * 事件源、监听器 需要被spring管理 * 监听器 需要实现 ApplicationListener&lt;ApplicationEvent&gt; * 可体现事件源和监听器之间的松耦合 仅依赖spring、ApplicationEvent *&#x2F; &#x2F;&#x2F; @Autowired &#x2F;&#x2F; ApplicationEventPublisher appEventPublisher; @Autowired ApplicationContext applicationContext; &#x2F;&#x2F;因为ApplicationContext实现了ApplicationEventPublisher接口因此常用它来发布事件 @Override public void run(ApplicationArguments args) throws Exception &#123; &#x2F;&#x2F;1.发布事件 applicationContext.publishEvent(new ApplicationEvent(this) &#123; &#125;); &#125; public static void main(String[] args) &#123; SpringApplication.run(Demo1App.class, args); &#125; &#125; @Component @Slf4j public class Demo1Listener implements ApplicationListener&lt;ApplicationEvent&gt; &#123; &#x2F;&#x2F;2.监听事件 @Override public void onApplicationEvent(ApplicationEvent event) &#123; log.info(&quot;[onApplicationEvent]监听到事件：&#123;&#125;&quot;, event.toString()); &#125; &#125; 2.自定义事件@Slf4j @SpringBootApplication public class Demo2App implements ApplicationRunner &#123; &#x2F;** * 自定义事件 Demo2Event * 继承 ApplicationEvent实现对指定类型事件进行监听 *&#x2F; @Autowired ApplicationContext applicationContext; @Override public void run(ApplicationArguments args) throws Exception &#123; &#x2F;&#x2F;1.发布自定义事件 applicationContext.publishEvent(new Demo2Event(this) &#123; &#125;); &#125; public static void main(String[] args) &#123; SpringApplication.run(Demo2App.class, args); &#125; &#125; public class Demo2Event extends ApplicationEvent &#123; public Demo2Event(Object source) &#123; super(source); &#125; &#125; @Component @Slf4j public class Demo2Listener implements ApplicationListener&lt;Demo2Event&gt; &#123; &#x2F;&#x2F;2.监听事件 @Override public void onApplicationEvent(Demo2Event event) &#123; log.info(&quot;[onApplicationEvent]监听到事件：&#123;&#125;&quot;, event.toString()); &#x2F;&#x2F;[onApplicationEvent]监听到事件：com.example.applicationeventstudy.event.demo2.Demo2App$xxxxxx &#125; &#125; 3.自定义事件添加参数@Slf4j @SpringBootApplication public class Demo3App implements ApplicationRunner &#123; &#x2F;** * 1.自定义事件 Demo3Event 添加业务参数 * 2.忽略事件源 根据实际业务情况而定 减少参数 * 3.使用 @EventListener 替换 implements ApplicationListener&lt;Demo2Event&gt; 增加监听者的可扩展性 *&#x2F; @Autowired ApplicationContext applicationContext; public static void main(String[] args) &#123; SpringApplication.run(Demo3App.class, args); &#125; @Override public void run(ApplicationArguments args) throws Exception &#123; &#x2F;&#x2F;1.发布自定义事件 applicationContext.publishEvent(new Demo3Event(this, &quot;自定义参数&quot;) &#123; &#125;); &#125; &#125; @ToString public class Demo3Event extends ApplicationEvent &#123; private String id; public Demo3Event(Object source) &#123; super(source); &#125; public Demo3Event(Object source, String id) &#123; super(source); this.id &#x3D; id; &#125; &#125; @Component @Slf4j public class Demo3Listener &#123; &#x2F;&#x2F;2.监听事件 &#x2F;&#x2F;使用注解可以不用实现ApplicationListener&lt;&gt; @EventListener() public void onApplicationEvent001(ApplicationEvent event) &#123; log.info(&quot;[onApplicationEvent](001)监听到事件：&#123;&#125;&quot;, event.toString()); &#125; @EventListener() public void onApplicationEvent002(Demo3Event event) &#123; log.info(&quot;[onApplicationEvent](002)监听到事件：&#123;&#125;&quot;, event.toString()); &#x2F;&#x2F;[onApplicationEvent]监听到事件：Demo3Event(id&#x3D;自定义参数) &#125; &#x2F;** * [onApplicationEvent](001)监听到事件：org.springframework.boot.context.event.*** * [onApplicationEvent](001)监听到事件：org.springframework.boot.availability.*** * [onApplicationEvent](001)监听到事件：Demo3Event(id&#x3D;自定义参数) * [onApplicationEvent](002)监听到事件：Demo3Event(id&#x3D;自定义参数) * [onApplicationEvent](001)监听到事件：org.springframework.boot.context.event.*** * [onApplicationEvent](001)监听到事件：org.springframework *&#x2F; &#125; 4.@EventListener@Slf4j @SpringBootApplication public class Demo4App implements ApplicationRunner &#123; &#x2F;** * @EventListener用法讲解 * 1.监听自定义事件 * 2. 注解中指定监听事件类型，可指定多个监听事件类型 * 3.注解中使用condition 根据特定条件进行监听 event的属性需要提供getter方法来访问它 * 4.根据特定条件进行监听 对事件进行修改后返回 *&#x2F; @Autowired ApplicationContext applicationContext; public static void main(String[] args) &#123; SpringApplication.run(Demo4App.class, args); &#125; @Override public void run(ApplicationArguments args) throws Exception &#123; &#x2F;&#x2F;1.发布自定义事件 applicationContext.publishEvent(new Demo4Event(this, &quot;001&quot;) &#123; &#125;); applicationContext.publishEvent(new Demo4Event(this, &quot;002&quot;) &#123; &#125;); &#125; &#125; @ToString public class Demo4Event extends ApplicationEvent &#123; private String id; public Demo4Event(Object source) &#123; super(source); &#125; public Demo4Event(Object source, String id) &#123; super(source); this.id &#x3D; id; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id &#x3D; id; &#125; &#125; @Component @Slf4j public class Demo4Listener &#123; &#x2F;&#x2F;2.监听事件 @EventListener(condition &#x3D; &quot;#event.id &#x3D;&#x3D; &#39;001&#39;&quot;) public void onApplicationEvent001(Demo4Event event) &#123; log.info(&quot;[onApplicationEvent](001)监听到事件：&#123;&#125;&quot;, event.toString()); &#125; @EventListener(condition &#x3D; &quot;#event.id &#x3D;&#x3D; &#39;002&#39;&quot;) public void onApplicationEvent002(Demo4Event event) &#123; log.info(&quot;[onApplicationEvent](002)监听到事件：&#123;&#125;&quot;, event.toString()); &#125; &#x2F;** * [onApplicationEvent](001)监听到事件：Demo4Event(id&#x3D;001) * [onApplicationEvent](002)监听到事件：Demo4Event(id&#x3D;002) *&#x2F; @EventListener(&#123;Demo4Event.class, ApplicationEvent.class&#125;) &#x2F;&#x2F;在注解中也可以使用event来置顶监听的事件类 public void onApplicationEvent003(Object event) &#123; log.info(&quot;[onApplicationEvent](003)监听到事件：&#123;&#125;&quot;, event.toString()); &#125; &#x2F;** * 2024-03-11T20:58:55.022+08:00 : [onApplicationEvent](003)监听到事件：org.springframework.boot.availability.AvailabilityChangeEvent[source&#x3D;org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@dbd8e44, started on Mon Mar 11 20:58:54 CST 2024] * 2024-03-11T20:58:55.039+08:00 : [onApplicationEvent](001)监听到事件：Demo4Event(id&#x3D;001) * 2024-03-11T20:58:55.039+08:00 : [onApplicationEvent](003)监听到事件：Demo4Event(id&#x3D;001) * 2024-03-11T20:58:55.040+08:00 : [onApplicationEvent](003)监听到事件：Demo4Event(id&#x3D;002) * 2024-03-11T20:58:55.040+08:00 : [onApplicationEvent](002)监听到事件：Demo4Event(id&#x3D;002) * 2024-03-11T20:58:55.040+08:00 : [onApplicationEvent](003)监听到事件：org.springframework.boot.context.event.ApplicationReadyEvent[source&#x3D;org.springframework.boot.SpringApplication@43bdaa1b] * 2024-03-11T20:58:55.040+08:00 : [onApplicationEvent](003)监听到事件：org.springframework.boot.availability.AvailabilityChangeEvent[source&#x3D;org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@dbd8e44, started on Mon Mar 11 20:58:54 CST 2024] *&#x2F; &#125; 5.异步监听、自定义事件@Slf4j @SpringBootApplication @EnableAsync public class Demo5App implements ApplicationRunner &#123; &#x2F;** * 异步监听 * 1.@0rder 指定执行顺序 在同步的情况下生效 * 2.@Async 异步执行 需要@EnableAsync 开启异步 * 自定义事件 不继承 ApplicationEvent[根据特定情况自行设计，由仅依赖 ApplicationEvent 转变为 依赖自定义事件类] *&#x2F; @Autowired ApplicationContext applicationContext; public static void main(String[] args) &#123; SpringApplication.run(Demo5App.class, args); &#125; @Override public void run(ApplicationArguments args) throws Exception &#123; &#x2F;&#x2F;1.发布自定义事件 applicationContext.publishEvent(new Demo5Event(this, &quot;001&quot;) &#123; &#125;); applicationContext.publishEvent(new Demo5Event(this, &quot;002&quot;) &#123; &#125;); applicationContext.publishEvent(new Demo6Event(&quot;666&quot;) &#123; &#125;); &#125; @EventListener() public void onApplicationEvent002(Demo6Event event) &#123; log.info(&quot;[onApplicationEvent](666)监听到事件：&#123;&#125;&quot;, event.toString()); &#125; &#x2F;** * public interface ApplicationEventPublisher &#123; * default void publishEvent(ApplicationEvent event) &#123; * this.publishEvent((Object)event); * &#125; * * void publishEvent(Object event); * &#125; * * log：[onApplicationEvent](666)监听到事件：Demo6Event(id&#x3D;666) *&#x2F; &#125; @ToString @Getter public class Demo5Event extends ApplicationEvent &#123; private String id; public Demo5Event(Object source) &#123; super(source); &#125; public Demo5Event(Object source, String id) &#123; super(source); this.id &#x3D; id; &#125; &#125; @Component @Slf4j public class Demo5Listener &#123; &#x2F;&#x2F;2.监听事件 &#x2F;&#x2F; @Order(1000) @Async @EventListener() public void onApplicationEvent001(Demo5Event event) throws Exception &#123; log.info(&quot;[onApplicationEvent](001)监听到事件：&#123;&#125;&quot;, event.toString()); Thread.sleep(1000); &#125; &#x2F;&#x2F; @Order(100) @Async @EventListener() public void onApplicationEvent002(Demo5Event event) throws Exception &#123; log.info(&quot;[onApplicationEvent](002)监听到事件：&#123;&#125;&quot;, event.toString()); &#125; &#x2F;** 使用Order() * 2024-03-11T21:06:25.694+08:00 c.e.a.event.demo5.Demo5Listener : [onApplicationEvent](002)监听到事件：Demo5Event(id&#x3D;001) * 2024-03-11T21:06:25.696+08:00 c.e.a.event.demo5.Demo5Listener : [onApplicationEvent](001)监听到事件：Demo5Event(id&#x3D;001) * 2024-03-11T21:06:25.696+08:00 c.e.a.event.demo5.Demo5Listener : [onApplicationEvent](002)监听到事件：Demo5Event(id&#x3D;002) * 2024-03-11T21:06:25.696+08:00 c.e.a.event.demo5.Demo5Listener : [onApplicationEvent](001)监听到事件：Demo5Event(id&#x3D;002) *&#x2F; &#x2F;** 使用Async() * 2024-03-11T21:16:41.516+08:00 main] c.e.a.event.demo5.Demo5App : [onApplicationEvent](666)监听到事件：Demo6Event(id&#x3D;666) * 2024-03-11T21:16:41.515+08:00 task-1] c.e.a.event.demo5.Demo5Listener : [onApplicationEvent](002)监听到事件：Demo5Event(id&#x3D;001) * 2024-03-11T21:16:41.515+08:00 task-3] c.e.a.event.demo5.Demo5Listener : [onApplicationEvent](002)监听到事件：Demo5Event(id&#x3D;002) * 2024-03-11T21:16:41.515+08:00 task-2] c.e.a.event.demo5.Demo5Listener : [onApplicationEvent](001)监听到事件：Demo5Event(id&#x3D;001) * 2024-03-11T21:16:41.516+08:00 task-4] c.e.a.event.demo5.Demo5Listener : [onApplicationEvent](001)监听到事件：Demo5Event(id&#x3D;002) *&#x2F; &#125; @ToString @Getter &#x2F;** * 不继承ApplicationEvent *&#x2F; public class Demo6Event &#123; private String id; public Demo6Event(String id) &#123; this.id &#x3D; id; &#125; &#125; 四.提升 Java 事件驱动模型实现 :: Rectcircle Blog 五.应用","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.ehzyil.xyz/tags/Spring/"}],"author":"ehzyil"},{"title":"TypeScript学习","slug":"2024/TypeScrip学习","date":"2024-03-11T20:22:46.000Z","updated":"2024-06-17T01:04:54.003Z","comments":true,"path":"2024/03/11/2024/TypeScrip学习/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/11/2024/TypeScrip%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"TypeScript学习创建TS项目前提需要先安装npm 安装typescript npm install -g typescript 命令框cmd,输入 tsc -v后，显示了版本信息后，就说明安装成功 安装ts-node npm install -g ts-node -v 命令框cmd,输入ts-node -v后，显示了版本信息后，就说明安装成功 创建项目 新建一个叫 demo的文件夹 在demo路径下打开cmd,并输入tsc –init 生成tsconfig.json文件 同样在cmd中输入npm init -y 生成package.json文件 运行 新建 index.ts 文件，并写下这样的代码： const hello: string = 'HelloWorld'; console.log(hello); 在cmd中输入tsc index.ts 后会生成文件，然后node .\\index.js命令行窗口即会显示HelloWorld TS 中的基础类型和任意类型 基础类型：Boolean、Number、String、null、undefined 以及 ES6 的 Symbol 和 ES10 的 BigInt。 1.字符串类型//string let a:string='123' let b:string='ddd$&#123;a&#125;' console.log(a); console.log(b); 2.数字类型//number // let c:number ='123' //error let d:number=123; let e:number=Infinity; let notAnumber:number=NaN; let decimal:number=0.5; let hex:number=0xf11; let binary:number=0b10101010; let octal:number=0o744; 3.布尔类型//boolean let f:boolean=false; let t1:boolean=Boolean(true); let t2:boolean=Boolean(1); console.log(t1); console.log(t2); //不能将类型“Boolean”分配给类型“boolean”。 // “boolean”是基元，但“Boolean”是包装器对象。如可能首选使用“boolean” // let createdBoolean1: boolean = new Boolean(1) //error let createdBoolean2: Boolean = new Boolean(1) 4.空值类型//void function voidFn():void &#123; console.log('test void') &#125; 5.Null和undefined类型let u:undefined=undefined; let n:null=null; //Null和undefined类型 let u1:null=null; let n1:undefined=undefined; 6.Any 类型 和 unknown 顶级类型 //any let a1:any='123'; a1=123; a1=true; let a2; a2=123; a2=true; //unknown let value:unknown; value=true; value=42; value='123' value=[ ]; value=&#123; &#125;; value=null; value=undefined; value=undefined; value=Symbol('test'); // unknow类型不能作为子类型只能作为父类型 any可以作为父类型和子类型 //unknown类型不能赋值给其他类型 let uk1:unknown='123'; // let uk3:string=uk1; //error //any类型是可以的 let an1:any='123'; let an2:string=an1; let uk3:unknown='123'; let an3:any='321'; //unknown可赋值对象只有unknown 和 any let uk4:unknown=an3; let an4:any=uk3; an3=uk3; //如果是any类型在对象没有这个属性的时候还在获取是不会报错的 let an5:any=&#123;a:1&#125;; an5.b; // 如果是unknow 是不能调用属性和方法 let uk5:unknown=&#123;a:1,ccc:():number=>123&#125;; // uk5.b; //error // uk5.ccc(); /error tips: unknow类型不能作为子类型只能作为父类型,any可以作为父类型和子类型 unknown可赋值对象只有unknown 和 any 接口和对象类型在typescript中，我们定义对象的方式要用关键字interface（接口），我的理解是使用interface来定义一种约束，让数据的结构满足约束的格式。定义方式如下： interface Person &#123; name: string; age: number; &#125; //这样写是会报错的 因为我们在person定义了name，age但是对象里面缺少age属性 //使用接口约束的时候不能多一个属性也不能少一个属性 //必须与接口保持一致 // const ErrorPerson:Person=&#123; // name:'张三', // &#125; const person:Person=&#123; name:'张三', age:18 &#125; //重名interface 可以合并 interface A &#123; name: string; &#125; interface A &#123; age: string; &#125; var a:A=&#123; name:'张三', age:'18' &#125; 继承//继承 interface B extends A &#123; sex: string; &#125; var b:B=&#123; name:'张三', age:'18', sex:'男' &#125; 可选属性//可选属性 使用?操作符 interface C &#123; b?:string, a:string &#125; const c:C = &#123; a:\"213\" &#125; 任意属性//任意属性 [propName: string] // 需要注意的是，一旦定义了任意属性，那么确定属性和可选属性的类型都必须是它的类型的子集： interface D1 &#123; name: string age: number [propName: string]: any &#125; const d1:D1 = &#123; name:\"123\", age:111, sex:\"男\" &#125; //下面这种写法的对象的所有属性的得变成string // interface D1 &#123; // name: string // age: number // [propName: string]: string // &#125; 只读属性//只读属性 readonly //readonly 只读属性是不允许被赋值的只能读取 interface E &#123; name?: string readonly age: string [propName: string]: any &#125; // const e:E = &#123; // name:\"123\", // age:111, age是只读的不允许重新赋值 // sex:\"男\" // &#125; 添加函数 // 添加函数 interface F &#123; name?: string cb:()=>string &#125; let f:F = &#123; name:\"123\", cb:()=>&#123; return 'function' &#125; &#125; 数组类型 //类型加中括号 let arr1:number[]=[1,2,3] //这样会报错定义了数字类型出现字符串是不允许的 // let arr2:number[] = [1,2,3,'1'] //error //操作方法添加也是不允许的 let arr2:number[] = [1,2,3,] // arr.unshift('1') //error var arr3: number[] = [1, 2, 3]; //数字类型的数组 var arr4: string[] = [\"1\", \"2\"]; //字符串类型的数组 var arr5: any[] = [1, \"2\", true]; //任意类型的数组 // 数组泛型 let arr6:Array&lt;number>=[1,2,3,4,5] let arr7:Array&lt;string>=['1','3','4'] //用接口表示数组 interface arr7&#123; [index:number]:number &#125; let arrs: arr7 = [1, 1, 2, 3, 5]; // 表示：只要索引的类型是数字时，那么值的类型必须是数字。 // 多维数组 let data:number[][]=[[1,1], [2,2],[3,3]] // arguments类数组 //error // function Arr(...args:any):void&#123; // console.log(arguments) // //错误的arguments 是类数组不能这样定义 // let arr:number[] = arguments // &#125; function Arr(...args:any): void &#123; console.log(arguments) //ts内置对象IArguments 定义 let arr:IArguments = arguments &#125; Arr(111, 222, 333) //其中 IArguments 是 TypeScript 中定义好了的类型，它实际上就是： interface IArguments &#123; [index: number]: any; length: number; callee: Function; &#125; // any 在数组中的应用 let list:any[ ]=['a',1,true] 函数类型&#x2F;&#x2F; 注意，参数不能多传，也不能少传 必须按照约定的类型来 const fn1 &#x3D;(name:string,age:number):string&#x3D;&gt;&#123; return &#96;姓名是$&#123;name&#125;，年龄是$&#123;age&#125;&#96; &#125; var r1&#x3D;fn1(&#39;张三&#39;,18) console.log(r1) &#x2F;&#x2F; 函数的可选参数? const fn2 &#x3D;(name:string,age:number,sex?:string):string&#x3D;&gt;&#123; return &#96;姓名是$&#123;name&#125;，年龄是$&#123;age&#125;+$&#123;sex&#125;&#96; &#125; var r2&#x3D;fn2(&#39;张三&#39;,18) console.log(r2) &#x2F;&#x2F; 函数参数的默认值 const fn3 &#x3D;(name:string,age:number,sex:string&#x3D;&#39;男&#39;):string&#x3D;&gt;&#123; return &#96;姓名是$&#123;name&#125;，年龄是$&#123;age&#125;$&#123;sex&#125;&#96; &#125; var r3&#x3D;fn3(&#39;张三&#39;,18) console.log(r3) const fn4 &#x3D; (name: string &#x3D; &quot;我是默认值&quot;): string &#x3D;&gt; &#123; return name &#125; var r4&#x3D;fn4() console.log(r4) &#x2F;&#x2F; 接口定义函数 interface Add&#123; (a: number, b: number):number &#125; const add: Add &#x3D; (a, b) &#x3D;&gt; &#123; return a + b &#125; console.log(add(1, 2)) interface User&#123; name: string, age: number, sex: string pwd: string &#125; function getUserInfo(user:User):User&#123; return user; &#125; &#x2F;&#x2F;定义剩余参数 const fn5 &#x3D; (array:number[],...items:any[]):any[] &#x3D;&gt; &#123; console.log(array,items) return items &#125; let a:number[] &#x3D; [1,2,3] fn5(a,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;) &#x2F;&#x2F; 函数重载 function fn(params: number): void function fn(params: string, params2: number): void function fn(params: any, params2?: any): void &#123; console.log(params) console.log(params2) &#125; fn(123) fn(&#39;123&#39;,456) &#x2F;&#x2F; 重载是方法名字相同，而参数不同，返回类型可以相同也可以不同。 &#x2F;&#x2F; 如果参数类型不同，则参数类型应设置为 any。 &#x2F;&#x2F; 参数数量不同你可以将不同的参数设置为可选。 联合类型 | 交叉类型 | 类型断言联合类型&#x2F;&#x2F;例如我们的手机号通常是13XXXXXXX 为数字类型 这时候产品说需要支持座机 &#x2F;&#x2F;所以我们就可以使用联合类型支持座机字符串 let phone1: number | string &#x3D; &#39;010-820&#39; &#x2F;&#x2F;这样写是会报错的应为我们的联合类型只有数字和字符串并没有布尔值 &#x2F;&#x2F; let phone2: number | string &#x3D; true &#x2F;&#x2F;error &#x2F;&#x2F; 函数使用联合类型 const fn1&#x3D;(sth:number|boolean):boolean&#x3D;&gt;&#123; return !!sth &#125; console.log(fn1(true)); &#x2F;&#x2F; 输出: true console.log(fn1(false)); &#x2F;&#x2F; 输出: false console.log(fn1(1)); &#x2F;&#x2F; 输出: true console.log(fn1(0)); &#x2F;&#x2F; 输出: false 交叉类型多种类型的集合，联合对象将具有所联合类型的所有成员 interface People &#123; age: number, height: number &#125; interface Man&#123; sex: string &#125; const ehzyil&#x3D; (people: People &amp; Man)&#x3D;&gt;&#123; console.log(people) &#125; ehzyil(&#123;age: 18,height: 180,sex: &#39;male&#39;&#125;); 类型断言语法： 值 as 类型 或 &lt;类型&gt;值 value as string &lt;string&gt;value interface A &#123; run: string &#125; interface B &#123; build: string &#125; &#x2F;&#x2F; const fn &#x3D; (type: A | B): string &#x3D;&gt; &#123; &#x2F;&#x2F; return type.run &#x2F;&#x2F;error &#x2F;&#x2F; &#125; &#x2F;&#x2F;这样写是有警告的应为B的接口上面是没有定义run这个属性的 interface A1 &#123; run: string &#125; interface B1 &#123; build: string &#125; const fn &#x3D; (type: A1 | B1): string &#x3D;&gt; &#123; return (type as A1).run &#125; &#x2F;&#x2F;可以使用类型断言来推断他传入的是A接口的值 &#x2F;&#x2F; 使用any临时断言 &#x2F;&#x2F; window.abc &#x3D; 123 &#x2F;&#x2F;error &#x2F;&#x2F;这样写会报错因为window没有abc这个东西 (window as any).abc &#x3D; 123 &#x2F;&#x2F;可以使用any临时断言在 any 类型的变量上，访问任何属性都是允许的。 &#x2F;&#x2F;as const const names &#x3D; &#39;abc&#39; names &#x3D; &#39;aa&#39; &#x2F;&#x2F;无法修改 error let names2 &#x3D; &#39;abc&#39; as const names2 &#x3D; &#39;aa&#39; &#x2F;&#x2F;无法修改 error &#x2F;&#x2F; 数组 let a1 &#x3D; [10, 20] as const; const a2 &#x3D; [10, 20]; a1.unshift(30); &#x2F;&#x2F; 错误，此时已经断言字面量为[10, 20],数据无法做任何修改 a2.unshift(30); &#x2F;&#x2F; 通过，没有修改指针 需要注意的是，类型断言只能够「欺骗」TypeScript 编译器，无法避免运行时的错误，反而滥用类型断言可能会导致运行时错误； 类型断言是不具影响力的 在下面的例子中，将 something 断言为 boolean 虽然可以通过编译，但是并没有什么用 并不会影响结果, 因为编译过程中会删除类型断言 function toBoolean(something: any): boolean &#123; return something as boolean; &#125; toBoolean(1); &#x2F;&#x2F; 返回值为 1 &#x2F;&#x2F; 内置对象ECMAScript 的内置对象Boolean、Number、string、RegExp、Date、Error let b:Boolean=new Boolean(true); console.log(b); let n:Number=new Number(10); console.log(n); let s:String=new String(\"Hello\"); console.log(s); let d:Date=new Date(); console.log(d); let a:Array&lt;number>=[1,2,3,4,5]; console.log(a); let r:RegExp=new RegExp(\"a\",\"g\"); console.log(r); let e:Error=new Error(\"Error\"); console.log(e); DOM 和 BOM 的内置对象Document、HTMLElement、Event、NodeList 等 let body: HTMLElement=document.body; let allDivs: NodeList =document.querySelectorAll(\"div\"); //读取div 这种需要类型断言 或者加个判断应为读不到返回null let div: HTMLElement =document.querySelector(\"div\") as HTMLDivElement; document.addEventListener(\"click\",function(e:MouseEvent)&#123; console.log(e.target); &#125;); let div:NodeListOf&lt;HTMLDivElement| HTMLElement> = document.querySelectorAll(\"div\"); let lo:Location=location; let local:Storage=localStorage; dom元素的映射表 HTMLElement：所有 HTML 元素的基类。 HTMLAnchorElement：&lt;a&gt; 元素（超链接）。 HTMLAreaElement：&lt;area&gt; 元素（图像地图中的区域）。 HTMLAudioElement：&lt;audio&gt; 元素（音频）。 HTMLBaseElement：&lt;base&gt; 元素（指定文档的基 URL）。 HTMLBodyElement：&lt;body&gt; 元素（文档的主体）。 HTMLBRElement：&lt;br&gt; 元素（换行）。 HTMLButtonElement：&lt;button&gt; 元素（按钮）。 HTMLCanvasElement：&lt;canvas&gt; 元素（用于绘制图形和图像）。 HTMLDataElement：&lt;data&gt; 元素（定义可机读的数据）。 HTMLDataListElement：&lt;datalist&gt; 元素（提供预定义选项列表）。 HTMLDetailsElement：&lt;details&gt; 元素（可折叠的详细信息）。 HTMLDialogElement：&lt;dialog&gt; 元素（模态对话框）。 HTMLDivElement：&lt;div&gt; 元素（通用容器）。 HTMLEmbedElement：&lt;embed&gt; 元素（嵌入外部内容）。 HTMLFieldSetElement：&lt;fieldset&gt; 元素（一组相关控件）。 HTMLFontElement：&lt;font&gt; 元素（已弃用，用于设置字体）。 HTMLFormElement：&lt;form&gt; 元素（用于收集用户输入）。 HTMLFrameElement：&lt;frame&gt; 元素（内联框架）。 HTMLFrameSetElement：&lt;frameset&gt; 元素（一组内联框架）。 HTMLHeadElement：&lt;head&gt; 元素（文档的头部）。 HTMLHeadingElement：&lt;h1&gt; 到 &lt;h6&gt; 元素（标题）。 HTMLHRElement：&lt;hr&gt; 元素（水平线）。 HTMLHtmlElement：&lt;html&gt; 元素（文档的根元素）。 HTMLIFrameElement：&lt;iframe&gt; 元素（内联框架）。 HTMLImageElement：&lt;img&gt; 元素（图像）。 HTMLInputElement：&lt;input&gt; 元素（用于收集用户输入）。 HTMLKeygenElement：&lt;keygen&gt; 元素（生成密钥对）。 HTMLLabelElement：&lt;label&gt; 元素（用于关联控件和标签）。 HTMLLegendElement：&lt;legend&gt; 元素（&lt;fieldset&gt; 元素的标题）。 HTMLLIElement：&lt;li&gt; 元素（列表项）。 HTMLLinkElement：&lt;link&gt; 元素（用于链接外部资源）。 HTMLMapElement：&lt;map&gt; 元素（定义图像地图）。 HTMLMarqueeElement：&lt;marquee&gt; 元素（已弃用，用于滚动文本）。 HTMLMediaElement：所有媒体元素（&lt;audio&gt;, &lt;video&gt;, &lt;track&gt;, &lt;source&gt;) 的基类。 HTMLMenuElement：&lt;menu&gt; 元素（上下文菜单）。 HTMLMetaElement：&lt;meta&gt; 元素（用于提供文档元数据）。 HTMLMeterElement：&lt;meter&gt; 元素（测量值指示器）。 HTMLModElement：&lt;mod&gt; 元素（已弃用，用于指示文本的修改）。 HTMLOListElement：&lt;ol&gt; 元素（有序列表）。 HTMLObjectElement：&lt;object&gt; 元素（嵌入外部内容）。 HTMLOptGroupElement：&lt;optgroup&gt; 元素（&lt;select&gt; 元素中的选项组）。 HTMLOptionElement：&lt;option&gt; 元素（&lt;select&gt; 元素中的选项）。 HTMLOutputElement：&lt;output&gt; 元素（用于显示结果）。 HTMLParagraphElement：&lt;p&gt; 元素（段落）。 HTMLParamElement：&lt;param&gt; 元素（用于 &lt;object&gt; 元素指定参数）。 HTMLPictureElement：&lt;picture&gt; 元素（用于响应式图像）。 HTMLPreElement：&lt;pre&gt; 元素（预格式化文本）。 HTMLProgressElement：&lt;progress&gt; 元素（进度条）。 HTMLQuoteElement：&lt;q&gt; 和 &lt;blockquote&gt; 元素（引用）。 HTMLScriptElement：&lt;script&gt; 元素（用于执行脚本）。 HTMLSelectElement：&lt;select&gt; 元素（下拉列表）。 HTMLSlotElement：&lt;slot&gt; 元素（用于 Web 组件的插槽）。 HTMLSourceElement：&lt;source&gt; 元素（用于 &lt;video&gt; 和 &lt;audio&gt; 元素指定媒体源）。 HTMLSpanElement：&lt;span&gt; 元素（内联容器）。 HTMLStyleElement：&lt;style&gt; 元素（用于定义样式）。 HTMLTableCaptionElement：&lt;caption&gt; 元素（表格标题）。 HTMLTableCellElement：&lt;td&gt; 和 &lt;th&gt; 元素（表格单元格）。 HTMLTableColElement：&lt;col&gt; 和 &lt;colgroup&gt; 元素（表格列）。 HTMLTableHeadElement：&lt;thead&gt; 元素（表格头部）。 HTMLTableRowElement：&lt;tr&gt; 元素（表格行）。 HTMLTableSectionElement：&lt;tbody&gt;, &lt;tfoot&gt;, &lt;thead&gt; 元素（表格部分）。 HTMLTemplateElement：&lt;template&gt; 元素（用于定义可重用的内容模板）。 HTMLTextAreaElement：&lt;textarea&gt; 元素（多行文本输入）。 HTMLTimeElement：&lt;time&gt; 元素（表示日期和时间）。 HTMLTitleElement：&lt;title&gt; 元素（文档的标题）。 HTMLTrackElement：&lt;track&gt; 元素（用于 &lt;video&gt; 和 &lt;audio&gt; 元素指定字幕或音轨）。 HTMLUListElement：&lt;ul&gt; 元素（无序列表）。 HTMLUnknownElement：用于未知或自定义元素。 HTMLVideoElement：&lt;video&gt; 元素（视频）。 SVG 元素 SVGElement：所有 SVG 元素的基类。 SVGAElement：&lt;a&gt; 元素（SVG 超链接）。 SVGAltGlyphElement：&lt;altGlyph&gt; 元素（备用字形）。 SVGAltGlyphDefElement：&lt;altGlyphDef&gt; 元素（备用字形定义）。 SVGAltGlyphItemElement：&lt;altGlyphItem&gt; 元素（备用字形项）。 SVGCircleElement：&lt;circle&gt; 元素（圆形）。 SVGClipPathElement：&lt;clipPath&gt; 元素（剪切路径）。 SVGCursorElement：&lt;cursor&gt; 元素（光标）。 SVGDefsElement：&lt;defs&gt; 元素（定义）。 SVGDescElement：&lt;desc&gt; 元素（描述）。 SVGEllipseElement：&lt;ellipse&gt; 元素（椭圆）。 SVGFEBlendElement：&lt;feBlend&gt; 元素（混合滤镜）。 SVGFEColorMatrixElement：&lt;feColorMatrix&gt; 元素（颜色矩阵滤镜）。 SVGFEComponentTransferElement：&lt;feComponentTransfer&gt; 元素（分量传输滤镜）。 SVGFECompositeElement：&lt;feComposite&gt; 元素（复合滤镜）。 SVGFEConvolveMatrixElement：&lt;feConvolveMatrix&gt; 元素（卷积矩阵滤镜）。 SVGFEDiffuseLightingElement：&lt;feDiffuseLighting&gt; 元素（漫射光照滤镜）。 SVGFEDisplacementMapElement：&lt;feDisplacementMap&gt; 元素（位移贴图滤镜）。 SVGFEDistantLightElement：&lt;feDistantLight&gt; 元素（远光源）。 SVGFEDropShadowElement：&lt;feDropShadow&gt; 元素（阴影滤镜）。 SVGFEFloodElement：&lt;feFlood&gt; 元素（泛光滤镜）。 SVGFEFuncAElement：&lt;feFuncA&gt; 元素（Alpha 分量函数）。 SVGFEFuncBElement：&lt;feFuncB&gt; 元素（B 分量函数）。 SVGFEFuncGElement：&lt;feFuncG&gt; 元素（G 分量函数）。 SVGFEFuncRElement：&lt;feFuncR&gt; 元素（R 分量函数）。 SVGFEGaussianBlurElement：&lt;feGaussianBlur&gt; 元素（高斯模糊滤镜）。 SVGFEImageElement：&lt;feImage&gt; 元素（图像滤镜）。 SVGFEMergeElement：&lt;feMerge&gt; 元素（合并滤镜）。 SVGFEMergeNodeElement：&lt;feMergeNode&gt; 元素（合并滤镜节点）。 SVGFEMorphologyElement：&lt;feMorphology&gt; 元素（形态学滤镜）。 SVGFEOffsetElement：&lt;feOffset&gt; 元素（偏移滤镜）。 SVGFEPointLightElement：&lt;fePointLight&gt; 元素（点光源）。 SVGFESpecularLightingElement：&lt;feSpecularLighting&gt; 元素（镜面光照滤镜）。 SVGFESpotLightElement：&lt;feSpotLight&gt; 元素（聚光灯）。 SVGFETileElement：&lt;feTile&gt; 元素（平铺滤镜）。 SVGFETurbulenceElement：&lt;feTurbulence&gt; 元素（湍流滤镜）。 SVGFilterElement：&lt;filter&gt; 元素（滤镜）。 SVGForeignObjectElement：&lt;foreignObject&gt; 元素（外部对象）。 SVGGElement：&lt;g&gt; 元素（组）。 SVGImageElement：&lt;image&gt; 元素（图像）。 SVGLineElement：&lt;line&gt; 元素（线）。 SVGLinearGradientElement：&lt;linearGradient&gt; 元素（线性渐变）。 SVGMarkerElement：&lt;marker&gt; 元素（标记）。 SVGMaskElement：&lt;mask&gt; 元素（遮罩）。 SVGMetadataElement：&lt;metadata&gt; 元素（元数据）。 SVGPathElement：&lt;path&gt; 元素（路径）。 SVGPolygonElement：&lt;polygon&gt; 元素（多边形）。 SVGPolylineElement：&lt;polyline&gt; 元素（折线）。 SVGRadialGradientElement：&lt;radialGradient&gt; 元素（径向渐变）。 SVGRectElement：&lt;rect&gt; 元素（矩形）。 SVGSVGElement：&lt;svg&gt; 元素（SVG 根元素）。 SVGScriptElement：&lt;script&gt; 元素（SVG 脚本）。 SVGStopElement：&lt;stop&gt; 元素（渐变停止）。 SVGStyleElement：&lt;style&gt; 元素（SVG 样式）。 SVGSwitchElement：&lt;switch&gt; 元素（开关）。 SVGSymbolElement：&lt;symbol&gt; 元素（符号）。 SVGTextElement：&lt;text&gt; 元素（文本）。 SVGTextPathElement：&lt;textPath&gt; 元素（文本路径）。 SVGTitleElement：&lt;title&gt; 元素（标题）。 SVGTspanElement：&lt;tspan&gt; 元素（文本跨度）。 SVGUseElement：&lt;use&gt; 元素（使用）。 SVGViewElement：&lt;view&gt; 元素（视图）。 其他元素 Document：文档对象。 DocumentFragment：文档片段。 DocumentType：文档类型声明。 Element：所有 HTML 和 SVG 元素的基类。 Node：所有 DOM 节点的基类。 ShadowRoot：Web 组件的影子 DOM。 Window：浏览器窗口。 XMLDocument：XML 文档。 定义Promise//如果我们不指定返回的类型TS是推断不出来返回的是什么类型 function promise():Promise&lt;string>&#123; return new Promise&lt;string>((resolve,reject)=>&#123; resolve(\"hello world\") &#125;) &#125; promise().then((data)=>&#123; console.log(data) &#125;) 代码雨案例&lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;title>代码雨案例&lt;/title> &lt;meta charset=\"UTF-8\"> &lt;style> * &#123; padding: 0; margin: 0; overflow: hidden; &#125; &lt;/style> &lt;/head> &lt;body> &lt;canvas id=\"canvas\">&lt;/canvas> &lt;script src=\"./index.js\">&lt;/script> &lt;/body> &lt;/html> // 获取页面上id为\"canvas\"的HTMLCanvasElement元素，并将其类型断言为HTMLCanvasElement let canvas: HTMLCanvasElement = document.querySelector(\"#canvas\") as HTMLCanvasElement; // 从canvas元素获取2D渲染上下文，并将其类型断言为CanvasRenderingContext2D let ctx: CanvasRenderingContext2D = canvas.getContext(\"2d\") as CanvasRenderingContext2D; // 设置canvas元素的高度和宽度分别为屏幕可用高度和宽度 canvas.height = screen.availHeight; canvas.width = screen.availWidth; // 定义一个字符串，包含要随机显示的字符 let str: string = \"abcdefghigklmn\"; // 创建一个数组，长度为canvas宽度除以10后的向上取整数，所有元素初始化为0 let arr: number[] = Array(Math.ceil(canvas.width / 10)).fill(0); // 输出初始的arr数组内容和2D渲染上下文对象 console.log(arr); console.log(ctx); // 定义绘制函数 const draw = (): void => &#123; // 设置背景填充颜色为半透明黑色 ctx.fillStyle = 'rgba(0,0,0,0.05)'; // 填充整个canvas区域作为背景 ctx.fillRect(0, 0, canvas.width, canvas.height); // 设置文字颜色为绿色 ctx.fillStyle = \"#0f0\"; // 遍历数组，根据数组中的每个值绘制随机字符 arr.forEach((item, index) => &#123; // 从str中随机选择一个字符 let randomCharIndex = Math.floor(Math.random() * str.length); // 在canvas上指定位置绘制字符 ctx.fillText(str[randomCharIndex], index * 10, item + 10); // 更新数组元素值，使其在canvas高度范围内随机分布（增加一定的随机性） arr[index] = item >= canvas.height || item > 20000 * Math.random() ? 0 : item + 10; &#125;); &#125; // 每隔50毫秒调用一次draw函数，实现动态绘制效果 setInterval(draw, 50); Class类ES6提供了更接近传统语言的写法，引入了Class（类）这个概念，作为对象的模板。通过class关键字，可以定义类。基本上，ES6的class可以看作只是一个语法糖，它的绝大部分功能，ES5都可以做到，新的class写法只是让对象原型的写法更加清晰、更像面向对象编程的语法而已。 TypeScript 类 TypeScript 类是一种创建对象的蓝图。它定义了对象的属性和方法。 语法 class ClassName &#123; // 成员变量 private name: string; // 构造函数 constructor(name: string) &#123; this.name = name; &#125; // 成员方法 public greet(): void &#123; console.log(`Hello, $&#123;this.name&#125;!`); &#125; &#125; 成员变量 成员变量是类的属性。它们存储有关对象状态的信息。在上面的示例中，name 是一个私有成员变量，这意味着它只能在类内部访问。 构造函数 构造函数是类的一个特殊方法，它在创建类的新实例时被调用。它用于初始化成员变量。在上面的示例中，构造函数接受一个字符串参数，并将该参数分配给 name 成员变量。 成员方法 成员方法是类的函数。它们用于操作对象的状态或执行其他任务。在上面的示例中，greet 方法是一个公共成员方法，这意味着它可以在类外部访问。它打印一条包含对象 name 属性的消息。 类实例 要创建类的实例，请使用 new 关键字： const person = new ClassName(\"John\"); 这将创建一个 ClassName 类的实例，并将 name 成员变量初始化为 “John”。 访问成员 可以使用点语法访问类的成员： 访问成员变量：person.name 调用成员方法：person.greet() 继承TypeScript 类支持继承，这允许你创建从现有类派生的新类。派生类继承基类的所有成员，并可以添加自己的成员。 语法 class DerivedClass extends BaseClass &#123; // 派生类特有的成员 &#125; 示例 class Animal &#123; name: string; constructor(name: string) &#123; this.name = name; &#125; speak(): void &#123; console.log(\"Animal is speaking\"); &#125; &#125; class Dog extends Animal &#123; breed: string; constructor(name: string, breed: string) &#123; super(name); // 调用基类构造函数 this.breed = breed; &#125; bark(): void &#123; console.log(\"Dog is barking\"); &#125; &#125; const dog = new Dog(\"Buddy\", \"Golden Retriever\"); dog.speak(); // 继承自基类 dog.bark(); // 派生类特有的方法 抽象类抽象类是不能被实例化的类。它们用于定义接口，子类必须实现这些接口。 语法 abstract class AbstractClass &#123; abstract method(): void; &#125; 示例 abstract class Shape &#123; abstract area(): number; &#125; class Circle extends Shape &#123; radius: number; constructor(radius: number) &#123; super(); this.radius = radius; &#125; area(): number &#123; return Math.PI * this.radius ** 2; &#125; &#125; 接口定义类接口是定义一组方法签名的契约。类必须实现接口中定义的所有方法才能被认为实现了该接口。 语法 interface InterfaceName &#123; method(): void; &#125; 示例 interface Drawable &#123; draw(): void; &#125; class Rectangle implements Drawable &#123; width: number; height: number; constructor(width: number, height: number) &#123; this.width = width; this.height = height; &#125; draw(): void &#123; console.log(\"Drawing a rectangle\"); &#125; &#125; 修饰符修饰符用于控制类、接口、方法和属性的访问权限。它们可以指定成员是公有的、私有的、受保护的或只读的。 访问权限级别 TypeScript 中有四种访问权限级别： public：成员可以在任何地方访问。 protected：成员只能在类本身及其派生类中访问。 private：成员只能在类本身内部访问。 readonly：成员只能在声明时初始化，之后不能修改。 修饰符语法 修饰符放在成员声明之前： public：无修饰符 protected：protected private：private readonly：readonly 示例 class Person &#123; public name: string; // 公有属性 protected age: number; // 受保护属性 private address: string; // 私有属性 readonly id: number; // 只读属性 &#125; 注意： 修饰符不能用于接口方法，因为接口方法始终是公有的。 修饰符不能用于枚举成员，因为枚举成员始终是公有、静态和常量的。 static 关键字static 关键字用于声明类中的静态成员。静态成员属于类本身，而不是类的实例。 静态成员类型 静态成员可以是： 属性：存储与类本身相关的数据。 方法：执行与类本身相关的操作。 静态成员的优点 类级作用域：静态成员可以在不创建类实例的情况下访问和使用。 代码重用：静态方法可以实现类级的功能，而无需创建实例。 内存效率：静态成员只存储在类本身中，而不是在每个实例中，从而节省内存。 静态成员的语法 在成员声明之前使用 static 关键字： class MyClass &#123; static property: number; // 静态属性 static method(): void; // 静态方法 &#125; 访问静态成员 可以使用类名直接访问静态成员： MyClass.property; // 访问静态属性 MyClass.method(); // 调用静态方法 示例 class MathUtils &#123; static PI: number = 3.14; // 静态属性 static calculateArea(radius: number): number &#123; return Math.PI * radius ** 2; // 静态方法 &#125; &#125; console.log(MathUtils.PI); // 3.14 const area = MathUtils.calculateArea(5); // 78.5 元组类型元组类型是一种有序集合类型，其中每个元素可以具有不同的类型。元组的长度和元素类型在编译时是固定的。 语法 let tuple: [type1, type2, ..., typeN]; 示例 let employee: [string, number, boolean] = [\"John Doe\", 30, true]; 在这个示例中，employee 元组包含三个元素： 第一个元素是一个字符串（姓名）。 第二个元素是一个数字（年龄）。 第三个元素是一个布尔值（是否已婚）。 访问元组元素 可以使用索引访问元组元素： console.log(employee[0]); // \"John Doe\" console.log(employee[1]); // 30 console.log(employee[2]); // true 解构元组 也可以使用解构来提取元组元素： const [name, age, isMarried] = employee; console.log(name); // \"John Doe\" console.log(age); // 30 console.log(isMarried); // true 示例 元组的一个常见用例是表示一个二维点： type Point = [number, number]; const point: Point = [10, 20]; console.log(point[0]); // 10 console.log(point[1]); // 20 枚举类型枚举类型是一种特殊的数据类型，它表示一组命名常量。枚举成员的值在编译时是固定的，并且通常是从 0 开始的整数。 语法 enum EnumName &#123; member1 = value1, member2 = value2, // ... memberN = valueN &#125; 枚举类型 TypeScript 支持以下类型的枚举： 数字枚举：枚举成员是数字值。 字符串枚举：枚举成员是字符串值。 异构枚举：枚举成员可以是数字或字符串值。 数字枚举 数字枚举的成员值从 0 开始自动增长。但是，也可以显式指定成员值： enum Colors &#123; Red = 1, Green = 2, Blue = 3 &#125; 字符串枚举 字符串枚举的成员值必须是字符串字面量或其他字符串枚举成员： enum Colors &#123; Red = 'red', Green = 'green', Blue = 'blue' &#125; 异构枚举 异构枚举可以混合数字和字符串成员： enum Types &#123; No = \"No\", Yes = 1 &#125; 接口枚举 枚举类型可以与接口一起使用来定义具有特定属性的对象： enum Types &#123; yyds &#125; interface A &#123; red: Types.yyds &#125; const obj: A = &#123; red: Types.yyds &#125; 常量枚举 常量枚举使用 const 修饰符定义，并且编译为常量值： const enum Types &#123; No = \"No\", Yes = 1 &#125; 反向映射 数字枚举和异构枚举具有反向映射，它允许你根据值获取枚举成员的名称： enum Enum &#123; fall &#125; const a = Enum.fall; // 0 const nameOfA = Enum[a]; // \"fall\" 类型推论 | 类型别名类型推论 声明了一个变量,但是没有定义类型 TypeScript 会在没有明确的指定类型的时候推测出一个类型，这就是类型推论 let str &#x3D; &quot;Hello World&quot;; &#x2F;&#x2F;TS帮我推断出来这是一个string类型 str&#x3D;135 &#x2F;&#x2F;error 不能将类型“number”分配给类型“string”。 如果声明变量没有定义类型也没有赋值这时候TS会推断成any类型可以进行任何操作 let ehzyil; ehzyil &#x3D; 123; ehzyil &#x3D; &quot;123&quot;; ehzyil &#x3D; true; 类型别名type 关键字（可以给一个类型定义一个名字）多用于复合类型 &#x2F;&#x2F; 定义类型别名 type str &#x3D; string let s:str &#x3D; &quot;123&quot; console.log(s); &#x2F;&#x2F; 定义函数别名 type fn &#x3D; () &#x3D;&gt; string let f: fn &#x3D; () &#x3D;&gt; &quot;123&quot; console.log(s); &#x2F;&#x2F; 定义联合类型别名 type c &#x3D; string | number let c1: c &#x3D; 123 let c2: c &#x3D; &#39;123&#39; console.log(c1 ,c2); &#x2F;&#x2F; 定义值的别名 type value &#x3D; boolean | 0 | &#39;213&#39; let v:value &#x3D; true &#x2F;&#x2F;变量v的值 只能是上面value定义的值 type 和 interface 还是一些区别的 虽然都可以定义类型 1.interface可以继承 type 只能通过 &amp; 交叉类型合并 2.type 可以定义 联合类型 和 可以使用一些操作符 interface不行 3.interface 遇到重名的会合并 type 不行 type高级用法 左边的值会作为右边值的子类型. type a &#x3D; 1 extends number ? 1 : 0 &#x2F;&#x2F;1 type a &#x3D; 1 extends Number ? 1 : 0 &#x2F;&#x2F;1 type a &#x3D; 1 extends Object ? 1 : 0 &#x2F;&#x2F;1 type a &#x3D; 1 extends any ? 1 : 0 &#x2F;&#x2F;1 type a &#x3D; 1 extends unknow ? 1 : 0 &#x2F;&#x2F;1 type a &#x3D; 1 extends never ? 1 : 0 &#x2F;&#x2F;0 never类型never 类型表示一个永远不会发生的值。它用于表示永远不会返回或终止的函数或操作。 语法 type never = never; 特点 never 类型的值永远不会存在。 任何类型都可以赋值给 never 类型，但 never 类型的值不能赋值给任何其他类型。 never 类型是任何其他类型的子类型。 用法 never 类型通常用于以下情况： 表示永远不会返回的函数，例如无限循环或抛出异常的函数。 表示永远不会终止的操作，例如死锁或未处理的 Promise。 作为类型系统中的哨兵值，表示无效或不可能的状态。 示例 永远不会返回的函数： const infiniteLoop = (): never => &#123; while (true) &#123; // 无限循环 &#125; &#125;; 永远不会终止的操作： const deadLock = (): never => &#123; const lock1 = new Lock(); const lock2 = new Lock(); lock1.acquire(); lock2.acquire(); // 死锁，因为无法同时获取两个锁 &#125;; 作为哨兵值： type State = \"active\" | \"inactive\" | \"error\"; const getState = (): State => &#123; // ... 获取状态的逻辑 if (// 发生错误) &#123; return \"error\"; &#125; else &#123; return \"active\"; // 永远不会返回 \"inactive\" &#125; &#125;; never 类型与其他类型的比较 类型 never 类型 void void 是一个特殊情况，表示没有返回值的函数。never 类型表示永远不会返回或终止的函数。 unknown never 类型是 unknown 类型的子类型，因为任何值都可以赋值给 never 类型。 any never 类型不是 any 类型的子类型，因为 never 类型的值永远不会存在。 never 与 void 的差异 //差异1 void类型只是没有返回值 但本身不会出错 function Void():void &#123; console.log(); &#125; //只会抛出异常没有返回值 function Never():never &#123; throw new Error('aaa') &#125; //差异2 当我们鼠标移上去的时候会发现 只有void和number never在联合类型中会被直接移除 type A = void | number | never 注意事项 never 类型不能用于表示可选值。使用 undefined 或 null 来表示可选值。 never 类型不能用于表示异步函数。使用 Promise&lt;never&gt; 来表示永远不会解析或拒绝的异步函数。 never 类型的一个应用场景kun 函数使用 never 类型来表示一个兜底逻辑，即当输入值 value 不属于枚举类型 bb 中的任何一个值时执行的逻辑。 具体来说，never 类型在这里的作用是： 类型检查：确保 value 变量只能取 bb 枚举类型中的值，否则会产生类型错误。 错误处理：当 value 不是有效的枚举值时，函数会抛出一个类型为 never 的错误。这可以帮助我们检测和处理无效输入。 哨兵值：never 类型作为一个哨兵值，表示一种不可能或无效的状态，在这种情况下，表示一个无效的输入值。 type bb = \"唱\" | \"跳\" | \"rap\" | \"篮球\" function kun(value: bb): void &#123; switch (value) &#123; case \"唱\": console.log(\"唱歌\"); break; case \"跳\": console.log(\"跳舞\"); break; case \"rap\": console.log(\"饶舌\"); break; default: //兜底逻辑 const error:never = value; return error &#125; &#125; Symbol类型自ECMAScript 2015起，symbol成为了一种新的原生类型，就像number和string一样。symbol类型的值是通过Symbol构造函数创建的。 可以传递参做为唯一标识 只支持 string 和 number类型的参数 let sym1 &#x3D; Symbol(); let sym2 &#x3D; Symbol(&quot;key&quot;); &#x2F;&#x2F; 可选的字符串key Symbol的值是唯一的 const s1 &#x3D; Symbol() const s2 &#x3D; Symbol() &#x2F;&#x2F; s1 &#x3D;&#x3D;&#x3D; s2 &#x3D;&gt;false console.log(s1 &#x3D;&#x3D;&#x3D; Symbol()) &#x2F;&#x2F; false 用作对象属性的键 let sym &#x3D; Symbol(); let obj &#x3D; &#123; [sym]: &quot;value&quot; &#125;; console.log(obj[sym]); &#x2F;&#x2F; &quot;value&quot; 使用symbol定义的属性，是不能通过如下方式遍历拿到的 const symbol1 &#x3D; Symbol(&#39;666&#39;) const symbol2 &#x3D; Symbol(&#39;777&#39;) const obj1&#x3D; &#123; [symbol1]: &#39;小满&#39;, [symbol2]: &#39;二蛋&#39;, age: 19, sex: &#39;女&#39; &#125; &#x2F;&#x2F; 1 for in 遍历 for (const key in obj1) &#123; &#x2F;&#x2F; 注意在console看key,是不是没有遍历到symbol1 console.log(key) &#x2F;&#x2F; console.log(obj1[key]) &#x2F;&#x2F;error &#125; &#x2F;&#x2F; 2 Object.keys 遍历 Object.keys(obj1) console.log(Object.keys(obj1)) &#x2F;&#x2F; 3 getOwnPropertyNames console.log(Object.getOwnPropertyNames(obj1)) &#x2F;&#x2F; 4 JSON.stringfy console.log(JSON.stringify(obj1)) 如何拿到 &#x2F;&#x2F; 1 拿到具体的symbol 属性,对象中有几个就会拿到几个 Object.getOwnPropertySymbols(obj1) console.log(Object.getOwnPropertySymbols(obj1)) &#x2F;&#x2F; 2 es6 的 Reflect 拿到对象的所有属性 Reflect.ownKeys(obj1) console.log(Reflect.ownKeys(obj1)) interface Item &#123; name: string age: number &#125; const array: Array&lt;Item&gt; &#x3D; [ &#123; name: &quot;张三&quot;, age: 18 &#125;, &#123; name: &quot;李四&quot;, age: 20 &#125; ] type mapType &#x3D; string | number const map: Map&lt;mapType, mapType&gt; &#x3D; new Map() map.set(&quot;0&quot;, &quot;张三&quot;) map.set(1, &quot;李四&quot;) map.set(2, &quot;王五&quot;) const obj &#x3D; &#123; aaa: 123, bbb: 456 &#125; const gen &#x3D; (args: any): void &#x3D;&gt; &#123; let it: Iterator&lt;any&gt; &#x3D; args[Symbol.iterator](); let next: any &#x3D; &#123; done: false &#125; while (!next.done) &#123; next &#x3D; it.next(); if (!next.done) &#123; console.log(next.value) &#125; &#125; &#125; gen(array) gen(map) &#x2F;&#x2F; gen(obj) &#x2F;&#x2F;error obj 并不是一个可迭代对象 我们平时开发中不会手动调用iterator 应为 他是有语法糖的就是for of 记住 for of 是不能循环对象的因为对象没有 iterator for (let value of array) &#123; console.log(value) &#125; 我们可以自己实现一个迭代器让对象支持for of const obj &#x3D; &#123; max: 5, current: 0, [Symbol.iterator]() &#123; return &#123; max: this.max, current: this.current, next() &#123; if (this.current &#x3D;&#x3D; this.max) &#123; return &#123; value: undefined, done: true &#125; &#125; else &#123; return &#123; value: this.current++, done: false &#125; &#125; &#125; &#125; &#125; &#125; console.log([...obj]) for (let val of obj) &#123; console.log(val); &#125; 众所周知的Symbols Symbol.hasInstance方法，会被instanceof运算符调用。构造器对象用来识别一个对象是否是其实例。 Symbol.isConcatSpreadable布尔值，表示当在一个对象上调用Array.prototype.concat时，这个对象的数组元素是否可展开。 Symbol.iterator方法，被for-of语句调用。返回对象的默认迭代器。 Symbol.match方法，被String.prototype.match调用。正则表达式用来匹配字符串。 Symbol.replace方法，被String.prototype.replace调用。正则表达式用来替换字符串中匹配的子串。 Symbol.search方法，被String.prototype.search调用。正则表达式返回被匹配部分在字符串中的索引。 Symbol.species函数值，为一个构造函数。用来创建派生对象。 Symbol.split方法，被String.prototype.split调用。正则表达式来用分割字符串。 Symbol.toPrimitive方法，被ToPrimitive抽象操作调用。把对象转换为相应的原始值。 Symbol.toStringTag方法，被内置方法Object.prototype.toString调用。返回创建对象时默认的字符串描述。 Symbol.unscopables对象，它自己拥有的属性会被with作用域排除在外。 泛型泛型允许你创建可重用的组件，这些组件可以在不修改源代码的情况下使用不同类型的数据。它们使代码更灵活、更可维护。 语法 function myFunction&lt;T&gt;(arg: T): T &#123; &#x2F;&#x2F; ... &#125; 在这个示例中，T 是一个类型变量，它可以被任何类型替换。这意味着 myFunction 可以接受和返回任何类型的数据 函数泛型我写了两个函数一个是数字类型的函数，另一个是字符串类型的函数,其实就是类型不同， 实现的功能是一样的，这时候我们就可以使用泛型来优化 function num (a:number,b:number) : Array&lt;number&gt; &#123; return [a ,b]; &#125; num(1,2) function str (a:string,b:string) : Array&lt;string&gt; &#123; return [a ,b]; &#125; str(&#39;独孤&#39;,&#39;求败&#39;) 泛型优化 语法为函数名字后面跟一个&lt;参数名&gt; 参数名可以随便写 例如我这儿写了T 当我们使用这个函数的时候把参数的类型传进去就可以了 （也就是动态类型） function Add&lt;T&gt;(a: T, b: T): Array&lt;T&gt; &#123; return [a,b] &#125; Add&lt;number&gt;(1,2) Add&lt;string&gt;(&#39;1&#39;,&#39;2&#39;) 我们也可以使用不同的泛型参数名，只要在数量上和使用方式上能对应上就可以。 function Sub&lt;T,U&gt;(a:T,b:U):Array&lt;T|U&gt; &#123; const params:Array&lt;T|U&gt; &#x3D; [a,b] return params &#125; Sub&lt;Boolean,number&gt;(false,1) 定义泛型接口声明接口的时候 在名字后面加一个&lt;参数&gt; 使用的时候传递类型 interface MyInter&lt;T&gt; &#123; (arg: T): T &#125; function fn&lt;T&gt;(arg: T): T &#123; return arg &#125; let result: MyInter&lt;number&gt; &#x3D; fn result(123) 对象字面量泛型let foo: &#123; &lt;T&gt;(arg: T): T &#125; foo &#x3D; function &lt;T&gt;(arg:T):T &#123; return arg &#125; foo(123) 泛型约束我们期望在一个泛型的变量上面，获取其length参数，但是，有的数据类型是没有length属性的 function getLegnth&lt;T&gt;(arg:T) &#123; return arg.length &#x2F;&#x2F; 类型“T”上不存在属性“length”。 &#125; 于是，我们就得对使用的泛型进行约束，我们约束其为具有length属性的类型，这里我们会用到interface,代码如下 interface Len &#123; length:number &#125; function getLegnth&lt;T extends Len&gt;(arg:T) &#123; return arg.length &#125; getLegnth&lt;string&gt;(&#39;123&#39;) 使用keyof 约束对象其中使用了TS泛型和泛型约束。首先定义了T类型并使用extends关键字继承object类型的子类型，然后使用keyof操作符获取T类型的所有键，它的返回 类型是联合 类型，最后利用extends关键字约束 K类型必须为keyof T联合类型的子类型 function prop&lt;T, K extends keyof T&gt;(obj: T, key: K) &#123; return obj[key] &#125; let o &#x3D; &#123; a: 1, b: 2, c: 3 &#125; prop(o, &#39;a&#39;) prop(o, &#39;d&#39;) &#x2F;&#x2F;此时就会报错发现找不到 泛型类声明方法跟函数类似名称后面定义&lt;类型&gt; 使用的时候确定类型new Sub() class Sub&lt;T&gt;&#123; attr: T[] &#x3D; []; add (a:T):T[] &#123; return [a] &#125; &#125; let s &#x3D; new Sub&lt;number&gt;() s.attr &#x3D; [1,2,3] s.add(123) let str &#x3D; new Sub&lt;string&gt;() str.attr &#x3D; [&#39;1&#39;,&#39;2&#39;,&#39;3&#39;] str.add(&#39;123&#39;) tsconfig.json配置文件生成tsconfig.json 文件这个文件是通过tsc –init命令生成的 配置详解 &quot;compilerOptions&quot;: &#123; &quot;incremental&quot;: true, &#x2F;&#x2F; TS编译器在第一次编译之后会生成一个存储编译信息的文件，第二次编译会在第一次的基础上进行增量编译，可以提高编译的速度 &quot;tsBuildInfoFile&quot;: &quot;.&#x2F;buildFile&quot;, &#x2F;&#x2F; 增量编译文件的存储位置 &quot;diagnostics&quot;: true, &#x2F;&#x2F; 打印诊断信息 &quot;target&quot;: &quot;ES5&quot;, &#x2F;&#x2F; 目标语言的版本 &quot;module&quot;: &quot;CommonJS&quot;, &#x2F;&#x2F; 生成代码的模板标准 &quot;outFile&quot;: &quot;.&#x2F;app.js&quot;, &#x2F;&#x2F; 将多个相互依赖的文件生成一个文件，可以用在AMD模块中，即开启时应设置&quot;module&quot;: &quot;AMD&quot;, &quot;lib&quot;: [&quot;DOM&quot;, &quot;ES2015&quot;, &quot;ScriptHost&quot;, &quot;ES2019.Array&quot;], &#x2F;&#x2F; TS需要引用的库，即声明文件，es5 默认引用dom、es5、scripthost,如需要使用es的高级版本特性，通常都需要配置，如es8的数组新特性需要引入&quot;ES2019.Array&quot;, &quot;allowJS&quot;: true, &#x2F;&#x2F; 允许编译器编译JS，JSX文件 &quot;checkJs&quot;: true, &#x2F;&#x2F; 允许在JS文件中报错，通常与allowJS一起使用 &quot;outDir&quot;: &quot;.&#x2F;dist&quot;, &#x2F;&#x2F; 指定输出目录 &quot;rootDir&quot;: &quot;.&#x2F;&quot;, &#x2F;&#x2F; 指定输出文件目录(用于输出)，用于控制输出目录结构 &quot;declaration&quot;: true, &#x2F;&#x2F; 生成声明文件，开启后会自动生成声明文件 &quot;declarationDir&quot;: &quot;.&#x2F;file&quot;, &#x2F;&#x2F; 指定生成声明文件存放目录 &quot;emitDeclarationOnly&quot;: true, &#x2F;&#x2F; 只生成声明文件，而不会生成js文件 &quot;sourceMap&quot;: true, &#x2F;&#x2F; 生成目标文件的sourceMap文件 &quot;inlineSourceMap&quot;: true, &#x2F;&#x2F; 生成目标文件的inline SourceMap，inline SourceMap会包含在生成的js文件中 &quot;declarationMap&quot;: true, &#x2F;&#x2F; 为声明文件生成sourceMap &quot;typeRoots&quot;: [], &#x2F;&#x2F; 声明文件目录，默认时node_modules&#x2F;@types &quot;types&quot;: [], &#x2F;&#x2F; 加载的声明文件包 &quot;removeComments&quot;:true, &#x2F;&#x2F; 删除注释 &quot;noEmit&quot;: true, &#x2F;&#x2F; 不输出文件,即编译后不会生成任何js文件 &quot;noEmitOnError&quot;: true, &#x2F;&#x2F; 发送错误时不输出任何文件 &quot;noEmitHelpers&quot;: true, &#x2F;&#x2F; 不生成helper函数，减小体积，需要额外安装，常配合importHelpers一起使用 &quot;importHelpers&quot;: true, &#x2F;&#x2F; 通过tslib引入helper函数，文件必须是模块 &quot;downlevelIteration&quot;: true, &#x2F;&#x2F; 降级遍历器实现，如果目标源是es3&#x2F;5，那么遍历器会有降级的实现 &quot;strict&quot;: true, &#x2F;&#x2F; 开启所有严格的类型检查 &quot;alwaysStrict&quot;: true, &#x2F;&#x2F; 在代码中注入&#39;use strict&#39; &quot;noImplicitAny&quot;: true, &#x2F;&#x2F; 不允许隐式的any类型 &quot;strictNullChecks&quot;: true, &#x2F;&#x2F; 不允许把null、undefined赋值给其他类型的变量 &quot;strictFunctionTypes&quot;: true, &#x2F;&#x2F; 不允许函数参数双向协变 &quot;strictPropertyInitialization&quot;: true, &#x2F;&#x2F; 类的实例属性必须初始化 &quot;strictBindCallApply&quot;: true, &#x2F;&#x2F; 严格的bind&#x2F;call&#x2F;apply检查 &quot;noImplicitThis&quot;: true, &#x2F;&#x2F; 不允许this有隐式的any类型 &quot;noUnusedLocals&quot;: true, &#x2F;&#x2F; 检查只声明、未使用的局部变量(只提示不报错) &quot;noUnusedParameters&quot;: true, &#x2F;&#x2F; 检查未使用的函数参数(只提示不报错) &quot;noFallthroughCasesInSwitch&quot;: true, &#x2F;&#x2F; 防止switch语句贯穿(即如果没有break语句后面不会执行) &quot;noImplicitReturns&quot;: true, &#x2F;&#x2F;每个分支都会有返回值 &quot;esModuleInterop&quot;: true, &#x2F;&#x2F; 允许export&#x3D;导出，由import from 导入 &quot;allowUmdGlobalAccess&quot;: true, &#x2F;&#x2F; 允许在模块中全局变量的方式访问umd模块 &quot;moduleResolution&quot;: &quot;node&quot;, &#x2F;&#x2F; 模块解析策略，ts默认用node的解析策略，即相对的方式导入 &quot;baseUrl&quot;: &quot;.&#x2F;&quot;, &#x2F;&#x2F; 解析非相对模块的基地址，默认是当前目录 &quot;paths&quot;: &#123; &#x2F;&#x2F; 路径映射，相对于baseUrl &#x2F;&#x2F; 如使用jq时不想使用默认版本，而需要手动指定版本，可进行如下配置 &quot;jquery&quot;: [&quot;node_modules&#x2F;jquery&#x2F;dist&#x2F;jquery.min.js&quot;] &#125;, &quot;rootDirs&quot;: [&quot;src&quot;,&quot;out&quot;], &#x2F;&#x2F; 将多个目录放在一个虚拟目录下，用于运行时，即编译后引入文件的位置可能发生变化，这也设置可以虚拟src和out在同一个目录下，不用再去改变路径也不会报错 &quot;listEmittedFiles&quot;: true, &#x2F;&#x2F; 打印输出文件 &quot;listFiles&quot;: true&#x2F;&#x2F; 打印编译的文件(包括引用的声明文件) &#125; &#x2F;&#x2F; 指定一个匹配列表（属于自动指定该路径下的所有ts相关文件） &quot;include&quot;: [ &quot;src&#x2F;**&#x2F;*&quot; ], &#x2F;&#x2F; 指定一个排除列表（include的反向操作） &quot;exclude&quot;: [ &quot;demo.ts&quot; ], &#x2F;&#x2F; 指定哪些文件使用该配置（属于手动一个个指定文件） &quot;files&quot;: [ &quot;demo.ts&quot; ] 介绍几个常用的 1.include指定编译文件默认是编译当前目录下所有的ts文件 2.exclude指定排除的文件 3.target指定编译js 的版本例如es5 es6 4.allowJS是否允许编译js文件 5.removeComments是否在编译过程中删除文件中的注释 6.rootDir编译文件的目录 7.outDir输出的目录 8.sourceMap代码源文件 9.strict严格模式 10.module默认common.js 可选es6模式 amd umd 等 命名空间命名空间是一种组织和封装相关代码的机制，它可以防止名称冲突并提高代码的可读性和可维护性。 TypeScript与ECMAScript 2015一样，任何包含顶级import或者export的文件都被当成一个模块。相反地，如果一个文件不带有顶级的import或者export声明，那么它的内容被视为全局可见的（因此对模块也是可见的） 创建命名空间 要创建命名空间，可以使用 namespace 关键字，后跟命名空间的名称： namespace MyNamespace &#123; // 命名空间代码 &#125; 访问命名空间成员 要访问命名空间中的成员，可以使用点运算符（.）： MyNamespace.myFunction(); 命名空间中通过export将想要暴露的部分导出 如果不用export 导出是无法读取其值的 namespace a &#123; export const Time: number = 1000 export const fn = &lt;T>(arg: T): T => &#123; return arg &#125; fn(Time) &#125; namespace b &#123; export const Time: number = 1000 export const fn = &lt;T>(arg: T): T => &#123; return arg &#125; fn(Time) &#125; a.Time b.Time 嵌套命名空间 命名空间可以嵌套，这意味着一个命名空间可以包含另一个命名空间： namespace a &#123; export namespace b &#123; export class Vue &#123; parameters: string constructor(parameters: string) &#123; this.parameters = parameters &#125; &#125; &#125; &#125; let v = a.b.Vue console.log(v) //[class Vue] new v('1') 默认命名空间 TypeScript 中存在一个默认命名空间，它包含没有明确指定命名空间的代码。默认命名空间的名称是全局对象（通常是 window）。 抽离命名空间 a.ts export namespace V &#123; export const a &#x3D; 1 &#125; b.ts import &#123;V&#125; from &#39;..&#x2F;observer&#x2F;index&#39; console.log(V); &#x2F;&#x2F;&#123;a:1&#125; 简化命名空间 namespace A &#123; export namespace B &#123; export const C &#x3D; 1; &#125; &#125; import x &#x3D; A.B.C; console.log(x); &#x2F;&#x2F;1 合并命名空间 重名的命名空间会合并 namespace A &#123; export namespace B &#123; export const C &#x3D; 1; &#125; &#125; namespace A &#123; export namespace B &#123; export const D &#x3D; 1; &#125; &#125; console.log(A); &#x2F;&#x2F;&#123; B: &#123; C: 1, D: 1 &#125; &#125; 命名空间和模块 命名空间与模块类似，但它们之间存在一些关键区别： 作用域： 命名空间是全局作用域的，这意味着它们可以在代码中的任何地方访问。模块是块作用域的，这意味着它们只能在定义它们的块内访问。 导入&#x2F;导出： 命名空间不需要导入或导出，而模块必须使用 import 和 export 语句进行导入或导出。 封装： 命名空间不提供封装，而模块通过模块系统提供封装。 使用命名空间的好处 使用命名空间的主要好处包括： 防止名称冲突： 命名空间有助于防止来自不同模块或库的名称冲突。 提高可读性： 命名空间可以将相关代码组织到有意义的组中，从而提高代码的可读性。 可维护性： 通过将代码组织到命名空间中，可以更容易地维护和更新代码。 前端模块化在 ES6 模块化规范之前，前端模块化有以下规范： CommonJS：一种用于 Node.js 的模块化规范，使用 require() 和 module.exports 来导出和导入模块。 // 导入 require(\"xxx\"); require(\"../xxx.js\"); // 导出 exports.xxxxxx= function() &#123;&#125;; module.exports = xxxxx; AMD (Asynchronous Module Definition)：一种用于在浏览器中加载异步模块的规范，使用 define() 和 require() 函数来定义和加载模块。 // 定义 define(\"module\", [\"dep1\", \"dep2\"], function(d1, d2) &#123;...&#125;); // 加载模块 require([\"module\", \"../app\"], function(module, app) &#123;...&#125;); CMD (Common Module Definition)：类似于 AMD，但更适合于在服务器端加载模块。 define(function(require, exports, module) &#123; var a = require('./a'); a.doSomething(); var b = require('./b'); b.doSomething(); &#125;); UMD (Universal Module Definition)：一种通用模块化规范，可以在浏览器和 Node.js 中使用。 (function (window, factory) &#123; // 检测是不是 Nodejs 环境 if (typeof module === 'object' &amp;&amp; typeof module.exports === \"objects\") &#123; module.exports = factory(); &#125; // 检测是不是 AMD 规范 else if (typeof define === 'function' &amp;&amp; define.amd) &#123; define(factory); &#125; // 使用浏览器环境 else &#123; window.eventUtil = factory(); &#125; &#125;)(this, function () &#123; //module ... &#125;); SystemJS：一种现代模块加载器，支持多种模块化规范，包括 ES6 模块。 这些规范允许前端开发者将代码分解为可重用的模块，从而提高代码的可维护性和可扩展性。然而，它们在语法和加载机制上存在差异，这给开发人员带来了额外的复杂性。 ES6 模块化规范（也称为 ECMAScript 模块）于 2015 年引入，它提供了一种更简单、更统一的方式来定义和加载模块。它使用 import 和 export 关键字，并且内置于现代浏览器和 Node.js 中，消除了对外部模块加载器的需要。 ES6 模块化规范出来之后上面这些模块化规范就用的比较少了,现在主要使用 import export . es6模块化规范用法 1.默认导出和引入 默认导出可以导出任意类型，这儿举例导出一个对象，并且默认导出只能有一个 引入的时候名字可以随便起 test.ts &#x2F;&#x2F;导出 export default &#123; a:1, &#125; &#x2F;&#x2F;引入 import test from &quot;.&#x2F;test&quot;; 2.分别导出 &#x2F;&#x2F;导出 index.ts export default &#123; a: 1, b: 2, &#125; export function add&lt;T extends number&gt;(a: T, b: T) &#123; return a + b &#125; export let xxx &#x3D; &quot;xxx&quot; &#x2F;&#x2F;引入 import obj, &#123; xxx, add &#125; from &#39;.&#x2F;index&#39; var b &#x3D; add(1, 2); console.log(obj); console.log(xxx); console.log(b); 3.重名问题 如果 导入的时候叫add但是已经有变量占用了可以用as重命名 import obj, &#123; xxx as str, add &#125; from &#39;.&#x2F;index&#39; console.log(str); 4.动态引入 import只能写在顶层，不能掺杂到逻辑里面，这时候就需要动态引入了 if(true)&#123; import(&#39;.&#x2F;test&#39;).then(res &#x3D;&gt; &#123; console.log(res) &#125;) &#125; 声明文件d.ts声明文件（也称为类型声明文件）是 TypeScript 中以 .d.ts 为扩展名的文件。它们包含有关 JavaScript 库、模块或 API 的类型信息，而无需实际实现这些库、模块或 API。 声明文件允许 TypeScript 编译器检查使用这些库、模块或 API 的 TypeScript 代码的类型正确性。它们提供有关函数、类、接口和变量的类型信息，从而使编译器可以确保代码中使用的类型与声明文件中定义的类型兼容。 当使用第三方库时，我们需要引用它的声明文件，才能获得对应的代码补全、接口提示等功能。 declare var 声明全局变量 declare function 声明全局方法 declare class 声明全局类 declare enum 声明全局枚举类型 declare namespace 声明（含有子属性的）全局对象 interface 和 type 声明全局类型 /// &lt;reference /> 三斜线指令 用途 声明文件用于以下用途： 类型检查：允许 TypeScript 编译器检查代码中使用的类型的正确性，即使这些类型来自外部库或 API。 自动完成：在使用外部库或 API 时提供代码自动完成和 IntelliSense。 重构：允许重构工具（例如 Visual Studio Code）重构使用外部库或 API 的代码。 文档：提供有关外部库或 API 的类型信息的文档。 创建声明文件 可以使用以下方法创建声明文件： 手动创建：手动编写声明文件，并确保它们与外部库或 API 的实际实现兼容。 使用工具：使用诸如 DefinitelyTyped 之类的工具自动生成声明文件。 从现有代码生成：使用诸如 dtsgenerator 之类的工具从现有 JavaScript 代码生成声明文件。 案例 初始化项目后，安装axios和express PS D:\\Study\\TS> npm i express added 64 packages in 2s PS D:\\Study\\TS> npm i axios added 7 packages in 1s 在Index.ts中引入两个库 import axios from \"axios\"; import express from \"express\"; //error 报错显示： 已声明“express”，但从未读取其值。ts(6133) 无法找到模块“express”的声明文件。“d:&#x2F;Study&#x2F;TS&#x2F;node_modules&#x2F;express&#x2F;index.js”隐式拥有 “any” 类型。 尝试使用 &#96;&#96; (如果存在)，或者添加一个包含 declare module &#39;express&#39;; 的新声明(.d.ts)文件ts(7016) 让我们去下载他的声明文件npm i --save-dev @types/express 那为什么axios 没有报错? 我们可以去node_modules 下面去找axios 的package json 发现axios已经指定了声明文件 所以没有报错可以直接用 通过语法declare 暴露我们声明的axios 对象declare const axios: AxiosStatic; 如果有一些第三方包确实没有声明文件我们可以自己去定义名称.d.ts 创建一个文件去声明 案例手写声明文件 index.ts import axios from \"axios\"; import express from \"express\"; const app = express(); const router = express.Router(); app.use(\"/\", router); router.get(\"/\", (req, res) => &#123; res.json(&#123; code: 200, message: \"Hello Worlld\" &#125;); &#125;); app.listen(3000, () => &#123; console.log(\"Server is running on port 3000\"); &#125;); fn(\"abc\") C.a express.d.ts declare module \"express\" &#123; interface Express &#123; (): App; Router(): Router; &#125; interface App &#123; use: (path: string, router: any) => void; listen(port: number, cb?: () => void): void &#125; interface Router &#123; get: (path: string, cb: (req: any, res: any) => void) => void; &#125; const express: Express; export default express; &#125; declare var a: number; declare function fn(param: type) &#123; &#125; declare class Vue &#123; &#125; declare enum C &#123; a, b &#125; Mixins混入（不熟悉）Mixins 是 TypeScript 中的一种设计模式，它允许将多个类的功能组合到一个类中，而无需使用继承。这使得可以轻松地向现有类添加新功能，而无需修改其源代码。 1.对象混入可以使用es6的Object.assign 合并多个对象 此时 people 会被推断成一个交差类型 Name &amp; Age &amp; sex; interface Name &#123; name: string &#125; interface Age &#123; age: number &#125; interface Sex &#123; sex: number &#125; let people1: Name &#x3D; &#123; name: &quot;小满&quot; &#125; let people2: Age &#x3D; &#123; age: 20 &#125; let people3: Sex &#x3D; &#123; sex: 1 &#125; const people &#x3D; Object.assign(people1,people2,people3) 2.类的混入首先声明两个mixins类 （严格模式要关闭不然编译不过）&#x3D;&gt; &quot;strict&quot;: false, class A &#123; type: boolean &#x3D; false; changeType() &#123; this.type &#x3D; !this.type &#125; &#125; class B &#123; name: string &#x3D; &#39;张三&#39;; getName(): string &#123; return this.name; &#125; &#125; 下面创建一个类，结合了这两个mixins 首先应该注意到的是，没使用extends而是使用implements。 把类当成了接口 我们可以这么做来达到目的，为将要mixin进来的属性方法创建出占位属性。 这告诉编译器这些成员在运行时是可用的。 这样就能使用mixin带来的便利，虽说需要提前定义一些占位属性 class C implements A,B&#123; type:boolean changeType:()&#x3D;&gt;void; name: string; getName:()&#x3D;&gt; string &#125; 最后，创建这个帮助函数，帮我们做混入操作。 它会遍历mixins上的所有属性，并复制到目标上去，把之前的占位属性替换成真正的实现代码 Object.getOwnPropertyNames()可以获取对象自身的属性，除去他继承来的属性，对它所有的属性遍历，它是一个数组，遍历一下它所有的属性名 Mixins(C, [A, B]) function Mixins(curCls: any, itemCls: any[]) &#123; itemCls.forEach(item &#x3D;&gt; &#123; Object.getOwnPropertyNames(item.prototype).forEach(name &#x3D;&gt; &#123; curCls.prototype[name] &#x3D; item.prototype[name] &#125;) &#125;) &#125; 装饰器DecoratorDecorator 装饰器是一项实验性特性，在未来的版本中可能会发生改变 它们不仅增加了代码的可读性，清晰地表达了意图，而且提供一种方便的手段，增加或修改类的功能 若要启用实验性的装饰器特性，你必须在命令行或tsconfig.json里启用编译器选项 &quot;experimentalDecorators&quot;: true, &quot;emitDecoratorMetadata&quot;: true, 所有装饰器类装饰器： @ClassDecorator：应用于类本身。 方法装饰器： @MethodDecorator：应用于类方法。 @Get：应用于类访问器（getter）方法。 @Set：应用于类访问器（setter）方法。 属性装饰器： @PropertyDecorator：应用于类属性。 @Field：应用于类字段（仅限于使用 --experimentalDecorators 标志）。 参数装饰器： @ParameterDecorator：应用于类方法的参数。 其他装饰器： @DecoratorFactory：用于创建自定义装饰器的工厂装饰器。 注意： @ClassDecorator、@MethodDecorator、@PropertyDecorator 和 @ParameterDecorator 是 TypeScript 中内置的通用装饰器类型。 @Get、@Set 和 @Field 是特定于特定库或框架的装饰器（例如 Angular 和 MobX）。 @DecoratorFactory 是一个元装饰器，用于创建自定义装饰器。 类装饰器 ClassDecorator 首先定义一个类 class A &#123; constructor() &#123; console.log(&#39;A&#39;); &#125; &#125; 定义一个类装饰器函数 const Base: ClassDecorator &#x3D; (target) &#x3D;&gt; &#123; &#x2F;&#x2F;target是类名 console.log(&quot;target:&quot; + target); target.prototype.name &#x3D; &#39;测试类装饰器&#39;; &#125; 使用的时候 直接通过@函数名使用 @Base class A &#123; constructor() &#123; console.log(&#39;A&#39;); &#125; &#125; 验证 const a &#x3D; new A() as any; console.log(a.name); &#x2F;&#x2F;测试类装饰器 &#x2F;&#x2F; target:class A &#123; &#x2F;&#x2F; constructor() &#123; &#x2F;&#x2F; console.log(&#39;A&#39;); &#x2F;&#x2F; &#125; &#x2F;&#x2F; &#125; &#x2F;&#x2F; A &#x2F;&#x2F; 测试类装饰器 装饰器工厂其实也就是一个高阶函数 外层的函数接受值 里层的函数最终接受类的构造函数 const Base &#x3D; (args: any) &#x3D;&gt; &#123; const fn: ClassDecorator &#x3D; (target: any) &#x3D;&gt; &#123; console.log(&quot;target:&quot; + target); console.log(&quot;args:&quot; + args); &#125; return fn; &#125; @Base(&quot;装饰器工厂&quot;) class A &#123; &#125; const a &#x3D; new A() as any; 属性装饰器 PropertyDecorator返回两个参数 对于静态成员来说是类的构造函数，对于实例成员是类的原型对象。 属性的名字。 const PropDecorator &#x3D; (args: any) &#x3D;&gt; &#123; const fn: PropertyDecorator &#x3D; (target, propertyKey) &#x3D;&gt; &#123; console.log(target); console.log(propertyKey); &#x2F;&#x2F;属性名 console.log(&quot;args:&quot; + args); &#x2F;&#x2F;参数 &#125; return fn; &#125; class A &#123; @PropDecorator(&quot;属性装饰器&quot;) name: string &#x3D; &quot;张三&quot;; constructor() &#123; this.name &#x3D; &quot;张三&quot;; &#125; &#125; const a &#x3D; new A() as any; console.log(a); &#x2F;&#x2F; &#123;&#125; &#x2F;&#x2F; name &#x2F;&#x2F; args:属性装饰器 &#x2F;&#x2F; A &#123; name: &#39;张三&#39; &#125; 参数装饰器 ParameterDecorator返回三个参数 对于静态成员来说是类的构造函数，对于实例成员是类的原型对象。 成员的名字。 参数在函数参数列表中的索引。 class A &#123; constructor() &#123; &#125; setParasm(@ParamDecorator method: string &#x3D; &#39;参数装饰器&#39;) &#123; &#125; &#125; const a &#x3D; new A() as any; function ParamDecorator(target: A, propertyKey: any, parameterIndex: number): void &#123; console.log(target); console.log(propertyKey); &#x2F;&#x2F;方法名 console.log(parameterIndex); &#x2F;&#x2F;方法的索引位置 &#125; 方法装饰器返回三个参数 对于静态成员来说是类的构造函数，对于实例成员是类的原型对象。 成员的名字。 成员的属性描述符。 class A &#123; constructor() &#123; &#125; @MethDecorator setParasm(method: string &#x3D; &#39;参数装饰器&#39;) &#123; &#125; &#125; const a &#x3D; new A() as any; function MethDecorator(target: A, propertyKey: &#39;setParasm&#39;, descriptor?: PropertyDescriptor): void &#123; console.log(target); console.log(propertyKey); &#x2F;&#x2F; 方法名 console.log(descriptor); &#x2F;&#x2F; 成员的属性描述符 &#125; &#x2F;&#x2F; &#123;&#125; &#x2F;&#x2F; setParasm &#x2F;&#x2F; &#123; &#x2F;&#x2F; value: [Function: setParasm], &#x2F;&#x2F; writable: true, &#x2F;&#x2F; enumerable: false, &#x2F;&#x2F; configurable: true &#x2F;&#x2F; &#125; 案例import axios from &#39;axios&#39;; import &#39;reflect-metadata&#39;; class Http &#123; constructor() &#123; &#125; @Get(&#39;https:&#x2F;&#x2F;api.apiopen.top&#x2F;api&#x2F;getHaoKanVideo?page&#x3D;0&amp;size&#x3D;5&#39;) getList(@Result data: any) &#123; console.log(data); &#125; &#125; const http &#x3D; new Http() as any; function Get(url: string) &#123; const fn: MethodDecorator &#x3D; function (target: any, propertyKey: string, descriptor: PropertyDescriptor) &#123; axios.get(url).then(res &#x3D;&gt; &#123; &#x2F;&#x2F; console.log(res); const getedKey &#x3D; Reflect.getMetadata(&#39;key&#39;, target); descriptor.value(getedKey ? res.data[getedKey] : res.data); &#125;) &#125; return fn; &#125; function Result(target: Object, propertyKey: string | symbol, parameterIndex: number) &#123; console.log(target, propertyKey, parameterIndex); Reflect.defineMetadata(&#39;key&#39;, &#39;result&#39;, target); &#125; webpack构建ts+vue3项目webpack构建ts+vue3项目 实战TS编写发布订阅模式on订阅&#x2F;监听 emit 发布&#x2F;注册 once 只执行一次 off解除绑定 interface EventFace &#123; on(event: string, callback: Function): void; emit(event: string, ...args: any): void; off(event: string, callback: Function): void; once(event: string, callback: Function): void; &#125; class Emitter implements EventFace &#123; events: Map&lt;string, Function[]>; constructor() &#123; this.events = new Map(); &#125; on(event: string, callback: Function) &#123; if (this.events.has(event)) &#123; const callbackList = this.events.get(event); callbackList &amp;&amp; callbackList.push(callback); &#125; else &#123; this.events.set(event, [callback]); &#125; &#125; emit(event: string, ...args: any) &#123; const callbackList = this.events.get(event); if (callbackList) &#123; callbackList.forEach(fn => &#123; fn(...args); &#125;); &#125; &#125;; off(event: string, callback: Function) &#123; const callbackList = this.events.get(event); // console.log(\"callbackList-off:\" + callbackList); if (callbackList) &#123; let index = callbackList.indexOf(callback); callbackList.splice(index, 1); &#125; &#125;; once(event: string, callback: Function) &#123; const fn = (...args: any) => &#123; callback(...args); this.off(event, callback); &#125;; this.on(event, fn); &#125;; &#125; const bus = new Emitter(); const fn = (b: boolean, n: number) => &#123; console.log(1, b, n); &#125;; bus.once('once', fn) bus.emit('once', false, Math.random()); bus.emit('once', false, Math.random()); bus.emit('once', false, Math.random()); console.log(bus); weakMap，weakSet，set，map在es5的时候常用的Array object ，在es6又新增了两个类型，Set和Map，类似于数组和对象。 WeakMap 和 WeakSetWeakMap 和 WeakSet 是 JavaScript 中的集合类型，其元素是弱引用的对象。这意味着： 当对象不再被任何其他变量引用时，它们会被垃圾回收。 WeakMap 和 WeakSet 自身不会阻止对象被垃圾回收。 用法： WeakMap 可用于将对象作为键存储值。 WeakSet 可用于存储对象，而无需关联任何值。 示例： // WeakMap const weakMap = new WeakMap(); const object = &#123;&#125;; weakMap.set(object, \"value\"); // WeakSet const weakSet = new WeakSet(); weakSet.add(object); Map 和 SetMap 和 Set 是 JavaScript 中的集合类型，其元素是强引用的值。这意味着： 只要 Map 或 Set 中包含元素，它们就不会被垃圾回收。 Map 和 Set 自身可以阻止元素被垃圾回收。 用法： Map 可用于将键值对存储在集合中。 Set 可用于存储唯一值。 示例： // Map const map = new Map(); map.set(\"key\", \"value\"); // Set const set = new Set(); set.add(\"value\"); 比较 特征 WeakMap&#x2F;WeakSet Map&#x2F;Set 元素类型 弱引用对象 强引用值 垃圾回收 不会阻止 会阻止 键 只能是对象 任意值 用途 存储对象，避免循环引用 存储键值对或唯一值 何时使用 使用 WeakMap&#x2F;WeakSet：当需要存储对象且不希望阻止它们被垃圾回收时。例如，在缓存中存储对象。 使用 Map&#x2F;Set：当需要存储强引用值时。例如，在对象中存储数据或跟踪唯一元素。 案例首先obj引用了这个对象 + 1，aahph也引用了 + 1，wmap也引用了，但是不会 + 1，应为他是弱引用，不会计入垃圾回收，因此 obj 和 aahph 释放了该引用 weakMap 也会随着消失的，但是有个问题你会发现控制台能输出，值是取不到的，应为V8的GC回收是需要一定时间的，你可以延长到500ms看一看，并且为了避免这个问题不允许读取键值，也不允许遍历，同理weakSet 也一样 let obj:any &#x3D; &#123;name:&#39;小满zs&#39;&#125; &#x2F;&#x2F;1 let aahph:any &#x3D; obj &#x2F;&#x2F;2 let wmap:WeakMap&lt;object,string&gt; &#x3D; new WeakMap() wmap.set(obj,&#39;爱安徽潘慧&#39;) &#x2F;&#x2F;2 他的键是弱引用不会计数的 obj &#x3D; null &#x2F;&#x2F; -1 aahph &#x3D; null;&#x2F;&#x2F;-1 &#x2F;&#x2F;v8 GC 不稳定 最少200ms setTimeout(()&#x3D;&gt;&#123; console.log(wmap) &#125;,500) proxy &amp; ReflectProxy Proxy对象用于创建一个对象的代理，从而实现基本操作的拦截和自定义（如属性查找、赋值、枚举、函数调用等） target 要使用 Proxy 包装的目标对象（可以是任何类型的对象，包括原生数组，函数，甚至另一个代理）。 handler 一个通常以函数作为属性的对象，各属性中的函数分别定义了在执行各种操作时代理 p 的行为。 handler.get() 本次使用的get 属性读取操作的捕捉器。 handler.set() 本次使用的set 属性设置操作的捕捉器。 Reflect与大多数全局对象不同Reflect并非一个构造函数，所以不能通过new运算符对其进行调用，或者将Reflect对象作为一个函数来调用。Reflect的所有属性和方法都是静态的（就像Math对象） Reflect.get(target, name, receiver)Reflect.get方法查找并返回target对象的name属性，如果没有该属性返回undefined Reflect.set(target, name,value, receiver)Reflect.set方法设置target对象的name属性等于value。 type Person &#x3D; &#123; name: string, age: number, text: string &#125; const proxy &#x3D; (object: any, key: any) &#x3D;&gt; &#123; return new Proxy(object, &#123; get: (target, prop, receiver) &#x3D;&gt; &#123; console.log(&#96;get key&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;$&#123;key&#125;&#96;); return Reflect.get(target, prop, receiver) &#125;, set: (target, prop, value, receiver) &#x3D;&gt; &#123; console.log(target) console.log(prop) console.log(value) console.log(&#96;set $&#123;key&#125;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;$&#123;value&#125;&#96;); return Reflect.set(target, prop, value, receiver) &#125; &#125;) &#125; &#x2F;&#x2F; const logAccess &#x3D; (object: Person, key: &#39;name&#39; | &#39;age&#39; | &#39;text&#39;) &#x3D;&gt; &#123; &#x2F;&#x2F; return proxy(object, key) &#x2F;&#x2F; &#125; &#x2F;&#x2F; 使用泛型+keyof优化 const logAccess &#x3D; &lt;T&gt;(object: T, key: keyof T): T &#x3D;&gt; &#123; return proxy(object, key) &#125; let man: Person &#x3D; logAccess(&#123; name: &#39;zhangsan&#39;, age: 18, text: &#39;hello&#39; &#125;, &quot;text&quot;) man.text &#x3D; &quot;hello world&quot; 案例简单实现一个mobx观察者模式 const list: Set&lt;Function&gt; &#x3D; new Set&lt;Function&gt;(); const autoRun &#x3D; (cb: Function) &#x3D;&gt; &#123; if (cb) &#123; list.add(cb); &#125; &#125; const obserbver &#x3D; &lt;T extends object&gt;(params: T) &#x3D;&gt; &#123; return new Proxy(params, &#123; set: (target: T, key: string, value: string, receiver: any) &#x3D;&gt; &#123; const result &#x3D; Reflect.set(target, key, value, receiver); list.forEach(cb &#x3D;&gt; cb()); return result; &#125; &#125;); &#125; const person &#x3D; obserbver(&#123; name: &quot;小满&quot;, attr: &quot;威猛先生&quot; &#125;) autoRun(() &#x3D;&gt; &#123; console.log(&#39;我变化了&#39;) console.log(person); &#125;); person.name &#x3D; &#39;zhangsan&#39;; 类型兼容所谓的类型兼容性，就是用于确定一个类型是否能赋值给其他的类型。typeScript中的类型兼容性是基于结构类型的（也就是形状），如果A要兼容B 那么A至少具有B相同的属性。 1.协变 也可以叫鸭子类型什么是鸭子类型？ 一只鸟 走路像鸭子 ，游泳也像，做什么都像，那么这只鸟就可以成为鸭子类型。 举例说明 interface A &#123; name:string age:number &#125; interface B &#123; name:string age:number sex:string &#125; let a:A &#x3D; &#123; name:&quot;老墨我想吃鱼了&quot;, age:33, &#125; let b:B &#x3D; &#123; name:&quot;老墨我不想吃鱼&quot;, age:33, sex:&quot;女&quot; &#125; a &#x3D; b A B 两个类型完全不同但是竟然可以赋值并无报错B类型充当A类型的子类型，当子类型里面的属性满足A类型就可以进行赋值，也就是说不能少可以多，这就是协变。 2.逆变逆变一般发生于函数的参数上面 举例说明 interface A &#123; name:string age:number &#125; interface B &#123; name:string age:number sex:string &#125; let a:A &#x3D; &#123; name:&quot;老墨我想吃鱼了&quot;, age:33, &#125; let b:B &#x3D; &#123; name:&quot;老墨我不想吃鱼&quot;, age:33, sex:&quot;女&quot; &#125; a &#x3D; b let fna &#x3D; (params:A) &#x3D;&gt; &#123; &#125; let fnb &#x3D; (params:B) &#x3D;&gt; &#123; &#125; fna &#x3D; fnb &#x2F;&#x2F;错误 fnb &#x3D; fna &#x2F;&#x2F;正确 这里比较绕，注意看fna 赋值 给 fnb 其实最后执行的还是fna 而 fnb的类型能够完全覆盖fna 所以这一定是安全的，相反fna的类型不能完全覆盖fnb少一个sex所以是不安全的。 3.双向协变tsconfig strictFunctionTypes 设置为false 支持双向协变 fna fnb 随便可以来回赋值 类型守卫在 TypeScript 中，类型守卫（Type Guards）是一种用于在运行时检查类型的机制。它们允许你在代码中执行特定的检查，以确定变量的类型，并在需要时执行相应的操作。 typeof 类型收缩 const isString &#x3D; (str:any) &#x3D;&gt; &#123; return typeof str &#x3D;&#x3D;&#x3D; &#39;string&#39;; &#125; 在这个例子里面我们声明一个函数可以接受任意类型，只筛选出字符串类型，进行类型收缩。 instanceof const isArr &#x3D; (value:unknown) &#x3D;&gt; &#123; if(value instanceof Array)&#123; value.length &#125; &#125; 使用 instanceof 类型守卫可以检查一个对象是否是特定类的实例 typeof 和 instanceof 区别typeof 和 instanceof 是 TypeScript 中用于类型检查的两个不同的操作符，它们有不同的作用和使用场景。 typeof 和 instanceof 是 TypeScript 中用于类型检查的两个不同的操作符，它们有不同的作用和使用场景。 const str &#x3D; &quot;Hello&quot;; console.log(typeof str); &#x2F;&#x2F; 输出: &quot;string&quot; const num &#x3D; 42; console.log(typeof num); &#x2F;&#x2F; 输出: &quot;number&quot; const bool &#x3D; true; console.log(typeof bool); &#x2F;&#x2F; 输出: &quot;boolean&quot; 注意事项：typeof 只能返回有限的字符串类型，包括 “string”、“number”、“boolean”、“symbol”、“undefined” 和 “object”。对于函数、数组、null 等类型，typeof 也会返回 “object”。因此，typeof 对于复杂类型和自定义类型的判断是有限的。 instanceof作用：instanceof 操作符用于检查一个对象是否是某个类的实例。它通过检查对象的原型链来确定对象是否由指定的类创建。 class Person &#123; name: string; constructor(name: string) &#123; this.name &#x3D; name; &#125; &#125; const person &#x3D; new Person(&quot;Alice&quot;); console.log(person instanceof Person); &#x2F;&#x2F; 输出: true const obj &#x3D; &#123;&#125;; console.log(obj instanceof Person); &#x2F;&#x2F; 输出: false 注意事项：instanceof 操作符主要用于检查对象是否是特定类的实例，它无法检查基本类型。此外，它也无法检查对象是通过字面量创建的，因为字面量对象没有显式的构造函数。 注意事项：instanceof 操作符主要用于检查对象是否是特定类的实例，它无法检查基本类型。此外，它也无法检查对象是通过字面量创建的，因为字面量对象没有显式的构造函数。 自定义守卫结合题目实现 实现一个函数支持任意类型如果是对象，就检查里面的属性，如果里面的属性是number就取两位，如果是string就去除左右空格如果是函数就执行 const isString &#x3D; (str:any)&#x3D;&gt; typeof str &#x3D;&#x3D;&#x3D; &#39;string&#39; const isNumber &#x3D; (num:any)&#x3D;&gt; typeof num &#x3D;&#x3D;&#x3D; &#39;number&#39; const isFn &#x3D; (fn:any)&#x3D;&gt; typeof fn &#x3D;&#x3D;&#x3D; &#39;function&#39; const isObj &#x3D; (obj:any)&#x3D;&gt; (&#123;&#125;).toString.call(obj) &#x3D;&#x3D;&#x3D; &#39;[object Object]&#39; const fn &#x3D; (data:any) &#x3D;&gt; &#123; let value; if(isObj(data))&#123; Object.keys(data).forEach(key&#x3D;&gt;&#123; value &#x3D; data[key] if(isString(value))&#123; data[key] &#x3D; value.trim() &#125; if(isNumber(value))&#123; data[key] &#x3D; value.toFixed(2) &#125; if(isFn(value))&#123; value() &#125; &#125;) &#125; &#125; const obj &#x3D; &#123; a: 100.22222, b: &#39; test &#39;, c: function () &#123; console.log(this.a); return this.a; &#125; &#125; fn(obj) 乍一看没啥问题 一运行就报错 乍一看没啥问题 一运行就报错 他说找不到a 当函数被单独调用时（例如 value()），函数内部的 this 会指向全局对象（在浏览器环境下是 window） 修改如下 const isString &#x3D; (str:any)&#x3D;&gt; typeof str &#x3D;&#x3D;&#x3D; &#39;string&#39; const isNumber &#x3D; (num:any)&#x3D;&gt; typeof num &#x3D;&#x3D;&#x3D; &#39;number&#39; const isFn &#x3D; (fn:any)&#x3D;&gt; typeof fn &#x3D;&#x3D;&#x3D; &#39;function&#39; const isObj &#x3D; (obj:any)&#x3D;&gt; (&#123;&#125;).toString.call(obj) &#x3D;&#x3D;&#x3D; &#39;[object Object]&#39; const fn &#x3D; (data:any) &#x3D;&gt; &#123; let value; if(isObj(data))&#123; Object.keys(data).forEach(key&#x3D;&gt;&#123; value &#x3D; data[key] if(isString(value))&#123; data[key] &#x3D; value.trim() &#125; if(isNumber(value))&#123; data[key] &#x3D; value.toFixed(2) &#125; if(isFn(value))&#123; data[key]() &#x2F;&#x2F;修改这儿 &#125; &#125;) &#125; &#125; const obj &#x3D; &#123; a: 100.22222, b: &#39; test &#39;, c: function () &#123; console.log(this); return this.a; &#125; &#125; fn(obj) 第一个问题解决了 第二个问题是我们编写的时候没有代码提示很烦 这时候就需要自定义守卫了 类型谓词的语法形式。它表示当 isString 返回 true 时，str 的类型被细化为 string 类型 const isString &#x3D; (str:any):str is string &#x3D;&gt; typeof str &#x3D;&#x3D;&#x3D; &#39;string&#39; const isNumber &#x3D; (num:any):num is number &#x3D;&gt; typeof num &#x3D;&#x3D;&#x3D; &#39;number&#39; const isFn &#x3D; (fn:any) &#x3D;&gt; typeof fn &#x3D;&#x3D;&#x3D; &#39;function&#39; const isObj &#x3D; (obj:any) &#x3D;&gt; (&#123;&#125;).toString.call(obj) &#x3D;&#x3D;&#x3D; &#39;[object Object]&#39; const fn &#x3D; (data:any) &#x3D;&gt; &#123; let value; if(isObj(data))&#123; Object.keys(data).forEach(key&#x3D;&gt;&#123; value &#x3D; data[key] if(isString(value))&#123; data[key] &#x3D; value.trim() &#125; if(isNumber(value))&#123; data[key] &#x3D; value.toFixed(2) &#125; if(isFn(value))&#123; data[key]() &#125; &#125;) &#125; &#125; const obj &#x3D; &#123; a: 100.22222, b: &#39; test &#39;, c: function () &#123; console.log(this); return this.a; &#125; &#125; fn(obj) 泛型工具infer关键字提取元素的妙用用法infer 递归实战插件编写","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"TypeScript","slug":"TypeScript","permalink":"https://blog.ehzyil.xyz/tags/TypeScript/"}],"author":"ehzyil"},{"title":"Vue学习","slug":"2024/Vue学习","date":"2024-03-11T20:22:46.000Z","updated":"2024-06-17T01:04:54.003Z","comments":true,"path":"2024/03/11/2024/Vue学习/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/11/2024/Vue%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"Vue学习Vite目录 &amp; Vue单文件组件 &amp; npm run dev 详解Vite目录public 下面的不会被编译 可以存放静态资源 assets 下面可以存放可编译的静态资源 components 下面用来存放我们的组件 App.vue 是全局组件 main ts 全局的ts文件 index.html 非常重要的入口文件 （webpack，rollup 他们的入口文件都是enrty input 是一个js文件 而Vite 的入口文件是一个html文件，他刚开始不会编译这些js文件 只有当你用到的时候 如script src&#x3D;”xxxxx.js” 会发起一个请求被vite拦截这时候才会解析js文件） vite config ts 这是vite的配置文件具体配置项 后面会详解 VsCode Vue3 插件推荐 Vue Language Features (Volar) SFC 语法规范*.vue 件都由三种类型的顶层语法块所组成：、、 每个 *.vue 文件最多可同时包含一个顶层 块。 其中的内容会被提取出来并传递给 @vue/compiler-dom，预编译为 JavaScript 的渲染函数，并附属到导出的组件上作为其 render 选项。 每一个 *.vue 文件可以有多个 块 (不包括)。 该脚本将作为 ES Module 来执行。 其默认导出的内容应该是 Vue 组件选项对象，它要么是一个普通的对象，要么是 defineComponent 的返回值。 每个 *.vue 文件最多只能有一个 块 (不包括常规的 ) 该脚本会被预处理并作为组件的 setup() 函数使用，也就是说它会在每个组件实例中执行。 的顶层绑定会自动暴露给模板。更多详情请查看 文档。 一个 *.vue 文件可以包含多个 标签。 标签可以通过 scoped 或 module attribute (更多详情请查看 SFC 样式特性) 将样式封装在当前组件内。多个不同封装模式的 标签可以在同一个组件中混 #### npm run dev 详解 在我们执行这个命令的时候会去找 package json 的scripts 然后执行对应的dev命令 &quot;scripts&quot;: &#123; &quot;dev&quot;: &quot;vite&quot;, &quot;build&quot;: &quot;run-p type-check \\&quot;build-only &#123;@&#125;\\&quot; --&quot;, &quot;preview&quot;: &quot;vite preview&quot;, &quot;build-only&quot;: &quot;vite build&quot;, &quot;type-check&quot;: &quot;vue-tsc --build --force&quot; &#125;, **那为什么我们不直接执行**vite 命令不是更方便吗 因为在我们的电脑上面并没有配置过相关命令 所以无法直接执行 其实在我们执行npm install 的时候（包含vite） 会在node_modules/.bin/ 创建好可执行文件 .bin 目录，这个目录不是任何一个 npm 包。目录下的文件，表示这是一个个软链接，打开文件可以看到文件顶部写着 #!/bin/sh ，表示这是一个脚本 在我们执行npm run xxx npm 会通过软连接 查找这个软连接存在于源码目录node_modules/vite 所以npm run xxx 的时候，就会到 node_modules/bin中找对应的映射文件，然后再找到相应的js文件来执行 1.查找规则是先从当前项目的node_modlue /bin去找, 2.找不到去全局的node_module/bin 去找 3.再找不到 去环境变量去找 node_modules/bin中 有三个vite文件。为什么会有三个文件呢？ # unix Linux macOS 系默认的可执行文件，必须输入完整文件名 vite # windows cmd 中默认的可执行文件，当我们不添加后缀名时，自动根据 pathext 查找文件 vite # Windows PowerShell 中可执行文件，可以跨平台 vite [Vite目录 & Vue单文件组件 & npm run dev 详解](https://xiaoman.blog.csdn.net/article/details/122771007) ### 模板方法 **第一种方法（选项式 API）：** * 使用 `data` 和 `methods` 选项来定义组件的状态和方法。 * `data` 返回一个对象，其中包含组件的状态。 * `methods` 返回一个对象，其中包含组件的方法。 export default &#123; data() &#123; return &#123; message: &#39;Welcome to Your Vue.js App&#39; &#125; &#125;,methods: &#123; &#x2F;&#x2F; ...methods &#125; &#125; **第二种方法（Composition API 的 `setup` 函数）：** * 使用 `setup` 函数来定义组件的状态和方法。 * `setup` 函数返回一个对象，其中包含组件的状态和方法。 * 不需要使用 `data` 或 `methods` 选项。 export default &#123; setup()&#123; const a&#x3D;&quot;Hello&quot; return&#123;a&#125; &#125; &#125; **第三种方法（Composition API 的 ``）：** * 使用 `` 块来定义组件的状态和方法。 * `` 块中的代码直接在组件模板中执行。 * 不需要使用 `setup` 函数或 `data` 和 `methods` 选项。 &lt;script setup lang&#x3D;&quot;ts&quot;&gt; const message:string&#x3D;&#39;ehzyil is here&#39; &lt;&#x2F;script&gt; **主要区别：** * **状态定义：**第一种方法使用 `data` 选项定义状态，而第二和第三种方法使用 `setup` 函数或 `` 块。 * **方法定义：**第一种方法使用 `methods` 选项定义方法，而第二和第三种方法使用 `setup` 函数或 `` 块中的代码。 * **语法：**第一种方法使用选项式 API 语法，而第二和第三种方法使用 Composition API 语法。 * **代码组织：**Composition API 的方法可以更好地组织和重用代码，因为它们与组件模板分离。 ### Vue 指令 ### Ref ### Reactive ### toRef toRefs toRaw ### computed &lt;template&gt; &lt;div&gt; &lt;p&gt;Name: &#123;&#123; name &#125;&#125;&lt;&#x2F;p&gt; &lt;p&gt;Reversed Name: &#123;&#123; reversedName &#125;&#125;&lt;&#x2F;p&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script&gt; export default &#123; data() &#123; return &#123; name: &#39;John Doe&#39; &#125; &#125;, computed: &#123; reversedName() &#123; return this.name.split(&#39;&#39;).reverse().join(&#39;&#39;); &#125; &#125; &#125; &lt;&#x2F;script&gt; 代码讲解 1. 模板 &lt;template> &lt;div> &lt;p>Name: &#123;&#123; name &#125;&#125;&lt;/p> &lt;p>Reversed Name: &#123;&#123; reversedName &#125;&#125;&lt;/p> &lt;/div> &lt;/template> * 模板是 Vue.js 组件的 HTML 结构。 * 在模板中，我们使用双大括号来插值数据。 * `name` 是一个数据属性，它在组件的 `data` 对象中定义。 * `reversedName` 是一个计算属性，它在组件的 `computed` 对象中定义。 2. 脚本 &lt;script> export default &#123; data() &#123; return &#123; name: 'John Doe' &#125; &#125;, computed: &#123; reversedName() &#123; return this.name.split('').reverse().join(''); &#125; &#125; &#125; &lt;/script> * 脚本是 Vue.js 组件的 JavaScript 代码。 * 在脚本中，我们使用 `export default` 来导出组件。 * `data` 对象用于定义组件的数据属性。 * `computed` 对象用于定义组件的计算属性。 3. 数据属性 data() &#123; return &#123; name: 'John Doe' &#125; &#125; * `data` 对象是一个函数，它返回一个对象。 * 这个对象包含了组件的数据属性。 * 在这个例子中，我们定义了一个名为 `name` 的数据属性，它的值为 `John Doe`。 4. 计算属性 computed: &#123; reversedName() &#123; return this.name.split('').reverse().join(''); &#125; &#125; * `computed` 对象是一个对象，它包含了组件的计算属性。 * 计算属性是根据其他数据属性计算出来的属性。 * 在这个例子中，我们定义了一个名为 `reversedName` 的计算属性，它的值是 `name` 属性值的反转。 5. 渲染 当 Vue.js 组件被渲染时，它会先执行 `data` 函数来获取数据属性，然后执行 `computed` 函数来计算计算属性。最后，它会使用模板来渲染组件。 在上面的例子中，组件会被渲染成如下 HTML： &lt;div> &lt;p>Name: John Doe&lt;/p> &lt;p>Reversed Name: eoD nhoJ&lt;/p> &lt;/div> #### computed购物车案例 &lt;template&gt; &lt;div&gt; &lt;input placeholder&#x3D;&quot;请输入关键字&quot; type&#x3D;&quot;text&quot; v-model&#x3D;&quot;keyWord&quot;&gt; &lt;table style&#x3D;&quot;margin-top:10px;&quot; width&#x3D;&quot;500&quot; cellspacing&#x3D;&quot;0&quot; cellpadding&#x3D;&quot;0&quot; border&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;物品&lt;&#x2F;th&gt; &lt;th&gt;单价&lt;&#x2F;th&gt; &lt;th&gt;数量&lt;&#x2F;th&gt; &lt;th&gt;总价&lt;&#x2F;th&gt; &lt;th&gt;操作&lt;&#x2F;th&gt; &lt;&#x2F;tr&gt; &lt;&#x2F;thead&gt; &lt;tbody&gt; &lt;tr v-for&#x3D;&quot;(item, index) in searchData&quot;&gt; &lt;td align&#x3D;&quot;center&quot;&gt;&#123;&#123; item.name &#125;&#125;&lt;&#x2F;td&gt; &lt;td align&#x3D;&quot;center&quot;&gt;&#123;&#123; item.price &#125;&#125;&lt;&#x2F;td&gt; &lt;td align&#x3D;&quot;center&quot;&gt; &lt;button @click&#x3D;&quot;item.num &gt; 0 ? item.num-- : null&quot;&gt;-&lt;&#x2F;button&gt; &#123;&#123; item.num &#125;&#125; &lt;button @click&#x3D;&quot;item.num++&quot;&gt;+&lt;&#x2F;button&gt; &lt;&#x2F;td&gt; &lt;td align&#x3D;&quot;center&quot;&gt;&#123;&#123; item.num * item.price &#125;&#125;&lt;&#x2F;td&gt; &lt;td align&#x3D;&quot;center&quot;&gt;&lt;button @click&#x3D;&quot;del(index)&quot;&gt;删除&lt;&#x2F;button&gt;&lt;&#x2F;td&gt; &lt;&#x2F;tr&gt; &lt;&#x2F;tbody&gt; &lt;tfoot&gt; &lt;tr&gt; &lt;td colspan&#x3D;&quot;5&quot; align&#x3D;&quot;right&quot;&gt; &lt;span&gt;总价：&#123;&#123; total &#125;&#125;&lt;&#x2F;span&gt; &lt;&#x2F;td&gt; &lt;&#x2F;tr&gt; &lt;&#x2F;tfoot&gt; &lt;&#x2F;table&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script setup lang&#x3D;&quot;ts&quot;&gt; import &#123; ref, reactive, computed &#125; from &#39;vue&#39; let keyWord &#x3D; ref&lt;string&gt;(&#39;&#39;) interface data &#123; name: string price: number num: number &#125; const data &#x3D; reactive&lt;data[]&gt;([ &#123; name: &quot;小满的绿帽子&quot;, price: 100, num: 1, &#125;, &#123; name: &quot;小满的红衣服&quot;, price: 200, num: 1, &#125;, &#123; name: &quot;小满的黑袜子&quot;, price: 300, num: 1, &#125; ]) let searchData &#x3D; computed(() &#x3D;&gt; &#123; return data.filter(item &#x3D;&gt; item.name.includes(keyWord.value)) &#125;) let total &#x3D; computed(() &#x3D;&gt; &#123; return searchData.value.reduce((pre, cur) &#x3D;&gt; &#123; console.log(&#39;pre&#39;, pre, &#39;cur&#39;, cur) return pre + cur.num * cur.price &#125;, 0) &#125;) const del &#x3D; (index: number) &#x3D;&gt; &#123; console.log(index) data.splice(index, 1) &#125; &lt;&#x2F;script&gt; ### watch侦听器 watch 需要侦听特定的数据源，并在单独的回调函数中执行副作用 watch第一个参数监听源 watch第二个参数回调函数cb（newVal,oldVal） watch第三个参数一个options配置项是一个对象{ immediate:true //是否立即调用一次 deep:true //是否开启深度监听 监听Ref 案例 import &#123; ref, watch &#125; from &#39;vue&#39; let message &#x3D; ref(&#123; nav:&#123; bar:&#123; name:&quot;&quot; &#125; &#125; &#125;) watch(message, (newVal, oldVal) &#x3D;&gt; &#123; console.log(&#39;新的值----&#39;, newVal); console.log(&#39;旧的值----&#39;, oldVal); &#125;,&#123; immediate:true, deep:true &#125;) 监听多个ref 注意变成数组啦 import &#123; ref, watch ,reactive&#125; from &#39;vue&#39; let message &#x3D; ref(&#39;&#39;) let message2 &#x3D; ref(&#39;&#39;) watch([message,message2], (newVal, oldVal) &#x3D;&gt; &#123; console.log(&#39;新的值----&#39;, newVal); console.log(&#39;旧的值----&#39;, oldVal); &#125;) 监听Reactive 使用reactive监听深层对象开启和不开启deep 效果一样 import &#123; ref, watch ,reactive&#125; from &#39;vue&#39; let message &#x3D; reactive(&#123; nav:&#123; bar:&#123; name:&quot;&quot; &#125; &#125; &#125;) watch(message, (newVal, oldVal) &#x3D;&gt; &#123; console.log(&#39;新的值----&#39;, newVal); console.log(&#39;旧的值----&#39;, oldVal); &#125;) 案例2 监听reactive 单一值 import &#123; ref, watch ,reactive&#125; from &#39;vue&#39; let message &#x3D; reactive(&#123; name:&quot;&quot;, name2:&quot;&quot; &#125;) watch(()&#x3D;&gt;message.name, (newVal, oldVal) &#x3D;&gt; &#123; console.log(&#39;新的值----&#39;, newVal); console.log(&#39;旧的值----&#39;, oldVal); &#125;) ### watchEffect高级侦听器 **Vue 3 中 watchEffect 函数** watchEffect 函数是一个响应式函数，它会在其依赖项发生变化时执行。 **语法** watchEffect(() => &#123; // 回调函数 &#125;) **参数** * 回调函数：一个将在依赖项发生变化时执行的函数。 **返回值** * 取消监视函数：一个可以用来停止监视依赖项的函数。 **示例** import &#123; watchEffect &#125; from 'vue' const count = ref(0) watchEffect(() => &#123; console.log(`count is now $&#123;count.value&#125;`) &#125;) count.value++ 在上面的示例中，我们使用 watchEffect 函数来监视 `count` 变量的变化。当 `count` 变量发生变化时，watchEffect 函数中的回调函数就会执行，并会将 `count` 变量的值打印到控制台。 **依赖项** watchEffect 函数的依赖项是指在回调函数中使用的任何响应式变量。在上面的示例中，`count` 变量是 watchEffect 函数的依赖项。 **取消监视** 要取消监视依赖项，可以使用 watchEffect 函数返回的取消监视函数。在上面的示例中，我们可以使用以下代码来取消监视 `count` 变量： const stopWatching = watchEffect(() => &#123; console.log(`count is now $&#123;count.value&#125;`) &#125;) stopWatching() **何时使用 watchEffect 函数** watchEffect 函数可以用来监视任何响应式变量的变化。它通常用于在响应式变量发生变化时执行一些操作，例如更新 UI 或执行异步操作。 **与 computed 函数的区别** watchEffect 函数与 computed 函数非常相似。它们都是响应式函数，都会在依赖项发生变化时执行。但是，watchEffect 函数和 computed 函数之间存在一些关键的区别： * watchEffect 函数的回调函数会在每次依赖项发生变化时执行，而 computed 函数的回调函数只会在依赖项发生变化时执行一次。 * watchEffect 函数没有返回值，而 computed 函数有返回值。 * watchEffect 函数可以用来执行异步操作，而 computed 函数不能。 **结论** watchEffect 函数是一个强大的工具，可以用来监视任何响应式变量的变化。它通常用于在响应式变量发生变化时执行一些操作，例如更新 UI 或执行异步操作。 **其他示例** * 使用 watchEffect 函数来更新 UI： import &#123; watchEffect &#125; from 'vue' const message = ref('Hello, world!') watchEffect(() => &#123; document.getElementById('app').innerHTML = message.value &#125;) message.value = 'Hello, Vue!' * 使用 watchEffect 函数来执行异步操作： import &#123; watchEffect &#125; from 'vue' const count = ref(0) watchEffect(async () => &#123; const data = await fetch('https://example.com/api/data') console.log(data) &#125;) count.value++ **注意** * watchEffect 函数是 Vue 3 中的新特性。在 Vue 2 中，可以使用 `watch` 选项来监视响应式变量的变化。 * watchEffect 函数是异步的。这意味着它不会立即执行。相反，它会在下一次事件循环中执行。 * watchEffect 函数可以用来监视多个响应式变量。只需在回调函数中使用这些变量即可。 ### ### **Vue 3 生命周期** **1. 创建阶段** - `beforeCreate`：在实例创建之前调用。 - `created`：在实例创建之后调用。 - `beforeMount`：在实例挂载之前调用。 **2. 挂载阶段** - `mounted`：在实例挂载之后调用。 **3. 更新阶段** - `beforeUpdate`：在实例更新之前调用。 - `updated`：在实例更新之后调用。 **4. 卸载阶段** - `beforeDestroy`：在实例销毁之前调用。 - `destroyed`：在实例销毁之后调用。 **5. 错误处理阶段** - `errorCaptured`：在捕获错误时调用。 **6. 其他生命周期钩子** - `activated`：在组件激活时调用。 - `deactivated`：在组件停用时调用。 - `renderTriggered`：在重新渲染触发时调用。 - `renderTracked`：在重新渲染被跟踪时调用。 **示例** &lt;template> &lt;div v-if=\"showComponent\" style=\"background-color: bisque;\"> &lt;span @click=\"num++\" v-if=\"num >= 0 &amp;&amp; num &lt; 3\">&#123;&#123; num &#125;&#125;&lt;/span> &lt;/div> &lt;button @click=\"change\">button&lt;/button> &lt;/template> &lt;script setup lang=\"ts\"> import &#123; ref, onBeforeMount, onMounted, onBeforeUpdate, onUpdated, onBeforeUnmount, onUnmounted, before &#125; from 'vue' var num = ref(0) let showComponent = ref(true); const change = () => &#123; showComponent.value = !showComponent.value; &#125; onBeforeMount(() => &#123; console.log(`the component is to mounted.`) &#125;) onMounted(() => &#123; console.log(`the component is now mounted.`) &#125;) onBeforeUpdate(() => &#123; console.log(`the component is about to update.`) &#125;) onUpdated(() => &#123; console.log(`the component is now updated.`) &#125;) onBeforeUnmount(() => &#123; console.log(`the component is to unmounted.`) &#125;) onUnmounted(() => &#123; console.log(`the component is now unmounted.`) &#125;) &lt;/script> **输出** ### **组件** 代码组件是 Vue 中可重用的组件，它们可以包含自己的模板、脚本和样式。 每一个.vue 文件呢都可以充当组件来使用 每一个组件都可以复用 **代码示例** &lt;template&gt; &lt;HelloWorld&gt;&lt;&#x2F;HelloWorld&gt; &lt;&#x2F;template&gt; &lt;script setup lang&#x3D;&quot;ts&quot;&gt; import HelloWorld from &#39;.&#x2F;components&#x2F;HelloWorld.vue&#39;; &lt;&#x2F;script&gt; ### bem架构 他是一种css架构 oocss 实现的一种 （面向对象css） ，BEM实际上是block、element、modifier的缩写，分别为块层、元素层、修饰符层，element UI 也使用的是这种架构 BEM 命名约定的模式是： .block &#123;&#125; .block__element &#123;&#125; .block--modifier &#123;&#125; #### 编写bem架构 bem.scss $block-sel: &quot;-&quot; !default; $element-sel: &quot;__&quot; !default; $modifier-sel: &quot;--&quot; !default; $namespace:&#39;xm&#39; !default; @mixin bfc &#123; height: 100%; overflow: hidden; &#125; &#x2F;&#x2F;混入 @mixin b($block) &#123; $B: $namespace + $block-sel + $block; &#x2F;&#x2F;变量 .#&#123;$B&#125;&#123; &#x2F;&#x2F;插值语法#&#123;&#125; @content; &#x2F;&#x2F;内容替换 &#125; &#125; @mixin flex &#123; display: flex; &#125; @mixin e($element) &#123; $selector:&amp;; @at-root &#123; #&#123;$selector + $element-sel + $element&#125; &#123; @content; &#125; &#125; &#125; @mixin m($modifier) &#123; $selector:&amp;; @at-root &#123; #&#123;$selector + $modifier-sel + $modifier&#125; &#123; @content; &#125; &#125; &#125; 全局扩充sass: vite.config.ts import &#123; defineConfig &#125; from &#39;vite&#39; import vue from &#39;@vitejs&#x2F;plugin-vue&#39; &#x2F;&#x2F; https:&#x2F;&#x2F;vitejs.dev&#x2F;config&#x2F; export default defineConfig(&#123; plugins: [vue()], css: &#123; preprocessorOptions: &#123; scss: &#123; additionalData: &quot;@import &#39;.&#x2F;src&#x2F;bem.scss&#39;;&quot; &#125; &#125; &#125; &#125;) Vue 组件用法 app.vue &lt;template&gt; &lt;div&gt; &lt;div class&#x3D;&quot;xm-test&quot;&gt;ehzyil-test &lt;br&gt; &lt;div class&#x3D;&quot;xm-test__inner&quot;&gt;ehzyil-test__inner&lt;&#x2F;div&gt; &lt;dir class&#x3D;&quot;xm-test--super&quot;&gt;super&lt;&#x2F;dir&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;Layout&gt;&lt;&#x2F;Layout&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import Layout from &#39;.&#x2F;Layout&#x2F;index.vue&#39; &lt;&#x2F;script&gt; &lt;style lang&#x3D;&quot;scss&quot;&gt; @include b(test) &#123; background-color: red; @include e(inner) &#123; background-color: rgba(0, 34, 255, 0.37); &#125; @include m(super) &#123; background-color: yellow; &#125; @include flex; &#125; #app &#123; @include bfc; &#125; &lt;&#x2F;style&gt; #### bem架构实现Layout布局 文件树：在app.vue中引入Layout.vue ├─.vscode ├─public └─src ├─assets ├─components └─Layout ├─Content ├─Header └─Menu 在index.html中添加样式 &lt;style&gt; * &#123; margin: 0; padding: 0; &#125; html, body&#123; height: 100%; overflow: hidden; &#125; &lt;&#x2F;style&gt; App.vue &lt;template&gt; &lt;Layout&gt;&lt;&#x2F;Layout&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import Layout from &#39;.&#x2F;Layout&#x2F;index.vue&#39; &lt;&#x2F;script&gt; &lt;style lang&#x3D;&quot;scss&quot;&gt; #app &#123; @include bfc; &#125; &lt;&#x2F;style&gt; Layout/index.vue &lt;template&gt; &lt;div class&#x3D;&quot;xm-wraps&quot;&gt; &lt;div&gt; &lt;Menu&gt;&lt;&#x2F;Menu&gt; &lt;&#x2F;div&gt; &lt;div class&#x3D;&quot;xm-wraps__right&quot;&gt; &lt;Header&gt;&lt;&#x2F;Header&gt; &lt;Content&gt;&lt;&#x2F;Content&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import &#123; ref, reactive &#125; from &quot;vue&quot; import Menu from &#39;.&#x2F;Menu&#x2F;index.vue&#39; import Content from &#39;.&#x2F;Content&#x2F;index.vue&#39; import Header from &#39;.&#x2F;Header&#x2F;index.vue&#39; &lt;&#x2F;script&gt; &lt;style lang&#x3D;&quot;scss&quot; scoped&gt; @include b(&#39;wraps&#39;) &#123; @include bfc; @include flex; @include e(right) &#123; flex: 1; display: flex; flex-direction: column; &#125; &#125; &lt;&#x2F;style&gt; /Menu/index.vue &lt;template&gt; &lt;div class&#x3D;&quot;xm-menu&quot;&gt; menu &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import type &#123; ref, reactive &#125; from &#39;vue&#39;; &lt;&#x2F;script&gt; &lt;style scoped lang&#x3D;&quot;scss&quot;&gt; @include b(menu) &#123; min-width: 200px; border-right: 1px solid #e6e6e6; height: 100%; &#125; &lt;&#x2F;style&gt; /Content/index.vue &lt;template&gt; &lt;div class&#x3D;&quot;xm-content&quot;&gt; Content &lt;div class&#x3D;&quot;xm-content__items&quot; c- v-for&#x3D;&quot;(item, index) in 50&quot; :key&#x3D;&quot;index&quot;&gt; &#123;&#123; index &#125;&#125; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import type &#123; ref, reactive &#125; from &#39;vue&#39;; &lt;&#x2F;script&gt; &lt;style scoped lang&#x3D;&quot;scss&quot;&gt; @include b(content) &#123; flex: 1; overflow: auto; @include e(items) &#123; padding: 10px; border: solid; margin: 10px; border-radius: 10px; &#125; &#125; &lt;&#x2F;style&gt; /Header/index.vue &lt;template&gt; &lt;div class&#x3D;&quot;xm-header&quot;&gt; header &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import type &#123; ref, reactive &#125; from &#39;vue&#39;; &lt;&#x2F;script&gt; &lt;style scoped lang&#x3D;&quot;scss&quot;&gt; @include b(header) &#123; height: 50px; border-bottom: 1px solid red; &#125; &lt;&#x2F;style&gt; ### 父子组件传参 #### 父传值 字符串类型是不需要v-bind/: &lt;Menu title&#x3D;&quot;我是标题&quot;&gt;&lt;&#x2F;Menu&gt; 传递非字符串类型需要加v-bind/简写 冒号 &lt;template&gt; &lt;div class&#x3D;&quot;layout&quot;&gt; &lt;Menu v-bind:data&#x3D;&quot;data&quot; title&#x3D;&quot;我是标题&quot;&gt;&lt;&#x2F;Menu&gt; &lt;div class&#x3D;&quot;layout-right&quot;&gt; &lt;Header&gt;&lt;&#x2F;Header&gt; &lt;Content&gt;&lt;&#x2F;Content&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script setup lang&#x3D;&quot;ts&quot;&gt; import Menu from &#39;.&#x2F;Menu&#x2F;index.vue&#39; import Header from &#39;.&#x2F;Header&#x2F;index.vue&#39; import Content from &#39;.&#x2F;Content&#x2F;index.vue&#39; import &#123; reactive &#125; from &#39;vue&#39;; const data &#x3D; reactive&lt;number[]&gt;([1, 2, 3]) &lt;&#x2F;script&gt; 若非字符串类型不加v-bind则**传递的字符串** &lt;Menu title&#x3D;&quot;这是title&quot; :arr&#x3D;&quot;data&quot; arr1&#x3D;data&gt;&lt;&#x2F;Menu&gt; #### 子组件接受值 通过defineProps 来接受 defineProps是无须引入的直接使用即可 如果我们使用的TypeScript,可以使用传递字面量类型的纯类型语法做为参数 const values &#x3D; defineProps&lt;&#123; title: string arr: number[] arr1: number[] &#125;&gt;(); 如果你使用的不是TS defineProps(&#123; title: &#123; type: String, default: &#39;menu&#39; &#125;, arr: &#123; type: Array, default: () &#x3D;&gt; [1, 2, 3] &#125;, arr1: &#123; type: Array, default: () &#x3D;&gt; [1, 2, 3] &#125; &#125;) TS 特有的默认值方式 type props &#x3D; &#123; title?: string arr?: number[] arr1?: number[] arr2?: number[] &#125; withDefaults(defineProps&lt;props&gt;(), &#123; &#x2F;&#x2F;设置默认值 title: &#39;menu&#39;, arr: () &#x3D;&gt; [1, 2, 3], arr1: () &#x3D;&gt; [1, 2, 3], arr2: () &#x3D;&gt; [1, 2, 3] &#125;) #### 代码 index.vue &lt;template&gt; &lt;div class&#x3D;&quot;xm-wraps&quot;&gt; &lt;div&gt; &lt;Menu title&#x3D;&quot;这是title&quot; :arr&#x3D;&quot;data&quot; arr1&#x3D;data&gt;&lt;&#x2F;Menu&gt; &lt;&#x2F;div&gt; &lt;div class&#x3D;&quot;xm-wraps__right&quot;&gt; &lt;Header&gt;&lt;&#x2F;Header&gt; &lt;Content&gt;&lt;&#x2F;Content&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import &#123; ref, reactive &#125; from &quot;vue&quot; import Menu from &#39;.&#x2F;Menu&#x2F;index.vue&#39; import Content from &#39;.&#x2F;Content&#x2F;index.vue&#39; import Header from &#39;.&#x2F;Header&#x2F;index.vue&#39; var data &#x3D; [1, 2, 3]; &lt;&#x2F;script&gt; &lt;style lang&#x3D;&quot;scss&quot; scoped&gt; @include b(&#39;wraps&#39;) &#123; @include bfc; @include flex; @include e(right) &#123; flex: 1; display: flex; flex-direction: column; &#125; &#125; &lt;&#x2F;style&gt; Menu/index.vue &lt;template&gt; &lt;div class&#x3D;&quot;xm-menu&quot;&gt; menu &lt;hr&gt; &#123;&#123; title &#125;&#125; &lt;hr&gt; &#123;&#123; values &#125;&#125; &lt;hr&gt; &#123;&#123; arr &#125;&#125; &lt;hr&gt; &#123;&#123; arr1 &#125;&#125; &lt;hr&gt; &#123;&#123; arr2 &#125;&#125; &lt;hr&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import type &#123; ref, reactive &#125; from &#39;vue&#39;; &#x2F;&#x2F; const values &#x3D; defineProps&lt;&#123; &#x2F;&#x2F; title: string &#x2F;&#x2F; arr: number[] &#x2F;&#x2F; arr1: number[] &#x2F;&#x2F; &#125;&gt;(); &#x2F;&#x2F; defineProps(&#123; &#x2F;&#x2F; title: &#123; &#x2F;&#x2F; type: String, &#x2F;&#x2F; default: &#39;menu&#39; &#x2F;&#x2F; &#125;, &#x2F;&#x2F; arr: &#123; &#x2F;&#x2F; type: Array, &#x2F;&#x2F; default: () &#x3D;&gt; [1, 2, 3] &#x2F;&#x2F; &#125;, &#x2F;&#x2F; arr1: &#123; &#x2F;&#x2F; type: Array, &#x2F;&#x2F; default: () &#x3D;&gt; [1, 2, 3] &#x2F;&#x2F; &#125; &#x2F;&#x2F; &#125;) type props &#x3D; &#123; title?: string arr?: number[] arr1?: number[] arr2?: number[] &#125; withDefaults(defineProps&lt;props&gt;(), &#123; &#x2F;&#x2F;设置默认值 title: &#39;menu&#39;, arr: () &#x3D;&gt; [1, 2, 3], arr1: () &#x3D;&gt; [1, 2, 3], arr2: () &#x3D;&gt; [1, 2, 3] &#125;) const fun &#x3D; () &#x3D;&gt; &#123; &#x2F;&#x2F; console.log(title) &#x2F;&#x2F;找不到名称“title”。 console.log(values.title) &#125; &lt;&#x2F;script&gt; &lt;style scoped lang&#x3D;&quot;scss&quot;&gt; @include b(menu) &#123; min-width: 200px; border-right: 1px solid #e6e6e6; height: 100%; &#125; &lt;&#x2F;style&gt; #### 子组件给父组件传参 是通过defineEmits派发一个事件 &lt;template&gt; &lt;div class&#x3D;&quot;menu&quot;&gt; &#123;&#123; title &#125;&#125; &lt;hr&gt; &lt;!-- 3.触发事件 --&gt; &lt;button @click&#x3D;&quot;clickTap&quot;&gt;派发给父组件&lt;&#x2F;button&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script setup lang&#x3D;&quot;ts&quot;&gt; import &#123; reactive &#125; from &#39;vue&#39; defineProps(&#123; title: &#123; type: String, default: &#39;啥都没传&#39; &#125;, &#125;) const list &#x3D; reactive&lt;number[]&gt;([4, 5, 6]) &#x2F;&#x2F; 1.定义一个名为 on-click 的自定义事件。 &#x2F;&#x2F; const emit &#x3D; defineEmits([&#39;on-click&#39;]) &#x2F;&#x2F; 如果用了ts可以这样 const emit &#x3D; defineEmits&lt;&#123; (e: &#39;on-click&#39;, list: number[]): void &#125;&gt;() &#x2F;&#x2F; 2.在 clickTap 方法中，使用 emit 方法派发 on-click 事件，并传递 list 数组作为参数 const clickTap &#x3D; () &#x3D;&gt; &#123; emit(&#39;on-click&#39;, list) &#125; &lt;&#x2F;script&gt; 我们在子组件绑定了一个click 事件 然后通过defineEmits 注册了一个自定义事件 点击click 触发 emit 去调用我们注册的事件 然后传递参数 #### 父组件接受子组件的事件 &lt;template&gt; &lt;div class&#x3D;&quot;xm-wraps&quot;&gt; &lt;div&gt; &lt;Menu @on-click&#x3D;&quot;handleClick&quot; title&#x3D;&quot;Menu&quot;&gt;&lt;&#x2F;Menu&gt; &lt;&#x2F;div&gt; &lt;div class&#x3D;&quot;xm-wraps__right&quot;&gt; &lt;Header&gt;&lt;&#x2F;Header&gt; &lt;Content&gt;&lt;&#x2F;Content&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import &#123; ref, reactive &#125; from &quot;vue&quot; import Menu from &#39;.&#x2F;Menu&#x2F;index.vue&#39; import Content from &#39;.&#x2F;Content&#x2F;index.vue&#39; import Header from &#39;.&#x2F;Header&#x2F;index.vue&#39; var data &#x3D; [1, 2, 3]; const handleClick &#x3D; (list: number[]) &#x3D;&gt; &#123; console.log(list, &#39;父组件接受子组件&#39;); &#125; &lt;&#x2F;script&gt; &lt;style lang&#x3D;&quot;scss&quot; scoped&gt; @include b(&#39;wraps&#39;) &#123; @include bfc; @include flex; @include e(right) &#123; flex: 1; display: flex; flex-direction: column; &#125; &#125; &lt;&#x2F;style&gt; 我们从Menu 组件接受子组件派发的事件on-click 后面是我们自己定义的函数名称handleClick 会把参数返回过来 #### 子组件暴露给父组件内部属性 子组件使用defineExpose暴露属性或方法 &lt;script setup lang&#x3D;&quot;ts&quot;&gt; import &#123; reactive &#125; from &#39;vue&#39; const aa &#x3D; reactive(&#123; &quot;aa&quot;: &quot;1&quot; &#125;) const list &#x3D; reactive&lt;number[]&gt;([4, 5, 6]) const clickTap &#x3D; () &#x3D;&gt; &#123; &#125; defineExpose(&#123; clickTap, list, aa, &#125;) &lt;&#x2F;script&gt; 父组件使用`const MenuVal = ref();`接受 &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import &#123; ref, reactive &#125; from &quot;vue&quot; import Menu from &#39;.&#x2F;Menu&#x2F;index.vue&#39; const handleClick &#x3D; (list: number[]) &#x3D;&gt; &#123; console.log(list, &#39;父组件接受子组件&#39;); console.log(MenuVal.value?.aa); console.log(MenuVal.value?.list); &#125; const MenuVal &#x3D; ref&lt;InstanceType&lt;typeof Menu&gt;&gt;(); &lt;&#x2F;script&gt; ### 组件 vite-demo\\src\\components\\Card\\index.vue &lt;template&gt; &lt;div class&#x3D;&quot;card&quot;&gt; &lt;div class&#x3D;&quot;card-header&quot;&gt; &lt;div&gt;标题&lt;&#x2F;div&gt; &lt;div&gt;副标题&lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;div v-if&#x3D;&#39;content&#39; class&#x3D;&quot;card-content&quot;&gt; &#123;&#123; content &#125;&#125; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script setup lang&#x3D;&quot;ts&quot;&gt; type Props &#x3D; &#123; content: string &#125; defineProps&lt;Props&gt;() &lt;&#x2F;script&gt; &lt;style scoped lang&#x3D;&#39;less&#39;&gt; @border: #ccc; .card &#123; width: 300px; border: 1px solid @border; border-radius: 3px; &amp;:hover &#123; box-shadow: 0 0 10px @border; &#125; &amp;-content &#123; padding: 10px; &#125; &amp;-header &#123; display: flex; justify-content: space-between; padding: 10px; border-bottom: 1px solid @border; &#125; &#125;&lt;&#x2F;style&gt; #### 全局组件 **组件注册方法** 在main.ts 引入我们的组件跟随在createApp(App) 后面 切记不能放到mount 后面这是一个链式调用用 其次调用 component 第一个参数组件名称 第二个参数组件实例 import './assets/main.css' import &#123; createApp &#125; from 'vue' import App from './App.vue' import Card from './components/Card/index.vue' createApp(App).component('Card', Card).mount('#app') 使用方法 直接在其他vue页面 立即使用即可 无需引入 &lt;template&gt; &lt;div class&#x3D;&quot;xm-content&quot;&gt; Content &lt;div class&#x3D;&quot;xm-content__items&quot;&gt; &lt;Card content&#x3D;&quot;这是一个卡片&quot;&gt;&lt;&#x2F;Card&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; **批量注册组件** 例如： 需要从 @element-plus/icons-vue 中导入所有图标并进行全局注册。 main.ts import * as ElementPlusIconsVue from &#39;@element-plus&#x2F;icons-vue&#39; const app &#x3D; createApp(App) for (const [key, component] of Object.entries(ElementPlusIconsVue)) &#123; app.component(key, component) &#125; #### 局部组件 就是在一个组件内（A） 通过import 去引入别的组件(B) 称之为局部组件 应为B组件只能在A组件内使用 所以是局部组件 如果C组件想用B组件 就需要C组件也手动import 引入 B 组件 例子：如`父组件接受子组件的事件` #### 递归组件 组件定义名称的方式： 1.在增加一个script 通过 export 添加name &lt;script lang&#x3D;&quot;ts&quot;&gt; export default &#123; name:&quot;TreeItem&quot; &#125; &lt;&#x2F;script&gt; 2.直接使用文件名当组件名 3.使用插件 [vue-macros](https://github.com/vue-macros/vue-macros) **案例递归树** 在父组件配置数据结构 数组对象格式 传给子组件 type TreeList = &#123; name: string; icon?: string; children?: TreeList[] | []; &#125;; const data = reactive&lt;TreeList[]>([ &#123; name: \"no.1\", children: [ &#123; name: \"no.1-1\", children: [ &#123; name: \"no.1-1-1\", &#125;, ], &#125;, ], &#125;, &#123; name: \"no.2\", children: [ &#123; name: \"no.2-1\", &#125;, ], &#125;, &#123; name: \"no.3\", &#125;, ]); 如： &lt;template&gt; &lt;Tree :data&#x3D;&quot;data&quot;&gt;&lt;&#x2F;Tree&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import &#123; reactive &#125; from &#39;vue&#39;; import Layout from &#39;.&#x2F;Layout&#x2F;index.vue&#39; import Tree from &#39;.&#x2F;components&#x2F;Tree&#x2F;Tree.vue&#39;; type TreeList &#x3D; &#123; name: string; icon?: string; children?: TreeList[] | []; &#125;; const data &#x3D; reactive&lt;TreeList[]&gt;([ &#123; name: &quot;no.1&quot;, children: [ &#123; name: &quot;no.1-1&quot;, children: [ &#123; name: &quot;no.1-1-1&quot;, &#125;, ], &#125;, ], &#125;, &#123; name: &quot;no.2&quot;, children: [ &#123; name: &quot;no.2-1&quot;, &#125;, ], &#125;, &#123; name: &quot;no.3&quot;, &#125;, ]); &lt;&#x2F;script&gt; 子组件接收值 type TreeList &#x3D; &#123; name: string; icon?: string; children?: TreeList[] | []; &#125;; type Props&lt;T&gt; &#x3D; &#123; data?: T[] | []; &#125;; defineProps&lt;Props&lt;TreeList&gt;&gt;(); const clickItem &#x3D; (item: TreeList) &#x3D;&gt; &#123; console.log(item) &#125; 如： &lt;template&gt; &lt;div style&#x3D;&quot;margin-left:10px;&quot; class&#x3D;&quot;tree&quot;&gt; &lt;div :key&#x3D;&quot;index&quot; v-for&#x3D;&quot;(item, index) in data&quot;&gt; &lt;div @click.stop&#x3D;&#39;clickItem(item)&#39;&gt; &#123;&#123; item.name &#125;&#125; &lt;&#x2F;div&gt; &lt;TreeItem @click.stop&#x3D;&#39;clickItem(item)&#39; v-if&#x3D;&#39;item?.children?.length&#39; :data&#x3D;&quot;item.children&quot;&gt;&lt;&#x2F;TreeItem&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import type &#123; ref, reactive &#125; from &#39;vue&#39;; type TreeList &#x3D; &#123; name: string; icon?: string; children?: TreeList[] | []; &#125;; type Props&lt;T&gt; &#x3D; &#123; data?: T[] | T; &#125; defineProps&lt;Props&lt;TreeList&gt;&gt;(); const clickItem &#x3D; (item: TreeList) &#x3D;&gt; &#123; console.log(item); &#125; &lt;&#x2F;script&gt; &lt;script lang&#x3D;&quot;ts&quot;&gt; export default &#123; name: &quot;TreeItem&quot; &#125; &lt;&#x2F;script&gt; &lt;style scoped lang&#x3D;&quot;scss&quot;&gt;&lt;&#x2F;style&gt; 页面展示如下： no.1 no.1-1 no.1-1-1 no.2 no.2-1 no.3 ### 动态组件 动态组件允许你根据一个动态值来渲染不同的组件。这对于创建可重用且灵活的组件非常有用。 要创建动态组件，可以使用 `is` 属性： &lt;component :is=\"value\">&lt;/component> 其中 `componentName` 是一个动态值，它将被求值为一个组件名称。 例如，以下代码将根据 `currentComponent` 的值渲染不同的组件： &lt;template> &lt;div class=\"box\" v-for=\"(item, index) in componentData\"> &lt;div class=\"tabs\" :class=\"[index == active ? 'active' : '']\" @click=\"value = item.com\"> &#123;&#123; item.name &#125;&#125; &lt;/div> &lt;/div> &lt;hr> &lt;component :is=\"value\">&lt;/component> &lt;/template> &lt;script lang=\"ts\" setup> import &#123; ref, reactive &#125; from 'vue'; import Layout from './Layout/index.vue' import V1 from './components/V1.vue' import V2 from './components/V2.vue' const value = ref(V1) const active = ref(0) var componentData = reactive([&#123; name: 'v1', com: V1 &#125;, &#123; name: 'v2', com: V2 &#125;]) const change = () => &#123; &#125; &lt;/script> &lt;style lang=\"less\"> //并列 .box &#123; display: inline-block; &#125; .active &#123; background-color: blue; &#125; .tabs &#123; border: 1px solid; padding: 10px 10px; margin: 20px; background-color: red; &#125; &lt;/style> ### 插槽slot #### 匿名插槽 1.在子组件放置一个插槽 &lt;template&gt; &lt;div&gt; &lt;slot&gt;&lt;&#x2F;slot&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; 2.在父组件给这个插槽填充内容 &lt;template &gt; &lt;Header&gt;&lt;&#x2F;Header&gt; &lt;Content&gt; &lt;template v-slot&gt; &lt;p&gt;这是子组件的内容&lt;&#x2F;p&gt; &lt;&#x2F;template&gt; &lt;&#x2F;Content&gt; &lt;&#x2F;template&gt; #### 具名插槽 具名插槽其实就是给插槽取个名字。一个子组件可以放多个插槽，而且可以放在不同的地方，而父组件填充内容时，可以根据这个名字把内容填充到对应插槽中 1.在子组件放置一个插槽 &lt;template> &lt;div> &lt;slot name=\"c\">&lt;/slot> &lt;/div> &lt;/template> 2.在父组件给这个插槽填充内容 &lt;template &gt; &lt;Header&gt;&lt;&#x2F;Header&gt; &lt;Content&gt; &lt;template v-slot:c&gt; &lt;p&gt;这是子组件的内容&lt;&#x2F;p&gt; &lt;&#x2F;template&gt; &lt;&#x2F;Content&gt; &lt;&#x2F;template&gt; 插槽简写 &lt;template &gt; &lt;Header&gt;&lt;&#x2F;Header&gt; &lt;Content&gt; &lt;template #c&gt; &lt;p&gt;这是子组件的内容&lt;&#x2F;p&gt; &lt;&#x2F;template&gt; &lt;&#x2F;Content&gt; &lt;&#x2F;template&gt; #### 作用域插槽 在子组件动态绑定参数派发给父组件的slot去使用 &lt;template&gt; &lt;div&gt; &lt;div v-for&#x3D;&quot;item in 100&quot;&gt; &lt;slot :data&#x3D;&quot;item&quot;&gt;&lt;&#x2F;slot&gt; &lt;&#x2F;div&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; 子组件 &lt;template v-slot&#x3D;&quot;data&quot;&gt; &lt;p&gt;&#123;&#123; data &#125;&#125;&lt;&#x2F;p&gt; &lt;&#x2F;template&gt; #### 动态组件 子组件： &lt;template&gt; &lt;div&gt; &lt;slot name&#x3D;&quot;c&quot;&gt;&lt;&#x2F;slot&gt; &lt;&#x2F;div&gt; &lt;&#x2F;template&gt; 父组件： &lt;template &gt; &lt;Header&gt;&lt;&#x2F;Header&gt; &lt;Content&gt; &lt;template #[val]&gt; &lt;p&gt;这是子组件的内容&lt;&#x2F;p&gt; &lt;&#x2F;template&gt; &lt;&#x2F;Content&gt; &lt;&#x2F;template&gt; &lt;script lang&#x3D;&quot;ts&quot; setup&gt; import &#123; ref, reactive &#125; from &quot;vue&quot; import Menu from &#39;.&#x2F;Menu&#x2F;index.vue&#39; import Content from &#39;.&#x2F;Content&#x2F;index.vue&#39; import Header from &#39;.&#x2F;Header&#x2F;index.vue&#39; const val &#x3D; ref(&#39;c&#39;) &lt;&#x2F;script&gt;","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"Vue","slug":"Vue","permalink":"https://blog.ehzyil.xyz/tags/Vue/"}],"author":"ehzyil"},{"title":"前端常用总结","slug":"2024/前端常用总结","date":"2024-03-11T20:22:46.000Z","updated":"2024-06-17T01:04:54.007Z","comments":true,"path":"2024/03/11/2024/前端常用总结/","link":"","permalink":"https://blog.ehzyil.xyz/2024/03/11/2024/%E5%89%8D%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%80%BB%E7%BB%93/","excerpt":"","text":"前端常用总结Refer jQuery API Documentation MDN Web Docs JS四种方法去除字符串最后的逗号&lt;script&gt; window.onload&#x3D;function() &#123; var obj &#x3D; &#123;name: &quot;xxx&quot;, age: 30, sex: &quot;female&quot;&#125;;&#x2F;&#x2F;定义一个object对象 var str &#x3D; &#39;&#39;&#x2F;&#x2F;定义一个空字符用来接收对象里的key或者value for(var item in obj) &#123;&#x2F;&#x2F;遍历item变量里的对象的属性和元素， str +&#x3D; obj[item] + &quot;,&quot;&#x2F;&#x2F;将obj对象的值遍历出来，并且追加到str字符中。 &#x2F;&#x2F;str +&#x3D; item + &quot;,&quot;&#x2F;&#x2F;将obj对象的属性遍历出来，并且追加到str字符中。 &#125; &#x2F;&#x2F;第一种方法、将字符串中最后一个元素&quot;,&quot;逗号去掉， &#x2F;&#x2F;str &#x3D; str.substring(0, str.lastIndexOf(&#39;,&#39;)); &#x2F;&#x2F;第二种方法、将字符串中最后一个元素&quot;,&quot;逗号去掉， &#x2F;&#x2F;str &#x3D; (str.substring(str.length - 1) &#x3D;&#x3D; &#39;,&#39;) ? str.substring(0, str.length - 1) : str; &#x2F;&#x2F;第三种方法、将字符串中最后一个元素&quot;,&quot;逗号去掉， &#x2F;&#x2F;var str&#x3D;str.substring(0,str.length-1);&#x2F;&#x2F;3、将字符串中最后一个元素&quot;,&quot;逗号去掉， &#x2F;&#x2F;第四种方法、将字符串中最后一个元素&quot;,&quot;逗号去掉， var reg&#x3D;&#x2F;,$&#x2F;gi; str&#x3D;str.replace(reg,&quot;&quot;); console.log(str) &#125; &lt;&#x2F;script&gt; 中文逗号，转英文逗号,正则let str &#x3D; &#39;奥术大师,让他忽然，阳台上发生,神鼎飞丹砂&#39; str.replace(&#x2F;，&#x2F;ig,&#39;,&#39;) overflow 的运用overflow CSS 属性用于控制元素中内容的溢出行为。它指定当内容超出元素的边界时如何处理。 语法： overflow: value; 取值： visible：内容超出元素边界时可见。 hidden：超出边界的任何内容都会被剪切。 scroll：在元素中显示滚动条，允许用户滚动查看超出边界的隐藏内容。 auto：如果内容超出边框，则显示滚动条。否则，内容将被剪切。 inherit：从父元素继承 overflow 属性。 用法： 1. 隐藏溢出内容 &lt;div style=\"overflow: hidden;\"> &lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas eget lacus eget nunc tincidunt laoreet.&lt;/p> &lt;/div> 2. 显示滚动条 &lt;div style=\"overflow: scroll;\"> &lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas eget lacus eget nunc tincidunt laoreet. Suspendisse potenti. Mauris eget lacus eget nunc tincidunt laoreet. Suspendisse potenti. Mauris eget lacus eget nunc tincidunt laoreet. Suspendisse potenti.&lt;/p> &lt;/div> 3. 自动显示滚动条 &lt;div style=\"overflow: auto;\"> &lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas eget lacus eget nunc tincidunt laoreet. Suspendisse potenti. Mauris eget lacus eget nunc tincidunt laoreet. Suspendisse potenti. Mauris eget lacus eget nunc tincidunt laoreet. Suspendisse potenti.&lt;/p> &lt;/div> 4. 继承 overflow 属性 &lt;div style=\"overflow: inherit;\"> &lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas eget lacus eget nunc tincidunt laoreet.&lt;/p> &lt;/div> 其他用法： overflow-x：仅控制水平方向上的溢出。 overflow-y：仅控制垂直方向上的溢出。 overflow-clip：裁剪超出边界的任何内容，使其不可见。 注意事项： 滚动条的显示取决于浏览器的默认样式。 overflow 属性不适用于行内元素。 overflow-clip 仅适用于非文本内容。 运算符 逻辑运算符： 感叹号 (!) 三个感叹号 (!!!) //取一个操作数的布尔值。如果操作数为真，则返回 true；如果操作数为假，则返回 false。 示例：!!!true 返回 false,!!!1 返回 false,!!!0 返回 true,!!!null 返回 true,!!!undefined 返回 true。 双感叹号 (!!) //将任何值转换为布尔值。如果该值是真值，则返回 true；否则返回 false。 示例：!!0 返回 false，!!\"\" 返回 false,!!\" \" 返回 true，，!![] 返回 true。 与运算符 (&amp;) 或运算符 (|) 异或运算符 (^) 比较运算符： 等号 (&#x3D;) 三等号 (&#x3D;&#x3D;&#x3D;) //严格相等比较两个操作数，包括数据类型。如果两个操作数相等且类型相同，则返回真；否则返回假。 示例：10 === 10 返回 true，10 === \"10\" 返回 false。 不等号 (!&#x3D;) 大于号 (&gt;) 小于号 (&lt;) 大于等于号 (&gt;&#x3D;) 小于等于号 (&lt;&#x3D;) 算术运算符： 加号 (+) 减号 (-) 乘号 (*) 除号 (&#x2F;) 取模运算符 (%) 其他运算符： 赋值运算符 (&#x3D;) 判断一个值是否为空&#x2F;&#x2F; 检查值是否为以下任何一种空值：null、undefined、空字符串 (&quot;&quot;)、0、NaN if (value &#x3D;&#x3D;&#x3D; null || value &#x3D;&#x3D;&#x3D; undefined || value &#x3D;&#x3D;&#x3D; &quot;&quot; || value &#x3D;&#x3D;&#x3D; 0 || Number.isNaN(value)) &#123; &#x2F;&#x2F; 值为空 &#125; else &#123; &#x2F;&#x2F; 值不为空 &#125; 数据校验数据类型校验 1. 数字 Number.isFinite(value)：检查值是否为有限数字（不包括 NaN 和 Infinity）。 Number.isInteger(value)：检查值是否为整数。 typeof value === &#39;number&#39;：检查值是否为数字类型（不包括 NaN）。 value % 1 === 0：检查值是否为整数（不包括 NaN）。 2. 字符串 typeof value === &#39;string&#39;：检查值是否为字符串类型。 value.length &gt; 0：检查字符串是否不为空。 value.trim().length &gt; 0：检查去除空格后的字符串是否不为空。 3. 布尔值 typeof value === &#39;boolean&#39;：检查值是否为布尔值。 value === true || value === false：检查值是否为 true 或 false。 4. 数组 Array.isArray(value)：检查值是否为数组。 value instanceof Array：检查值是否为数组的实例。 typeof value === &#39;object&#39; &amp;&amp; value.length !== undefined：检查值是否为对象且具有 length 属性。 5. 对象 typeof value === &#39;object&#39;：检查值是否为对象（包括数组）。 value !== null &amp;&amp; typeof value === &#39;object&#39;：检查值是否为对象且不为 null。 Object.keys(value).length &gt; 0：检查对象是否不为空。 6. 日期 value instanceof Date：检查值是否为 Date 对象。 typeof value === &#39;object&#39; &amp;&amp; value.getTime()：检查值是否为对象且具有 getTime() 方法。 7. 函数 typeof value === &#39;function&#39;：检查值是否为函数。 value instanceof Function：检查值是否为 Function 对象。 8.自定义函数 function isNull(value) &#123; if (value === null || value === undefined || value === \"\" || value === 0 || Number.isNaN(value)) &#123; console.log('值为空'); &#125; else &#123; console.log('值不为空'); &#125; &#125; function isEmptyString(string) &#123; return string === \"\"; &#125;; function isArray(value) &#123; return Array.isArray(value); &#125; /*判断是否为数字，是则返回true,否则返回false*/ function f_check_number(obj) &#123; if (/^\\d+$/.test(obj.value)) &#123; return true; &#125; else &#123; console.log(obj, \"请输入数字\"); return false; &#125; &#125; /** * 取得字符串的字节长度 *对于双音节字符+2 其余+1 */ function strlen(str) &#123; var i; var len; len = 0; for (i = 0; i &lt; str.length; i++) &#123; if (str.charCodeAt(i) > 255) len += 2; else len++; &#125; return len; &#125; /* 用途：检查输入字符串是否只由汉字组成 如果通过验证返回true,否则返回false */ function f_check_zh(obj) &#123; if (/^[\\u4e00-\\u9fa5]+$/.test(obj.value)) &#123; return true; &#125; console.log(obj, \"请输入汉字\"); return false; &#125; javaScript通用数据类型校验 - wl666lw - 博客园 监听器所有能绑定监听器的元素 HTML 元素（例如 &lt;div&gt;, &lt;button&gt;, &lt;input&gt;) SVG 元素（例如 &lt;svg&gt;, &lt;circle&gt;, &lt;path&gt;) XML 元素（例如 &lt;xml&gt;, &lt;element&gt;, &lt;attribute&gt;) 窗口对象（window, document) 自定义元素（使用 customElements.define() 定义的元素） 具体来说，HTML 元素包括： &lt;a&gt; 锚点 &lt;abbr&gt; 缩写 &lt;acronym&gt; 首字母缩略词 &lt;address&gt; 地址 &lt;applet&gt; 小程序 &lt;area&gt; 图像映射区域 &lt;article&gt; 文章 &lt;aside&gt; 侧边栏 &lt;audio&gt; 音频 &lt;b&gt; 加粗 &lt;base&gt; 基准 &lt;bdi&gt; 双向隔离 &lt;bdo&gt; 双向覆盖 &lt;big&gt; 放大 &lt;blockquote&gt; 引用 &lt;body&gt; 主体 &lt;br&gt; 换行 &lt;button&gt; 按钮 &lt;canvas&gt; 画布 &lt;caption&gt; 表格标题 &lt;center&gt; 居中 &lt;cite&gt; 引用 &lt;code&gt; 代码 &lt;col&gt; 列 &lt;colgroup&gt; 列组 &lt;command&gt; 命令 &lt;content&gt; 内容 &lt;data&gt; 数据 &lt;datalist&gt; 数据列表 &lt;dd&gt; 定义描述 &lt;del&gt; 删除线 &lt;details&gt; 详细信息 &lt;dfn&gt; 定义 &lt;dialog&gt; 对话框 &lt;dir&gt; 目录 &lt;div&gt; 块级元素 &lt;dl&gt; 定义列表 &lt;dt&gt; 定义术语 &lt;em&gt; 强调 &lt;embed&gt; 嵌入 &lt;fieldset&gt; 字段集 &lt;figcaption&gt; 图注 &lt;figure&gt; 图形 &lt;font&gt; 字体 &lt;footer&gt; 页脚 &lt;form&gt; 表单 &lt;frame&gt; 框架 &lt;frameset&gt; 框架集 &lt;h1-h6&gt; 标题 &lt;head&gt; 头部 &lt;header&gt; 页眉 &lt;hr&gt; 水平线 &lt;html&gt; HTML 文档 &lt;i&gt; 斜体 &lt;iframe&gt; 内嵌框架 &lt;image&gt; 图像 &lt;img&gt; 图像 &lt;input&gt; 输入 &lt;ins&gt; 插入 &lt;kbd&gt; 键盘 &lt;label&gt; 标签 &lt;legend&gt; 图例 &lt;li&gt; 列表项 &lt;link&gt; 链接 &lt;main&gt; 主内容 &lt;map&gt; 图像映射 &lt;mark&gt; 标记 &lt;marquee&gt; 跑马灯 &lt;menu&gt; 菜单 &lt;meta&gt; 元数据 &lt;meter&gt; 仪表 &lt;nav&gt; 导航 &lt;noscript&gt; 无脚本 &lt;object&gt; 对象 &lt;ol&gt; 有序列表 &lt;optgroup&gt; 选项组 &lt;option&gt; 选项 &lt;output&gt; 输出 &lt;p&gt; 段落 &lt;param&gt; 参数 &lt;picture&gt; 图片 &lt;pre&gt; 预格式化文本 &lt;progress&gt; 进度条 &lt;q&gt; 引用 &lt;rp&gt; 替换前的文本 &lt;rt&gt; 替换后的文本 &lt;ruby&gt; 红宝石注释 &lt;s&gt; 删除线 &lt;samp&gt; 示例 &lt;script&gt; 脚本 &lt;section&gt; 节 &lt;select&gt; 选择 &lt;shadow&gt; 影子 &lt;small&gt; 小号字体 &lt;source&gt; 媒体源 &lt;span&gt; 跨度 &lt;strike&gt; 删除线 &lt;strong&gt; 加粗 &lt;style&gt; 样式 &lt;sub&gt; 下标 &lt;summary&gt; 摘要 &lt;sup&gt; 上标 &lt;svg&gt; 可缩放矢量图形 &lt;table&gt; 表格 &lt;tbody&gt; 表格主体 &lt;td&gt; 表格数据 &lt;template&gt; 模板 &lt;textarea&gt; 文本区域 &lt;tfoot&gt; 表格页脚 &lt;th&gt; 表格标题 &lt;thead&gt; 表格标题 &lt;time&gt; 时间 &lt;title&gt; 标题 &lt;tr&gt; 表格行 &lt;track&gt; 轨道 &lt;u&gt; 下划线 &lt;ul&gt; 无序列表 &lt;var&gt; 变量 &lt;video&gt; 视频 &lt;wbr&gt; 换行符 请注意，并非所有元素都支持所有类型的监听器。例如，&lt;input&gt; 元素不支持 mousemove 事件监听器。 可以被绑定的事件HTML 元素支持多种事件，包括： click：当用户单击元素时触发。 dblclick：当用户双击元素时触发。 mouseenter：当鼠标指针进入元素时触发。 mouseleave：当鼠标指针离开元素时触发。 mousemove：当鼠标指针在元素内移动时触发。 mouseover：当鼠标指针悬停在元素上时触发。 mouseout：当鼠标指针离开元素时触发。 mousedown：当用户在元素上按下鼠标按钮时触发。 mouseup：当用户在元素上释放鼠标按钮时触发。 keydown：当用户按下键盘上的某个键时触发。 keypress：当用户按下并释放键盘上的某个键时触发。 keyup：当用户释放键盘上的某个键时触发。 focus：当元素获得焦点时触发。 blur：当元素失去焦点时触发。 change：当元素的值发生更改时触发。 submit：当用户提交表单时触发。 reset：当用户重置表单时触发。 select：当用户选择元素中的文本时触发。 load：当页面加载完成时触发。 unload：当页面卸载时触发。 beforeunload：在页面卸载之前触发。 error：当页面加载过程中发生错误时触发。 abort：当页面加载被用户中止时触发。 scroll：当用户滚动页面时触发。 你还可以使用 jQuery 的 on() 方法来绑定自定义事件。 例如，以下代码绑定了一个名为 myCustomEvent 的自定义事件： $('#myElement').on('myCustomEvent', function() &#123; // ... &#125;); 然后，你可以使用 trigger() 方法触发此事件： $('#myElement').trigger('myCustomEvent'); 这将调用与该事件关联的所有事件处理程序。 trigger() 方法是 jQuery 中用于手动触发事件的方法。它允许你模拟事件的发生，即使该事件不是由用户交互或其他外部因素触发的。 trigger() 方法可以触发任何类型的事件，包括： DOM 事件：例如，click、change、submit 自定义事件：由开发人员创建和触发的事件 jQuery 事件：由 jQuery 库创建和触发的事件，例如 ready 和 ajaxComplete 要使用 trigger() 方法，你需要指定要触发的事件的名称作为第一个参数。还可以传递其他参数来提供有关事件的附加信息。 事件处理程序函数事件处理程序函数是当触发事件时调用的函数。这些函数通常以 on 前缀开头，后面跟着事件名称。例如，onclick 函数将在单击元素时调用。 以下是常见的事件处理程序： onclick：在单击元素时触发。 ondblclick：在双击元素时触发。 onmouseenter：当鼠标指针进入元素时触发。 onmouseleave：当鼠标指针离开元素时触发。 onmousemove：当鼠标指针在元素内移动时触发。 onmouseover：当鼠标指针悬停在元素上时触发。 onmouseout：当鼠标指针离开元素时触发。 onmousedown：当用户在元素上按下鼠标按钮时触发。 onmouseup：当用户在元素上释放鼠标按钮时触发。 onkeydown：当用户按下键盘上的某个键时触发。 onkeypress：当用户按下并释放键盘上的某个键时触发。 onkeyup：当用户释放键盘上的某个键时触发。 onfocus：当元素获得焦点时触发。 onblur：当元素失去焦点时触发。 onchange：当元素的值发生更改时触发。 onsubmit：当用户提交表单时触发。 onreset：当用户重置表单时触发。 onselect：当用户选择元素中的文本时触发。 onload：当页面加载完成时触发。 onunload：当页面卸载时触发。 onbeforeunload：在页面卸载之前触发。 onerror：当页面加载过程中发生错误时触发。 onabort：当页面加载被用户中止时触发。 onscroll：当用户滚动页面时触发。 代码示例 以下代码示例演示了如何将 onclick 事件处理程序函数绑定到 HTML 元素： &lt;button onclick=\"myFunction()\">点击我&lt;/button> &lt;script> function myFunction() &#123; alert(\"你点击了按钮！\"); &#125; &lt;/script> 当用户单击按钮时，将调用 myFunction 函数，并显示一个警报框。 jQuery 绑定和解绑监听器的方法绑定监听器 bind()：绑定一个或多个事件处理程序到选定的元素。 on()：绑定一个或多个事件处理程序到选定的元素或其后代。 live()：绑定一个或多个事件处理程序到当前和未来匹配选择器的元素。 delegate()：绑定一个或多个事件处理程序到选定的元素，但只对匹配选择器的后代元素触发。 解绑监听器 unbind()：解绑一个或多个事件处理程序，之前必须使用 bind() 绑定。 off()：解绑一个或多个事件处理程序，之前必须使用 on() 绑定。 die()：解绑一个或多个事件处理程序，之前必须使用 live() 绑定。 undelegate()：解绑一个或多个事件处理程序，之前必须使用 delegate() 绑定。 示例 绑定一个 click 事件处理程序到具有 id 为 “myButton” 的按钮： $(\"#myButton\").click(function() &#123; // 事件处理程序代码 &#125;); 解绑之前绑定的 click 事件处理程序： $(\"#myButton\").unbind(\"click\"); 绑定一个 mouseover 事件处理程序到所有具有 class 为 “myClass” 的元素： $(\".myClass\").on(\"mouseover\", function() &#123; // 事件处理程序代码 &#125;); 解绑之前绑定的 mouseover 事件处理程序： $(\".myClass\").off(\"mouseover\"); 绑定一个 live() 事件处理程序到当前和未来具有 id 为 “myElement” 的元素： $(\"#myElement\").live(\"click\", function() &#123; // 事件处理程序代码 &#125;); 解绑之前绑定的 live() 事件处理程序： $(\"#myElement\").die(\"click\"); 绑定一个 delegate() 事件处理程序到具有 id 为 “myContainer” 的元素，但只对匹配选择器 “p” 的后代元素触发： $(\"#myContainer\").delegate(\"p\", \"click\", function() &#123; // 事件处理程序代码 &#125;); 解绑之前绑定的 delegate() 事件处理程序： $(\"#myContainer\").undelegate(\"p\", \"click\"); $('#staffTeamTab .sn-transfer-search-input').off('keypress').on('click',function() ); 操纵元素创建元素： const newElement = document.createElement(\"p\"); &lt;input type=\"text\" id=\"phoneTpl\" value=\"Hello world\"> &lt;style> #phoneTpl &#123; value: ''; &#125; &lt;/style> 获取元素： // 根据 ID 获取元素 const elementById = document.getElementById(\"my-element\"); // 根据标签名获取元素 const elementsByTagName = document.getElementsByTagName(\"p\"); // 根据类名获取元素 const elementsByClassName = document.getElementsByClassName(\"my-class\"); // 根据 CSS 选择器获取第一个匹配的元素 const elementByQuery = document.querySelector(\".my-class\"); // 根据 CSS 选择器获取所有匹配的元素 const elementsByQueryAll = document.querySelectorAll(\".my-class\"); 修改元素： 属性： // 设置元素的 id 属性 element.setAttribute(\"id\", \"new-id\"); // 获取元素的 class 属性值 const classValue = element.getAttribute(\"class\"); // 移除元素的 style 属性 element.removeAttribute(\"style\"); 内容： // 设置元素的 HTML 内容 element.innerHTML = \"&lt;p>Hello world&lt;/p>\"; // 设置元素的文本内容 element.textContent = \"Hello world\"; // 设置元素的 innerText 内容 element.innerText = \"Hello world\"; // 设置表单元素的值 element.value = \"Hello world\"; 样式： // 设置元素的背景色 element.style.backgroundColor = \"red\"; // 向元素添加一个类名 element.classList.add(\"my-class\"); // 从元素中删除一个类名 element.classList.remove(\"my-class\"); // 在元素中切换一个类名 element.classList.toggle(\"my-class\"); 添加和删除元素： // 将一个元素作为子元素添加到另一个元素的末尾 parent.appendChild(child); // 将一个元素作为子元素添加到另一个元素的指定位置之前 parent.insertBefore(child, referenceNode); // 从元素中删除一个子元素 parent.removeChild(child); // 用一个元素替换另一个元素 parent.replaceChild(newChild, oldChild); 克隆元素： // 克隆一个元素，创建一个新的元素及其所有子元素的副本 const clone = element.cloneNode(true); 其他： // 将元素滚动到可视区域 element.scrollIntoView(); // 获取元素在页面上的边界框 const boundingClientRect = element.getBoundingClientRect(); // 将焦点设置到元素上 element.focus(); // 从元素上移除焦点 element.blur(); 使用 jQuery： // 获取元素及其所有子元素的 HTML 内容 const html = $(\"#my-element\").html(); // 设置元素及其所有子元素的文本内容 $(\"#my-element\").text(\"Hello world\"); // 获取表单元素的值 const value = $(\"#my-input\").val(); // 向元素添加一个或多个类名 $(\"#my-element\").addClass(\"my-class\"); // 从元素中删除一个或多个类名 $(\"#my-element\").removeClass(\"my-class\"); // 将一个元素作为子元素添加到另一个元素的末尾 $(\"#parent\").append($(\"#child\")); // 从元素中删除一个子元素 $(\"#child\").remove(); 清空元素的值使用 jQuery： $(&#39;#phoneTpl&#39;).val(&#39;&#39;); 案例 1： 清空文本输入框的内容 $(&#39;#phoneTpl&#39;).val(&#39;&#39;); &#x2F;&#x2F; 清空文本输入框的内容 $(&#39;#phoneTpl&#39;).empty(); 案例 1： 清空元素中的所有子元素 $(&#39;#phoneTpl&#39;).empty(); &#x2F;&#x2F; 清空元素中的所有子元素 $(&#39;#phoneTpl&#39;).html(); 案例 1： 获取元素中的 HTML 内容 const htmlContent &#x3D; $(&#39;#phoneTpl&#39;).html(); &#x2F;&#x2F; 获取元素中的 HTML 内容 案例 2： 设置元素中的 HTML 内容 $(&#39;#phoneTpl&#39;).html(&#39;&lt;p&gt;新内容&lt;&#x2F;p&gt;&#39;); &#x2F;&#x2F; 设置元素中的 HTML 内容 使用原生 JavaScript： document.getElementById(&#39;phoneTpl&#39;).value = &#39;&#39;; 案例：清空一个文本输入框的内容。 &lt;input type=\"text\" id=\"phoneTpl\" value=\"1234567890\"> document.getElementById('phoneTpl').value = ''; document.getElementById(&#39;phoneTpl&#39;).textContent; 案例：获取一个元素的文本内容。 &lt;div id=\"phoneTpl\">1234567890&lt;/div> const phoneText = document.getElementById('phoneTpl').textContent; console.log(phoneText); // 输出：1234567890 document.getElementById(&#39;phoneTpl&#39;).innerHTML; 案例：获取一个元素的 HTML 内容。 &lt;div id=\"phoneTpl\">1234567890 &lt;span>Mobile&lt;/span>&lt;/div> const phoneHtml = document.getElementById('phoneTpl').innerHTML; console.log(phoneHtml); // 输出：1234567890 &lt;span>Mobile&lt;/span> document.querySelector(&#39;#phoneTpl&#39;).value = &#39;&#39;; 案例：清空一个文本输入框的内容（CSS 选择器）。 &lt;input type=\"text\" id=\"phoneTpl\" value=\"1234567890\"> document.querySelector('#phoneTpl').value = ''; document.querySelector(&#39;#phoneTpl&#39;).textContent; 案例：获取一个元素的文本内容（CSS 选择器）。 &lt;div id=\"phoneTpl\">1234567890&lt;/div> const phoneText = document.querySelector('#phoneTpl').textContent; console.log(phoneText); // 输出：1234567890 document.querySelector(&#39;#phoneTpl&#39;).innerHTML; 案例：获取一个元素的 HTML 内容（CSS 选择器）。 &lt;div id=\"phoneTpl\">1234567890 &lt;span>Mobile&lt;/span>&lt;/div> const phoneHtml = document.querySelector('#phoneTpl').innerHTML; console.log(phoneHtml); // 输出：1234567890 &lt;span>Mobile&lt;/span> 使用 HTML： &lt;input id=&quot;phoneTpl&quot; value=&quot;&quot;&gt; 使用 CSS： #phoneTpl &#123; value: &#39;&#39;; &#125; &lt;input type&#x3D;&quot;text&quot; id&#x3D;&quot;phoneTpl&quot; value&#x3D;&quot;Hello world&quot;&gt; &lt;style&gt; #phoneTpl &#123; value: &#39;&#39;; &#125; &lt;&#x2F;style&gt; 注意： val() 方法适用于 &lt;input&gt;、&lt;select&gt; 和 &lt;textarea&gt; 元素。 empty() 方法删除元素的所有子元素和文本内容。 text() 方法获取元素的文本内容，不包括 HTML 标签。 html() 方法获取元素的 HTML 内容，包括 HTML 标签。 HTML 方法仅适用于 &lt;input&gt; 元素。 CSS 方法仅适用于 &lt;input&gt; 元素，并且需要浏览器支持。 操纵元素的样式1. 内联样式 &lt;p style=\"color: red; font-size: 20px;\">这是内联样式&lt;/p> 2. 内置样式表 &lt;style> p &#123; color: blue; font-size: 16px; &#125; &lt;/style> 3. 外部样式表 &lt;link rel=\"stylesheet\" href=\"style.css\"> 4. DOM 操作 // 获取元素 const element = document.getElementById(\"my-element\"); // 设置样式 element.style.color = \"green\"; element.style.fontSize = \"24px\"; **getComputedStyle(element).getPropertyValue(“property”)**： const element = document.getElementById(\"my-element\"); const computedStyle = getComputedStyle(element); const fontSize = computedStyle.getPropertyValue(\"font-size\"); console.log(fontSize); // 输出：16px **element.style.getPropertyValue(“property”)**： const element = document.getElementById(\"my-element\"); const fontSize = element.style.getPropertyValue(\"font-size\"); console.log(fontSize); // 输出：16px* **element.style.setProperty(“property”, “value”)**： const element = document.getElementById(\"my-element\"); element.style.setProperty(\"color\", \"red\"); **element.style.getPropertyValue(“property”)**： const element = document.getElementById(\"my-element\"); const color = element.style.getPropertyValue(\"color\"); console.log(color); // 输出：red **element.setAttribute(“style”, “property: value;”)**： const element = document.getElementById(\"my-element\"); element.setAttribute(\"style\", \"color: red;\"); **element.getAttribute(“style”)**： const element = document.getElementById(\"my-element\"); const style = element.getAttribute(\"style\"); console.log(style); // 输出：color: red; **element.classList.add(“class-name”)**： const element = document.getElementById(\"my-element\"); element.classList.add(\"active\"); **element.classList.remove(“class-name”)**： const element = document.getElementById(\"my-element\"); element.classList.remove(\"active\"); **element.classList.toggle(“class-name”)**： const element = document.getElementById(\"my-element\"); element.classList.toggle(\"active\"); **element.classList.contains(“class-name”)**： const element = document.getElementById(\"my-element\"); const hasActiveClass = element.classList.contains(\"active\"); console.log(hasActiveClass); // 输出：true 5. CSS 类 &lt;p class=\"my-class\">这是使用 CSS 类的样式&lt;/p> .my-class &#123; color: purple; font-size: 18px; &#125; 6. CSS 变量 :root &#123; --primary-color: #ff0000; --font-size: 14px; &#125; p &#123; color: var(--primary-color); font-size: var(--font-size); &#125; 7. CSS 预处理器 $primary-color: #ff0000; $font-size: 14px; p &#123; color: $primary-color; font-size: $font-size; &#125; 8. JavaScript 框架 // 使用 React import React from \"react\"; const MyComponent = () => &#123; return &lt;p style=&#123;&#123; color: \"orange\", fontSize: \"22px\" &#125;&#125;>这是使用 React 框架的样式&lt;/p>; &#125;; // 使用 Vue import Vue from \"vue\"; new Vue(&#123; el: \"#app\", data: &#123; style: &#123; color: \"yellow\", fontSize: \"16px\", &#125;, &#125;, template: `&lt;p :style=\"style\">这是使用 Vue 框架的样式&lt;/p>`, &#125;); 9.jQuery 以下是一些使用 jQuery 操纵元素样式的方法： css() 方法：获取或设置元素的样式属性。 addClass() 方法：向元素添加一个或多个 CSS 类。 removeClass() 方法：从元素中删除一个或多个 CSS 类。 toggleClass() 方法：在元素上切换一个或多个 CSS 类。 hasClass() 方法：检查元素是否具有指定的 CSS 类。 示例： // 获取元素的 \"color\" 样式属性 const color = $(\"#my-element\").css(\"color\"); // 设置元素的 \"font-size\" 样式属性 $(\"#my-element\").css(\"font-size\", \"20px\"); // 向元素添加 \"my-class\" CSS 类 $(\"#my-element\").addClass(\"my-class\"); // 从元素中删除 \"my-class\" CSS 类 $(\"#my-element\").removeClass(\"my-class\"); // 在元素上切换 \"active\" CSS 类 $(\"#my-element\").toggleClass(\"active\"); // 检查元素是否具有 \"hidden\" CSS 类 const hasHiddenClass = $(\"#my-element\").hasClass(\"hidden\"); 注意：jQuery 的 css() 方法还可以用于同时获取或设置多个样式属性。例如： // 获取元素的 \"color\" 和 \"font-size\" 样式属性 const styles = $(\"#my-element\").css([\"color\", \"font-size\"]); // 设置元素的 \"color\" 和 \"font-size\" 样式属性 $(\"#my-element\").css(&#123; color: \"red\", fontSize: \"24px\" &#125;); HTML 元素的事件HTML 元素可以绑定的事件有很多，具体取决于元素的类型。以下是 HTML 元素最常见的事件： 通用事件 click：当用户单击元素时触发。 dblclick：当用户双击元素时触发。 mousedown：当用户在元素上按下鼠标按钮时触发。 mouseup：当用户在元素上释放鼠标按钮时触发。 mouseover：当鼠标指针悬停在元素上时触发。 mouseout：当鼠标指针离开元素时触发。 mousemove：当鼠标指针在元素上移动时触发。 keydown：当用户按下键盘上的某个键时触发。 keyup：当用户释放键盘上的某个键时触发。 focus：当元素获得焦点时触发。 blur：当元素失去焦点时触发。 click：当用户单击元素时触发。 例子：为按钮添加单击事件处理程序，在单击时显示警报： &lt;button id=\"my-button\">Click me&lt;/button> &lt;script> const button = document.getElementById(\"my-button\"); button.addEventListener(\"click\", function() &#123; alert(\"Button clicked!\"); &#125;); &lt;/script> dblclick：当用户双击元素时触发。 例子：为段落添加双击事件处理程序，在双击时更改其文本颜色： &lt;p id=\"my-paragraph\">Double-click me&lt;/p> &lt;script> const paragraph = document.getElementById(\"my-paragraph\"); paragraph.addEventListener(\"dblclick\", function() &#123; paragraph.style.color = \"red\"; &#125;); &lt;/script> mousedown：当用户在元素上按下鼠标按钮时触发。 例子：为图像添加 mousedown 事件处理程序，在按下鼠标按钮时显示图像的原始大小： &lt;img id=\"my-image\" src=\"image.jpg\"> &lt;script> const image = document.getElementById(\"my-image\"); image.addEventListener(\"mousedown\", function() &#123; alert(`Original image size: $&#123;image.naturalWidth&#125;px x $&#123;image.naturalHeight&#125;px`); &#125;); &lt;/script> mouseup：当用户在元素上释放鼠标按钮时触发。 例子：为链接添加 mouseup 事件处理程序，在释放鼠标按钮时打开新选项卡： &lt;a id=\"my-link\" href=\"https://www.example.com\">Example website&lt;/a> &lt;script> const link = document.getElementById(\"my-link\"); link.addEventListener(\"mouseup\", function() &#123; window.open(link.href, \"_blank\"); &#125;); &lt;/script> mouseover：当鼠标指针悬停在元素上时触发。 例子：为段落添加 mouseover 事件处理程序，在鼠标指针悬停在其上时显示工具提示： &lt;p id=\"my-paragraph\">Hover over me&lt;/p> &lt;script> const paragraph = document.getElementById(\"my-paragraph\"); paragraph.addEventListener(\"mouseover\", function() &#123; paragraph.title = \"This is a tooltip\"; &#125;); &lt;/script> mouseout：当鼠标指针离开元素时触发。 例子：为按钮添加 mouseout 事件处理程序，在鼠标指针离开其上时隐藏工具提示： &lt;button id=\"my-button\">Show tooltip&lt;/button> &lt;script> const button = document.getElementById(\"my-button\"); button.addEventListener(\"mouseout\", function() &#123; button.title = \"\"; &#125;); &lt;/script> mousemove：当鼠标指针在元素上移动时触发。 例子：为文档添加 mousemove 事件处理程序，在鼠标指针移动时显示其坐标： &lt;body> &lt;script> document.addEventListener(\"mousemove\", function(e) &#123; console.log(`Mouse position: $&#123;e.clientX&#125;px, $&#123;e.clientY&#125;px`); &#125;); &lt;/script> keydown：当用户按下键盘上的某个键时触发。 例子：为输入字段添加 keydown 事件处理程序，在用户按下 Enter 键时提交表单： &lt;form id=\"my-form\"> &lt;input type=\"text\" id=\"my-input\"> &lt;/form> &lt;script> const input = document.getElementById(\"my-input\"); input.addEventListener(\"keydown\", function(e) &#123; if (e.key === \"Enter\") &#123; document.getElementById(\"my-form\").submit(); &#125; &#125;); &lt;/script> keyup：当用户释放键盘上的某个键时触发。 例子：为文本区域添加 keyup 事件处理程序，在用户释放键盘上的某个键时计算文本长度： &lt;textarea id=\"my-textarea\">&lt;/textarea> &lt;script> const textarea = document.getElementById(\"my-textarea\"); textarea.addEventListener(\"keyup\", function() &#123; const textLength = textarea.value.length; console.log(`Text length: $&#123;textLength&#125;`); &#125;); &lt;/script> focus：当元素获得焦点时触发。 例子：为输入字段添加 focus 事件处理程序，在输入字段获得焦点时显示占位符： &lt;input type=\"text\" id=\"my-input\" placeholder=\"Enter your name\"> &lt;script> const input = document.getElementById(\"my-input\"); input.addEventListener(\"focus\", function() &#123; input.placeholder = \"\"; &#125;); &lt;/script> blur：当元素失去焦点时触发。 例子：为按钮添加 blur 事件处理程序，在按钮失去焦点时隐藏下拉菜单： &lt;button id=\"my-button\">Show dropdown&lt;/button> &lt;div id=\"my-dropdown\" style=\"display: none;\"> &lt;ul> &lt;li>Item 1&lt;/li> &lt;li>Item 2&lt;/li> &lt;li>Item 3&lt;/li> &lt;/ul> &lt;/div> &lt;script> const button = document.getElementById(\"my-button\"); const dropdown = document.getElementById(\"my-dropdown\"); button.addEventListener(\"blur\", function() &#123; dropdown.style.display = \"none\"; &#125;); &lt;/script> 表单元素事件 change 事件： * 当用户在文本输入框中输入内容时 * 当用户从下拉列表中选择一个选项时 * 当用户选中或取消选中复选框时 input 事件： * 当用户在文本输入框中键入每个字符时 * 当用户在文本区域中键入每个字符时 submit 事件： * 当用户单击提交按钮时 * 当用户按 Enter 键时（如果表单元素具有焦点） reset 事件： * 当用户单击重置按钮时 代码示例 &lt;form> &lt;input type=\"text\" id=\"name\" onchange=\"handleChange(event)\"> &lt;input type=\"submit\" value=\"Submit\" onclick=\"handleSubmit(event)\"> &lt;input type=\"reset\" value=\"Reset\"> &lt;/form> JavaScript 函数 function handleChange(event) &#123; // 获取触发事件的元素 const element = event.target; // 获取元素的值 const value = element.value; // 执行其他操作，例如更新 UI 或发送数据到服务器 &#125; function handleSubmit(event) &#123; // 阻止表单提交 event.preventDefault(); // 获取表单数据 const data = new FormData(event.target); // 执行其他操作，例如验证数据或发送数据到服务器 &#125; const form = document.querySelector('form'); form.addEventListener('reset', (event) => &#123; // 阻止表单重置 event.preventDefault(); // 重置表单 form.reset(); // 执行其他操作，例如显示确认消息 &#125;); 其他事件 load：当页面加载完成时触发。 unload：当页面卸载时触发。 scroll：当用户滚动页面时触发。 resize：当窗口大小发生更改时触发。 可以绑定事件的 HTML 元素 原则上，任何 HTML 元素都可以绑定事件，但某些事件仅适用于特定类型的元素。例如，”click” 事件可以绑定到任何元素，而 “submit” 事件只能绑定到表单元素。 以下是一些常见元素可以绑定的事件示例： &lt;a&gt; 元素：click、dblclick、mousedown、mouseup、mouseover、mouseout &lt;button&gt; 元素：click、dblclick、mousedown、mouseup、mouseover、mouseout &lt;input&gt; 元素：change、input、focus、blur &lt;select&gt; 元素：change、focus、blur &lt;textarea&gt; 元素：change、input、focus、blur &lt;form&gt; 元素：submit、reset &lt;body&gt; 元素：load、unload、scroll、resize 要绑定事件，可以使用以下语法： element.addEventListener(\"event-name\", function) 例如，要为 &lt;button&gt; 元素绑定一个单击事件处理程序，可以使用以下代码： const button = document.getElementById(\"my-button\"); button.addEventListener(\"click\", function() &#123; // 事件处理程序代码 &#125;); jQuery 选择器jQuery 是一个 JavaScript 库，它提供了许多用于操作 DOM（文档对象模型）的方法。jQuery 选择器用于选择 HTML 文档中的元素。 $(“.staffTeamab”) **$()**：表示 jQuery 对象。 **$(“.staffTeamab”)**：选择所有具有 class&#x3D;”staffTeamab” 的元素。 $(“#staffTeamTab”) **$()**：表示 jQuery 对象。 **$(“#staffTeamTab”)**：选择具有 id&#x3D;”staffTeamTab” 的元素。 区别 类选择器 vs. ID 选择器：$(&quot;.staffTeamab&quot;) 是一个类选择器，它选择所有具有指定类名的元素。$(&quot;#staffTeamTab&quot;) 是一个 ID 选择器，它选择具有指定 ID 的单个元素。 匹配多个元素 vs. 匹配单个元素：类选择器通常匹配多个元素，而 ID 选择器通常匹配单个元素。 性能：ID 选择器比类选择器更快，因为浏览器可以更有效地查找具有唯一 ID 的元素。 示例 假设 HTML 文档中包含以下代码： &lt;div class=\"staffTeamab\">员工团队 A&lt;/div> &lt;div class=\"staffTeamab\">员工团队 B&lt;/div> &lt;div id=\"staffTeamTab\">员工团队选项卡&lt;/div> $(“.staffTeamab”) 将选择两个具有 class&#x3D;”staffTeamab” 的元素（即 “员工团队 A” 和 “员工团队 B”）。 $(“#staffTeamTab”) 将选择具有 id&#x3D;”staffTeamTab” 的单个元素（即 “员工团队选项卡”）。 最佳实践 对于需要唯一标识的元素，请使用 ID 选择器。 对于需要匹配多个元素的元素，请使用类选择器。 为了提高性能，尽量使用 ID 选择器而不是类选择器。 判断tab是否含有active&lt;div class=\"staffTeamab active\">&lt;/div> &lt;div class=\"staffTeamab\">&lt;/div> &lt;div id=\"staffTeamTab\" class=\"active\">&lt;/div> $(&quot;.staffTeamab&quot;).hasClass(&quot;active&quot;) 将返回 true，因为集合中有一个元素具有类名 “active”。 $(&quot;#staffTeamTab&quot;).hasClass(&quot;active&quot;) 也将返回 true，因为具有 ID “staffTeamTab” 的元素具有类名 “active”。 何时使用每种方法： 如果您需要检查元素集合中是否有任何元素具有指定的类名，请使用 $(&quot;.staffTeamab&quot;).hasClass(&quot;active&quot;)。 如果您需要检查单个元素是否有指定的类名，请使用 $(&quot;#staffTeamTab&quot;).hasClass(&quot;active&quot;)。 AJAXAJAX（Asynchronous JavaScript and XML）是一种用于在不重新加载整个页面的情况下与服务器通信的技术。它使用 XMLHttpRequest 对象在后台发送和接收数据。 AJAX 常用方法以下是在 AJAX 中常用的方法： XMLHttpRequest.open()：打开一个与服务器的连接，并指定请求类型（例如 GET 或 POST）和请求的 URL。 XMLHttpRequest.send()：将请求发送到服务器。 XMLHttpRequest.abort()：中止当前请求。 XMLHttpRequest.getResponseHeader()：获取服务器响应中的特定头部信息。 XMLHttpRequest.getAllResponseHeaders()：获取服务器响应中的所有头部信息。 XMLHttpRequest.overrideMimeType()：覆盖服务器响应的 MIME 类型。 XMLHttpRequest.setRequestHeader()：设置请求头部信息。 XMLHttpRequest.onreadystatechange：一个事件处理程序，在请求状态发生变化时触发。 XMLHttpRequest.open() 语法： XMLHttpRequest.open(method, url, async, user, password); 参数： method：请求类型，例如 “GET” 或 “POST”。 url：请求的 URL。 async：一个布尔值，表示请求是否异步（true）或同步（false）。默认值为 true。 user：可选的用户名，用于 HTTP 基本身份验证。 password：可选的密码，用于 HTTP 基本身份验证。 示例： var xhr = new XMLHttpRequest(); xhr.open('GET', 'https://example.com/api/data'); 这将打开一个到 https://example.com/api/data 的 GET 请求。 XMLHttpRequest.send() 语法： XMLHttpRequest.send(data); 参数： data：可选的数据，将作为请求正文发送到服务器。对于 GET 请求，此参数应为 null。 示例： xhr.send(); 这将发送请求到服务器。 XMLHttpRequest.abort() 语法： XMLHttpRequest.abort(); 参数： 无 示例： xhr.abort(); 这将中止当前请求。 XMLHttpRequest.getResponseHeader() 语法： XMLHttpRequest.getResponseHeader(headerName); 参数： headerName：要获取的头部名称。 示例： var contentType = xhr.getResponseHeader('Content-Type'); 这将获取响应的 “Content-Type” 头部。 XMLHttpRequest.getAllResponseHeaders() 语法： XMLHttpRequest.getAllResponseHeaders(); 参数： 无 示例： var allHeaders = xhr.getAllResponseHeaders(); 这将获取响应的所有头部。 XMLHttpRequest.overrideMimeType() 语法： XMLHttpRequest.overrideMimeType(mimeType); 参数： mimeType：要覆盖的 MIME 类型。 示例： xhr.overrideMimeType('text/plain'); 这将覆盖响应的 MIME 类型为 “text&#x2F;plain”。 XMLHttpRequest.setRequestHeader() 语法： XMLHttpRequest.setRequestHeader(headerName, headerValue); 参数： headerName：要设置的头部名称。 headerValue：要设置的头部值。 示例： xhr.setRequestHeader('Content-Type', 'application/json'); 这将设置请求的 “Content-Type” 头部为 “application&#x2F;json”。 XMLHttpRequest.onreadystatechange 语法： XMLHttpRequest.onreadystatechange &#x3D; function() &#123; ... &#125;; 参数： 一个事件处理程序函数，在请求状态发生变化时触发。 示例： xhr.onreadystatechange = function() &#123; if (xhr.readyState === 4 &amp;&amp; xhr.status === 200) &#123; // 请求成功，处理响应数据 &#125; else &#123; // 请求失败，处理错误 &#125; &#125;; 这将创建一个事件处理程序，在请求状态发生变化时触发。当请求完成并且成功时（状态为 4，状态代码为 200），它将处理响应数据。 其他常用方法除了上述方法外，还有一些其他在 AJAX 中常用的方法： $.ajax()：jQuery 库中用于执行 AJAX 请求的方法。它简化了 AJAX 请求的语法。 fetch()：一个原生 JavaScript 方法，用于执行异步请求。它比 XMLHttpRequest 更现代且易于使用。 axios.get()：axios 库中用于执行 GET 请求的方法。它是一个流行的第三方 AJAX 库，提供了许多有用的功能。 示例一以下是一个使用 XMLHttpRequest 发送 GET 请求的示例： var xhr = new XMLHttpRequest(); xhr.open('GET', 'https://example.com/api/data'); xhr.onload = function() &#123; if (xhr.status === 200) &#123; // 请求成功，处理响应数据 &#125; else &#123; // 请求失败，处理错误 &#125; &#125;; xhr.send(); 注意：在使用 AJAX 时，请务必注意跨域请求的安全问题。如果您正在向与您的网站不同的域发送请求，则需要使用 CORS（跨域资源共享）来允许该请求。 示例二&lt;Html&gt; &lt;head&gt; &lt;script src&#x3D;&quot;&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;protobufjs@7.X.X&#x2F;dist&#x2F;protobuf.js&quot;&gt;&lt;&#x2F;script&gt; &lt;&#x2F;head&gt; &lt;body&gt; &lt;a href&#x3D;&quot;a.json&quot; id&#x3D;&quot;ajax&quot;&gt;ajax&lt;&#x2F;a&gt; &lt;form method&#x3D;&quot;post&quot; id&#x3D;&quot;form&quot;&gt; &lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;data&quot; placeholder&#x3D;&quot;data&quot;&gt; &lt;button type&#x3D;&quot;submit&quot;&gt;Submit&lt;&#x2F;button&gt; &lt;&#x2F;form&gt; &lt;button id&#x3D;&quot;get&quot;&gt;get&lt;&#x2F;button&gt; &lt;button id&#x3D;&quot;memos&quot;&gt;POST&lt;&#x2F;button&gt; &lt;&#x2F;body&gt; &lt;script text&#x3D;&quot;text&#x2F;javascript&quot;&gt; window.onload &#x3D; function () &#123; document.getElementById(&#39;ajax&#39;).onclick &#x3D; function () &#123; var request &#x3D; new XMLHttpRequest(); var url &#x3D; this.href; var method &#x3D; &#39;GET&#39;; request.open(method, url); request.send(null); request.onreadystatechange &#x3D; function () &#123; if (request.readyState &#x3D;&#x3D; 4) &#123; if (request.status &#x3D;&#x3D; 200 || request.status &#x3D;&#x3D; 304) &#123; alert(request.responseText); &#125; else &#123; alert(&#39;error&#39;); &#125; &#125; &#125;; return false; &#125;; document.getElementById(&#39;get&#39;).onclick &#x3D; function () &#123; var xhr &#x3D; new XMLHttpRequest(); xhr.open(&#39;GET&#39;, &#39;https:&#x2F;&#x2F;memobbs.app&#x2F;memos.json&#39;); xhr.send(JSON.stringify(&#123; data: &#39;test&#39; &#125;)); xhr.onload &#x3D; function () &#123; if (xhr.status &#x3D;&#x3D;&#x3D; 200) &#123; console.log(xhr.responseText); &#125; else &#123; console.error(xhr.statusText); &#125; &#125;; &#125; &#125; &lt;&#x2F;script&gt; &lt;&#x2F;Html&gt; references:- Ajax简介 - CRainyDay- Ajax数据格式 - CRainyDay jQuery Validate 使用 references: - jQuery Validate | 菜鸟教程 - jQuery验证控件jquery.validate.js使用说明+中文API - 微微_echo - 博客园 自定义校验规则前端代码： &lt;form id&#x3D;&quot;form&quot;&gt; &lt;label for&#x3D;&quot;bookname&quot;&gt;图书名称&lt;&#x2F;label&gt; &lt;input type&#x3D;&quot;text&quot; id&#x3D;&quot;bookname&quot; name&#x3D;&quot;bookname&quot; placeholder&#x3D;&quot;书名&quot; required&gt; &lt;input type&#x3D;&quot;hidden&quot; id&#x3D;&quot;bookid&quot; name&#x3D;&quot;bookid&quot;&gt; &lt;label for&#x3D;&quot;borrowername&quot;&gt;借阅人&lt;&#x2F;label&gt; &lt;input type&#x3D;&quot;text&quot; id&#x3D;&quot;borrowername&quot; name&#x3D;&quot;borrowername&quot; placeholder&#x3D;&quot;借阅人&quot; required&gt; &lt;input type&#x3D;&quot;hidden&quot; id&#x3D;&quot;borrowerid&quot; name&#x3D;&quot;borrowerid&quot;&gt; &lt;label for&#x3D;&quot;begindate&quot;&gt;借阅时间&lt;&#x2F;label&gt; &lt;input type&#x3D;&quot;text&quot; id&#x3D;&quot;begindate&quot; name&#x3D;&quot;begindate&quot; placeholder&#x3D;&quot;借阅时间&quot; required&gt; &lt;label for&#x3D;&quot;enddate&quot;&gt;计划还书间时&lt;&#x2F;label&gt; &lt;input type&#x3D;&quot;text&quot; id&#x3D;&quot;enddate&quot; name&#x3D;&quot;enddate&quot; placeholder&#x3D;&quot;计划还书时间&quot; required&gt; &lt;input type&#x3D;&quot;submit&quot; value&#x3D;&quot;保存&quot; onclick&#x3D;&quot;submitx(this, &#39;form&#39;)&quot;&gt; &lt;input type&#x3D;&quot;button&quot; value&#x3D;&quot;返回&quot; onclick&#x3D;&quot;back(&#39;$&#123;ctx&#125;&#x2F;train&#x2F;xsgl&#x2F;tsxx&#39;)&quot;&gt; &lt;&#x2F;form&gt; 自定义验证方法 addMethod(name,method,message)方法参数 name 是添加的方法的名字。参数 method 是一个函数，接收三个参数 (value,element,param) 。value 是元素的值，element 是元素本身，param 是参数。 要求校验enddate与begindate的时间不能相差30天 jQuery.validator.addMethod(&quot;compareDate&quot;, function (date1Str, element, date2Str) &#123; debugger console.log(date1Str, element, date2Str) const date1 &#x3D; new Date(date1Str.substr(0, 4) + &#39;-&#39; + date1Str.substr(4, 2) + &#39;-&#39; + date1Str.substr(6, 2)); const date2 &#x3D; new Date(date2Str.substr(0, 4) + &#39;-&#39; + date2Str.substr(4, 2) + &#39;-&#39; + date2Str.substr(6, 2)); const differenceInTime &#x3D; Math.abs(date2.getTime() - date1.getTime()); const differenceInDays &#x3D; differenceInTime &#x2F; (1000 * 3600 * 24); if (differenceInDays &gt;&#x3D; 30) &#123; return false; &#125; else &#123; return true; &#125; &#125;, &quot;借书时间不允许超过30天&quot;); 添加校验规则 validate &#x3D; &#123; rules: &#123; bookname: &#123;required: true&#125;, borrowername: &#123;required: true&#125;, begindate: &#123;required: true&#125;, enddate: &#123; required: true, compareDate: $(&#39;#begindate&#39;).val() &#125;, &#125; &#125; $(&quot;#form&quot;).validate(validate); if ($(&quot;#&quot; + formid).validate(validate).form()) &#123; ... &#125; 在原有的校验规则上再添加校验规则当使用外部脚本引入校验方法的时候，想要在原有的校验规则上再添加校验规则时的方法。 1.直接修改validate的内容如下： validate.rules.enddate.compareDate &#x3D; $(&#39;#begindate&#39;).val(); $(&quot;#form&quot;).validate(validate); if ($(&quot;#&quot; + formid).validate(validate).form()) &#123; ... &#125; 2. 使用 extend 方法： $(&quot;#form&quot;).validate(validate); $.extend($(&quot;#form&quot;).validate().settings.rules, &#123; enddate: &#123; &#x2F;&#x2F; 添加新的校验规则 compareDate: $(&#39;#begindate&#39;).val() &#125; &#125;); if ($(&quot;#&quot; + formid).validate(validate).form()) &#123; ... &#125; 使用 extend 方法时，可以在表单验证之前或之后添加新的校验规则. 当添加messages时可以用下面的方法 $.extend($.validator.messages, &#123; compareDate: &quot;测试借书时间不允许超过30天&quot; &#125;);","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"前端","slug":"前端","permalink":"https://blog.ehzyil.xyz/tags/%E5%89%8D%E7%AB%AF/"}],"author":"ehzyil"},{"title":"Frp内网穿透","slug":"2024/Frp内网穿透","date":"2024-01-28T20:22:46.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/01/28/2024/Frp内网穿透/","link":"","permalink":"https://blog.ehzyil.xyz/2024/01/28/2024/Frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/","excerpt":"","text":"Frp本地安装查看服务器架构使用cat /proc/version查看服务器架构，可以使用下列的其他两种 root@iZj6caytd8hmoddao18kslZ:/data# cat /proc/version Linux version 5.10.0-15-amd64 (debian-kernel@lists.debian.org) (gcc-10 (Debian 10.2.1-6) 10.2.1 20210110, GNU ld (GNU Binutils for Debian) 2.35.2) #1 SMP Debian 5.10.120-1 (2022-06-09) root@iZj6caytd8hmoddao18kslZ:/data# uname -a Linux iZj6caytd8hmoddao18kslZ 5.10.0-15-amd64 #1 SMP Debian 5.10.120-1 (2022-06-09) x86_64 GNU/Linux root@iZj6caytd8hmoddao18kslZ:/data# uname -r 5.10.0-15-amd64 下载release包：根据设备和frp版本下载： wget https://github.com/fatedier/frp/releases/download/v0.53.2/frp_0.53.2_linux_amd64.tar.gz 查看当前文件夹下的文件 root@iZj6caytd8hmoddao18kslZ:&#x2F;data# ls frp_0.53.2_linux_amd64.tar.gz 解压： tar -zxvf frp_0.53.2_linux_amd64.tar.gz 相关文件： root@iZj6caytd8hmoddao18kslZ:&#x2F;data# cd frp_0.53.2_linux_amd64&#x2F; root@iZj6caytd8hmoddao18kslZ:&#x2F;data&#x2F;frp_0.53.2_linux_amd64# ls frpc frpc.toml frps frps.toml LICENSE frps*：是frp服务器相关文件。 frpc*：是frp客户端相关文件。 配置FRP服务端修改服务端配置： vim frps.ini 文件内容： # [common] is integral section [common] server_addr &#x3D; 8.210.2.174 # 如果有多个IP，可以选择绑定到不同的ip上 bind_port &#x3D; 7000 # 虚拟主机配置，不能和系统中已监听的端口冲突。http和https可以设置成同一个 vhost_http_port &#x3D; 7080 vhost_https_port &#x3D; 7081 # 服务端web面板 dashboard_addr &#x3D; 0.0.0.0 dashboard_port &#x3D; 7500 # 设置用户名密码，默认都是admin，请注意做修改 dashboard_user &#x3D; admin dashboard_pwd &#x3D; Li021712 # 普罗米修斯运维服务，go语言相关监控，可以关闭 enable_prometheus &#x3D; false # 设置日志文件地址 log_file &#x3D; &#x2F;data&#x2F;frp&#x2F;frps.log # trace, debug, info, warn, error log_level &#x3D; info log_max_days &#x3D; 3 # disable log colors when log_file is console, default is false disable_log_color &#x3D; false # DetailedErrorsToClient defines whether to send the specific error (with debug info) to frpc. By default, this value is true. detailed_errors_to_client &#x3D; true # 最新版本支持的验证方式比较多，这里还是选用token模式 # AuthenticationMethod specifies what authentication method to use authenticate frpc with frps. # If &quot;token&quot; is specified - token will be read into login message. authentication_method &#x3D; token # AuthenticateHeartBeats specifies whether to include authentication token in heartbeats sent to frps. By default, this value is false. authenticate_heartbeats &#x3D; false # AuthenticateNewWorkConns specifies whether to include authentication token in new work connections sent to frps. By default, this value is false. authenticate_new_work_conns &#x3D; false # auth token 相当于密码，请注意保护 token &#x3D; 123456 # 允许配置绑定的端口 # only allow frpc to bind ports you list, if you set nothing, there won&#39;t be any limit # allow_ports &#x3D; 2000-3000,3001,3003,4000-50000 # pool_count in each proxy will change to max_pool_count if they exceed the maximum value max_pool_count &#x3D; 5 # max ports can be used for each client, default value is 0 means no limit max_ports_per_client &#x3D; 0 # TlsOnly specifies whether to only accept TLS-encrypted connections. By default, the value is false. tls_only &#x3D; false Docker安装配置FRP服务端修改服务端配置： vim frps.ini 文件内容： # [common] is integral section [common] server_addr &#x3D; 8.210.2.174 # 如果有多个IP，可以选择绑定到不同的ip上 bind_port &#x3D; 7000 # 虚拟主机配置，不能和系统中已监听的端口冲突。http和https可以设置成同一个 vhost_http_port &#x3D; 7080 vhost_https_port &#x3D; 7081 # 服务端web面板 dashboard_addr &#x3D; 0.0.0.0 dashboard_port &#x3D; 7500 # 设置用户名密码，默认都是admin，请注意做修改 dashboard_user &#x3D; admin dashboard_pwd &#x3D; Li021712 # 普罗米修斯运维服务，go语言相关监控，可以关闭 enable_prometheus &#x3D; false # 设置日志文件地址 log_file &#x3D; &#x2F;data&#x2F;frp&#x2F;frps.log # trace, debug, info, warn, error log_level &#x3D; info log_max_days &#x3D; 3 # disable log colors when log_file is console, default is false disable_log_color &#x3D; false # DetailedErrorsToClient defines whether to send the specific error (with debug info) to frpc. By default, this value is true. detailed_errors_to_client &#x3D; true # 最新版本支持的验证方式比较多，这里还是选用token模式 # AuthenticationMethod specifies what authentication method to use authenticate frpc with frps. # If &quot;token&quot; is specified - token will be read into login message. authentication_method &#x3D; token # AuthenticateHeartBeats specifies whether to include authentication token in heartbeats sent to frps. By default, this value is false. authenticate_heartbeats &#x3D; false # AuthenticateNewWorkConns specifies whether to include authentication token in new work connections sent to frps. By default, this value is false. authenticate_new_work_conns &#x3D; false # auth token 相当于密码，请注意保护 token &#x3D; 123456 # 允许配置绑定的端口 # only allow frpc to bind ports you list, if you set nothing, there won&#39;t be any limit # allow_ports &#x3D; 2000-3000,3001,3003,4000-50000 # pool_count in each proxy will change to max_pool_count if they exceed the maximum value max_pool_count &#x3D; 5 # max ports can be used for each client, default value is 0 means no limit max_ports_per_client &#x3D; 0 # TlsOnly specifies whether to only accept TLS-encrypted connections. By default, the value is false. tls_only &#x3D; false 拉取docker镜像并运行docker run --restart&#x3D;always --network host -d -v &#x2F;data&#x2F;frp&#x2F;frps.ini:&#x2F;etc&#x2F;frp&#x2F;frps.toml --name frps snowdreamtech&#x2F;frps 配置FRP客户端下载客户端 # FRP客户端 [common] server_addr &#x3D; 8.210.2.174 # 与frps.ini的bind_port一致 server_port &#x3D; 7000 # 与frps.ini的token一致 token &#x3D; 123456 # [一个例子_这里尽量全英文] # type &#x3D; 链接协议默认就是tcp不需要改 # local_ip &#x3D; 如果你要内网穿透的就是本机地址就写127.0.0.1 # local_port &#x3D; 假设你是要远程桌面(windows)那就在这里写3389 # remote_port &#x3D; 如果你是按照我博客的步骤填写的是30000-40000，在这里你可以随便写一个30000-40000之间的数字 [forum] type &#x3D; tcp local_ip &#x3D; 127.0.0.1 local_port &#x3D; 8080 remote_port &#x3D; 80 [palword] type &#x3D; udp local_ip &#x3D; 127.0.0.1 local_port &#x3D; 8211 remote_port &#x3D; 36422 运行frpc.exe -c frpc.ini","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"内网穿透","slug":"内网穿透","permalink":"https://blog.ehzyil.xyz/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"}],"author":"ehzyil"},{"title":"使用StringUtils.isNotEmpty遇到的问题","slug":"2024/使用StringUtils.isNotEmpty遇到的问题","date":"2024-01-12T00:00:00.000Z","updated":"2024-06-17T01:04:54.007Z","comments":true,"path":"2024/01/12/2024/使用StringUtils.isNotEmpty遇到的问题/","link":"","permalink":"https://blog.ehzyil.xyz/2024/01/12/2024/%E4%BD%BF%E7%94%A8StringUtils.isNotEmpty%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"在判断map中的key的时候遇到的问题 下面的代码想要判断params中的键dashes是否存在，传入的空字符串，调用StringUtils.isNotEmpty(dashes)方法却返回true，即不为空导致实际业务出现了问题。 HashMap&lt;String, Object> params = new HashMap&lt;>(); params.put(\"阿萨\", \"\"); Object o = params.get(\"dashes\"); System.out.println(o); //空 String dashes = String.valueOf(o); System.out.println(dashes); // null System.out.println(StringUtils.isNotEmpty(dashes)); //true System.out.println(StringUtils.isNotBlank(dashes)); //true if (StringUtils.isNotEmpty(String.valueOf(params.get(\"dashes\")))) &#123; //可以进来 System.out.println(\"可以进来\"); &#125; 通过查看StringUtils.isNotEmpty()的源码 public static boolean isNotEmpty(CharSequence cs) &#123; return !isEmpty(cs); &#125; public static boolean isEmpty(CharSequence cs) &#123; return cs &#x3D;&#x3D; null || cs.length() &#x3D;&#x3D; 0; &#125; 并没有发现什么问题，以为这个方法不能检查出问题，于是网上搜索了相关问题，发现并不是StringUtils.isNotEmpty()的问题。问题要归结于String.valueOf(o)方法，下面看看其源码： //当参数是Integer、Long等范型时 public static String valueOf(Object obj) &#123; return (obj == null) ? \"null\" : obj.toString(); &#125; //当参数为基本类型时,如int public static String valueOf(int i) &#123; return Integer.toString(i); &#125; 因此当参数是Integer、Long等泛型时，如果参数为null，此方法会会把参数转化成”null” 所以如果用StringUtils.isNotEmpty去判断使用String.ValueOf转化成”null”的参数，是true的，因为此方法并没有去判断传入值为”null”的情况。 其实在判断一个map中的key的时候应该先判断key是否存在，再去判断它的值，下面要判断map中的一个key对应的值是否是”true”，可以使用： HashMap&lt;String, Object> params = new HashMap&lt;>(); params.put(\"dashes\", \"true\"); if (params.containsKey(\"dashes\") &amp;&amp; StringUtils.isNotEmpty(params.get(\"dashes\").toString())) &#123; System.out.println(\"1\"); &#125; if (params.containsKey(\"dashes\") &amp;&amp; \"true\".equals(params.get(\"dashes\").toString())) &#123; System.out.println(\"2\"); &#125; Object o = Optional.ofNullable(params.get(\"dashes\")).orElse(\"\"); if (StringUtils.isNotEmpty(o.toString())) &#123; System.out.println(\"3\"); &#125; if (\"true\".equals(o.toString())) &#123; System.out.println(\"4\"); &#125; 只有当params.put(&quot;dashes&quot;, &quot;true&quot;);传入的值为”true”时才会进入if结构内。","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"},{"name":"踩坑","slug":"踩坑","permalink":"https://blog.ehzyil.xyz/tags/%E8%B8%A9%E5%9D%91/"}],"author":"ehzyil"},{"title":"SpringBoot实现微信公众号自动登录","slug":"2024/SpringBoot实现微信公众号自动登录","date":"2024-01-11T00:00:00.000Z","updated":"2024-06-17T01:04:54.003Z","comments":true,"path":"2024/01/11/2024/SpringBoot实现微信公众号自动登录/","link":"","permalink":"https://blog.ehzyil.xyz/2024/01/11/2024/SpringBoot%E5%AE%9E%E7%8E%B0%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%95/","excerpt":"","text":"微信公众平台配置使用natapp开启内网穿透使用教程 我的配置如下： 注册微信公众号并开启服务器配置注册完公众号进入基本配置&#x2F; 填写服务器配置 填写内容示例 具体要求可以参考 接入指南 填写完点击提交的时候会发现报错，原因是WX要调用你填写的URL并传入这几个参数 将其进行加密后和echostr比对后并返回echostr才能通过验证，当然你直接返回echostr字段也可可以。 这个文档里有具体的内容但他是用python写的具体看这里。 附上Java代码 @Slf4j @RestController @RequestMapping(path = \"wx\") public class WxCallbackRestController &#123; private static final String TOKEN = \"ehzyil\"; // 请按照公众平台官网\\基本配置中信息填写 /** * 微信的公众号接入 token 验证，即返回echostr的参数值 * * @param request * @return */ @GetMapping public String check(HttpServletRequest request) &#123; try &#123; // 获取请求参数 String signature = request.getParameter(\"signature\"); String timestamp = request.getParameter(\"timestamp\"); String nonce = request.getParameter(\"nonce\"); String echostr = request.getParameter(\"echostr\"); // 将token、timestamp、nonce进行字典序排序 String[] list = &#123;TOKEN, timestamp, nonce&#125;; Arrays.sort(list); // 将三个参数字符串拼接成一个字符串 String str = list[0] + list[1] + list[2]; // 将字符串进行SHA1加密 MessageDigest sha1 = null; try &#123; sha1 = MessageDigest.getInstance(\"SHA-1\"); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; // Update the message digest with each element in the list for (String element : list) &#123; sha1.update(element.getBytes()); &#125; // Generate the hash code byte[] hashcode = sha1.digest(); // Convert the hash code to a hexadecimal string StringBuilder hexString = new StringBuilder(); for (byte b : hashcode) &#123; hexString.append(String.format(\"%02X\", b)); &#125; // 比较加密后的字符串和signature是否一致 if (hexString.toString().equalsIgnoreCase(signature)) &#123; // 如果一致，则返回echostr return echostr; &#125; else &#123; // 如果不一致，则返回空字符串 return \"\"; &#125; &#125; catch (Exception e) &#123; // 如果发生异常，则返回异常信息 log.error(\"微信公众号登录token验证失败:\", e.getMessage()); return \"\"; &#125; &#125; &#125; 成功并启用后显示如下： 下面正式开启实践。 实践策略 使用SSE建立前后端的半长连接,在前端生成验证码并展示二维码，用户扫描二维码并输入验证码，服务通过回调接口接收用户的验证码，让用户自动登录。 具体实践SSE建立前后端的半长连接-前端前端代码 &lt;div class=\"tabpane-container\"> &lt;h2 class=\"title\">微信扫码/长按识别登录&lt;/h2> &lt;div class=\"first\"> &lt;img class=\"signin-qrcode\" th:src=\"$&#123;global.siteInfo.contactMeWxQrCode&#125;\"/> &lt;/div> &lt;div class=\"explain\"> &lt;bold>输入验证码&lt;/bold> &lt;span id=\"code\">&lt;/span> &lt;div>&lt;span id=\"state\">有效期五分钟 👉&lt;/span> &lt;a id=\"refreshCode\">手动刷新&lt;/a>&lt;/div> &lt;/div> &lt;/div> 这段代码用于建立一个长连接。 首先，它检查是否存在一个名为sseSource的变量，如果存在，则尝试关闭它。然后，它检查是否存在一个名为deviceId的变量，如果没有，则从cookie中获取它。 接下来，它创建一个新的EventSource对象，并将其赋值给sseSource变量。这个对象用于建立与服务器的连接。 然后，它为sseSource对象添加一个onmessage事件监听器，当服务器发送消息时，这个事件监听器就会被触发。在事件监听器中，它将消息的数据部分替换所有双引号，并将其修剪。然后，它根据消息的内容执行不同的操作。 如果消息以refresh#开头，则表示服务器发送了一个新的验证码。它将新的验证码显示在页面上，并更新状态标签。 如果消息是scan，则表示用户已经扫描了二维码。它更新状态标签。 如果消息以login#开头，则表示用户已经成功登录。它将服务器发送的cookie保存到本地，并刷新页面。 如果消息以init#开头，则表示服务器发送了一个初始验证码。它将初始验证码显示在页面上，并更新状态标签。 如果消息不是以上任何一种，则它会将新的验证码显示在页面上，并更新状态标签。 最后，它为sseSource对象添加一个onopen事件监听器，当连接建立时，这个事件监听器就会被触发。在事件监听器中，它将设备ID打印到控制台。 它还为sseSource对象添加一个onerror事件监听器，当连接出错时，这个事件监听器就会被触发。在事件监听器中，它将错误信息打印到控制台，并更新状态标签。 最后，它创建一个名为intHook的变量，并将其赋值为一个每秒钟执行一次的setInterval函数。这个函数用于每秒钟向服务器请求一次验证码。 let fetchCodeCnt = 0; function fetchCode() &#123; if (deviceId) &#123; if (++fetchCodeCnt > 5) &#123; // 为了避免不停的向后端发起请求，做一个最大的重试计数限制 try &#123; window.clearInterval(intHook); &#125; catch (e) &#123; &#125; return; &#125; $.ajax(&#123; url: \"/login/fetch?deviceId=\" + deviceId, type: \"get\", dataType: \"text\", success: function (data) &#123; console.log(\"data>>>>>>>>: \", data); if (data != 'fail') &#123; codeTag.text(data); try &#123; window.clearInterval(intHook); &#125; catch (e) &#123; &#125; &#125; &#125;, error: function (e) &#123; console.log(\"some error! \", e); &#125; &#125;); &#125; else &#123; console.log(\"deviceId未获取，稍后再试!\"); &#125; &#125; 这段代码用于每秒钟向服务器请求一次验证码。 首先，它检查deviceId变量是否存在，如果存在，则继续执行。 然后，它检查fetchCodeCnt变量的值，如果fetchCodeCnt大于5，则表示已经重试了5次，它将清除intHook定时器，并返回。 如果fetchCodeCnt小于或等于5，则它将向服务器发送一个AJAX请求，请求验证码。 在AJAX请求的success回调函数中，它将服务器返回的验证码显示在页面上，并清除intHook定时器。 在AJAX请求的error回调函数中，它将错误信息打印到控制台。 最后，如果deviceId变量不存在，则它将打印一条消息到控制台，表示稍后再试。 这段代码的作用是每秒钟向服务器请求一次验证码，直到服务器返回一个有效的验证码，或者重试次数达到5次。 function refreshCode() &#123; $.ajax(&#123; url: &quot;&#x2F;login&#x2F;refresh?deviceId&#x3D;&quot; + deviceId, dataType: &quot;json&quot;, type: &quot;get&quot;, success: function (data) &#123; const code &#x3D; data[&#39;result&#39;][&#39;code&#39;]; const reconnect &#x3D; data[&#39;result&#39;][&#39;reconnect&#39;] console.log(&quot;验证码刷新完成: &quot;, data); if (reconnect) &#123; &#x2F;&#x2F; 重新建立连接 buildConnect(); $(&#39;#state&#39;).text(&quot;已刷新!&quot;); &#125; else if (code) &#123; if (codeTag.text() !&#x3D;&#x3D; code) &#123; console.log(&quot;主动刷新验证码!&quot;); codeTag.text(code); stateTag.text(&quot;已刷新!&quot;); &#125; else &#123; console.log(&quot;验证码已刷新了!&quot;); &#125; &#125; &#125; &#125;) &#125; refreshCode()函数用于刷新验证码。 它首先向服务器发送一个AJAX请求，请求新的验证码。 在AJAX请求的success回调函数中，它将服务器返回的验证码数据解析出来，并存储在code和reconnect变量中。 然后，它将code和reconnect变量的值打印到控制台。 如果reconnect为真，则表示需要重新建立长连接，它将调用buildConnect()函数重新建立长连接，并更新状态标签的文本为“已刷新！”。 如果reconnect为假，则表示不需要重新建立长连接，它将检查code变量的值是否与当前验证码标签上的文本不同，如果不同，则表示验证码已刷新，它将更新验证码标签上的文本和状态标签的文本为“已刷新！”。 如果code变量的值与当前验证码标签上的文本相同，则表示验证码没有刷新，它将打印一条消息到控制台，表示验证码已刷新了。 这段代码的作用是刷新验证码，并根据服务器返回的数据更新验证码标签和状态标签的文本。 它可以用于在用户点击刷新验证码按钮时刷新验证码，也可以用于在长连接断开时重新建立长连接并刷新验证码。 SSE建立前后端的半长连接-后端初始化 /** * key = 验证码, value = 长连接 */ private LoadingCache&lt;String, SseEmitter> verifyCodeCache; /** * key = 设备 value = 验证码 */ private LoadingCache&lt;String, String> deviceCodeCache; public WxLoginHelper(LoginService loginService) &#123; // 注入登录服务 this.sessionService = loginService; // 设定令牌验证码缓存，最大容量为300，过期时间为5分钟 verifyCodeCache = CacheBuilder.newBuilder().maximumSize(300) .expireAfterWrite(5, TimeUnit.MINUTES).build(new CacheLoader&lt;String, SseEmitter>() &#123; @Override public SseEmitter load(String s) throws Exception &#123; // 在Guava库中抛出异常\"No Vla In Guava Exception\"，异常信息包含参数s throw new NoVlaInGuavaException(\"no val: \" + s); &#125; &#125;); // 设定验证码缓存，最大容量为300，过期时间为5分钟 deviceCodeCache = CacheBuilder.newBuilder().maximumSize(300) .expireAfterWrite(5, TimeUnit.MINUTES).build(new CacheLoader&lt;String, String>() &#123; @Override public String load(String s) &#123; // 记录日志：load deviceCodeCache: &#123;&#125; log.info(\"load deviceCodeCache: &#123;&#125;\", s); // 循环生成验证码 while (true) &#123; // 调用CodeGenerateUtil类的genCode()方法生成验证码 String code = CodeGenerateUtil.genCode(); // 判断缓存中是否已经存在该验证码 if (!verifyCodeCache.asMap().containsKey(code)) &#123; // 若缓存中不存在该验证码，则返回该验证码 return code; &#125; &#125; &#125; &#125;); &#125; 在WxLoginHelper初始化的同时给verifyCodeCache和deviceCodeCache初始化。 LoadingCache是Guava库中的一种缓存，它可以自动加载和过期缓存项。 在你的代码中，verifyCodeCache用于存储验证验证码和SSE Emitter对象的映射，deviceCodeCache用于存储设备ID和验证码的映射。 这两个缓存的最大容量都是300，过期时间都是5分钟。 verifyCodeCache的加载器是一个匿名内部类，它实现了CacheLoader接口。 CacheLoader接口定义了一个load()方法，该方法用于加载缓存项。 在verifyCodeCache的加载器中，load()方法会抛出一个NoVlaInGuavaException异常，该异常表示缓存中没有该验证码对应的值。 deviceCodeCache的加载器也是一个匿名内部类，它也实现了CacheLoader接口。 在deviceCodeCache的加载器中，load()方法会循环生成验证码，直到生成一个不存在于缓存中的验证码。 这段代码的作用是初始化两个缓存，以便在后续的代码中使用它们来存储验证验证码和设备验证码。 订阅方法获取订阅设备的SseEmitter /** * 订阅设备的SseEmitter * @param deviceId 设备ID * @return 订阅设备的SseEmitter对象 * @throws IOException 输入输出异常 */ public SseEmitter subscribe(String deviceId) throws IOException &#123; // 由于传来的deviceId并没有用，可以从前一个类ReqInfoContext中获取 deviceId = ReqInfoContext.getReqInfo().getDeviceId(); // get方法会在缓存中检查该值是否存在，如果不存在，则返回null。 // getUnchecked方法不会检查该值是否存在，而是直接返回该值,如果不存在，则会抛出异常。 String realCode = deviceCodeCache.getUnchecked(deviceId); // 创建SseEmitter对象 SseEmitter sseEmitter = new SseEmitter(SSE_EXPIRE_TIME); // 检查并关闭旧的SSE Emitter SseEmitter oldSss = verifyCodeCache.getIfPresent(deviceId); if (oldSss != null) &#123; oldSss.complete(); &#125; // 将新的SSE Emitter放入缓存 verifyCodeCache.put(realCode, sseEmitter); // 设置SSE Emitter的超时和错误处理程序 sseEmitter.onTimeout(() -> &#123; log.info(\"sse 超时中断 --> &#123;&#125;\", realCode); verifyCodeCache.invalidate(realCode); sseEmitter.complete(); &#125;); sseEmitter.onError((e) -> &#123; log.warn(\"sse 错误! --> &#123;&#125;\", realCode, e); verifyCodeCache.invalidate(realCode); sseEmitter.complete(); &#125;); // 向客户端发送初始化消息 sseEmitter.send(\"initCode!\"); sseEmitter.send(\"init#\" + realCode); // 返回SSE Emitter对象 return sseEmitter; &#125; 当前端调用deviceCodeCache.getUnchecked(deviceId);方法时生成并将验证码存在deviceCodeCache中。verifyCodeCache验证码和新创建的sseEmitter，完成之后向客户端发送初始化消息并返回SSE Emitter对象。 获取验证码 public String resendCode() throws IOException &#123; &#x2F;&#x2F; 获取旧的验证码，注意不使用 getUnchecked, 避免重新生成一个验证码 String deviceId &#x3D; ReqInfoContext.getReqInfo().getDeviceId(); String oldCode &#x3D; deviceCodeCache.getIfPresent(deviceId); SseEmitter lastSse &#x3D; oldCode &#x3D;&#x3D; null ? null : verifyCodeCache.getIfPresent(oldCode); if (lastSse !&#x3D; null) &#123; lastSse.send(&quot;resend!&quot;); lastSse.send(&quot;init#&quot; + oldCode); return oldCode; &#125; return &quot;fail&quot;; &#125; 根据deviceId从verifyCodeCache中获取SseEmitter，如果为空返回”fail”。否则重新发送验证码。 重新获取验证码 public String refreshCode() throws IOException &#123; ReqInfoContext.ReqInfo reqInfo &#x3D; ReqInfoContext.getReqInfo(); &#x2F;&#x2F;获取设备ID和旧的验证码。 String deviceId &#x3D; ReqInfoContext.getReqInfo().getDeviceId(); &#x2F;&#x2F;检查旧的验证码是否有效。 String oldCode &#x3D; deviceCodeCache.getIfPresent(deviceId); &#x2F;&#x2F;如果旧的验证码无效，则返回null。 SseEmitter lastSse &#x3D; (oldCode &#x3D;&#x3D; null ? null : verifyCodeCache.getIfPresent(oldCode)); if (lastSse &#x3D;&#x3D; null) &#123; log.info(&quot;last deviceId:&#123;&#125;, code:&#123;&#125;, sse closed!&quot;, deviceId, oldCode); deviceCodeCache.invalidate(deviceId); return null; &#125; &#x2F;&#x2F;生成一个新的验证码。 deviceCodeCache.invalidate(deviceId); String newCode &#x3D; deviceCodeCache.getUnchecked(deviceId); log.info(&quot;generate new loginCode! deviceId:&#123;&#125;, oldCode:&#123;&#125;, code:&#123;&#125;&quot;, deviceId, oldCode, newCode); &#x2F;&#x2F;向客户端发送一条消息，通知客户端更新验证码。 lastSse.send(&quot;updateCode!&quot;); lastSse.send(&quot;refresh#&quot; + newCode); &#x2F;&#x2F;将新的验证码放入缓存。 verifyCodeCache.invalidate(oldCode); verifyCodeCache.put(newCode, lastSse); &#x2F;&#x2F;返回新的验证码。 return newCode; &#125; 根据deviceId去deviceCodeCache中获取旧的验证码，如果旧的验证码无效就返回null，否则说明之前获取过验证码，就将验证码和设备码在deviceId中移除并重新生成，返回验证码，将验证码和ssemitter的缓存进verifyCodeCache，最后返回验证码。 关注公众号并发送验证码通过公众号的WEB接口和公众号发送信息发现两个的请求路径并不相同，为了都能接受到，在请求上做了兼容。 &#x2F;&#x2F;Request URL: http:&#x2F;&#x2F;gtmf86.natappfree.cc&#x2F;wx&#x2F;callback &#x2F;&#x2F;Request URL: http:&#x2F;&#x2F;gtmf86.natappfree.cc&#x2F;wx 回调接口代码如下： @PostMapping(path &#x3D; &#123;&quot;&quot;, &quot;callback&quot;&#125;, consumes &#x3D; &#123;&quot;application&#x2F;xml&quot;, &quot;text&#x2F;xml&quot;&#125;, produces &#x3D; &quot;application&#x2F;xml;charset&#x3D;utf-8&quot;) public BaseWxMsgResVo callBack(@RequestBody WxTxtMsgReqVo msg) &#123; String content &#x3D; msg.getContent(); if (&quot;subscribe&quot;.equals(msg.getEvent()) || &quot;scan&quot;.equalsIgnoreCase(msg.getEvent())) &#123; String key &#x3D; msg.getEventKey(); if (StringUtils.isNotBlank(key) || key.startsWith(&quot;qrscene_&quot;)) &#123; &#x2F;&#x2F; 带参数的二维码，扫描、关注事件拿到之后，直接登录，省却输入验证码这一步 &#125; &#125; BaseWxMsgResVo res &#x3D; wxHelper.buildResponseBody(msg.getEvent(), content, msg.getFromUserName()); fillResVo(res, msg); return res; &#125; buildResponseBody(): &#x2F;** * 返回自动响应的文本 * * @return *&#x2F; public BaseWxMsgResVo buildResponseBody(String eventType, String content, String fromUser) &#123; &#x2F;&#x2F; 返回的文本消息 String textRes &#x3D; null; &#x2F;&#x2F; 返回的是图文消息 List&lt;WxImgTxtItemVo&gt; imgTxtList &#x3D; null; if (&quot;subscribe&quot;.equalsIgnoreCase(eventType)) &#123; &#x2F;&#x2F; 订阅 textRes &#x3D; &quot;欢迎订阅！&quot;; &#125; &#x2F;&#x2F; 微信公众号登录 else if (CodeGenerateUtil.isVerifyCode(content)) &#123; sessionService.autoRegisterWxUserInfo(fromUser); if (qrLoginHelper.login(content)) &#123; textRes &#x3D; &quot;登录成功，开始愉快的玩耍技术派吧！&quot;; log.info(&quot;用户:登录成功&quot;, fromUser); &#125; else &#123; textRes &#x3D; &quot;验证码过期了，刷新登录页面重试一下吧&quot;; log.info(&quot;用户:登录失败，验证码过期了！&quot;, fromUser); &#125; &#125; else &#123; textRes &#x3D; content; &#125; if (textRes !&#x3D; null) &#123; WxTxtMsgResVo vo &#x3D; new WxTxtMsgResVo(); vo.setContent(textRes); return vo; &#125; else &#123; WxImgTxtMsgResVo vo &#x3D; new WxImgTxtMsgResVo(); vo.setArticles(imgTxtList); vo.setArticleCount(imgTxtList.size()); return vo; &#125; &#125; 用户登录逻辑用户注册&#x2F;登录 sessionService.autoRegisterWxUserInfo(fromUser); 实现方法如下 @Override public Long autoRegisterWxUserInfo(String uuid) &#123; UserSaveReq req &#x3D; new UserSaveReq().setLoginType(0).setThirdAccountId(uuid); Long userId &#x3D; registerOrGetUserInfo(req); ReqInfoContext.getReqInfo().setUserId(userId); return userId; &#125; &#x2F;** * 没有注册时，先注册一个用户；若已经有，则登录 * * @param req *&#x2F; private Long registerOrGetUserInfo(UserSaveReq req) &#123; &#x2F;&#x2F;查询用户是否存在 UserDO user &#x3D; userDao.getByThirdAccountId(req.getThirdAccountId()); if (user &#x3D;&#x3D; null) &#123; &#x2F;&#x2F;不存在就注册 return registerService.registerByWechat(req.getThirdAccountId()); &#125; return user.getId(); &#125; 这个方法返回用户的userId. 下面就是重点内容，先判断用户是否登录成功 if (qrLoginHelper.login(content)) &#123; textRes &#x3D; &quot;登录成功，开始愉快的玩耍技术派吧！&quot;; log.info(&quot;用户:登录成功&quot;, fromUser); &#125; else &#123; textRes &#x3D; &quot;验证码过期了，刷新登录页面重试一下吧&quot;; log.info(&quot;用户:登录失败，验证码过期了！&quot;, fromUser); &#125; 用户登的逻辑如下 public boolean login(String verifyCode) &#123; &#x2F;&#x2F;通过验证码找到对应的SSE Emitter。 SseEmitter sseEmitter &#x3D; verifyCodeCache.getIfPresent(verifyCode); if (sseEmitter &#x3D;&#x3D; null) &#123; return false; &#125; &#x2F;&#x2F;登录微信，并获取用户的session。 String session &#x3D; sessionService.loginByWx(ReqInfoContext.getReqInfo().getUserId()); try &#123; &#x2F;&#x2F; 登录成功，写入session sseEmitter.send(session); &#x2F;&#x2F; 设置cookie的路径 sseEmitter.send(&quot;login#&quot; + LoginService.SESSION_KEY + &quot;&#x3D;&quot; + session + &quot;;path&#x3D;&#x2F;;&quot;); return true; &#125; catch (Exception e) &#123; log.error(&quot;登录异常: &#123;&#125;&quot;, verifyCode, e); &#125; finally &#123; &#x2F;&#x2F;关闭SSE Emitter，并从缓存中移除验证码。 sseEmitter.complete(); verifyCodeCache.invalidate(verifyCode); &#125; return false; &#125; 上述代码的关键地方在于 String session = sessionService.loginByWx(ReqInfoContext.getReqInfo().getUserId()); 根据uderId去生成session，并写回前端sseEmitter.send(&quot;login#&quot; + LoginService.SESSION_KEY + &quot;=&quot; + session + &quot;;path=/;&quot;); &#x2F;** * 生成会话字符串 * * @param userId 用户ID * @return 生成的会话字符串 *&#x2F; public String genSession(Long userId) &#123; &#x2F;&#x2F; 创建一个包含SelfTraceIdGenerator生成的会话ID和用户ID的Map对象 String session &#x3D; JsonUtil.toStr(MapUtils.create(&quot;s&quot;, SelfTraceIdGenerator.generate(), &quot;u&quot;, userId)); &#x2F;&#x2F; 使用JWT创建一个令牌 String token &#x3D; JWT.create() .withIssuer(jwtProperties.getIssuer()) &#x2F;&#x2F; 设置令牌发布者 .withSubject(session) &#x2F;&#x2F; 设置令牌主题为会话字符串 .withClaim(&quot;userId&quot;, userId) &#x2F;&#x2F; 添加用户ID作为声明 .withExpiresAt(new Date(System.currentTimeMillis() + jwtProperties.getExpire())) &#x2F;&#x2F; 设置令牌过期时间 .sign(algorithm);&#x2F;&#x2F; 签署令牌 &#x2F;&#x2F;TODO 存redis return token; &#125; 在前端收到session后写入cookie else if (text.startsWith(&#39;login#&#39;)) &#123; &#x2F;&#x2F; 登录格式为 login#cookie console.log(&quot;登录成功,保存cookie&quot;, text) document.cookie &#x3D; text.substring(6); source.close(); refreshPage(); &#125; 如图写入的session格式如下 至此整个登录实现完成，这个项目是后续登录逻辑是登录完成后重定向到首页，后端实现filter进行拦截，拦截时获取cookie并对其进行解析从而获取用户的信息，写入请求上下文。","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"}],"author":"ehzyil"},{"title":"Docker安装Nginx","slug":"2024/Docker安装Nginx","date":"2024-01-09T20:22:46.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2024/01/09/2024/Docker安装Nginx/","link":"","permalink":"https://blog.ehzyil.xyz/2024/01/09/2024/Docker%E5%AE%89%E8%A3%85Nginx/","excerpt":"","text":"创建挂载目录mkdir -p &#x2F;data&#x2F;docker&#x2F;nginx&#x2F;conf mkdir -p &#x2F;data&#x2F;docker&#x2F;nginx&#x2F;nginx mkdir -p &#x2F;data&#x2F;docker&#x2F;nginx&#x2F;html 拉取docker镜像docker pull nginx:latest 创建Nginx容器并运行docker run -d --name nginx -p 81:80 -v &#x2F;data&#x2F;docker&#x2F;nginx:&#x2F;data nginx 容器中的nginx.conf文件、conf.d文件和html文件夹复制到宿主机 docker cp nginx:&#x2F;etc&#x2F;nginx&#x2F;nginx.conf &#x2F;home&#x2F;ehzyil&#x2F;data&#x2F;docker_data&#x2F;nginx&#x2F; docker cp nginx:&#x2F;etc&#x2F;nginx&#x2F;conf.d &#x2F;home&#x2F;ehzyil&#x2F;data&#x2F;docker_data&#x2F;nginx&#x2F; docker cp nginx:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html &#x2F;home&#x2F;ehzyil&#x2F;data&#x2F;docker_data&#x2F;nginx&#x2F; 关闭并删除Nginx容器# 关闭该容器 docker stop nginx # 删除该容器 docker rm nginx 创建Nginx容器，指定挂载点和端口号并运行docker run \\ -p 81:80 \\ --name nginx \\ -v &#x2F;data&#x2F;docker&#x2F;nginx&#x2F;nginx.conf:&#x2F;etc&#x2F;nginx&#x2F;nginx.conf \\ -v &#x2F;data&#x2F;docker&#x2F;nginx&#x2F;conf.d:&#x2F;etc&#x2F;nginx&#x2F;conf.d \\ -v &#x2F;home&#x2F;nginx&#x2F;nginx:&#x2F;var&#x2F;log&#x2F;nginx \\ -v &#x2F;data&#x2F;docker&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\ -d nginx:latest Nginx默认配置(备份) user nginx; worker_processes auto; error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log notice; pid &#x2F;var&#x2F;run&#x2F;nginx.pid; events &#123; worker_connections 1024; &#125; http &#123; include &#x2F;etc&#x2F;nginx&#x2F;mime.types; default_type application&#x2F;octet-stream; log_format main &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39; &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39; &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;; access_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on;a include &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;*.conf; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"git","slug":"git","permalink":"https://blog.ehzyil.xyz/tags/git/"},{"name":"docker","slug":"docker","permalink":"https://blog.ehzyil.xyz/tags/docker/"}],"author":"ehzyil"},{"title":"记录SQL执行日志","slug":"2024/记录SQL执行日志","date":"2024-01-06T20:22:46.000Z","updated":"2024-06-17T01:04:54.007Z","comments":true,"path":"2024/01/06/2024/记录SQL执行日志/","link":"","permalink":"https://blog.ehzyil.xyz/2024/01/06/2024/%E8%AE%B0%E5%BD%95SQL%E6%89%A7%E8%A1%8C%E6%97%A5%E5%BF%97/","excerpt":"","text":"记录SQL执行日志MybaitPlus执行日志因为技术派使用MybatisPlus作为ORM框架进行数据源的CURD，因此我们可以直接使用mybatis提供的日志输出 如开启控制台输出 mybatis: configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl map-underscore-to-camel-case: true 日志结果： JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@62f94ee2] will not be managed by Spring &#x3D;&#x3D;&gt; Preparing: select * from user where id &#x3D;? &#x3D;&#x3D;&gt; Parameters: 1(Integer) &lt;&#x3D;&#x3D; Columns: id, third_account_id, user_name, password, login_type, deleted, create_time, update_time &lt;&#x3D;&#x3D; Row: 1, master, admin, admin, 0, 0, 2023-12-28 15:53:47, 2023-12-29 11:23:18 &lt;&#x3D;&#x3D; Total: 1 使用Mybatis插件Mybatis插件机制在Mybatis中，插件机制提供了非常强大的扩展能力，在sql最终执行之前，提供了四个拦截点，支持不同场景的功能扩展 Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) Mybatis插件实现流程1.实现接口Interceptorpublic class SqlStateInterceptor implements Interceptor &#123;&#125; 2.添加注解@Intercepts(value &#x3D; &#123;@Signature(type &#x3D; Executor.class, method &#x3D; &quot;query&quot;, args &#x3D; &#123;MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class&#125;), @Signature(type &#x3D; Executor.class, method &#x3D; &quot;update&quot;, args &#x3D; &#123;MappedStatement.class, Object.class&#125;), &#125;) 类上的@Intercepts注解，它表明这个类是一个mybatis的插件类，通过@Signature来指定切点 其中的type, method, args用来精确命中切点的具体方法 首先从切点为Executor，然后两个方法的执行会被拦截；这两个方法的方法名分别是query, update，参数类型也一并定义了，通过这些信息，可以精确匹配Executor接口上定义的类，如下 &#x2F;&#x2F; org.apache.ibatis.executor.Executor &#x2F;&#x2F; 对应第一个@Signature &lt;E&gt; List&lt;E&gt; query(MappedStatement var1, Object var2, RowBounds var3, ResultHandler var4) throws SQLException; &#x2F;&#x2F; 对应第二个@Signature int update(MappedStatement var1, Object var2) throws SQLException; mybatis提供了四个切点，那么他们之间有什么区别，什么样的场景选择什么样的切点呢？ 一般来讲，拦截ParameterHandler是最常见的，虽然上面的实例是拦截Executor，切点的选择，主要与它的功能强相关，想要更好的理解它，需要从mybatis的工作原理出发，这里将只做最基本的介绍，待后续源码进行详细分析 Executor：代表执行器，由它调度StatementHandler、ParameterHandler、ResultSetHandler等来执行对应的SQL，其中StatementHandler是最重要的。 StatementHandler：作用是使用数据库的Statement（PreparedStatement）执行操作，它是四大对象的核心，起到承上启下的作用，许多重要的插件都是通过拦截它来实现的。 ParameterHandler：是用来处理SQL参数的。 ResultSetHandler：是进行数据集（ResultSet）的封装返回处理的，它非常的复杂，好在不常用。 借用网上的一张mybatis执行过程来辅助说明 3.重载方法public class SqlStateInterceptor implements Interceptor &#123; @Override public Object intercept(Invocation invocation) throws Throwable &#123; return null; &#125; @Override public Object plugin(Object target) &#123; return null; &#125; @Override public void setProperties(Properties properties) &#123; &#125; &#125; 4.拦截器注册自定义的拦截器实现，还需要注册到应用中让它生效，一般有下面几种姿势 1.spring bean对象 @Bean public SqlStateInterceptor sqlStateInterceptor() &#123; return new SqlStateInterceptor(); &#125; 2.SqlSessionFactory 除了上面的姿势之外还可以直接再sql会话工厂中指定 @Bean(name &#x3D; &quot;sqlSessionFactory&quot;) public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean bean &#x3D; new SqlSessionFactoryBean(); bean.setDataSource(dataSource); bean.setMapperLocations( &#x2F;&#x2F; 设置mybatis的xml所在位置，这里使用mybatis注解方式，没有配置xml文件 new PathMatchingResourcePatternResolver().getResources(&quot;classpath*:mapping&#x2F;*.xml&quot;)); &#x2F;&#x2F; 注册typehandler，供全局使用 bean.setTypeHandlers(new Timestamp2LongHandler()); bean.setPlugins(new SqlStatInterceptor()); return bean.getObject(); &#125; 3.xml配置 对于习惯使用myabtis的同学而言，这一种方式不少见，直接再myabtis-config.xml中进行定义 @Bean public SqlStateInterceptor sqlStateInterceptor() &#123; return new SqlStateInterceptor(); &#125; 实现package com.example.senstive.senstive; import com.alibaba.druid.pool.DruidPooledPreparedStatement; import com.baomidou.mybatisplus.core.MybatisParameterHandler; import com.example.senstive.util.DateUtil; import com.example.senstive.util.DruidCheckUtil; import com.mysql.cj.MysqlConnection; import com.zaxxer.hikari.pool.HikariProxyConnection; import com.zaxxer.hikari.pool.HikariProxyPreparedStatement; import lombok.extern.slf4j.Slf4j; import nonapi.io.github.classgraph.reflection.ReflectionUtils; import org.apache.ibatis.executor.statement.StatementHandler; import org.apache.ibatis.mapping.BoundSql; import org.apache.ibatis.mapping.MappedStatement; import org.apache.ibatis.mapping.ParameterMapping; import org.apache.ibatis.mapping.ParameterMode; import org.apache.ibatis.plugin.*; import org.apache.ibatis.reflection.MetaObject; import org.apache.ibatis.scripting.defaults.DefaultParameterHandler; import org.apache.ibatis.session.Configuration; import org.apache.ibatis.session.ResultHandler; import org.springframework.util.CollectionUtils; import java.sql.Connection; import java.sql.Statement; import java.util.Date; import java.util.List; import java.util.Properties; import java.util.regex.Matcher; // Define a custom MyBatis plugin to handle data masking. /** * SQL状态拦截器 */ @Intercepts(&#123; @Signature(type = StatementHandler.class, method = \"query\", args = &#123;Statement.class, ResultHandler.class&#125;), @Signature(type = StatementHandler.class, method = \"update\", args = &#123;Statement.class&#125;) &#125;) @Slf4j public class SqlStateInterceptor implements Interceptor &#123; /** * 拦截方法 * * @param invocation 调用的Invocation对象 * @return Object * @throws Throwable 异常 */ @Override public Object intercept(Invocation invocation) throws Throwable &#123; long time = System.currentTimeMillis(); final List&lt;Object> results = (List&lt;Object>) invocation.proceed(); if (results.isEmpty()) &#123; return results; &#125; StatementHandler statementHandler = (StatementHandler) invocation.getTarget(); String sql = buildSql(statementHandler); //判断数据库连接池类型 Object[] args = invocation.getArgs(); String uname = \"\"; if (args[0] instanceof HikariProxyPreparedStatement) &#123; HikariProxyConnection connection = (HikariProxyConnection) ((HikariProxyPreparedStatement) invocation.getArgs()[0]).getConnection(); uname = connection.getMetaData().getUserName(); &#125; else if (DruidCheckUtil.hasDuridPkg()) &#123; if (args[0] instanceof DruidPooledPreparedStatement) &#123; Connection connection = ((DruidPooledPreparedStatement) args[0]).getStatement().getConnection(); if (connection instanceof MysqlConnection) &#123; Properties properties = ((MysqlConnection) connection).getProperties(); uname = properties.getProperty(\"user\"); &#125; &#125; &#125; Object rs = null; try &#123; rs = invocation.proceed(); &#125; catch (Throwable e) &#123; log.error(\"error sql: \" + sql, e); throw e; &#125; finally &#123; long cost = System.currentTimeMillis() - time; sql = this.replaceContinueSpace(sql); // 这个方法的总耗时 log.info(\"\\n\\n ============= \\nsql ----> &#123;&#125;\\n&#123;&#125;\\nuser ----> &#123;&#125;\\ncost ----> &#123;&#125;\\n ============= \\n\", sql, getResult(rs.toString()), uname, cost); &#125; return rs; &#125; /** * 获取结果 * * @param input 输入 * @return 结果 */ private String getResult(String input) &#123; return input.substring(input.indexOf(\"(\") + 1, input.lastIndexOf(\")\")); &#125; /** * 替换连续的空白 * * @param str 字符串 * @return 替换后的字符串 */ private String replaceContinueSpace(String str) &#123; StringBuilder builder = new StringBuilder(str.length()); boolean preSpace = false; for (int i = 0, len = str.length(); i &lt; len; i++) &#123; char ch = str.charAt(i); boolean isSpace = Character.isWhitespace(ch); if (preSpace &amp;&amp; isSpace) &#123; continue; &#125; if (preSpace) &#123; // 前面的是空白字符，当前的不是的 preSpace = false; builder.append(ch); &#125; else if (isSpace) &#123; // 当前字符为空白字符，前面的那个不是的 preSpace = true; builder.append(\" \"); &#125; else &#123; // 前一个和当前字符都非空白字符 builder.append(ch); &#125; &#125; return builder.toString(); &#125; /** * 构建SQL * * @param statementHandler StatementHandler对象 * @return SQL语句 */ private String buildSql(StatementHandler statementHandler) &#123; BoundSql boundSql = statementHandler.getBoundSql(); Configuration configuration = null; if (statementHandler.getParameterHandler() instanceof DefaultParameterHandler) &#123; DefaultParameterHandler handler = (DefaultParameterHandler) statementHandler.getParameterHandler(); configuration = (Configuration) new ReflectionUtils().getFieldVal(false, handler, \"configuration\"); &#125; else if (statementHandler.getParameterHandler() instanceof MybatisParameterHandler) &#123; MybatisParameterHandler paramHandler = (MybatisParameterHandler) statementHandler.getParameterHandler(); configuration = ((MappedStatement) new ReflectionUtils().getFieldVal(false, paramHandler, \"mappedStatement\")).getConfiguration(); &#125; if (configuration == null) &#123; return boundSql.getSql(); &#125; return buildSql(boundSql, configuration); &#125; /** * 构建SQL * * @param boundSql BoundSql对象 * @param configuration Configuration对象 * @return SQL语句 */ private String buildSql(BoundSql boundSql, Configuration configuration) &#123; String sql = boundSql.getSql(); //获取参数 Object parameterObject = boundSql.getParameterObject(); List&lt;ParameterMapping> parameterMappings = boundSql.getParameterMappings(); //没有参数直接返回 if (CollectionUtils.isEmpty(parameterMappings) || parameterObject == null) &#123; return sql; &#125; MetaObject mo = configuration.newMetaObject(boundSql.getParameterObject()); for (ParameterMapping parameterMapping : parameterMappings) &#123; //只拦截 输出参数 if (parameterMapping.getMode() == ParameterMode.OUT) &#123; continue; &#125; //参数值 Object value; //获取参数名称 String propertyName = parameterMapping.getProperty(); if (boundSql.hasAdditionalParameter(propertyName)) &#123; value = boundSql.getAdditionalParameter(propertyName); &#125; else if (configuration.getTypeHandlerRegistry().hasTypeHandler(parameterObject.getClass())) &#123; //如果是单个值则直接赋值 value = parameterObject; &#125; else &#123; value = mo.getValue(propertyName); &#125; String param = Matcher.quoteReplacement(getParameter(value)); sql = sql.replaceFirst(\"\\\\?\", param); &#125; sql += \";\"; return sql; &#125; /** * 获取参数值 * * @param parameter 参数 * @return 参数值 */ private String getParameter(Object parameter) &#123; if (parameter instanceof String) &#123; return \"'\" + parameter + \"'\"; &#125; else if (parameter instanceof Date) &#123; return \"'\" + DateUtil.format(DateUtil.DB_FORMAT, ((Date) parameter).getTime()) + \"'\"; &#125; else if (parameter instanceof java.util.Date) &#123; return \"'\" + DateUtil.format(DateUtil.DB_FORMAT, ((java.util.Date) parameter).getTime()) + \"'\"; &#125; return parameter.toString(); &#125; /** * 插件方法 * * @param o 对象 * @return 插件后的对象 */ @Override public Object plugin(Object o) &#123; return Plugin.wrap(o, this); &#125; /** * 设置属性 * * @param properties 属性 */ @Override public void setProperties(Properties properties) &#123; &#125; &#125; 当插件的intercept()方法被调用时，它会执行拦截的SQL语句。在这个方法中，会首先获取拦截的SQL语句并根据数据库连接池的类型获取登录用户的用户名。然后，会执行原始的SQL语句，并在执行过程中捕获任何异常并记录日志。最后，会返回原始SQL语句的结果。插件的plugin()方法用于将插件应用到目标对象上。它使用MyBatis提供的Plugin类对目标对象进行包装，从而实现插件的功能。setProperties()方法用于设置插件的属性。 为什么 buildSql()要获取Configuration获取 Configuration 对象是为了能够访问 MyBatis 的配置信息，例如： 类型处理器注册表 (TypeHandlerRegistry) 对象工厂 (ObjectFactory) 对象包装工厂 (ObjectWrapperFactory) 插件列表 (InterceptorChain) 映射语句注册表 (MappedStatementRegistry) 环境 (Environment) 在 buildSql() 方法中，需要使用 Configuration 对象来获取类型处理器注册表，以便能够将参数值转换为数据库可识别的格式。 例如，在以下代码中，使用 Configuration 对象来获取类型处理器注册表，并使用类型处理器注册表来获取字符串类型参数的类型处理器： Configuration configuration = ...; TypeHandlerRegistry typeHandlerRegistry = configuration.getTypeHandlerRegistry(); TypeHandler&lt;String> stringTypeHandler = typeHandlerRegistry.getTypeHandler(String.class); 然后，可以使用 stringTypeHandler 来将字符串类型参数值转换为数据库可识别的格式。 String parameterValue = ...; String databaseValue = stringTypeHandler.setParameter(parameterValue, null); 获取 Configuration 对象还可以用于访问其他 MyBatis 配置信息，例如： 允许自动映射的别名列表 (typeAliases) 允许自动映射的结果映射列表 (resultMaps) 允许自动映射的缓存列表 (caches) 这些信息可以用于构建更复杂的 SQL 语句或执行其他操作。 下列代码的作用MetaObject mo &#x3D; configuration.newMetaObject(boundSql.getParameterObject()); for (ParameterMapping parameterMapping : parameterMappings) &#123; &#x2F;&#x2F;只拦截 输出参数 if (parameterMapping.getMode() &#x3D;&#x3D; ParameterMode.OUT) &#123; continue; &#125; &#x2F;&#x2F;参数值 Object value; &#x2F;&#x2F;获取参数名称 String propertyName &#x3D; parameterMapping.getProperty(); if (boundSql.hasAdditionalParameter(propertyName)) &#123; value &#x3D; boundSql.getAdditionalParameter(propertyName); &#125; else if (configuration.getTypeHandlerRegistry().hasTypeHandler(parameterObject.getClass())) &#123; &#x2F;&#x2F;如果是单个值则直接赋值 value &#x3D; parameterObject; &#125; else &#123; value &#x3D; mo.getValue(propertyName); &#125; String param &#x3D; Matcher.quoteReplacement(getParameter(value)); sql &#x3D; sql.replaceFirst(&quot;\\\\?&quot;, param); &#125; 这段代码用于将 MyBatis 查询语句中的参数值替换为实际值。 首先，使用 configuration.newMetaObject(boundSql.getParameterObject()) 创建一个 MetaObject 对象，该对象封装了查询语句的参数对象。 然后，遍历查询语句中的参数映射列表 (parameterMappings)。对于每个参数映射，如果参数模式为 ParameterMode.OUT，则跳过该参数，因为输出参数不需要替换。 对于其他参数，根据参数映射的属性名称 (propertyName) 获取参数值。参数值可能来自以下几个地方： 如果查询语句中包含附加参数，则从附加参数中获取。 如果 MyBatis 配置中注册了参数对象的类型处理器，则直接使用参数对象作为参数值。 否则，从参数对象中获取参数值。 最后，使用 Matcher.quoteReplacement(getParameter(value)) 将参数值转换为字符串，并用该字符串替换查询语句中的第一个问号 (?)。 工具类package com.example.senstive.util; import java.sql.Timestamp; import java.time.Instant; import java.time.LocalDateTime; import java.time.ZoneId; import java.time.format.DateTimeFormatter; public class DateUtil &#123; public static final DateTimeFormatter UTC_FORMAT &#x3D; DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSS&#39;Z&#39;&quot;); public static final DateTimeFormatter DB_FORMAT &#x3D; DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;); public static final DateTimeFormatter BLOG_TIME_FORMAT &#x3D; DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日 HH:mm&quot;); public static final DateTimeFormatter BLOG_DATE_FORMAT &#x3D; DateTimeFormatter.ofPattern(&quot;yyyy年MM月dd日&quot;); &#x2F;** * 一天对应的毫秒数 *&#x2F; public static final Long ONE_DAY_MILL &#x3D; 86400_000L; public static final Long ONE_DAY_SECONDS &#x3D; 86400L; public static final Long ONE_MONTH_SECONDS &#x3D; 31 * 86400L; public static final Long THREE_DAY_MILL &#x3D; 3 * ONE_DAY_MILL; &#x2F;** * 毫秒转日期 * * @param timestamp * @return *&#x2F; public static String time2day(long timestamp) &#123; return format(BLOG_TIME_FORMAT, timestamp); &#125; public static String time2day(Timestamp timestamp) &#123; return time2day(timestamp.getTime()); &#125; public static LocalDateTime time2LocalTime(long timestamp) &#123; return LocalDateTime.ofInstant(Instant.ofEpochMilli(timestamp), ZoneId.systemDefault()); &#125; public static String time2utc(long timestamp) &#123; return format(UTC_FORMAT, timestamp); &#125; public static String time2date(long timestamp) &#123; return format(BLOG_DATE_FORMAT, timestamp); &#125; public static String time2date(Timestamp timestamp) &#123; return time2date(timestamp.getTime()); &#125; public static String format(DateTimeFormatter format, long timestamp) &#123; LocalDateTime time &#x3D; time2LocalTime(timestamp); return format.format(time); &#125; &#125; import org.springframework.util.ClassUtils public class DruidCheckUtil &#123; &#x2F;** * 判断是否包含durid相关的数据包 * * @return *&#x2F; public static boolean hasDuridPkg() &#123; return ClassUtils.isPresent(&quot;com.alibaba.druid.pool.DruidDataSource&quot;, DataSourceConfig.class.getClassLoader()); &#125; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://blog.ehzyil.xyz/tags/Mybatis/"}],"author":"ehzyil"},{"title":"通用敏感词替换","slug":"2024/通用敏感词替换","date":"2024-01-06T20:22:46.000Z","updated":"2024-06-17T01:04:54.007Z","comments":true,"path":"2024/01/06/2024/通用敏感词替换/","link":"","permalink":"https://blog.ehzyil.xyz/2024/01/06/2024/%E9%80%9A%E7%94%A8%E6%95%8F%E6%84%9F%E8%AF%8D%E6%9B%BF%E6%8D%A2/","excerpt":"","text":"本文基于开源项目sensitive-word，基础使用方法请看文档。 敏感词服务类新增一个敏感词配置类，处理自定义的敏感词、与白名单 @Data @Component @ConfigurationProperties(prefix &#x3D; &quot;forum.sensitive&quot;) public class SensitiveProperty &#123; &#x2F;** * true 表示开启敏感词校验 *&#x2F; private Boolean enable; &#x2F;** * 自定义的敏感词 *&#x2F; private List&lt;String&gt; deny; &#x2F;** * 自定义的非敏感词 *&#x2F; private List&lt;String&gt; allow; &#125; 封装敏感词的服务类，在项目启动时进行初始化 @Slf4j @Service public class SensitiveService &#123; /** * 敏感词命中计数统计 */ private static final String SENSITIVE_WORD_CNT_PREFIX = \"sensitive_word\"; private volatile SensitiveWordBs sensitiveWordBs; @Autowired private SensitiveProperty sensitiveConfig; @PostConstruct public void refresh() &#123; IWordDeny deny = () -> &#123; List&lt;String> sub = WordDenySystem.getInstance().deny(); sub.addAll(sensitiveConfig.getDeny()); return sub; &#125;; IWordAllow allow = () -> &#123; List&lt;String> sub = WordAllowSystem.getInstance().allow(); sub.addAll(sensitiveConfig.getAllow()); return sub; &#125;; sensitiveWordBs = SensitiveWordBs.newInstance() .wordDeny(deny) .wordAllow(allow) .init(); log.info(\"敏感词初始化完成!\"); &#125; /** * 判断是否包含敏感词 * * @param txt 需要校验的文本 * @return 返回命中的敏感词 */ public List&lt;String> contains(String txt) &#123; if (!BooleanUtils.isTrue(sensitiveConfig.getEnable())) &#123; return Collections.emptyList(); &#125; List&lt;String> ans = sensitiveWordBs.findAll(txt); if (CollectionUtils.isEmpty(ans)) &#123; return ans; &#125; return ans; &#125; /** * 敏感词替换 * * @param txt * @return */ public String replace(String txt) &#123; if (BooleanUtils.isTrue(sensitiveConfig.getEnable())) &#123; return sensitiveWordBs.replace(txt); &#125; return txt; &#125; /** * 查询文本中所有命中的敏感词 * * @param txt 校验文本 * @return 命中的敏感词 */ public List&lt;String> findAll(String txt) &#123; return sensitiveWordBs.findAll(txt); &#125; &#125; 调用测试在application.yml中配置敏感词 调用下列接口测试 @GetMapping(path &#x3D; &quot;sensitive&#x2F;check&quot;) public List&lt;String&gt; check(String txt) &#123; return sensitiveService.findAll(txt); &#125; @GetMapping(path &#x3D; &quot;sensitive&#x2F;contains&quot;) public List&lt;String&gt; contains(String txt) &#123; return sensitiveService.contains(txt); &#125; @GetMapping(path &#x3D; &quot;sensitive&#x2F;replace&quot;) public String replace(String txt) &#123; return sensitiveService.replace(txt); &#125; 自定义的数据库敏感词替换方案 数据脱敏是指使用各种技术手段对敏感数据进行处理，使其无法被未经授权的人员访问或使用。在实际项目中，需要数据脱敏的原因有很多，在此我们在实现Springboot项目中实现敏感词替换。 实际生产项目中使用的数据脱敏机制，为了安全以及某些大家都知道的原因，db中有很多信息是不能存储明文的，比如身份证、银行卡等需要加密之后存储到数据库中，然后再读取时，解密返回对应的明文那么如何实现呢？ 直接编码实现，每次写的时候、读取的时候手动进行加解密 ？ 实现一个通用的解决方案，比如在需要脱敏的字段上加一个标识，然后再实际写db&#x2F;从db读取时，自动实现加解密 我们这里介绍的是第二个解决方案 基于mybatis拦截器的敏感词替换实现方案整体的方案思路比较清晰： 实现一个自定义注解，放在需要脱敏的数据库实体对象的成员上 实现查询拦截器，再返回数据库内容到数据库实体对象上时，若判断成员上有对应注解，则赋值敏感词替换之后的内容 再具体的实现层，我们会增加一下缓存，减少每次都对实体对象的成员都进行判定，是否需要脱敏 具体实现实现一个自定义注解通过自定义注解，灵活地对需要脱敏的数据库实体对象成员进行标记。 @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.FIELD) public @interface SensitiveField &#123; String bind() default &quot;&quot;; &#125; 实现敏感数据缓存为了提高脱敏效率，使用缓存系统存储脱敏后的数据，减少重复脱敏的计算。 敏感词缓存类： public class SensitiveMetaCache &#123; &#x2F;&#x2F; 敏感元数据缓存 private static ConcurrentHashMap&lt;String, SensitiveObjectMeta&gt; CACHE &#x3D; new ConcurrentHashMap&lt;&gt;(); &#x2F;** * 获取指定key对应的敏感元数据 * * @param key 键 * @return 指定key对应的敏感元数据 *&#x2F; public static SensitiveObjectMeta get(String key) &#123; return CACHE.get(key); &#125; &#x2F;** * 将敏感元数据放入缓存 * * @param key 键 * @param meta 敏感元数据 *&#x2F; public static void put(String key, SensitiveObjectMeta meta) &#123; CACHE.put(key, meta); &#125; &#x2F;** * 从缓存中移除指定key对应的敏感元数据 * * @param key 键 *&#x2F; public static void remove(String key) &#123; CACHE.remove(key); &#125; &#x2F;** * 检查缓存中是否存在指定key对应的敏感元数据 * * @param key 键 * @return 缓存中是否存在指定key对应的敏感元数据 *&#x2F; public static boolean contains(String key) &#123; return CACHE.containsKey(key); &#125; &#x2F;** * 将敏感元数据放入缓存，如果键已存在，则更新对应的敏感元数据 * * @param key 键 * @param meta 敏感元数据 * @return 新的敏感元数据 *&#x2F; public static SensitiveObjectMeta putIfAbsent(String key, SensitiveObjectMeta meta) &#123; return CACHE.putIfAbsent(key, meta); &#125; &#x2F;** * 从缓存中计算并获取指定key对应的敏感元数据 * * @param key 键 * @param function 计算函数 * @return 指定key对应的敏感元数据 *&#x2F; public static SensitiveObjectMeta computeIfAbsent(String key, Function&lt;String, SensitiveObjectMeta&gt; function) &#123; return CACHE.computeIfAbsent(key, function); &#125; &#125; computeIfAbsent 方法的作用是将一个键值对添加到缓存中，如果键已经存在，则不执行任何操作。 代码分解 public static SensitiveObjectMeta computeIfAbsent(String key, Function&lt;String, SensitiveObjectMeta&gt; function)：这是一个公共静态方法，它接受两个参数：key 是要添加到缓存中的键，function 是一个 lambda 表达式，用于计算键对应的值。 return CACHE.computeIfAbsent(key, function);: 使用 ConcurrentHashMap 的 computeIfAbsent 方法来将键值对添加到缓存中。如果键已经存在，则不执行任何操作。否则，使用 lambda 表达式 function 来计算键对应的值，并将键值对添加到缓存中。 @Data public class SensitiveObjectMeta &#123; private static final String JAVA_LANG_OBJECT &#x3D; &quot;java.lang.object&quot;; &#x2F;** * 是否启用脱敏 *&#x2F; private Boolean enabledSensitiveReplace; &#x2F;** * 类名 *&#x2F; private String className; &#x2F;** * 标注 SensitiveField 的成员 *&#x2F; private List&lt;SensitiveFieldMeta&gt; sensitiveFieldMetaList; &#x2F;** * 构建 SensitiveObjectMeta 对象 * * @param param 对象实例 * @return Optional&lt;SensitiveObjectMeta&gt; 可选的 SensitiveObjectMeta 对象 *&#x2F; public static Optional&lt;SensitiveObjectMeta&gt; buildSensitiveObjectMeta(Object param) &#123; if (isNull(param)) &#123; return Optional.empty(); &#125; Class&lt;?&gt; clazz &#x3D; param.getClass(); SensitiveObjectMeta sensitiveObjectMeta &#x3D; new SensitiveObjectMeta(); sensitiveObjectMeta.setClassName(clazz.getName()); List&lt;SensitiveFieldMeta&gt; sensitiveFieldMetaList &#x3D; newArrayList(); sensitiveObjectMeta.setSensitiveFieldMetaList(sensitiveFieldMetaList); boolean sensitiveField &#x3D; parseAllSensitiveField(clazz, sensitiveFieldMetaList); sensitiveObjectMeta.setEnabledSensitiveReplace(sensitiveField); return Optional.of(sensitiveObjectMeta); &#125; &#x2F;** * 解析类中的所有 SensitiveField 字段 * * @param clazz 类实例 * @param sensitiveFieldMetaList SensitiveFieldMeta 对象列表 * @return boolean 是否包含 SensitiveField *&#x2F; private static boolean parseAllSensitiveField(Class&lt;?&gt; clazz, List&lt;SensitiveFieldMeta&gt; sensitiveFieldMetaList) &#123; Class&lt;?&gt; tempClazz &#x3D; clazz; boolean hasSensitiveField &#x3D; false; while ((nonNull(tempClazz)) &amp;&amp; !JAVA_LANG_OBJECT.equalsIgnoreCase(tempClazz.getName())) &#123; for (java.lang.reflect.Field field : tempClazz.getDeclaredFields()) &#123; SensitiveField sensitiveField &#x3D; field.getAnnotation(SensitiveField.class); if (nonNull(sensitiveField)) &#123; SensitiveFieldMeta sensitiveFieldMeta &#x3D; new SensitiveFieldMeta(); sensitiveFieldMeta.setName(field.getName()); sensitiveFieldMeta.setBindField(sensitiveField.bind()); sensitiveFieldMetaList.add(sensitiveFieldMeta); hasSensitiveField &#x3D; true; &#125; &#125; tempClazz &#x3D; tempClazz.getSuperclass(); &#125; return hasSensitiveField; &#125; @Data public static class SensitiveFieldMeta &#123; &#x2F;** * 默认根据字段名，找db中同名的字段 *&#x2F; private String name; &#x2F;** * 绑定的数据库字段别名 *&#x2F; private String bindField; &#125; &#125; 实现查询拦截器在查询拦截器中，对查询结果进行拦截，并根据自定义注解对敏感数据进行脱敏处理。 拦截器的工作原理 这个拦截器的工作原理如下： 当 MyBatis 执行 SQL 语句时，拦截器会首先被调用。 拦截器会检查 SQL 语句是否需要进行敏感词替换。 如果需要进行敏感词替换，拦截器会对查询结果中的敏感词进行替换。 拦截器会将替换后的查询结果返回给 MyBatis。 代码实现细节 @Intercepts(&#123; @Signature(type = ResultSetHandler.class, method = \"handleResultSets\", args = &#123;java.sql.Statement.class&#125;) &#125;) @Component @Slf4j public class SensitiveReadInterceptor implements Interceptor &#123; private static final String MAPPED_STATEMENT = \"mappedStatement\"; @Autowired private SensitiveService sensitiveService; @Override public Object intercept(Invocation invocation) throws Throwable &#123; final List&lt;Object> results = (List&lt;Object>) invocation.proceed(); if (results.isEmpty()) &#123; return results; &#125; final ResultSetHandler statementHandler = realTarget(invocation.getTarget()); // 获取 statementHandler 对象的元对象。 final MetaObject metaObject = SystemMetaObject.forObject(statementHandler); //获取 statementHandler 对象的 MAPPED_STATEMENT 属性 final MappedStatement mappedStatement = (MappedStatement) metaObject.getValue(MAPPED_STATEMENT); // 从 results 列表中找到第一个非空元素，并将其包装在一个 Optional 对象中。 Optional firstOpt = results.stream().filter(Objects::nonNull).findFirst(); //检查 Optional 对象是否包含值 if (!firstOpt.isPresent()) &#123; return results; &#125; Object firstObject = firstOpt.get(); // 找到需要进行敏感词替换的数据库实体类的成员信息 SensitiveObjectMeta sensitiveObjectMeta = findSensitiveObjectMeta(firstObject); // 执行替换的敏感词替换 replaceSensitiveResults(results, mappedStatement, sensitiveObjectMeta); // 找到需要进行敏感词替换的数据库实体类的成员信息 return results; &#125; @Override public Object plugin(Object o) &#123; return Plugin.wrap(o, this); &#125; @Override public void setProperties(Properties properties) &#123; &#125; &#125; 拦截器注解： @Intercepts 注解用于指定拦截器要拦截的方法。在本例中，拦截器要拦截 ResultSetHandler.handleResultSets 方法。 组件扫描： @Component 注解用于将拦截器注册为 Spring Bean。 日志记录： @Slf4j 注解用于启用日志记录。 拦截方法： intercept 方法是拦截器的主方法。在这个方法中，拦截器会对查询结果进行拦截、检查是否需要进行敏感词替换、对查询结果中的敏感词进行替换，并将替换后的查询结果返回给 MyBatis。 插件方法： plugin 方法用于将拦截器包装成一个插件。 属性设置方法： setProperties 方法用于设置拦截器的属性。 final MetaObject metaObject &#x3D; SystemMetaObject.forObject(statementHandler); 这行代码的作用是获取 statementHandler 对象的元对象。 在 Java 中，元对象是一种包含了对象元数据信息的对象。元数据信息包括对象的类型、属性、方法等。 SystemMetaObject.forObject 方法可以获取一个对象的元对象。这个方法是 MetaObject 类的静态方法。 statementHandler 对象是一个 org.apache.ibatis.executor.statement.StatementHandler 对象。StatementHandler 对象负责将 SQL 语句发送到数据库并处理结果。 获取 statementHandler 对象的元对象后，就可以通过元对象来获取 statementHandler 对象的属性和方法。 查询对象中，携带有 @SensitiveField 的成员，进行敏感词替换： findSensitiveObjectMeta 方法用于查询对象中，携带有 @SensitiveField 的成员，并进行敏感词替换。 /** * 查询对象中，携带有 @SensitiveField 的成员，进行敏感词替换 * * @param firstObject 待查询的对象 * @return 返回对象的敏感词元数据 */ private SensitiveObjectMeta findSensitiveObjectMeta(Object firstObject) &#123; // 缓存中不存在，则计算键对应的值，并缓存 SensitiveMetaCache.computeIfAbsent(firstObject.getClass().getName(), s -> &#123; Optional&lt;SensitiveObjectMeta> sensitiveObjectMeta = SensitiveObjectMeta.buildSensitiveObjectMeta(firstObject); return sensitiveObjectMeta.orElse(null); &#125;); // 从缓存中获取 SensitiveObjectMeta 对象。 return SensitiveMetaCache.get(firstObject.getClass().getName()); &#125; 执行具体的敏感词替换： replaceSensitiveResults 方法用于执行具体的敏感词替换。 &#x2F;** * 执行具体的敏感词替换 * * @param results 结果列表 * @param mappedStatement MappedStatement 对象 * @param sensitiveObjectMeta SensitiveObjectMeta 对象 *&#x2F; private void replaceSensitiveResults(Collection&lt;Object&gt; results, MappedStatement mappedStatement, SensitiveObjectMeta sensitiveObjectMeta) &#123; for (Object obj : results) &#123; if (sensitiveObjectMeta.getSensitiveFieldMetaList() &#x3D;&#x3D; null) continue; MetaObject objMetaObject &#x3D; mappedStatement.getConfiguration().newMetaObject(obj); sensitiveObjectMeta.getSensitiveFieldMetaList().forEach(i -&gt; &#123; Object value &#x3D; objMetaObject.getValue(StringUtils.isBlank(i.getBindField()) ? i.getName() : i.getBindField()); if (value &#x3D;&#x3D; null) &#123; return; &#125; else if (value instanceof String) &#123; String strValue &#x3D; (String) value; &#x2F;&#x2F; 替换字符串中的敏感词 String processVal &#x3D; sensitiveService.replace(strValue); objMetaObject.setValue(i.getName(), processVal); &#125; else if (value instanceof Collection) &#123; Collection listValue &#x3D; (Collection) value; if (CollectionUtils.isNotEmpty(listValue)) &#123; Optional firstValOpt &#x3D; listValue.stream().filter(Objects::nonNull).findFirst(); if (firstValOpt.isPresent()) &#123; &#x2F;&#x2F; 找到第一个非空元素的敏感对象成员信息; SensitiveObjectMeta valSensitiveObjectMeta &#x3D; findSensitiveObjectMeta(firstValOpt.get()); if (Boolean.TRUE.equals(valSensitiveObjectMeta.getEnabledSensitiveReplace()) &amp;&amp; CollectionUtils.isNotEmpty(valSensitiveObjectMeta.getSensitiveFieldMetaList())) &#123; &#x2F;&#x2F; 递归进行敏感词替换 replaceSensitiveResults(listValue, mappedStatement, valSensitiveObjectMeta); &#125; &#125; &#125; &#125; else if (ClassUtils.isPrimitiveOrWrapper(value.getClass())) &#123; &#x2F;&#x2F; 对于非基本类型的，需要对其内部进行敏感词替换 SensitiveObjectMeta valSensitiveObjectMeta &#x3D; findSensitiveObjectMeta(value); if (Boolean.TRUE.equals(valSensitiveObjectMeta.getEnabledSensitiveReplace()) &amp;&amp; CollectionUtils.isNotEmpty(valSensitiveObjectMeta.getSensitiveFieldMetaList())) &#123; replaceSensitiveResults(newArrayList(value), mappedStatement, valSensitiveObjectMeta); &#125; &#125; &#125;); &#125; &#125; 获取代理对象的真实目标对象： realTarget 方法用于获取代理对象的真实目标对象。 &#x2F;** * 获取代理对象的真实目标对象。 * * @param target * @param &lt;T&gt; * @return *&#x2F; public static &lt;T&gt; T realTarget(Object target) &#123; if (Proxy.isProxyClass(target.getClass())) &#123; &#x2F;&#x2F;获取一个对象的元对象 MetaObject metaObject &#x3D; SystemMetaObject.forObject(target); &#x2F;&#x2F;如果 target 是一个代理对象，那么 realTarget 方法会递归地获取代理对象的真实目标对象，直到找到一个不是代理对象的对象。 return realTarget(metaObject.getValue(&quot;h.target&quot;)); &#125; return (T) target; &#125; 在 Java 中，代理对象是一种可以代替真实对象执行操作的对象。代理对象可以用来增强真实对象的某些功能，例如添加日志记录、安全检查等。 Proxy.isProxyClass 方法可以判断一个对象是否是一个代理对象。如果 target 是一个代理对象，那么 realTarget 方法会递归地获取代理对象的真实目标对象，直到找到一个不是代理对象的对象。 SystemMetaObject.forObject 方法可以获取一个对象的元对象。元对象包含了该对象的一些元数据信息，例如对象的类型、属性、方法等。 getValue 方法可以从元对象中获取一个属性的值。在 realTarget 方法中，getValue 方法被用来获取代理对象的 h.target 属性的值。h.target 属性的值就是代理对象的真实目标对象。 最后，realTarget 方法将代理对象的真实目标对象返回。 这个方法可以用于各种场景，例如： 获取代理对象的真实目标对象，以便直接调用真实目标对象的方法。 在代理对象上添加额外的功能，例如日志记录、安全检查等。 在代理对象上进行性能优化，例如缓存代理对象的方法调用结果。 测试在实体类上添加注解 @Data @EqualsAndHashCode(callSuper &#x3D; false) @Accessors(chain &#x3D; true) @TableName(&quot;user&quot;) public class User implements Serializable &#123; private static final long serialVersionUID &#x3D; 1L; List&lt;User&gt; users; &#x2F;** * 主键ID *&#x2F; @TableId(value &#x3D; &quot;id&quot;, type &#x3D; IdType.AUTO) private Integer id; &#x2F;** * 第三方用户ID *&#x2F; @SensitiveField(bind &#x3D; &quot;thirdAccountId&quot;) private String thirdAccountId; &#x2F;** * 用户名 *&#x2F; @SensitiveField(bind &#x3D; &quot;userName&quot;) private String userName; &#x2F;** * 密码 *&#x2F; @SensitiveField() private String password; &#x2F;** * 登录方式: 0-微信登录，1-账号密码登录 *&#x2F; @SensitiveField() private Integer loginType; &#x2F;** * 是否删除 *&#x2F; private Integer deleted; &#x2F;** * 创建时间 *&#x2F; private LocalDateTime createTime; &#x2F;** * 最后更新时间 *&#x2F; private LocalDateTime updateTime; &#125; @SensitiveField()的字段可以指定也可以不指定，因为上述代码中已经做了判断 Object value &#x3D; objMetaObject.getValue(StringUtils.isBlank(i.getBindField()) ? i.getName() : i.getBindField()); 调用下述接口 @GetMapping() public Object getAllUsers() &#123; return userService.getAllUsers(); &#125; SQL日志如下 &#x3D;&#x3D;&gt; Preparing: select * from user &#x3D;&#x3D;&gt; Parameters: &lt;&#x3D;&#x3D; Columns: id, third_account_id, user_name, password, login_type, deleted, create_time, update_time &lt;&#x3D;&#x3D; Row: 1, fuckman, admin, admin, 0, 0, 2023-12-28 15:53:47, 2024-01-04 12:19:49 &lt;&#x3D;&#x3D; Row: 2, a7cb7200-0f85-4dd5-845c-700f3740e91, admin1, admin1, 10, 0, 2023-12-28 15:53:47, 2024-01-04 12:53:47 &lt;&#x3D;&#x3D; Row: 3, third_account_id, user_name, password, 20, 0, 2024-01-02 15:41:22, 2024-01-04 12:53:50 &lt;&#x3D;&#x3D; Row: 4, third_account_id, user_name, password, 0, 0, 2024-01-02 15:41:24, 2024-01-02 15:41:24 &lt;&#x3D;&#x3D; Row: 5, hello, user_name, password, 0, 0, 2024-01-02 15:41:25, 2024-01-04 12:19:52 &lt;&#x3D;&#x3D; Row: 6, master, admin, admin, 0, 0, 2023-12-28 15:53:47, 2023-12-29 11:23:18 &lt;&#x3D;&#x3D; Row: 7, big, admin, admin, 0, 0, 2023-12-28 15:53:47, 2024-01-04 12:19:56 &lt;&#x3D;&#x3D; Total: 7 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@370642cb] 显示结果如下:","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://blog.ehzyil.xyz/tags/Mybatis/"}],"author":"ehzyil"},{"title":"基于AbstractRoutingDataSource与AOP实现多数据源切换","slug":"2023/基于AbstractRoutingDataSource与AOP实现多数据源切换","date":"2023-12-29T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/12/29/2023/基于AbstractRoutingDataSource与AOP实现多数据源切换/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/29/2023/%E5%9F%BA%E4%BA%8EAbstractRoutingDataSource%E4%B8%8EAOP%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E5%88%87%E6%8D%A2/","excerpt":"","text":"环境准备环境配置代码地址 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;&#x2F;groupId&gt; &lt;artifactId&gt;guava&lt;&#x2F;artifactId&gt; &lt;version&gt;30.1-jre&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-plus-generator&lt;&#x2F;artifactId&gt; &lt;version&gt;3.4.1&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;&#x2F;groupId&gt; &lt;artifactId&gt;commons-lang3&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.2&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.2.16&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;&#x2F;groupId&gt; &lt;artifactId&gt;lombok&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; application.yml # 默认的数据库名 database: name: story spring: dynamic: primary: master11 # 这个表示默认的数据源 datasource: master: # 数据库名，从配置 database.name 中获取 url: jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;story?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai username: root password: 666666 # 注意下面这个type,选择DruidDataSource时，请确保项目中添加了druid相关依赖 type: com.alibaba.druid.pool.DruidDataSource #DruidDataSource自有属性 filters: stat initialSize: 0 minIdle: 1 maxActive: 200 maxWait: 10000 time-between-eviction-runs-millis: 60000 min-evictable-idle-time-millis: 200000 testWhileIdle: true testOnBorrow: true validationQuery: select 1 shop: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;shop?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai username: root password: 666666 blog: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;sg_blog?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai username: root password: 666666 logging: level: default: debug mybatis: configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl map-underscore-to-camel-case: true 基础知识点主要借助AbstractRoutingDataSource来实现动态数据源，简单介绍下它： AbstractRoutingDataSource 是 Spring 提供的一个抽象类，它可以让我们动态切换数据源。在使用 AbstractRoutingDataSource 时，我们需要实现它的 determineCurrentLookupKey() 方法，该方法返回当前线程使用的数据源的名称或标识符。 AbstractRoutingDataSource 两个重要的成员变量： defaultTargetDataSource：默认数据源，即当无法确定使用哪个数据源时，将使用该默认数据源。在AbstractRoutingDataSource中，我们可以通过调用setDefaultTargetDataSource()方法来设置默认数据源。 targetDataSources：一个Map类型的成员变量，其中保存了多个数据源实例。在AbstractRoutingDataSource中，我们可以通过调用setTargetDataSources()方法来设置多个数据源实例，其中Map的key为数据源标识符或名称，value为对应的数据源实例。 使用AbstractRoutingDataSource实现数据源切换的原理如下： 在应用启动时，我们需要配置多个数据源，并将它们注册到AbstractRoutingDataSource中。 当应用发起数据库操作时，AbstractRoutingDataSource会根据determineCurrentLookupKey()方法返回的值，查找对应的数据源。 在determineCurrentLookupKey()方法中，我们可以根据不同的策略来决定使用哪个数据源。例如，可以根据当前请求的用户信息、请求的URL、当前时间等来选择不同的数据源。 其核心代码如： // 返回选中的数据源 protected DataSource determineTargetDataSource() &#123; Assert.notNull(this.resolvedDataSources, \"DataSource router not initialized\"); Object lookupKey = this.determineCurrentLookupKey(); DataSource dataSource = (DataSource)this.resolvedDataSources.get(lookupKey); if (dataSource == null &amp;&amp; (this.lenientFallback || lookupKey == null)) &#123; dataSource = this.resolvedDefaultDataSource; &#125; if (dataSource == null) &#123; throw new IllegalStateException(\"Cannot determine target DataSource for lookup key [\" + lookupKey + \"]\"); &#125; else &#123; return dataSource; &#125; &#125; @Nullable protected abstract Object determineCurrentLookupKey(); 设计方案基于上面的动态数据源的几个核心知识点，所以当我们需要实现动态数据源切换时，自然而然可以想到的一个方案就是在每次db执行之前，塞入这个希望使用的数据源key ![](..&#x2F;..&#x2F;..&#x2F;images&#x2F;2023&#x2F;assets&#x2F;d82673ae-8d87-48ba-8fbc-288ebdcb237b (1).png) 如上的设计思路，主要借助aop的思想来实现，通过上下文来存储当前方法的执行，具体希望使用的数据源，然后再执行的sql的时候，AbstractRoutingDataSource直接从上下文中获取key，以此来抉择具体的数据源 基于上面的方案，接下来我们补充一下细节 1.数据源选择方式通过AOP的方式进行拦截，写入数据源选择，这种方式适用于方法级别的粒度，基于此常见的实现方式就是通过自定义注解，来标注需要使用的数据源 如自定义注解，通过设置ds值来指定这个方法执行时选择的数据源 @Retention(RetentionPolicy.RUNTIME) @Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;) public @interface DsAno &#123; &#x2F;** * 启用的数据源，默认主库 * * @return *&#x2F; MasterSlaveDsEnum value() default MasterSlaveDsEnum.MASTER; &#x2F;** * 启用的数据源，如果存在，则优先使用它来替换默认的value * * @return *&#x2F; String ds() default &quot;&quot;; &#125; 2.上下文通过上下文来保存当前选中的数据源，因此可以借助 ThreadLocal 来实现一个简单的数据源选择上下文，使用继承的线程上下文，支持异步时选择传递。 private static final ThreadLocal&lt;String&gt; CONTEXT_HOLDER &#x3D; new InheritableThreadLocal&lt;&gt;(); 3.动态数据源选择public class MyRoutingDataSource extends AbstractRoutingDataSource &#123; @Nullable @Override protected Object determineCurrentLookupKey() &#123; return DsContextHolder.get(); &#125; &#125; 4.aop拦截借助AOP，拦截方法、类上拥有DS注解的方法执行，然后从这个注解中读取选中的数据源，然后写入上下文；再方法执行完毕之后，清空上下文 @Aspect public class DsAspect &#123; &#x2F;** * 切入点, 拦截类上、方法上有注解的方法，用于切换数据源 *&#x2F; @Pointcut(&quot;@annotation(com.example.multidatasource.dal.DsAno)||@within(com.example.multidatasource.dal.DsAno)&quot;) public void pointcut() &#123; &#125; @Around(&quot;pointcut()&quot;) public Object around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable &#123; DsAno ds &#x3D; getDsAno(proceedingJoinPoint); try &#123; if (ds !&#x3D; null &amp;&amp; (StringUtils.isNotBlank(ds.ds()) || ds.value() !&#x3D; null)) &#123; &#x2F;&#x2F; 当上下文中没有时，则写入线程上下文，应该用哪个DB DsContextHolder.set(StringUtils.isNoneBlank(ds.ds()) ? ds.ds() : ds.value().name()); &#125; return proceedingJoinPoint.proceed(); &#125; finally &#123; &#x2F;&#x2F; 清空上下文信息 if (ds !&#x3D; null) &#123; DsContextHolder.reset(); &#125; &#125; &#125; private DsAno getDsAno(ProceedingJoinPoint proceedingJoinPoint) &#123; MethodSignature signature &#x3D; (MethodSignature) proceedingJoinPoint.getSignature(); Method method &#x3D; signature.getMethod(); DsAno ds &#x3D; method.getAnnotation(DsAno.class); if (ds &#x3D;&#x3D; null) &#123; &#x2F;&#x2F; 获取类上的注解 ds &#x3D; (DsAno) proceedingJoinPoint.getSignature().getDeclaringType().getAnnotation(DsAno.class); &#125; return ds; &#125; &#125; 多数据源实现1.多数据源定义对于多数据源，因此无法直接使用默认的数据源配置，我们借助默认的配置规则，加一个多数据源版本的设计对应的多数据源的配置规则如下 spring: dynamic: primary: master # 这个表示默认的数据源 datasource: story: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/$&#123;database.name&#125;?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai username: root password: 666666 shop: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/shop?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai username: root password: 666666 注意上面的定义： spring.dynamic.primary: 指定默认启用的数据源 spring.dynamic.datasource.数据源名.配置: 这里的数据源名不能重复（忽略大小写），数据源下的配置与默认的配置参数要求一致 对应的数据源配置加载方式直接借助了Spring的ConfigurationProperties来实现 2.多数据源注册上面是数据源的配置定义与加载，但是我们需要基于上面的数据源配置来实例化对应的datasource。 @Slf4j @Configuration @ConditionalOnProperty(prefix &#x3D; &quot;spring.dynamic&quot;, name &#x3D; &quot;primary&quot;) @EnableConfigurationProperties(DsProperties.class) public class DataSourceConfig &#123; public DataSourceConfig() &#123; log.info(&quot;动态数据源初始化!&quot;); &#125; @Bean public DsAspect dsAspect() &#123; return new DsAspect(); &#125; &#x2F;** * 整合主从数据源 * * @param dsProperties * @return 1 *&#x2F; @Bean @Primary public DataSource dataSource(DsProperties dsProperties) &#123; Map&lt;Object, Object&gt; targetDataSources &#x3D; Maps.newHashMapWithExpectedSize(dsProperties.getDatasource().size()); dsProperties.getDatasource().forEach((k, v) -&gt; targetDataSources.put(k.toUpperCase(), v.initializeDataSourceBuilder().build())); if (CollectionUtils.isEmpty(targetDataSources)) &#123; throw new IllegalStateException(&quot;多数据源配置，请以 spring.dynamic 开头&quot;); &#125; MyRoutingDataSource myRoutingDataSource &#x3D; new MyRoutingDataSource(); Object key &#x3D; dsProperties.getPrimary().toUpperCase(); if (!targetDataSources.containsKey(key)) &#123; if (targetDataSources.containsKey(MasterSlaveDsEnum.MASTER.name())) &#123; &#x2F;&#x2F; 当们没有配置primary对应的数据源时，存在MASTER数据源，则将主库作为默认的数据源 key &#x3D; MasterSlaveDsEnum.MASTER.name(); &#125; else &#123; key &#x3D; targetDataSources.keySet().iterator().next(); &#125; &#125; log.info(&quot;动态数据源，默认启用为： &quot; + key); myRoutingDataSource.setDefaultTargetDataSource(targetDataSources.get(key)); myRoutingDataSource.setTargetDataSources(targetDataSources); return myRoutingDataSource; &#125; &#125; @ConditionalOnProperty: 这是Spring Boot框架提供的条件注解，它表示只有当指定的属性满足特定条件时，才会创建这个配置类所定义的bean。 @EnableConfigurationProperties(DsProperties.class): 这个注解告诉Spring Boot去启用特定的配置属性类（DsProperties），使其可以被注入到其他bean中。 @Primary: 这个注解用于标识在多个bean候选者中，优先选择被注解的bean作为自动装配的首选项。 datasource实例化方式 DataSourceProperties.initializeDataSourceBuilder().build() 通过上面的方式创建的DataSource为默认的HikariDataSource 3.上下文定义public class DsContextHolder &#123; &#x2F;** * 使用继承的线程上下文，支持异步时选择传递 * 使用DsNode，支持链式的数据源切换，如最外层使用master数据源，内部某个方法使用slave数据源；但是请注意，对于事务的场景，不要交叉 *&#x2F; private static final ThreadLocal&lt;String&gt; CONTEXT_HOLDER &#x3D; new InheritableThreadLocal&lt;&gt;(); private DsContextHolder() &#123; &#125; public static void set(String dbType) &#123; CONTEXT_HOLDER.set(dbType); &#125; public static String get() &#123; return CONTEXT_HOLDER.get(); &#125; public static void set(DS ds) &#123; set(ds.name().toUpperCase()); &#125; &#x2F;** * 移除上下文 *&#x2F; public static void reset() &#123; CONTEXT_HOLDER.remove(); &#125; &#125; 4.AOP实现借助AOP来简化数据源的选择，因此我们先定义一个注解DsAno，可以放在类上，表示这个类下所有的共有方法，都走某个数据源；也可以放在方法上，且方法上的优先级大于类上的注解 public interface DS &#123; &#x2F;** * 使用的数据源名 * * @return *&#x2F; String name(); &#125; @Retention(RetentionPolicy.RUNTIME) @Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;) public @interface DsAno &#123; &#x2F;** * 启用的数据源，默认主库 * * @return *&#x2F; MasterSlaveDsEnum value() default MasterSlaveDsEnum.MASTER; &#x2F;** * 启用的数据源，如果存在，则优先使用它来替换默认的value * * @return *&#x2F; String ds() default &quot;&quot;; &#125; @Aspect public class DsAspect &#123; &#x2F;** * 切入点, 拦截类上、方法上有注解的方法，用于切换数据源 *&#x2F; @Pointcut(&quot;@annotation(com.example.multidatasource.dal.DsAno)||@within(com.example.multidatasource.dal.DsAno)&quot;) public void pointcut() &#123; &#125; @Around(&quot;pointcut()&quot;) public Object around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable &#123; DsAno ds &#x3D; getDsAno(proceedingJoinPoint); try &#123; if (ds !&#x3D; null &amp;&amp; (StringUtils.isNotBlank(ds.ds()) || ds.value() !&#x3D; null)) &#123; &#x2F;&#x2F; 当上下文中没有时，则写入线程上下文，应该用哪个DB DsContextHolder.set(StringUtils.isNoneBlank(ds.ds()) ? ds.ds() : ds.value().name()); &#125; return proceedingJoinPoint.proceed(); &#125; finally &#123; &#x2F;&#x2F; 清空上下文信息 if (ds !&#x3D; null) &#123; DsContextHolder.reset(); &#125; &#125; &#125; private DsAno getDsAno(ProceedingJoinPoint proceedingJoinPoint) &#123; MethodSignature signature &#x3D; (MethodSignature) proceedingJoinPoint.getSignature(); Method method &#x3D; signature.getMethod(); DsAno ds &#x3D; method.getAnnotation(DsAno.class); if (ds &#x3D;&#x3D; null) &#123; &#x2F;&#x2F; 获取类上的注解 ds &#x3D; (DsAno) proceedingJoinPoint.getSignature().getDeclaringType().getAnnotation(DsAno.class); &#125; return ds; &#125; &#125; 上面的设计中，针对常见的主从切换做了一个简单的兼容，定义了一个主从数据源的枚举，同样也是基于简化使用体验的出发，枚举类设计如下： public enum MasterSlaveDsEnum implements DS &#123; MASTER, SHOP, STORY; &#125; 5.编程式数据源选择与小结import java.util.function.Supplier; &#x2F;** * 手动指定数据源的用法 *&#x2F; public class DsSelectExecutor &#123; &#x2F;** * 有返回结果 * * @param ds 数据源 * @param supplier 供应商 * @param &lt;T&gt; 泛型类型 * @return 返回结果 *&#x2F; public static &lt;T&gt; T submit(DS ds, Supplier&lt;T&gt; supplier) &#123; DsContextHolder.set(ds); try &#123; return supplier.get(); &#125; finally &#123; DsContextHolder.reset(); &#125; &#125; &#x2F;** * 无返回结果 * * @param ds 数据源 * @param call 可调用对象 *&#x2F; public static void execute(DS ds, Runnable call) &#123; DsContextHolder.set(ds); try &#123; call.run(); &#125; finally &#123; DsContextHolder.reset(); &#125; &#125; &#125; submit方法：接收一个数据库连接和一个Supplier接口实例作为参数，返回Supplier接口的返回值。该方法的目的是使用给定的数据库连接执行传入的Supplier接口实例，然后返回该接口的返回值。在方法中，首先通过DsContextHolder类将数据库连接设置到连接池中，然后使用supplier.get()方法获取Supplier接口的返回值。最后，在finally块中重置数据库连接池的状态，以确保数据库连接被正确释放。 execute方法：接收一个数据库连接和一个Runnable接口实例作为参数，无返回值。该方法的目的是使用给定的数据库连接执行传入的Runnable接口实例中的代码。在方法中，首先通过DsContextHolder类将数据库连接设置到连接池中，然后使用call.run()方法执行Runnable接口的代码。最后，在finally块中重置数据库连接池的状态，以确保数据库连接被正确释放。 小结小结一下整个的设计实现，主要基于AOP + 上下文的方式，配合AbstractRoutingDataSource来实现多数据源的自由选择 但是请注意，上面的实现，还有几个缺陷： 如果一个方法上有数据源选择注解，但是内部开了子线程做异步处理，那么上下文存储的数据源会失效 一个方法上指定了数据源，它又调用其他有也指定了数据源的方法，当前的方案会存在数据源的选择冲突 druid数据源怎么兼容？ 优化支持嵌套的数据源选择方案前面提出的一个非常大的问题就是再整个链路中，只支持选择一次的数据源，显然这种方式缺陷比较大，也很容易出现问题； 若我们希望实现在选择了一个数据源之后，在执行某个代码片段时，还可以继续再选择其他的数据源，那么应该怎么实现呢？ 首先我们来看一下之前的实现方案 ![d82673ae-8d87-48ba-8fbc-288ebdcb237b (1)](..&#x2F;..&#x2F;..&#x2F;images&#x2F;2023&#x2F;assets&#x2F;d82673ae-8d87-48ba-8fbc-288ebdcb237b (1).png) 在执行sql时，从上下文中获取当前选中的数据源,前面设计的上下文直接使用线程上线文保存 private static final ThreadLocal&lt;String&gt; CONTEXT_HOLDER &#x3D; new InheritableThreadLocal&lt;&gt;(); 从上面的这个实现来看，上下文赋值只能有一次，当我们希望可以实现标题的效果，自然我们应该想到的数据结构就是栈 – 后进先出 我们定义一个DsNode节点来存储选中的数据源，区别于之前的String，它除了记录选中的数据源之外，还记录前一次选中的数据源 public static class DsNode &#123; DsNode pre; String ds; public DsNode(DsNode parent, String ds) &#123; pre &#x3D; parent; this.ds &#x3D; ds; &#125; &#125; 移除上下文则不能像之前那么粗暴，而是要实现栈的弹出效果 如果无选中的数据源，直接返回 若之前选中的DsNode，存在上一个节点，则将上一个节点重新写入上下文 若之前选中的DsNode，不存在上一个节点，则清空上下文 public static String get() &#123; DsNode dsNode &#x3D; CONTEXT_HOLDER.get(); log.info(&quot;get dbType:&#123;&#125;&quot;, CONTEXT_HOLDER.get()); return dsNode&#x3D;&#x3D;null? null:dsNode.ds; &#125; public static void set(String dbType) &#123; DsNode current &#x3D; CONTEXT_HOLDER.get(); CONTEXT_HOLDER.set(new DsNode(current,dbType)); &#125; public static void set(DS ds) &#123; set(ds.name().toUpperCase()); &#125; &#x2F;** * 移除上下文 *&#x2F; public static void reset() &#123; DsNode ds &#x3D; CONTEXT_HOLDER.get(); if (ds &#x3D;&#x3D; null) &#123; return; &#125; if (ds.pre !&#x3D; null) &#123; &#x2F;&#x2F; 退出当前的数据源选择，切回去走上一次的数据源配置 CONTEXT_HOLDER.set(ds.pre); &#125; else &#123; CONTEXT_HOLDER.remove(); &#125; &#125; Druid数据源支持dsProperties.getDatasource().forEach((k, v) -&gt; targetDataSources.put(k.toUpperCase(), initDataSource(k, v))); 上面代码中使用v.initializeDataSourceBuilder().build()初始化的数据源默认是HikarDataSource,当我们使用Druid数据源时就无法适配，因此需要做以下修改。 1.检查是否引入Druid相关资源 public class DruidCheckUtil &#123; &#x2F;** * 判断是否包含durid相关的数据包 * * @return *&#x2F; public static boolean hasDuridPkg() &#123; return ClassUtils.isPresent(&quot;com.alibaba.druid.pool.DruidDataSource&quot;, DataSourceConfig.class.getClassLoader()); &#125; &#125; 2.创建InitDatasourse方法用来判断是否初始化Druid数据源 &#x2F;** * 初始化数据源 * @param prefix 数据源前缀 * @param properties 数据源属性 * @return 数据源 *&#x2F; public DataSource initDataSource(String prefix, DataSourceProperties properties) &#123; &#x2F;&#x2F; 检查是否包含Druid包 if (!DruidCheckUtil.hasDuridPkg()) &#123; log.info(&quot;实例化HikarDataSource: &#123;&#125;&quot;, prefix); &#x2F;&#x2F; 使用数据源属性初始化数据源构建器并构建数据源 return properties.initializeDataSourceBuilder().build(); &#125; &#x2F;&#x2F; 如果数据源类型为空或者数据源类型不是DruidDataSource类 if (properties.getType() &#x3D;&#x3D; null || !properties.getType().isAssignableFrom(DruidDataSource.class)) &#123; log.info(&quot;实例化HikarDataSource: &#123;&#125;&quot;, prefix); &#x2F;&#x2F; 使用数据源属性初始化数据源构建器并构建数据源 return properties.initializeDataSourceBuilder().build(); &#125; log.info(&quot;实例化DruidDataSource: &#123;&#125;&quot;, prefix); &#x2F;&#x2F; 使用Spring Binder 绑定或创建一个 &quot;spring.dynamic.datasource.&quot; + prefix 的DruidDataSource对象 return Binder.get(environment).bindOrCreate(&quot;spring.dynamic.datasource.&quot; + prefix, DruidDataSource.class); &#125; 上面方法会读取spring.dynamic.datasource.xxx的配置信息，我们只需添加type属性和Druid的配置信息即可初始化DruidDataSource。 配置文件如下： # 默认的数据库名 database: name: story spring: dynamic: primary: master # 这个表示默认的数据源 datasource: master: # 数据库名，从配置 database.name 中获取 url: jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;$&#123;database.name&#125;?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai username: root password: 666666 # 注意下面这个type,选择DruidDataSource时，请确保项目中添加了druid相关依赖 type: com.alibaba.druid.pool.DruidDataSource #DruidDataSource自有属性 filters: stat initialSize: 0 minIdle: 1 maxActive: 200 maxWait: 10000 time-between-eviction-runs-millis: 60000 min-evictable-idle-time-millis: 200000 testWhileIdle: true testOnBorrow: true validationQuery: select 1 story: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;$&#123;database.name&#125;?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai username: root password: 666666 shop: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;shop?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai username: root password: 666666 blog: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;sg_blog?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai username: root password: 666666","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"}],"author":"ehzyil"},{"title":"数据库表自动初始化","slug":"2023/数据库表自动初始化","date":"2023-12-28T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/12/28/2023/数据库表自动初始化/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/28/2023/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E8%87%AA%E5%8A%A8%E5%88%9D%E5%A7%8B%E5%8C%96/","excerpt":"","text":"数据库表自动初始化Liquibase数据库表版本管理依赖配置在Maven的pom文件中添加Liquibase依赖 &lt;dependency> &lt;groupId>org.liquibase&lt;/groupId> &lt;artifactId>liquibase-core&lt;/artifactId> &lt;/dependency> 配置数据源和Liquibase 在application.yml文件中添加配置，liquibase: change-log: classpath:liquibase/master.xml用于指定change-log的目录。 # 默认的数据库名 database: name: story spring: liquibase: change-log: classpath:liquibase/master.xml enabled: true # 当实际使用的数据库不支持liquibase，如 mariadb 时，将这个参数设置为false datasource: url: jdbc:mysql://127.0.0.1:3306/$&#123;database.name&#125;?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai username: root password: 666666 说明：●对于不支持liquibase的数据库，如mariadb，请将上面的 spring.liquibase.enabled 设置为 false●change-log: 对应的是核心的数据库版本变更配置 在rescources下创建以下的配置文件。 master.xml 文件中的内容如下 &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;utf-8&quot;?&gt; &lt;databaseChangeLog xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.liquibase.org&#x2F;xml&#x2F;ns&#x2F;dbchangelog&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.liquibase.org&#x2F;xml&#x2F;ns&#x2F;dbchangelog http:&#x2F;&#x2F;www.liquibase.org&#x2F;xml&#x2F;ns&#x2F;dbchangelog&#x2F;dbchangelog-3.5.xsd&quot;&gt; &lt;include file&#x3D;&quot;liquibase&#x2F;changelog&#x2F;000_initial_schema.xml&quot; relativeToChangelogFile&#x3D;&quot;false&quot;&#x2F;&gt; &lt;&#x2F;databaseChangeLog&gt; 说明：●对于不支持liquibase的数据库，如mariadb，请将上面的 spring.liquibase.enabled 设置为 false●change-log: 对应的是核心的数据库版本变更配置 注意上面这个 include， 这里就是告诉liquibase，所有的变更记录，都放在了 liquibase&#x2F;changelog&#x2F;000_initial_schema.xml 这个文件中 changelog文件 changelog文件，用于定义数据库的结构和初始数据。可以使用XML、YAML或者JSON格式来编写changelog文件，例如000_initial_schema.xml。 &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;utf-8&quot;?&gt; &lt;databaseChangeLog xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.liquibase.org&#x2F;xml&#x2F;ns&#x2F;dbchangelog&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.liquibase.org&#x2F;xml&#x2F;ns&#x2F;dbchangelog http:&#x2F;&#x2F;www.liquibase.org&#x2F;xml&#x2F;ns&#x2F;dbchangelog&#x2F;dbchangelog-3.5.xsd&quot;&gt; &lt;property name&#x3D;&quot;now&quot; value&#x3D;&quot;now()&quot; dbms&#x3D;&quot;mysql&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;autoIncrement&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;changeSet id&#x3D;&quot;00000000000001&quot; author&#x3D;&quot;ehzyil&quot;&gt; &lt;sqlFile dbms&#x3D;&quot;mysql&quot; endDelimiter&#x3D;&quot;;&quot; encoding&#x3D;&quot;UTF-8&quot; path&#x3D;&quot;liquibase&#x2F;data&#x2F;init-schema.sql&quot;&#x2F;&gt; &lt;&#x2F;changeSet&gt; &lt;changeSet id&#x3D;&quot;00000000000002&quot; author&#x3D;&quot;ehzyil&quot;&gt; &lt;sqlFile dbms&#x3D;&quot;mysql&quot; endDelimiter&#x3D;&quot;;&quot; encoding&#x3D;&quot;UTF-8&quot; path&#x3D;&quot;liquibase&#x2F;data&#x2F;init-schema-1.sql&quot;&#x2F;&gt; &lt;&#x2F;changeSet&gt; &lt;changeSet id&#x3D;&quot;00000000000003&quot; author&#x3D;&quot;ehzyil&quot;&gt; &lt;sqlFile dbms&#x3D;&quot;mysql&quot; endDelimiter&#x3D;&quot;;&quot; encoding&#x3D;&quot;UTF-8&quot; path&#x3D;&quot;liquibase&#x2F;data&#x2F;init-data.sql&quot;&#x2F;&gt; &lt;&#x2F;changeSet&gt; &lt;!-- 可以添加更多的changeSet来定义其他表结构和初始数据 --&gt; &lt;&#x2F;databaseChangeLog&gt; 说明：●changeSet 标签，id必须唯一，不能出现冲突●sqlFile 里面的path，对应的可以是标准的sql文件，也可以是xml格式的数据库表定义、数据库操作文件●一旦写上去，changeSet的顺序不要调整 init-schema.sql CREATE TABLE &#96;user&#96; ( &#96;id&#96; int unsigned NOT NULL AUTO_INCREMENT COMMENT &#39;主键ID&#39;, &#96;third_account_id&#96; varchar(128) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;第三方用户ID&#39;, &#96;user_name&#96; varchar(64) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;用户名&#39;, &#96;password&#96; varchar(128) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;密码&#39;, &#96;login_type&#96; tinyint NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;登录方式: 0-微信登录，1-账号密码登录&#39;, &#96;deleted&#96; tinyint NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;是否删除&#39;, &#96;create_time&#96; timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;创建时间&#39;, &#96;update_time&#96; timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#39;最后更新时间&#39;, PRIMARY KEY (&#96;id&#96;), KEY &#96;key_third_account_id&#96; (&#96;third_account_id&#96;), KEY &#96;user_name&#96; (&#96;user_name&#96;) ) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8mb4 COMMENT&#x3D;&#39;用户登录表&#39;; 项目演示项目启动之后，一切正常的话，直接连上数据库可以看到库表创建成功，数据也初始化完成，当然也可以直接观察控制台的输出 注意事项非常重要的一个点是，上面的每个ChangeSet只会执行一次，因此当执行完毕之后发现不对，要回滚怎么办？或者又需要修改怎么办？ liquibase 提供了回滚的机制，当ChangeSet执行完毕之后，对应的sql文件&#x2F;xml文件（即path定义的文件）不允许再修改，因为db中会记录这个文件的md5，当修改这个文件之后，这个md5也会随之发生改变 因此两个解决方案：新增一个changeSet 删除 DATABASECHANGELOG 表中 changeSet id对应的记录，然后重新走一遍 DataSourcelnitializer首次初始化方案 如何使用DataSourceInitializer来实现自主可控的数据初始化 项目搭建依赖首先搭建一个标准的SpringBoot项目工程，相关版本以及依赖如下 &lt;dependency&gt; &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt; &lt;version&gt;8.0.31&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; 配置注意实现初始化数据库表操作的核心配置就在下面，重点关注 配置文件： resources/application.yml # 默认的数据库名 database: name: story spring: datasource: url: jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;$&#123;database.name&#125;?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai username: root password: 666666 logging: level: root: info org: springframework: jdbc: core: debug resources/init_data/init-schema.sql 对应的初始化ddl CREATE TABLE &#96;user&#96; ( &#96;id&#96; int unsigned NOT NULL AUTO_INCREMENT COMMENT &#39;主键ID&#39;, &#96;third_account_id&#96; varchar(128) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;第三方用户ID&#39;, &#96;user_name&#96; varchar(64) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;用户名&#39;, &#96;password&#96; varchar(128) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;密码&#39;, &#96;login_type&#96; tinyint NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;登录方式: 0-微信登录，1-账号密码登录&#39;, &#96;deleted&#96; tinyint NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;是否删除&#39;, &#96;create_time&#96; timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;创建时间&#39;, &#96;update_time&#96; timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#39;最后更新时间&#39;, PRIMARY KEY (&#96;id&#96;), KEY &#96;key_third_account_id&#96; (&#96;third_account_id&#96;), KEY &#96;user_name&#96; (&#96;user_name&#96;) ) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8mb4 COMMENT&#x3D;&#39;用户登录表&#39;; resources/init_data/init-data.sql 对用的初始化dml INSERT INTO &#96;user&#96; (id, third_account_id, &#96;user_name&#96;, &#96;password&#96;, login_type, deleted) VALUES (1, &#39;a7cb7228-0f85-4dd5-845c-7c5df3746e92&#39;, &#39;admin&#39;, &#39;admin&#39;, 0, 0); 初始化初始化配置 @Slf4j @Configuration public class DataSourceInit &#123; //需要初始化的表结构 @Value(\"classpath:init_data/init-schema.sql\") private Resource initDataSource; //初始化数据 @Value(\"classpath:init_data/init-data.sql\") private Resource initData; //数据库名 @Value(\"$&#123;database.name&#125;\") private String dataBaseName; @Bean public DataSourceInitializer dataSourceInitializer(final DataSource dataSource) &#123; final DataSourceInitializer initializer = new DataSourceInitializer(); // 设置数据源 initializer.setDataSource(dataSource); // 设置数据库Populator initializer.setDatabasePopulator(databasePopulator()); // true表示需要执行，false表示不需要初始化 initializer.setEnabled(needInit(dataSource)); return initializer; &#125; /** * 设置数据源 * &lt;span>创建了一个ResourceDatabasePopulator对象，并添加了两个SQL脚本文件到其中，用分号作为分隔符。最后将这个populator对象返回。&lt;/span> * * @return */ private DatabasePopulator databasePopulator() &#123; final ResourceDatabasePopulator populator = new ResourceDatabasePopulator(); populator.addScripts(initDataSource); populator.addScripts(initData); populator.setSeparator(\";\"); return populator; &#125; &#125; 我们这里主要是借助 DataSourceInitializer 来实现初始化，其核心有两个配置 DatabasePopulator: 通过addScripts来指定对应的sql文件 DataSourceInitializer#setEnabled: 判断是否需要执行初始化 接下来重点需要看的就是needInit方法，我们再这个方法里面，需要判断数据库是否存在，若不存在时，则创建数据库；然后再判断表是否存在，以此来决定是否需要执行初始化方法 /** * 判断是否需要初始化 * * @param dataSource * @return */ private boolean needInit(DataSource dataSource) &#123; if (autoInitDatabase()) &#123; return true; &#125; // 根据是否存在表来判断是否需要执行sql操作 JdbcTemplate jdbcTemplate = new JdbcTemplate(dataSource); List&lt;Map&lt;String, Object>> list = jdbcTemplate.queryForList(\"SELECT table_name FROM information_schema.TABLES where table_name = 'user' and table_schema = '\" + dataBaseName + \"'\"); boolean init = CollectionUtils.isEmpty(list); if (init) &#123; log.info(\"库表不存在，执行建表及数据初始化\"); &#125; else &#123; log.info(\"表结构已存在，无需初始化\"); &#125; return init; &#125; /** * 数据库不存在时，尝试创建数据库 * * @return */ private boolean autoInitDatabase() &#123; // 查询失败，可能是数据库不存在，尝试创建数据库之后再次测试 //jdbc:mysql://127.0.0.1:3306/story?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai //mysql://127.0.0.1:3306/story?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai URI url = URI.create(SpringUtil.getConfig(\"spring.datasource.url\").substring(5)); String uname = SpringUtil.getConfig(\"spring.datasource.username\"); String pwd = SpringUtil.getConfig(\"spring.datasource.password\"); try (Connection connection = DriverManager.getConnection(\"jdbc:mysql://\" + url.getHost() + \":\" + url.getPort() + \"?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false\", uname, pwd); Statement statement = connection.createStatement()) &#123; ResultSet set = statement.executeQuery(\"select schema_name from information_schema.schemata where schema_name='\" + dataBaseName + \"'\"); if (!set.next()) &#123; String createDb = \"create database if not exists \" + dataBaseName; connection.setAutoCommit(false); statement.execute(createDb); connection.commit(); log.info(\"创建数据库（&#123;&#125;）成功\", createDb); if (set.isClosed()) &#123; set.close(); &#125; return true; &#125; set.close(); log.info(\"数据库已存在，无需初始化\"); return false; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; 上面的数据库判断是否存在以及初始化的过程相对基础，直接使用了基础的Connection进行操作；这里借助了SpringUtil来获取配置信息，对应的类源码如下 @Component public class SpringUtil implements ApplicationContextAware, EnvironmentAware &#123; private static ApplicationContext context; private static Environment environment; /** * 获取bean * * @param bean * @param &lt;T> * @return */ public static &lt;T> T getBean(Class&lt;T> bean) &#123; return context.getBean(bean); &#125; public static Object getBean(String beanName) &#123; return context.getBean(beanName); &#125; /** * 获取配置 * * @param key * @return */ public static String getConfig(String key) &#123; return environment.getProperty(key); &#125; /** * 发布事件消息 * * @param event */ public static void publishEvent(ApplicationEvent event) &#123; context.publishEvent(event); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; SpringUtil.context = applicationContext; &#125; @Override public void setEnvironment(Environment environment) &#123; SpringUtil.environment = environment; &#125; &#125; Springboot启动类配置如下： @Slf4j @SpringBootApplication public class LiquibaseApplication implements ApplicationRunner &#123; @Autowired private JdbcTemplate jdbcTemplate; public static void main(String[] args) &#123; SpringApplication.run(LiquibaseApplication.class); &#125; @Override public void run(ApplicationArguments args) throws Exception &#123; List list &#x3D; jdbcTemplate.queryForList(&quot;select * from user limit 2&quot;); log.info(&quot;启动成功，初始化数据: &#123;&#125;\\n&#123;&#125;&quot;, list.size(), list); &#125; &#125; 项目演示删除原有的story数据库，启动项目，成功执行结果如下 两种初始化方式结合的实践方案 第一次启动项目时选择未开启liquibase，第二次开启liquibase时会报错，因为 databasechangelog表未创建，因此请选准一种启动方式。 库初始化/** * 数据库不存在时，尝试创建数据库&lt;br> * &lt;span> * autoInitDatabase方法用于自动创建数据库。在方法内部，首先从配置文件中获取数据库连接URL、用户名和密码，并通过DriverManager创建数据库连接和Statement对象。 * 然后通过执行查询语句判断数据库是否存在，如果不存在则执行创建数据库的sql语句，并打印相应日志。 * &lt;/span> * * @return 是否成功创建数据库 */ private boolean autoInitDatabase() &#123; // 查询失败，可能是数据库不存在，尝试创建数据库之后再次测试 //jdbc:mysql://127.0.0.1:3306/story?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai //mysql://127.0.0.1:3306/story?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai URI url = URI.create(SpringUtil.getConfig(\"spring.datasource.url\").substring(5)); String uname = SpringUtil.getConfig(\"spring.datasource.username\"); String pwd = SpringUtil.getConfig(\"spring.datasource.password\"); try (Connection connection = DriverManager.getConnection(\"jdbc:mysql://\" + url.getHost() + \":\" + url.getPort() + \"?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false\", uname, pwd); Statement statement = connection.createStatement()) &#123; ResultSet set = statement.executeQuery(\"select schema_name from information_schema.schemata where schema_name='\" + database + \"'\"); if (!set.next()) &#123; String createDb = \"create database if not exists \" + database; connection.setAutoCommit(false); statement.execute(createDb); connection.commit(); log.info(\"创建数据库（&#123;&#125;）成功\", createDb); if (set.isClosed()) &#123; set.close(); &#125; return true; &#125; set.close(); log.info(\"数据库已存在，无需初始化\"); return false; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; 上面的实现比较清晰了，首先是判断库是否存在，autoInitDatabase方法用于自动创建数据库。在方法内部，首先从配置文件中获取数据库连接URL、用户名和密码，并通过DriverManager创建数据库连接和Statement对象。然后通过执行查询语句判断数据库是否存在，如果不存在则执行创建数据库的sql语句，并打印相应日志。 为什么不直接使用 spring.datasource.url 来创建连接？因为库不存在时，直接使用下面这个url进行连接会抛连接异常 表初始化表的初始化，其实可以理解为项目启动之后执行一些sql，这时主要借助的就是 initializer.setDatabasePopulator核心知识点我们使用DbChangeSetLoader 类来实现初始化sql的加载，但实际上，若你完全抛开Liquibase，单纯的希望项目启动后执行某些sql，可以非常简单的实现，直接用DataSourcelnitializer首次初始化方案中写的通过 @Value 来加载需要初始化的sql文件，直接通过 ResourceDatabasePoplulator 添加sql资源就可以了 /** * 检测一下数据库中表是否存在，若存在则不初始化； * * @param dataSource * @return true 表示需要初始化； false 表示无需初始化 */ private boolean needInit(DataSource dataSource) &#123; if (autoInitDatabase()) &#123; return true; //不存在且已创建 &#125; //已存在的执行逻辑 // 根据是否存在表来判断是否需要执行sql操作 JdbcTemplate jdbcTemplate = new JdbcTemplate(dataSource); if (!liquibaseEnable) &#123; // 非liquibase做数据库版本管理的，根据用户来判断是否有初始化 List list = jdbcTemplate.queryForList(\"SELECT table_name FROM information_schema.TABLES where table_name = 'user_info' and table_schema = '\" + database + \"';\"); return CollectionUtils.isEmpty(list); &#125; // 对于liquibase做数据版本管控的场景，若使用的不是默认的pai_coding，则需要进行修订 List&lt;Map&lt;String, Object>> record = jdbcTemplate.queryForList(\"select * from DATABASECHANGELOG where ID='00000000000020' limit 1;\"); if (CollectionUtils.isEmpty(record)) &#123; // 首次启动，需要初始化库表，直接返回 return true; &#125; // 非首次启动时，判断记录对应的md5是否准确 if (Objects.equals(record.get(0).get(\"MD5SUM\"), \"8:a1a2d9943b746acf58476ae612c292fc\")) &#123; jdbcTemplate.update(\"update DATABASECHANGELOG set MD5SUM='8:bb81b67a5219be64eff22e2929fed540' where ID='00000000000020'\"); &#125; return false; &#125; needInit方法用于判断是否需要初始化数据库。该方法的参数是一个DataSource对象，表示数据源。在方法内部，先判断是否需要自动创建数据库，如果需要则返回true。然后通过JdbcTemplate查询是否存在表，如果不存在则需要初始化数据库，并打印相应日志，否则不需要初始化并打印相应日志。 核心知识点Liquibase兼容方案dataSourceInitializer(final DataSource dataSource)用于初始化数据源，返回一个DataSourceInitializer的Bean。该方法的参数是一个DataSource对象，表示数据源。在方法内部，创建了一个DataSourceInitializer对象，设置数据源为传入的参数，并根据是否需要初始化数据库设置启用状态和数据库结构初始化的Populator。 databasePopulator方法用于创建DatabasePopulator对象，用于数据库结构初始化。该方法的参数是一个是否需要初始化的标志，根据该标志进行不同的操作。如果不需要初始化且不使用Liquibase管理数据库，则创建一个ResourceDatabasePopulator对象，并通过addScripts方法添加需要执行的sql脚本，通过setSeparator方法设置sql语句的分隔符。 @Value(\"$&#123;database.name&#125;\") private String database; // 从配置文件中读取数据库名称 @Value(\"$&#123;spring.liquibase.enabled:true&#125;\") private Boolean liquibaseEnable; // 从配置文件中读取是否使用Liquibase管理数据库的配置 @Value(\"$&#123;spring.liquibase.change-log&#125;\") private String liquibaseChangeLog; // 从配置文件中读取Liquibase的change-log配置 /** * 初始化数据源，返回DataSourceInitializer Bean&lt;br> * * * @param dataSource 数据源 * @return DataSourceInitializer */ @Bean public org.springframework.jdbc.datasource.init.DataSourceInitializer dataSourceInitializer(final DataSource dataSource) &#123; final org.springframework.jdbc.datasource.init.DataSourceInitializer initializer = new org.springframework.jdbc.datasource.init.DataSourceInitializer(); // 设置数据源 initializer.setDataSource(dataSource); boolean enable = needInit(dataSource); initializer.setEnabled(enable); initializer.setDatabasePopulator(databasePopulator(enable)); return initializer; &#125; /** * 创建DatabasePopulator对象，用于数据库结构初始化&lt;br> * @param initEnable 是否需要初始化 * @return DatabasePopulator */ private DatabasePopulator databasePopulator(boolean initEnable) &#123; final ResourceDatabasePopulator populator = new ResourceDatabasePopulator(); if (!liquibaseEnable &amp;&amp; initEnable) &#123; // fixme: 首次启动时, 对于不支持liquibase的数据库，如mariadb，采用主动初始化 // fixme 这种方式不支持后续动态的数据表结构更新、数据变更 populator.addScripts(XmlParserUtils.loadDbChangeSetResources(liquibaseChangeLog).toArray(new ClassPathResource[]&#123;&#125;)); populator.setSeparator(\";\"); log.info(\"非Liquibase管理数据库，请手动执行数据库表初始化!\"); &#125; return populator; &#125; XmlParserUtils.loadDbChangeSetResources(liquibaseChangeLog).toArray(new ClassPathResource[]&#123;&#125;)用于读取配置文件中的change-log: classpath:liquibase/master.xml来获取liquibase的ChangeLog从而获取到所有的sql脚本。 注意上面的实现：我们依然借助了Liquibase 的xml文件来解析来加载对应的数据库表变更历史sql。但是需要注意的是，采用DataSourceInitializer初始化方案，只会执行一次；当你从github上拉了代码本地执行之后，后续再拉新的代码，有新的变更时，这些新的变更都不会被执行，需要我们删除数据库或删除 DATABASECHANGELOG 表中 changeSet id对应的记录，然后重新走一遍。 执行结果 未创建数据库的情况 已创建数据库的情况 不启用liquibase 未建库 spring: liquibase: change-log: classpath:liquibase&#x2F;master.xml enabled: false # 当实际使用的数据库不支持liquibase，如 mariadb 时，将这个参数设置为false 附上主要类的代码 @Slf4j @Configuration public class ForumDataSourceInitializer &#123; @Value(&quot;$&#123;database.name&#125;&quot;) private String database; &#x2F;&#x2F; 从配置文件中读取数据库名称 @Value(&quot;$&#123;spring.liquibase.enabled:true&#125;&quot;) private Boolean liquibaseEnable; &#x2F;&#x2F; 从配置文件中读取是否使用Liquibase管理数据库的配置 @Value(&quot;$&#123;spring.liquibase.change-log&#125;&quot;) private String liquibaseChangeLog; &#x2F;&#x2F; 从配置文件中读取Liquibase的change-log配置 &#x2F;** * 初始化数据源，返回DataSourceInitializer Bean&lt;br&gt; * &lt;span&gt;用于初始化数据源，返回一个DataSourceInitializer的Bean。 * 该方法的参数是一个DataSource对象，表示数据源。在方法内部，创建了一个DataSourceInitializer对象， * 设置数据源为传入的参数，并根据是否需要初始化数据库设置启用状态和数据库结构初始化的Populator。 &lt;&#x2F;span&gt; * * @param dataSource 数据源 * @return DataSourceInitializer *&#x2F; @Bean public org.springframework.jdbc.datasource.init.DataSourceInitializer dataSourceInitializer(final DataSource dataSource) &#123; final org.springframework.jdbc.datasource.init.DataSourceInitializer initializer &#x3D; new org.springframework.jdbc.datasource.init.DataSourceInitializer(); &#x2F;&#x2F; 设置数据源 initializer.setDataSource(dataSource); boolean enable &#x3D; needInit(dataSource); initializer.setEnabled(enable); initializer.setDatabasePopulator(databasePopulator(enable)); return initializer; &#125; &#x2F;** * 创建DatabasePopulator对象，用于数据库结构初始化&lt;br&gt; * &lt;span&gt;databasePopulator方法用于创建DatabasePopulator对象，用于数据库结构初始化。该方法的参数是一个是否需要初始化的标志，根据该标志进行不同的操作。 * 如果不需要初始化且不使用Liquibase管理数据库，则创建一个ResourceDatabasePopulator对象，并通过addScripts方法添加需要执行的sql脚本， * 通过setSeparator方法设置sql语句的分隔符。&lt;&#x2F;span&gt; * * @param initEnable 是否需要初始化 * @return DatabasePopulator *&#x2F; private DatabasePopulator databasePopulator(boolean initEnable) &#123; final ResourceDatabasePopulator populator &#x3D; new ResourceDatabasePopulator(); if (!liquibaseEnable &amp;&amp; initEnable) &#123; &#x2F;&#x2F; fixme: 首次启动时, 对于不支持liquibase的数据库，如mariadb，采用主动初始化 &#x2F;&#x2F; fixme 这种方式不支持后续动态的数据表结构更新、数据变更 populator.addScripts(XmlParserUtils.loadDbChangeSetResources(liquibaseChangeLog).toArray(new ClassPathResource[]&#123;&#125;)); populator.setSeparator(&quot;;&quot;); log.info(&quot;非Liquibase管理数据库，请手动执行数据库表初始化!&quot;); &#125; return populator; &#125; &#x2F;** * 判断是否需要初始化数据库&lt;br&gt; * * &lt;span&gt; * needInit方法用于判断是否需要初始化数据库。该方法的参数是一个DataSource对象，表示数据源。在方法内部，先判断是否需要自动创建数据库，如果需要则返回true。 * 然后通过JdbcTemplate查询是否存在表，如果不存在则需要初始化数据库，并打印相应日志，否则不需要初始化并打印相应日志。 * &lt;&#x2F;span&gt; * * @param dataSource 数据源 * @return 是否需要初始化 *&#x2F; private boolean needInit(DataSource dataSource) &#123; &#x2F;&#x2F; 创建数据库 if (autoInitDatabase()) &#123; return true; &#125; &#x2F;&#x2F; 根据是否存在表来判断是否需要执行sql操作 JdbcTemplate jdbcTemplate &#x3D; new JdbcTemplate(dataSource); List&lt;Map&lt;String, Object&gt;&gt; list &#x3D; jdbcTemplate.queryForList(&quot;SELECT table_name FROM information_schema.TABLES where table_name &#x3D; &#39;user&#39; and table_schema &#x3D; &#39;&quot; + database + &quot;&#39;&quot;); boolean init &#x3D; CollectionUtils.isEmpty(list); if (init) &#123; log.info(&quot;库表不存在，即将执行建表及数据初始化&quot;); &#125; else &#123; log.info(&quot;表结构已存在，无需初始化&quot;); &#125; return init; &#125; &#x2F;** * 数据库不存在时，尝试创建数据库&lt;br&gt; * &lt;span&gt; * autoInitDatabase方法用于自动创建数据库。在方法内部，首先从配置文件中获取数据库连接URL、用户名和密码，并通过DriverManager创建数据库连接和Statement对象。 * 然后通过执行查询语句判断数据库是否存在，如果不存在则执行创建数据库的sql语句，并打印相应日志。 * &lt;&#x2F;span&gt; * * @return 是否成功创建数据库 *&#x2F; private boolean autoInitDatabase() &#123; &#x2F;&#x2F; 查询失败，可能是数据库不存在，尝试创建数据库之后再次测试 &#x2F;&#x2F;jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;story?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai &#x2F;&#x2F;mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;story?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;Asia&#x2F;Shanghai URI url &#x3D; URI.create(SpringUtil.getConfig(&quot;spring.datasource.url&quot;).substring(5)); String uname &#x3D; SpringUtil.getConfig(&quot;spring.datasource.username&quot;); String pwd &#x3D; SpringUtil.getConfig(&quot;spring.datasource.password&quot;); try (Connection connection &#x3D; DriverManager.getConnection(&quot;jdbc:mysql:&#x2F;&#x2F;&quot; + url.getHost() + &quot;:&quot; + url.getPort() + &quot;?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;useSSL&#x3D;false&quot;, uname, pwd); Statement statement &#x3D; connection.createStatement()) &#123; ResultSet set &#x3D; statement.executeQuery(&quot;select schema_name from information_schema.schemata where schema_name&#x3D;&#39;&quot; + database + &quot;&#39;&quot;); if (!set.next()) &#123; String createDb &#x3D; &quot;create database if not exists &quot; + database; connection.setAutoCommit(false); statement.execute(createDb); connection.commit(); log.info(&quot;创建数据库（&#123;&#125;）成功&quot;, createDb); if (set.isClosed()) &#123; set.close(); &#125; return true; &#125; set.close(); log.info(&quot;数据库已存在，无需初始化&quot;); return false; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; &#125; public class XmlParserUtils &#123; private static final Log logger &#x3D; LogFactory.getLog(XmlParserUtils.class); &#x2F;&#x2F; 私有化getInstance方法，用于获取XMLReader实例 private static XMLReader xmlReader; private XmlParserUtils() &#123; &#x2F;&#x2F; 防止实例化 &#125; public static List&lt;ClassPathResource&gt; loadDbChangeSetResources(String source) &#123; try &#123; XMLReader reader &#x3D; getXMLReader(); ChangeHandler logHandler &#x3D; new ChangeHandler(&quot;include&quot;, &quot;file&quot;); reader.setContentHandler(logHandler); Path path &#x3D; new ClassPathResource(source.replace(&quot;classpath:&quot;, &quot;&quot;)).getFile().toPath(); reader.parse(path.toString()); List&lt;String&gt; changeSetFiles &#x3D; logHandler.getSets(); List&lt;ClassPathResource&gt; result &#x3D; new ArrayList&lt;&gt;(); ChangeHandler setHandler &#x3D; new ChangeHandler(&quot;sqlFile&quot;, &quot;path&quot;); for (String set : changeSetFiles) &#123; reader.setContentHandler(setHandler); Path setPath &#x3D; new ClassPathResource(set).getFile().toPath(); reader.parse(setPath.toString()); result.addAll(setHandler.getSets().stream().map(ClassPathResource::new).collect(Collectors.toList())); setHandler.reset(); &#125; return result; &#125; catch (Exception e) &#123; throw new IllegalStateException(&quot;加载初始化脚本异常!&quot;, e); &#125; &#125; private static XMLReader getXMLReader() throws ParserConfigurationException, SAXException &#123; if (xmlReader &#x3D;&#x3D; null) &#123; SAXParserFactory factory &#x3D; SAXParserFactory.newInstance(); xmlReader &#x3D; factory.newSAXParser().getXMLReader(); &#125; return xmlReader; &#125; public static class ChangeHandler extends DefaultHandler &#123; private final String tag; private final String attr; private List&lt;String&gt; sets &#x3D; new ArrayList&lt;&gt;(); &#x2F;** * 构造函数 * * @param tag 标签 * @param attr 属性 *&#x2F; public ChangeHandler(String tag, String attr) &#123; this.tag &#x3D; tag; this.attr &#x3D; attr; &#125; @Override public void startElement(String uri, String localName, String qName, Attributes attributes) throws SAXException &#123; if (tag.equals(qName)) &#123; sets.add(attributes.getValue(attr)); &#125; &#125; &#x2F;** * 获取变更脚本列表 * * @return 变更脚本列表 *&#x2F; public List&lt;String&gt; getSets() &#123; return sets; &#125; &#x2F;** * 重置变更脚本列表 *&#x2F; public void reset() &#123; sets.clear(); &#125; &#125; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"}],"author":"ehzyil"},{"title":"Docker指令报错的解决方法'permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock:'","slug":"2023/Docker指令报错的解决方法：'permission denied while trying to connect to the Docker...'","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.975Z","comments":true,"path":"2023/12/26/2023/Docker指令报错的解决方法：'permission denied while trying to connect to the Docker...'/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/Docker%E6%8C%87%E4%BB%A4%E6%8A%A5%E9%94%99%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%EF%BC%9A'permission%20denied%20while%20trying%20to%20connect%20to%20the%20Docker...'/","excerpt":"","text":"使用azure的服务器查看docker容器内的进程的时候出现一下错误， 意思是试图连接unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock:，但权限不够。 原因分析：这是因为你当前的用户没有这个权限。默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。即我们当前的用户不是root用户。 解决办法：把我们当前的用户添加到docker组中就可以了 第一步： sudo gpasswd -a username docker #将普通用户username加入到docker组中，username这个字段也可以直接换成$USER。 第二步：更新docker组 newgrp docker #更新docker组 第三步：再执行你报错的命令，此时就不会报错了。 ehzyil@AzureVM:~$ docker ps permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json\": dial unix /var/run/docker.sock: connect: permission denied ehzyil@AzureVM:~$ sudo gpasswd -a ehzyil docker Adding user ehzyil to group docker ehzyil@AzureVM:~$ newgrp docker ehzyil@AzureVM:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.ehzyil.xyz/tags/Docker/"},{"name":"linux","slug":"linux","permalink":"https://blog.ehzyil.xyz/tags/linux/"},{"name":"异常","slug":"异常","permalink":"https://blog.ehzyil.xyz/tags/%E5%BC%82%E5%B8%B8/"}],"author":"ehzyil"},{"title":"JS常用日期函数","slug":"2023/JS常用日期函数","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.975Z","comments":true,"path":"2023/12/26/2023/JS常用日期函数/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/JS%E5%B8%B8%E7%94%A8%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/","excerpt":"","text":"JS常用日期函数 &#x2F;&#x2F;获取完整的日期 var date&#x3D;new Date; var year&#x3D;date.getFullYear(); var month&#x3D;date.getMonth()+1; month &#x3D;(month&lt;10 ? &quot;0&quot;+month:month); var mydate &#x3D; (year.toString()+month.toString()); 注意，year.toString()+month.toString()不能写成year+month。不然如果月份大于等于10，则月份为数字，会和年份相加，如201210，则会变为2022，需要加.toString() 以下是搜到的有用内容： var myDate &#x3D; new Date(); myDate.getYear(); &#x2F;&#x2F;获取当前年份(2位) myDate.getFullYear(); &#x2F;&#x2F;获取完整的年份(4位,1970-????) myDate.getMonth(); &#x2F;&#x2F;获取当前月份(0-11,0代表1月) myDate.getDate(); &#x2F;&#x2F;获取当前日(1-31) myDate.getDay(); &#x2F;&#x2F;获取当前星期X(0-6,0代表星期天) myDate.getTime(); &#x2F;&#x2F;获取当前时间(从1970.1.1开始的毫秒数) myDate.getHours(); &#x2F;&#x2F;获取当前小时数(0-23) myDate.getMinutes(); &#x2F;&#x2F;获取当前分钟数(0-59) myDate.getSeconds(); &#x2F;&#x2F;获取当前秒数(0-59) myDate.getMilliseconds(); &#x2F;&#x2F;获取当前毫秒数(0-999) myDate.toLocaleDateString(); &#x2F;&#x2F;获取当前日期 var mytime&#x3D;myDate.toLocaleTimeString(); &#x2F;&#x2F;获取当前时间 myDate.toLocaleString( ); &#x2F;&#x2F;获取日期与时间 &lt;script language&#x3D;&quot;JavaScript&quot;&gt;function monthnow()&#123; var now &#x3D; new Date(); var monthn &#x3D; now.getMonth(); var yearn &#x3D; now.getYear(); window.location.href&#x3D;&quot;winnNamelist.jsp?getMonth&#x3D;&quot;+monthn+&quot;&amp;getYear&#x3D;&quot;+yearn; &#125;&lt;&#x2F;script&gt;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java Script","slug":"Java-Script","permalink":"https://blog.ehzyil.xyz/tags/Java-Script/"}],"author":"ehzyil"},{"title":"Java String hashCode生成负值","slug":"2023/Java String hashCode生成负值","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.975Z","comments":true,"path":"2023/12/26/2023/Java String hashCode生成负值/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/Java%20String%20hashCode%E7%94%9F%E6%88%90%E8%B4%9F%E5%80%BC/","excerpt":"","text":"转载 java String hashCode遇到的坑 在进行数据交换时，如果主键不是整型，需要对字符串，或联合主键拼接为字符串，进行hash，再进行取模分片，使用的是String自带的hashCode()方法，本来是件很方便的事，但是有些字符串取hashCode竟然是负数，使得分片为负数，找不到对应的分片，我们先看一下String 生成hashCode的代码： /** * Returns a hash code for this string. The hash code for a * &#123;@code String&#125; object is computed as * &lt;blockquote>&lt;pre> * s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1] * &lt;/pre>&lt;/blockquote> * using &#123;@code int&#125; arithmetic, where &#123;@code s[i]&#125; is the * &lt;i>i&lt;/i>th character of the string, &#123;@code n&#125; is the length of * the string, and &#123;@code ^&#125; indicates exponentiation. * (The hash value of the empty string is zero.) * * @return a hash code value for this object. */ public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length > 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h; &#125; 主要是根据字符串中字符的ascii码值来计算的，即 31 * hash + 字符的ASCII码值，int型的值取值范围为Integer.MIN_VALUE(-2147483648)～Integer.MAX_VALUE(2147483647)，所以如果字符串比较长，计算的数值就可能超出Integer.MAX_VALUE，造成数值溢出，值变成负数 几种比较极端的字符串hashCode值： String hashStr0 = \"35953305172933/\"; System.out.println(hashStr0.hashCode()); // 2147483647 Integer.MAX_VALUE System.out.println(Math.abs(hashStr0.hashCode())); // 2147483647 Integer.MAX_VALUE System.out.println(\"-------------------\"); String hashStr = \"359533051729330\"; System.out.println(hashStr.hashCode()); // -2147483648 Integer.MIN_VALUE System.out.println(Math.abs(hashStr.hashCode())); // -2147483648 Integer.MIN_VALUE System.out.println(\"-------------------\"); String hashStr2 = \"56800004874\"; System.out.println(hashStr2.hashCode()); // -2082984168 System.out.println(Math.abs(hashStr2.hashCode())); // 2082984168 System.out.println(\"-------------------\"); String hashStr3 = \"\"; System.out.println(hashStr3.hashCode()); // 0 System.out.println(Math.abs(hashStr3.hashCode())); // 0 System.out.println(\"-------------------\"); 当 key.hashCode() 值为 Interger.MIN_VALUE时候Maths.abs(key.hashCode()) 取 hash值为负。 对于字符串“359533051729330”的hashCode为Integer.MIN_VALUE，我们使用取绝对值还是超出Integer.MAX_VALUE，还是Integer.MIN_VALUE，所以针对这种极端情况是不可用的 要想利用hashCode为非负数，可以Integer.MAX_VALUE和与操作，这样最大正整数的符号位为0，与任何数与操作都是0，即是正数 int hash &#x3D; str.hashCode() &amp; Integer.MAX_VALUE;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"}],"author":"ehzyil"},{"title":"Java 实现 AES 加密和解密完整示例","slug":"2023/Java 实现 AES 加密和解密完整示例","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/12/26/2023/Java 实现 AES 加密和解密完整示例/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/Java%20%E5%AE%9E%E7%8E%B0%20AES%20%E5%8A%A0%E5%AF%86%E5%92%8C%E8%A7%A3%E5%AF%86%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"Java 实现 AES 加密和解密完整示例 遇到需求需要给已AES加密的用户姓名解密，并给出了python代码，网上找了个很全的Java 实现 AES 加密和解密完整示例，记录以下方便以后查阅。 转载Java 实现 AES 加密和解密完整示例 其他详细资料 1、简介AES，全称为 Advanced Encryption Standard，是一种分组密码算法，用于保护敏感数据的传输和存储。AES 分为 128 位和 256 位两种密钥长度，可以对数据进行加密和解密，保证数据的安全性和完整性。AES 主要应用于电子商务、移动支付、网络安全等领域，被广泛运用于现代社会的各个方面。AES 算法被设计为高度安全，可以在理论上保证其分组密码的安全性。然而，由于其复杂性和密钥长度，AES 算法的实现和应用也具有一定的技术难度。因此，在应用 AES算法时，需要注意加强密钥管理和安全性保障。 这个标准用来替代原先的 DES（Data Encryption Standard），已经被多方分析且广为全世界所使用。 AES 算法具有很多优点，例如快速、安全、可靠等。它可以加密大量数据，而不会因为加密过程中的数据量过大而变得缓慢。此外，AES 算法还支持块大小的自动调整，可以处理不同大小的数据块。 2、AES 加密模式2.1、加密方式ECB（Electronic Codebook）模式：这种模式是将整个明文分成若干段相同的小段，然后对每一小段进行加密。加密时，使用一个密钥，将明文中的每个字符与密钥中对应位置的字符进行异或运算，得到密文。 CBC（Cipher Block Chaining）模式：这种模式是先将明文切分成若干小段，然后每一小段与初始块或者上一段的密文段进行异或运算后，再与密钥进行加密。加密时，使用一个密钥和一个初始化向量（IV），初始化向量是一个16字节的向量，包含了加密算法所需的所有信息。 CFB（Cipher Feedback）模式：这种模式是一种较为复杂的加密模式，它结合了CBC和CTR两种模式的优点。在CFB模式中，加密过程中使用一个密钥和一个随机生成的初始化向量（IV），然后对明文进行加密。在加密完成后，通过对明文进行非对称加密来生成密文的向量。随后，通过对密文进行反向操作，将密文的向量与明文的向量进行异或运算，得到解密所需的密钥。 需要注意的是，ECB、CBC、CFB等模式都是对称加密算法，加密和解密使用相同的密钥。在使用这些算法时，需要注意保护密钥的安全，避免被恶意获取。 2.2、安全性ECB 不够安全，只适合于短数据的加密，而 CBC 和 CFB 相较于 ECB 更加安全，因为前一个密文块会影响当前明文块，使攻击者难以预测密文的结构。 2.3、速度ECB 是最简单的加密方式，速度最快，但由于安全性差不建议使用，CBC 因为每个明文块都要与前一个密文块进行异或操作，比 ECB 要慢一些，CFB 因为需要反复加密和解密，速度可能会更慢。 总的来说，选择 AES 的算法模式需要根据加密需要的安全性和速度来进行选择，通常推荐使用CBC 或 CFB 模式，而不是 ECB 模式。 3、Java 实现完整示例在 Java 中，可以使用 javax.crypto 包中的 Cipher 类来实现 AES 加密和解密。完整代码如下： import javax.crypto.Cipher; import javax.crypto.spec.IvParameterSpec; import javax.crypto.spec.SecretKeySpec; import java.nio.charset.StandardCharsets; import java.util.Base64; import java.util.Random; /** * &lt;h1>AES 加密和解密示例代码&lt;/h1> * Created by woniu * */ public class AESExample &#123; /** 加密模式之 ECB，算法/模式/补码方式 */ private static final String AES_ECB = \"AES/ECB/PKCS5Padding\"; /** 加密模式之 CBC，算法/模式/补码方式 */ private static final String AES_CBC = \"AES/CBC/PKCS5Padding\"; /** 加密模式之 CFB，算法/模式/补码方式 */ private static final String AES_CFB = \"AES/CFB/PKCS5Padding\"; /** AES 中的 IV 必须是 16 字节（128位）长 */ private static final Integer IV_LENGTH = 16; /*** * &lt;h2>空校验&lt;/h2> * @param str 需要判断的值 */ public static boolean isEmpty(Object str) &#123; return null == str || \"\".equals(str); &#125; /*** * &lt;h2>String 转 byte&lt;/h2> * @param str 需要转换的字符串 */ public static byte[] getBytes(String str)&#123; if (isEmpty(str)) &#123; return null; &#125; try &#123; return str.getBytes(StandardCharsets.UTF_8); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /*** * &lt;h2>初始化向量（IV），它是一个随机生成的字节数组，用于增加加密和解密的安全性&lt;/h2> */ public static String getIV()&#123; // String str = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"; // Random random = new Random(); // StringBuffer sb = new StringBuffer(); // for(int i = 0 ; i &lt; IV_LENGTH ; i++)&#123; // int number = random.nextInt(str.length()); // sb.append(str.charAt(number)); // &#125; // return sb.toString(); return \"1234567876543210\"; &#125; /*** * &lt;h2>获取一个 AES 密钥规范&lt;/h2> */ public static SecretKeySpec getSecretKeySpec(String key)&#123; SecretKeySpec secretKeySpec = new SecretKeySpec(getBytes(key), \"AES\"); return secretKeySpec; &#125; /** * &lt;h2>加密 - 模式 ECB&lt;/h2> * @param text 需要加密的文本内容 * @param key 加密的密钥 key * */ public static String encrypt(String text, String key)&#123; if (isEmpty(text) || isEmpty(key)) &#123; return null; &#125; try &#123; // 创建AES加密器 Cipher cipher = Cipher.getInstance(AES_ECB); SecretKeySpec secretKeySpec = getSecretKeySpec(key); cipher.init(Cipher.ENCRYPT_MODE, secretKeySpec); // 加密字节数组 byte[] encryptedBytes = cipher.doFinal(getBytes(text)); // 将密文转换为 Base64 编码字符串 return Base64.getEncoder().encodeToString(encryptedBytes); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * &lt;h2>解密 - 模式 ECB&lt;/h2> * @param text 需要解密的文本内容 * @param key 解密的密钥 key * */ public static String decrypt(String text, String key)&#123; if (isEmpty(text) || isEmpty(key)) &#123; return null; &#125; // 将密文转换为16字节的字节数组 byte[] textBytes = Base64.getDecoder().decode(text); try &#123; // 创建AES加密器 Cipher cipher = Cipher.getInstance(AES_ECB); SecretKeySpec secretKeySpec = getSecretKeySpec(key); cipher.init(Cipher.DECRYPT_MODE, secretKeySpec); // 解密字节数组 byte[] decryptedBytes = cipher.doFinal(textBytes); // 将明文转换为字符串 return new String(decryptedBytes, StandardCharsets.UTF_8); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * &lt;h2>加密 - 自定义加密模式&lt;/h2> * @param text 需要加密的文本内容 * @param key 加密的密钥 key * @param iv 初始化向量 * @param mode 加密模式 * */ public static String encrypt(String text, String key, String iv, String mode)&#123; if (isEmpty(text) || isEmpty(key) || isEmpty(iv)) &#123; return null; &#125; try &#123; // 创建AES加密器 Cipher cipher = Cipher.getInstance(mode); SecretKeySpec secretKeySpec = getSecretKeySpec(key); cipher.init(Cipher.ENCRYPT_MODE, secretKeySpec, new IvParameterSpec(getBytes(iv))); // 加密字节数组 byte[] encryptedBytes = cipher.doFinal(getBytes(text)); // 将密文转换为 Base64 编码字符串 return Base64.getEncoder().encodeToString(encryptedBytes); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * &lt;h2>解密 - 自定义加密模式&lt;/h2> * @param text 需要解密的文本内容 * @param key 解密的密钥 key * @param iv 初始化向量 * @param mode 加密模式 * */ public static String decrypt(String text, String key, String iv, String mode)&#123; if (isEmpty(text) || isEmpty(key) || isEmpty(iv)) &#123; return null; &#125; // 将密文转换为16字节的字节数组 byte[] textBytes = Base64.getDecoder().decode(text); try &#123; // 创建AES加密器 Cipher cipher = Cipher.getInstance(mode); SecretKeySpec secretKeySpec = getSecretKeySpec(key); cipher.init(Cipher.DECRYPT_MODE, secretKeySpec, new IvParameterSpec(getBytes(iv))); // 解密字节数组 byte[] decryptedBytes = cipher.doFinal(textBytes); // 将明文转换为字符串 return new String(decryptedBytes, StandardCharsets.UTF_8); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public static void main(String[] args) &#123; String text = \"411025200102175014\"; String key = \"rHx3pZwRTAb33psX\"; // 16字节的密钥 String iv = getIV(); String encryptTextEBC = encrypt(text, key); System.out.println(\"EBC 加密后内容：\" + encryptTextEBC); System.out.println(\"EBC 解密后内容：\" + decrypt(encryptTextEBC, key)); System.out.println(); String encryptTextCBC = encrypt(text, key, iv, AES_CBC); System.out.println(\"CBC 加密IV：\" + iv); System.out.println(\"CBC 加密后内容：\" + encryptTextCBC); System.out.println(\"CBC 解密后内容：\" + decrypt(encryptTextCBC, key, iv, AES_CBC)); System.out.println(); String encryptTextCFB = encrypt(text, key, iv, AES_CFB); System.out.println(\"CFB 加密IV：\" + iv); System.out.println(\"CFB 加密后内容：\" + encryptTextCFB); System.out.println(\"CFB 解密后内容：\" + decrypt(encryptTextCFB, key, iv, AES_CFB)); &#125; &#125; public static String decryptAES(String text) &#123; if (StringUtils.isEmpty(text)) return \"\"; text = text.substring(4); System.out.println(text); String mode = \"AES/CBC/PKCS5Padding\"; String iv = \"1234567876543210\"; String key = \"rHx3pZwRTAb33psX\"; // 16字节的密钥 // 将密文转换为16字节的字节数组 byte[] textBytes = Base64.getDecoder().decode(text); try &#123; // 创建AES加密器 Cipher cipher = Cipher.getInstance(mode); SecretKeySpec secretKeySpec = getSecretKeySpec(key); cipher.init(Cipher.DECRYPT_MODE, secretKeySpec, new IvParameterSpec(getBytes(iv))); // 解密字节数组 byte[] decryptedBytes = cipher.doFinal(textBytes); // 将明文转换为字符串 return new String(decryptedBytes, StandardCharsets.UTF_8); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; 需求中给出的python代码 AesEncryption.py import base64 # 导入base64模块 from Crypto.Cipher import AES # 从Crypto模块中导入AES加密算法 import StringUtil # 导入一个自定义的StringUtil模块 class AesEncryption: __AES_IV = '1234567876543210' # 初始化一个AES IV向量 def __init__(self, password: str = 'rHx3pZwRTAb33psX', prefix: str = 'AES:', encoding_protocol: str = 'ASCII'): self.__password = password # 初始化密码 self.__encoding_protocol = encoding_protocol # 初始化编码协议 self.__prefix = prefix # 初始化前缀 # AES加密方法 def aes_encrypt(self, text: str): block_size = 16 # 定义块大小为16 # 数据进行 PKCS5Padding 的填充 pad = lambda s: (s + (block_size - len(s) % block_size) * chr(block_size - len(s) % block_size)) # 定义填充函数 data = pad(text) # 对文本进行填充 # 创建加密对象 cipher = AES.new(self.__password.encode(), AES.MODE_CBC, self.__AES_IV.encode()) # 创建AES加密对象 # 得到加密后的字节码 encrypted_text = cipher.encrypt(bytes(data, self.__encoding_protocol)) # 进行加密 # base64 编码 encode_text = base64.b64encode(encrypted_text).decode(self.__encoding_protocol) # 进行base64编码 if StringUtil.is_str_not_null(self.__prefix): # 判断前缀是否不为空 encode_text = self.__prefix + encode_text # 如果前缀不为空，则添加前缀 return encode_text # 返回加密后的文本 # AES解密方法 def aes_decrypt(self, text: str): # 去掉 PKCS5Padding 的填充 un_pad = lambda s: s[:-ord(s[len(s) - 1:])] # 定义去除填充的函数 # 创建加密对象 cipher = AES.new(self.__password.encode(), AES.MODE_CBC, self.__AES_IV.encode()) # 创建AES解密对象 # base64 解码 text = StringUtil.split_by_first_str(text, self.__prefix) # 通过前缀分割文本 decode_text = base64.b64decode(str(text).encode()) # 进行base64解码 decrypt_text = un_pad(cipher.decrypt(decode_text)).decode(self.__encoding_protocol) # 进行解密 return decrypt_text # 返回解密后的文本 # 类方法：AES加密 @classmethod def aes_encrypt_cls(cls, text: str, password: str = 'rHx3pZwRTAb33psX', prefix: str = 'AES:', encoding_protocol: str = 'ASCII'): aes_en = AesEncryption(password=password, prefix=prefix, encoding_protocol=encoding_protocol) # 创建AesEncryption实例 return aes_en.aes_encrypt(text) # 调用实例方法进行加密 # 类方法：AES解密 @classmethod def aes_decrypt_cls(cls, text: str, password: str = 'rHx3pZwRTAb33psX', prefix: str = 'AES:', encoding_protocol: str = 'ASCII'): aes_de = AesEncryption(password=password, prefix=prefix, encoding_protocol=encoding_protocol) # 创建AesEncryption实例 return aes_de.aes_decrypt(text) # 调用实例方法进行解密 # 要加密的文本 text_to_encrypt = \"411025200102175014\" # 使用类方法进行加密 encrypted_text = AesEncryption.aes_encrypt_cls(text_to_encrypt) # 输出加密后的文本 print(\"Encrypted Text:\", encrypted_text) # 使用类方法进行解密 decrypted_text = AesEncryption.aes_decrypt_cls(encrypted_text) # 输出解密后的文本 print(\"Decrypted Text:\", decrypted_text) #StringUtil.py def is_str_not_null(text): if text is None: return False else: if type(text) == str: if text.strip() == '': return False else: return True else: raise Exception('text需为字符串') def split_by_first_str(text, prefix): if is_str_not_null(text) and is_str_not_null(prefix): sub_arr = text.split(prefix, 1) if len(sub_arr) > 1: return sub_arr[1] else: return text else: return text","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"},{"name":"AES","slug":"AES","permalink":"https://blog.ehzyil.xyz/tags/AES/"}],"author":"ehzyil"},{"title":"Java将list转字符串的方法","slug":"2023/Java将list转字符串的方法","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/12/26/2023/Java将list转字符串的方法/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/Java%E5%B0%86list%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%96%B9%E6%B3%95/","excerpt":"","text":"第一种：join()String.join(&quot;,&quot;, strList) 第二种：循环插入逗号public static &lt;T&gt; String parseListToStr(List&lt;T&gt; list)&#123; StringBuffer sb &#x3D; new StringBuffer(); if(listIsNotNull(list)) &#123; for(int i&#x3D;0;i&lt;&#x3D;list.size()-1;i++)&#123; if(i&lt;list.size()-1)&#123; sb.append(list.get(i) + &quot;,&quot;); &#125;else &#123; sb.append(list.get(i)); &#125; &#125; &#125; return sb.toString(); &#125; 第三种：stream流public static &lt;T&gt; String parseListToStr3(List&lt;T&gt; list)&#123; String result &#x3D; list.stream().map(String::valueOf).collect(Collectors.joining(&quot;,&quot;)); 第四种：使用谷歌Joiner方法import com.google.common.base.Joiner; public static &lt;T&gt; String parseListToStr(List&lt;T&gt; list)&#123; String result &#x3D; Joiner.on(&quot;,&quot;).join(list); return result; &#125; 导入com.google.common.base.Joiner; public static String strategy（String strategy）&#123; String result &#x3D; Joiner.on（“，”）.join（list）; 返回结果; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"}],"author":"ehzyil"},{"title":"Java循环删除List中元素的正确方式","slug":"2023/Java循环删除List中元素","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/12/26/2023/Java循环删除List中元素/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/Java%E5%BE%AA%E7%8E%AF%E5%88%A0%E9%99%A4List%E4%B8%AD%E5%85%83%E7%B4%A0/","excerpt":"","text":"Java循环删除List中元素的正确方式There are a few different ways to correctly delete elements from a List in Java. Here are some common methods: Using an Iterator: You can use an Iterator to loop through the List and remove elements based on a condition. This is the safest way to modify a List while iterating over it. List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); list.add(&quot;A&quot;); list.add(&quot;B&quot;); list.add(&quot;C&quot;); Iterator&lt;String&gt; iterator &#x3D; list.iterator(); while (iterator.hasNext()) &#123; String element &#x3D; iterator.next();&#x2F;&#x2F;删除之前必须显示调用 if (element.equals(&quot;B&quot;)) &#123; iterator.remove(); &#125; &#125; System.out.println(list); &#x2F;&#x2F; Output: [A, C] Using the removeIf() method: Java 8 introduced the removeIf() method in the List interface, which allows you to remove elements based on a specified condition using a Predicate. List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); list.add(&quot;A&quot;); list.add(&quot;B&quot;); list.add(&quot;C&quot;); list.removeIf(element -&gt; element.equals(&quot;B&quot;)); System.out.println(list); &#x2F;&#x2F; Output: [A, C] Using the removeAll() method: You can also use the removeAll() method to remove all occurrences of a specific element from the List. List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); list.add(&quot;A&quot;); list.add(&quot;B&quot;); list.add(&quot;C&quot;); list.removeAll(Collections.singleton(&quot;B&quot;)); System.out.println(list); &#x2F;&#x2F; Output: [A, C] Using a for loop in reverse order:You can iterate through the List in reverse order and remove elements that meet a certain condition. This approach is useful when you want to avoid using an Iterator. List&lt;String> list = new ArrayList&lt;>(); list.add(\"A\"); list.add(\"B\"); list.add(\"C\"); for (int i = list.size() - 1; i >= 0; i--) &#123; if (list.get(i).equals(\"B\")) &#123; list.remove(i); &#125; &#125; System.out.println(list); // Output: [A, C] Using the subList() method:You can utilize the subList() method to create a sub-list and then remove elements from it. This approach is helpful when you want to remove a range of elements from the List. List&lt;String> list = new ArrayList&lt;>(); list.add(\"A\"); list.add(\"B\"); list.add(\"C\"); list.subList(1, 2).clear(); System.out.println(list); // Output: [A, C] Using the Apache Commons Collections library:If you are open to using external libraries, the Apache Commons Collections library provides a CollectionUtils class with various methods for manipulating collections, including removing elements based on conditions. List&lt;String> list = new ArrayList&lt;>(); list.add(\"A\"); list.add(\"B\"); list.add(\"C\"); CollectionUtils.filter(list, object -> !object.equals(\"B\")); System.out.println(list); // Output: [A, C] 题外话其他方法 通过for循环删除（错误） 使用foreach循环删除（错误） 原因见: Java循环删除List中元素的正确方式","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"}],"author":"ehzyil"},{"title":"Java数组转List的三种方式","slug":"2023/Java数组转List的三种方式","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/12/26/2023/Java数组转List的三种方式/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/Java%E6%95%B0%E7%BB%84%E8%BD%ACList%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F/","excerpt":"","text":"Java数组转List的三种方式 需求中常用到数组转集合的操作,为了避免不出错并使用高效方法去网上找了篇非常不错的文章。 转载 Java数组转List的三种方式及对比 一.最常见方式（未必最佳）通过 Arrays.asList(strArray) 方式,将数组转换List后，不能对List增删，只能查改，否则抛异常。 关键代码： List list &#x3D; Arrays.asList(strArray); private void testArrayCastToListError() &#123; String[] strArray &#x3D; new String[2]; List list &#x3D; Arrays.asList(strArray); &#x2F;&#x2F;对转换后的list插入一条数据 list.add(&quot;1&quot;); System.out.println(list); &#125; 执行结果： Exception in thread &quot;main&quot; java.lang.UnsupportedOperationException at java.util.AbstractList.add(AbstractList.java:148) at java.util.AbstractList.add(AbstractList.java:108) at com.darwin.junit.Calculator.testArrayCastToList(Calculator.java:19) at com.darwin.junit.Calculator.main(Calculator.java:44) 程序在list.add(“1”)处，抛出异常：UnsupportedOperationException。 原因解析：Arrays.asList(strArray)返回值是java.util.Arrays类中一个私有静态内部类java.util.Arrays.ArrayList，它并非java.util.ArrayList类。java.util.Arrays.ArrayList类具有 set()，get()，contains()等方法，但是不具有添加add()或删除remove()方法,所以调用add()方法会报错。 使用场景：Arrays.asList(strArray)方式仅能用在将数组转换为List后，不需要增删其中的值，仅作为数据源读取使用。 二.数组转为List后，支持增删改查的方式通过ArrayList的构造器，将Arrays.asList(strArray)的返回值由java.util.Arrays.ArrayList转为java.util.ArrayList。 关键代码：ArrayList list &#x3D; new ArrayList(Arrays.asList(strArray)) ; private void testArrayCastToListRight() &#123; String[] strArray &#x3D; new String[2]; ArrayList&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;(Arrays.asList(strArray)) ; list.add(&quot;1&quot;); System.out.println(list); &#125; 执行结果：成功追加一个元素“1”。 使用场景：需要在将数组转换为List后，对List进行增删改查操作，在List的数据量不大的情况下，可以使用。 三.通过集合工具类Collections.addAll()方法(最高效)通过Collections.addAll(arrayList, strArray)方式转换，根据数组的长度创建一个长度相同的List，然后通过Collections.addAll()方法，将数组中的元素转为二进制，然后添加到List中，这是最高效的方法。 关键代码： ArrayList&lt; String&gt; arrayList &#x3D; new ArrayList&lt;String&gt;(strArray.length); Collections.addAll(arrayList, strArray); 测试： private void testArrayCastToListEfficient()&#123; String[] strArray &#x3D; new String[2]; ArrayList&lt; String&gt; arrayList &#x3D; new ArrayList&lt;String&gt;(strArray.length); Collections.addAll(arrayList, strArray); arrayList.add(&quot;1&quot;); System.out.println(arrayList); &#125; 执行结果：同样成功追加一个元素“1”。 [null, null, 1] 使用场景：需要在将数组转换为List后，对List进行增删改查操作，在List的数据量巨大的情况下，优先使用，可以提高操作速度。 四.Java8可通过stream流将3种基本类型数组转为List如果JDK版本在1.8以上，可以使用流stream来将下列3种数组快速转为List，分别是int[]、long[]、double[]，其他数据类型比如short[]、byte[]、char[]，在JDK1.8中暂不支持。由于这只是一种常用方法的封装，不再纳入一种崭新的数组转List方式，暂时算是java流送给我们的常用工具方法吧。 转换代码示例如下： List&lt;Integer&gt; intList&#x3D; Arrays.stream(new int[] &#123; 1, 2, 3, &#125;).boxed().collect(Collectors.toList()); List&lt;Long&gt; longList&#x3D; Arrays.stream(new long[] &#123; 1, 2, 3 &#125;).boxed().collect(Collectors.toList()); List&lt;Double&gt; doubleList&#x3D; Arrays.stream(new double[] &#123; 1, 2, 3 &#125;).boxed().collect(Collectors.toList()); 如果是String数组，可以使用Stream流这样转换： String[] arrays &#x3D; &#123;&quot;tom&quot;, &quot;jack&quot;, &quot;kate&quot;&#125;; List&lt;String&gt; stringList&#x3D; Stream.of(arrays).collect(Collectors.toList()); 题外话下列代码为什么会报错？ String[] strArray &#x3D; new String[2]; Arrays.stream(strArray).boxed().collect(Collectors.toList()); 这段代码会报错是因为在将String数组转换为Stream后，使用boxed()方法将基本数据类型的Stream转换为包装类型的Stream。然而，String本身并不是基本数据类型，因此不需要使用boxed()方法。应该直接使用Arrays.stream(strArray).collect(Collectors.toList())即可将String数组转换为List。","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"}],"author":"ehzyil"},{"title":"Spring读取Resource下文件的几种方式","slug":"2023/Spring读取Resource下文件的几种方式","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/12/26/2023/Spring读取Resource下文件的几种方式/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/Spring%E8%AF%BB%E5%8F%96Resource%E4%B8%8B%E6%96%87%E4%BB%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/","excerpt":"","text":"Spring读取Resource下文件的几种方式第一种：通过当前线程的类加载器获取资源文件； InputStream inputStream &#x3D; Thread.currentThread().getContextClassLoader().getResourceAsStream(&quot;excleTemplate&#x2F;test.xlsx&quot;); 第二种：通过当前类的类加载器获取资源文件 InputStream inputStream &#x3D; this.getClass().getResourceAsStream(&quot;&#x2F;excleTemplate&#x2F;test.xlsx&quot;); 前两种方式都是相对路径，需要注意资源文件的位置和调用代码的位置。 第三种：ClassPathResource这是使用Spring框架提供的方式，通过ClassPathResource来读取资源文件。它提供了更多的灵活性和功能，比如可以直接获取输入流。 ClassPathResource classPathResource &#x3D; new ClassPathResource(&quot;excleTemplate&#x2F;test.xlsx&quot;); InputStream inputStream &#x3D;classPathResource.getInputStream(); 第四种：ResourceUtils这种方式使用了ResourceUtils的getFile方法，但是需要注意的是ResourceUtils的getFile方法在生产环境中可能会出现问题。因为它使用了classpath:前缀，而这种方式在生产环境中并不适用。 File file &#x3D; ResourceUtils.getFile(&quot;classpath:excleTemplate&#x2F;test.xlsx&quot;); InputStream inputStream &#x3D; new FileInputStream(file); 应用实例：spring读取json文件public static com.alibaba.fastjson.JSONObject getProblemType() throws IOException &#123; &#x2F;&#x2F; 使用 ResourceUtils 工具类获取类路径下的 problemType.json 文件 File file &#x3D; ResourceUtils.getFile(&quot;classpath:data&#x2F;problemType.json&quot;); &#x2F;&#x2F; 使用 FileUtils 工具类将文件内容读取为字符串 String json &#x3D; FileUtils.readFileToString(file, &quot;UTF-8&quot;); &#x2F;&#x2F; 使用 fastjson 库将字符串解析为 JSONObject 对象 com.alibaba.fastjson.JSONObject jsonObject &#x3D; JSON.parseObject(json); &#x2F;&#x2F; 返回解析后的 JSONObject 对象 return jsonObject; &#125; 这段代码的作用是从类路径中读取一个名为 problemType.json 的文件，并将其内容解析为 JSONObject 对象，然后返回该对象。以下是注释的详细解释： public static com.alibaba.fastjson.JSONObject getProblemType() throws IOException &#123;: 这是一个公共的静态方法，返回类型为 com.alibaba.fastjson.JSONObject，并且可能会抛出 IOException 异常。 File file = ResourceUtils.getFile(&quot;classpath:data/problemType.json&quot;);: 使用 Spring 框架的 ResourceUtils 工具类获取类路径下的 problemType.json 文件。classpath: 前缀表示文件位于类路径下的 data 目录中。 String json = FileUtils.readFileToString(file, &quot;UTF-8&quot;);: 使用 Apache Commons IO 工具类 FileUtils 将文件内容读取为字符串。这里指定了文件编码为 UTF-8。 com.alibaba.fastjson.JSONObject jsonObject = JSON.parseObject(json);: 使用阿里巴巴的 fastjson 库将字符串解析为 JSONObject 对象。 return jsonObject;: 返回解析后的 JSONObject 对象。 替换成更保险的方法 public static com.alibaba.fastjson.JSONObject getProblemType() throws IOException &#123; &#x2F;&#x2F; 使用 ClassPathResource 工具类获取类路径下的 problemType.json 文件 ClassPathResource resource &#x3D; new ClassPathResource(&quot;data&#x2F;problemType.json&quot;); &#x2F;&#x2F; 使用 IOUtils 工具类将文件内容读取为字符串 String json &#x3D; IOUtils.toString(resource.getInputStream(), &quot;UTF-8&quot;); &#x2F;&#x2F; 使用 fastjson 库将字符串解析为 JSONObject 对象 com.alibaba.fastjson.JSONObject jsonObject &#x3D; JSON.parseObject(json); &#x2F;&#x2F; 返回解析后的 JSONObject 对象 return jsonObject; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.ehzyil.xyz/tags/Spring/"}],"author":"ehzyil"},{"title":"一条SQL更新语句是如何执行的？ to connect to the Docker daemon socket at unix:///var/run/docker.sock:'","slug":"2023/一条SQL更新语句是如何执行的？","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.991Z","comments":true,"path":"2023/12/26/2023/一条SQL更新语句是如何执行的？/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/%E4%B8%80%E6%9D%A1SQL%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F/","excerpt":"","text":"一条SQL更新语句是如何执行的？ 转载MySQL实战45讲| 02 | 日志系统：一条SQL更新语句是如何执行的？ 从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键 ID 和一个整型字段 c： mysql&gt; create table T(ID int primary key, c int); 如果要将 ID&#x3D;2 这一行的值加 1，SQL 语句就会这么写： mysql&gt; update T set c&#x3D;c+1 where ID&#x3D;2; 从前面的一条SQL查询语句是如何执行的？可以知道更新语句也会走一遍相同的流程。 执行语句前要先连接数据库，这是连接器的工作。 前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。如果接触 MySQL，那这两个词肯定是绕不过的，我后面的内容里也会不断地和你强调。不过话说回来，redo log 和 binlog 在设计上有很多有意思的地方，这些设计思路也可以用到你自己的程序里。 重要的日志模块：redo log不知道你还记不记得《孔乙己》这篇文章，酒店掌柜有一个粉板，专门用来记录客人的赊账记录。如果赊账的人不多，那么他可以把顾客名和账目写在板上。但如果赊账的人多了，粉板总会有记不下的时候，这个时候掌柜一定还有一个专门记录赊账的账本。 如果有人要赊账或者还账的话，掌柜一般有两种做法： 一种做法是直接把账本翻出来，把这次赊的账加上去或者扣除掉； 另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算。 在生意红火柜台很忙时，掌柜一定会选择后者，因为前者操作实在是太麻烦了。首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找，找到之后再拿出算盘计算，最后再将结果写回到账本上。 这整个过程想想都麻烦。相比之下，还是先在粉板上记一下方便。你想想，如果掌柜没有粉板的帮助，每次记账都得翻账本，效率是不是低得让人难以忍受？ 同样，在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。 而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。 与此类似，InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 要理解 crash-safe 这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 重要的日志模块：binlog前面我们讲过，MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。 我想你肯定会问，为什么会有两份日志呢？ 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID&#x3D;2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 有了对这两个日志的概念性理解，我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。 update T set c=c+1 where ID=2; 执行器先找引擎取 ID&#x3D;2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID&#x3D;2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。 你可能注意到了，最后三步看上去有点“绕”，将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是”两阶段提交”。 两阶段提交为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得从怎样让数据库恢复到半个月内任意一秒的状态？说起。 前面我们说过了，binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做： 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库； 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。 这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。 好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 仍然用前面的 update 语句来做例子。假设当前 ID&#x3D;2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？ 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。 你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？ 其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 小结redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。 sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 我还跟你介绍了与 MySQL 日志系统密切相关的“两阶段提交”。两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。 思考题前面说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？ 好处是“最长恢复时间”更短。 在一天一备的模式里，最坏情况下需要应用一天的 binlog。比如，你每天 0 点做一次全量备份，而要恢复出一个到昨天晚上 23 点的备份。 一周一备最坏情况就要应用一周的 binlog 了。 系统的对应指标就是RTO（恢复目标时间）。 当然这个是有成本的，因为更频繁全量备份需要消耗更多存储空间，所以这个 RTO 是成本换来的，就需要你根据业务重要性来评估了。","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"}],"author":"ehzyil"},{"title":"一条SQL查询语句是如何执行的？","slug":"2023/一条SQL查询语句是如何执行的？","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.991Z","comments":true,"path":"2023/12/26/2023/一条SQL查询语句是如何执行的？/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/%E4%B8%80%E6%9D%A1SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F/","excerpt":"","text":"一条SQL查询语句是如何执行的？ 转载MySQL实战45讲| 01 | 基础架构：一条SQL查询语句是如何执行的？ 平时我们使用数据库，看到的通常都是一个整体。比如，你有个最简单的表，表里只有一个 ID 字段，在执行下面这个查询语句时： mysql> update T set c=c+1 where ID=2; 我们看到的只是输入一条语句，返回一个结果，却不知道这条语句在 MySQL 内部的执行过程。 今天来把 MySQL 拆解一下，看看里面都有哪些“零件”，希望借由这个拆解过程，让我们对MySQL 有更深入的理解。这样当我们碰到 MySQL 的一些异常或者问题时，就能够直戳本质，更为快速地定位并解决问题。 下面是 MySQL 的基本架构示意图，从中可以清楚地看到 SQL 语句在 MySQL 的各个功能模块中的执行过程。 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 也就是说，你执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是 InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table 语句中使用 engine&#x3D;memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同，在后面的文章中，我们会讨论到引擎的选择。 从图中不难看出，不同的存储引擎共用一个Server 层，也就是从连接器到执行器的部分。你可以先对每个组件的名字有个印象，接下来我会结合开头提到的那条 SQL 语句，带你走一遍整个执行流程，依次看下每个组件的作用。 连接器第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的： mysql -h$ip -P$port -u$user -p 输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在 -p 后面写在命令行中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。 连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。 如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 常见的有远程连接不上mysql(host为 % 表示不限制ip localhost表示本机使用 )，这时就需要修改root的权限 mysql&gt; select host,user,plugin,authentication_string from mysql.user; mysql&gt; alter user &#39;root&#39;@&#39;%&#39; identified with mysql_native_password by &#39;123456&#39;; mysql&gt; FLUSH PRIVILEGES; 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。文本中这个图是 show processlist 的结果，其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。 如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。 但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 怎么解决这个问题呢？你可以考虑以下两种方案。 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。 MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。 但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 好在 MySQL 也提供了这种“按需使用”的方式。你可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下面这个语句一样： mysql&gt; select SQL_CACHE * from T where ID&#x3D;10； 需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。 分析器如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。 MySQL 从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。 mysql&gt; elect * from t where ID&#x3D;1; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;elect * from t where ID&#x3D;1&#39; at line 1 一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。 优化器经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join： mysql&gt; select * from t1 join t2 using(ID) where t1.c&#x3D;10 and t2.d&#x3D;20; 既可以先从表 t1 里面取出 c&#x3D;10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d&#x3D;20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等等，没关系，我会在后面的文章中单独展开说明优化器的内容。 执行器MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。 mysql&gt; select * from T where ID&#x3D;10; ERROR 1142 (42000): SELECT command denied to user &#39;b&#39;@&#39;localhost&#39; for table &#39;T&#39; 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的： 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。 在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。我们后面会专门有一篇文章来讲存储引擎的内部机制，里面会有详细的说明。 课后问题如果表 T 中没有字段 k，而你执行了这个语句 select * from T where k&#x3D;1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？ 分析器阶段，在分析器阶段主要会进行两项事，一是语法分析，根据mysql的关键字进行验证和解析；二是词法分析，会在词法解析的基础上进一步做表名和字段名称的验证和解析，判断你输入的这个 SQL 语句是否满足 MySQL 语法。因为没有k这一列所以在语法分析时会报错。","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"}],"author":"ehzyil"},{"title":"事务隔离：为什么你改了我还看不见？","slug":"2023/事务隔离：为什么你改了我还看不见？","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.991Z","comments":true,"path":"2023/12/26/2023/事务隔离：为什么你改了我还看不见？/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81%EF%BC%9F/","excerpt":"","text":"事务隔离：为什么你改了我还看不见？ 转载MySQL实战45讲| 03 | 事务隔离：为什么你改了我还看不见？ 提到事务，你肯定不陌生，和数据库打交道的时候，我们总是会用到事务。最经典的例子就是转账，你要给朋友小王转 100 块钱，而此时你的银行卡只有 100 块钱。 转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这 100 块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。 简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。 今天的文章里，我将会以 InnoDB 为例，剖析 MySQL 在事务支持方面的特定实现，并基于原理给出相应的实践建议，希望这些案例能加深你对 MySQL 事务原理的理解。 隔离性与隔离级别提到事务，你肯定会想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中 I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面我逐一为你解释： 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表 T 中只有一列，其中一行的值为 1，下面是按照时间顺序执行两个事务的行为。 mysql&gt; create table T(c int) engine&#x3D;InnoDB; insert into T(c) values(1); 我们来看看在不同的隔离级别下，事务 A 会有哪些不同的返回结果，也就是图里面 V1、V2、V3 的返回值分别是什么。 若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。 若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。 若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，你一定要记得将 MySQL 的隔离级别设置为“读提交”。 配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。你可以用 show variables 来查看当前的值。 mysql&gt; show variables like &#39;transaction_isolation&#39;; +-----------------------+----------------+ | Variable_name | Value | +-----------------------+----------------+ | transaction_isolation | READ-COMMITTED | +-----------------------+----------------+ 总结来说，存在即合理，哪个隔离级别都有它自己的使用场景，你要根据自己的业务情况来定。我想你可能会问那什么时候需要“可重复读”的场景呢？我们来看一个数据校对逻辑的案例。 假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。 这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。 事务隔离的实现理解了事务的隔离级别，我们再来看看事务隔离具体是怎么实现的。这里我们展开说明“可重复读”。 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC)。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。 同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。 你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。 事务的启动方式如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL 的事务启动方式有以下几种： 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit&#x3D;0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 有些客户端连接框架会默认连接成功后先执行一个 set autocommit&#x3D;0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。 因此，我会建议你总是使用 set autocommit&#x3D;1, 通过显式语句的方式来启动事务。 但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用 commit work and chain 语法。 在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。 select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60 复制代码 小结这篇文章里面，介绍了 MySQL 的事务隔离级别的现象和实现，根据实现原理分析了长事务存在的风险，以及如何用正确的方式避免长事务。希望我举的例子能够帮助你理解事务，并更好地使用 MySQL 的事务特性。 课后题你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？ 这个问题，我们可以从应用开发端和数据库端来看。 首先，从应用开发端来看： 确认是否使用了 set autocommit&#x3D;0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin&#x2F;commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例） 其次，从数据库端来看： 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 &#x2F; 或者 kill； Percona 的 pt-kill 这个工具不错，推荐使用； 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题； 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"}],"author":"ehzyil"},{"title":"开启 Azure 虚拟机的 root 账户","slug":"2023/开启 Azure 虚拟机的 root 账户","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/12/26/2023/开启 Azure 虚拟机的 root 账户/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/%E5%BC%80%E5%90%AF%20Azure%20%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%20root%20%E8%B4%A6%E6%88%B7/","excerpt":"","text":"使用finalshell连接白嫖的azure虚拟服务器发现root账号忘了，于是查询得知下面的教程。 默认情况微软 Azure 云是没有开启 root 账户的，root 账户是禁用状态。 首先执行：sudo passwd root 命令初始化root用户密码 sudo passwd root 修改 sshd_config 文件 开启 root 访问权限 sudo vim &#x2F;etc&#x2F;ssh&#x2F;sshd_config 在 sshd_config 文件里的 Authentication 部分加上以下内容： PermitRootLogin yes 编辑完毕后，重启 ssh 服务，执行如下命令： sudo systemctl restart sshd # 重启 ssh 服务以应用更改 最后尝试以 root 用户登陆，显示登陆成功，并且用户为root账户。 执行的代码和结果如下： ehzyil@AzureVM:~&#x2F;data&#x2F;script$ sudo passwd root New password: Retype new password: passwd: password updated successfully ehzyil@AzureVM:~&#x2F;data&#x2F;script$ sudo vim &#x2F;etc&#x2F;ssh&#x2F;sshd_config ehzyil@AzureVM:~&#x2F;data&#x2F;script$ sudo systemctl restart sshd ehzyil@AzureVM:~&#x2F;data&#x2F;script$ su Password: root@AzureVM:&#x2F;home&#x2F;ehzyil&#x2F;data&#x2F;script# bash &lt;(curl -L https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;v2fly&#x2F;fhs-install-v2ray&#x2F;master&#x2F;install-release.sh) 相似系统启用Root用户 ​ Ubuntu 及 Ubuntu SSH 启用 Root 用户 - 欧尼酱的小屋","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.ehzyil.xyz/tags/linux/"},{"name":"Azure","slug":"Azure","permalink":"https://blog.ehzyil.xyz/tags/Azure/"}],"author":"ehzyil"},{"title":"深入浅出索引（上）","slug":"2023/深入浅出索引（上）","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/12/26/2023/深入浅出索引（上）/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"深入浅出索引（上） 转载MySQL实战45讲| 04 | 深入浅出索引（上） 提到数据库索引，我想你并不陌生，在日常工作中会经常接触到。比如某一个 SQL 查询比较慢，分析完原因之后，你可能就会说“给某个字段加个索引吧”之类的解决方案。但到底什么是索引，索引又是如何工作的呢？今天就让我们一起来聊聊这个话题吧。 数据库索引的内容比较多，我分成了上下两篇文章。索引是数据库系统里面最重要的概念之一，所以我希望你能够耐心看完。在后面的实战文章中，我也会经常引用这两篇文章中提到的知识点，加深你对数据库索引的理解。 一句话简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。一本 500 页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。 索引的常见模型索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了索引模型的概念。可以用于提高读写效率的数据结构很多，这里我先给你介绍三种常见、也比较简单的数据结构，它们分别是哈希表、有序数组和搜索树。 下面我主要从使用的角度，为你简单分析一下这三种模型的区别。 哈希表是一种以键 - 值（key-value）存储数据的结构，我们只要输入待查找的值即 key，就可以找到其对应的值即 Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。 不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。 假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示： 图中，User2 和 User4 根据身份证号算出来的值都是 N，但没关系，后面还跟了一个链表。假设，这时候你要查 ID_card_n2 对应的名字是什么，处理步骤就是：首先，将 ID_card_n2 通过哈希函数算出 N；然后，按顺序遍历，找到 User2。 需要注意的是，图中四个 ID_card_n 的值并不是递增的，这样做的好处是增加新的 User 时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很慢的。 你可以设想下，如果你现在要找身份证号在 [ID_card_X, ID_card_Y] 这个区间的所有用户，就必须全部扫描一遍了。 所以，哈希表这种结构适用于只有等值查询的场景，比如 Memcached 及其他一些 NoSQL 引擎。 而有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示： 这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你要查 ID_card_n2 对应的名字，用二分法就可以快速得到，这个时间复杂度是 O(log(N))。 同时很显然，这个索引结构支持范围查询。你要查身份证号在 [ID_card_X, ID_card_Y] 区间的 User，可以先用二分法找到 ID_card_X（如果不存在 ID_card_X，就找到大于 ID_card_X 的第一个 User），然后向右遍历，直到查到第一个大于 ID_card_Y 的身份证号，退出循环。 如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。 所以，有序数组索引只适用于静态存储引擎，比如你要保存的是 2017 年某个城市的所有人口信息，这类不会再修改的数据。 二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们用二叉搜索树来实现的话，示意图如下所示： 二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。这样如果你要查 ID_card_n2 的话，按照图中的搜索顺序就是按照 UserA -&gt; UserC -&gt; UserF -&gt; User2 这个路径得到。这个时间复杂度是 O(log(N))。 当然为了维持 O(log(N)) 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 O(log(N))。 树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。 你可以想象一下一棵 100 万节点的平衡二叉树，树高 20。一次查询可能需要访问 20 个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要 10 ms 左右的寻址时间。也就是说，对于一个 100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要 20 个 10 ms 的时间，这个查询可真够慢的。 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。 以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。 N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。 不管是哈希还是有序数组，或者 N 叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库技术发展到今天，跳表、LSM 树等数据结构也被用于引擎设计中，这里我就不再一一展开了。 你心里要有个概念，数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。 截止到这里，我用了半篇文章的篇幅和你介绍了不同的数据结构，以及它们的适用场景，你可能会觉得有些枯燥。但是，我建议你还是要多花一些时间来理解这部分内容，毕竟这是数据库处理数据的核心概念之一，在分析问题的时候会经常用到。当你理解了索引的模型后，就会发现在分析问题的时候会有一个更清晰的视角，体会到引擎设计的精妙之处。 现在，我们一起进入相对偏实战的内容吧。 在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于 InnoDB 存储引擎在 MySQL 数据库中使用最为广泛，所以下面我就以 InnoDB 为例，和你分析一下其中的索引模型。 InnoDB 的索引模型在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。 每一个索引在 InnoDB 里面对应一棵 B+ 树。 假设，我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引。 这个表的建表语句是： mysql&gt; create table T( id int primary key, k int not null, name varchar(16), index (k))engine&#x3D;InnoDB; 表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。 从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。 根据上面的索引结构说明，我们来讨论一个问题：基于主键索引和普通索引的查询有什么区别？ 如果语句是 select * from T where ID&#x3D;500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树； 如果语句是 select * from T where k&#x3D;5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。 也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。 索引维护B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。 而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。 除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。 当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。 基于上面的索引维护过程说明，我们来讨论一个案例： 你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自增主键。当然事无绝对，我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。 插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。 也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。 除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？ 由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的： 只有一个索引； 该索引必须是唯一索引。 你一定看出来了，这就是典型的 KV 场景。 由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。 这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。 小结今天，我跟你分析了数据库引擎可用的数据结构，介绍了 InnoDB 采用的 B+ 树结构，以及为什么 InnoDB 要这么选择。B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。 由于 InnoDB 是索引组织表，一般情况下我会建议你创建一个自增主键，这样非主键索引占用的空间最小。但事无绝对，我也跟你讨论了使用业务逻辑字段做主键的应用场景。 课后题对于上面例子中的 InnoDB 表 T，如果你要重建索引 k，你的两个 SQL 语句可以这么写： alter table T drop index k; alter table T add index(k); 如果你要重建主键索引，也可以这么写： alter table T drop primary key; alter table T add primary key(id); 我的问题是，对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好的方法是什么？ 为什么要重建索引。我们文章里面有提到，索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。 重建索引 k 的做法是合理的，可以达到省空间的目的。但是，重建主键的过程不合理。不论是删除主键还是创建主键，都会将整个表重建。所以连着执行这两个语句的话，第一个语句就白做了。这两个语句，你可以用这个语句代替 ： alter table T engine&#x3D;InnoDB。","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"}],"author":"ehzyil"},{"title":"深入浅出索引（下）","slug":"2023/深入浅出索引（下）","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/12/26/2023/深入浅出索引（下）/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95%EF%BC%88%E4%B8%8B%EF%BC%89/","excerpt":"","text":"转载MySQL实战45讲 | 05 | 深入浅出索引（下） 在上一篇文章中，我和你介绍了 InnoDB 索引的数据结构模型，今天我们再继续聊聊跟 MySQL 索引有关的概念。 在开始这篇文章之前，我们先来看一下这个问题： 在下面这个表 T 中，如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？ 下面是这个表的初始化语句。 mysql&gt; create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT &#39;&#39;, index k(k)) engine&#x3D;InnoDB; insert into T values(100,1, &#39;aa&#39;),(200,2,&#39;bb&#39;),(300,3,&#39;cc&#39;),(500,5,&#39;ee&#39;),(600,6,&#39;ff&#39;),(700,7,&#39;gg&#39;); 现在，我们一起来看看这条 SQL 查询语句的执行流程： 在 k 索引树上找到 k&#x3D;3 的记录，取得 ID &#x3D; 300； 再到 ID 索引树查到 ID&#x3D;300 对应的 R3； 在 k 索引树取下一个值 k&#x3D;5，取得 ID&#x3D;500； 再回到 ID 索引树查到 ID&#x3D;500 对应的 R4； 在 k 索引树取下一个值 k&#x3D;6，不满足条件，循环结束。 在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。 在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？ 覆盖索引如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。 由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 需要注意的是，在引擎内部使用覆盖索引在索引 k 上其实读了三个记录，R3~R5（对应的索引 k 上的记录项），但是对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2。 备注：关于如何查看扫描行数的问题，我将会在第 16 文章《如何正确地显示随机消息？》中，和你详细讨论。 基于上面覆盖索引的说明，我们来讨论一个问题：在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？ 假设这个市民表的定义是这样的： CREATE TABLE &#96;tuser&#96; ( &#96;id&#96; int(11) NOT NULL, &#96;id_card&#96; varchar(32) DEFAULT NULL, &#96;name&#96; varchar(32) DEFAULT NULL, &#96;age&#96; int(11) DEFAULT NULL, &#96;ismale&#96; tinyint(1) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;), KEY &#96;id_card&#96; (&#96;id_card&#96;), KEY &#96;name_age&#96; (&#96;name&#96;,&#96;age&#96;) ) ENGINE&#x3D;InnoDB 我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？ 如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。 当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。 最左前缀原则看到这里你一定有一个疑问，如果为每一种查询都设计一个索引，索引是不是太多了。如果我现在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但总不能让它走全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的索引又感觉有点浪费。应该怎么做呢？ 这里，我先和你说结论吧。B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。 为了直观地说明这个概念，我们用（name，age）这个联合索引来分析。 可以看到，索引项是按照索引定义里面出现的字段顺序排序的。 当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果。 如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是”where name like ‘张 %’”。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。 可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 基于上面对最左前缀索引的说明，我们来讨论一个问题：在建立联合索引的时候，如何安排索引内的字段顺序。 这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 所以现在你知道了，这段开头的问题里，我们要为高频请求创建 (身份证号，姓名）这个联合索引，并用这个索引支持“根据身份证号查询地址”的需求。 那么，如果既有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护 (a,b)、(b) 这两个索引。 这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引。 索引下推上一段我们说到满足最左前缀原则的时候，最左前缀可以用于在索引中定位记录。这时，你可能要问，那些不符合最左前缀的部分，会怎么样呢？ 我们还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的： mysql&gt; select * from tuser where name like &#39;张 %&#39; and age&#x3D;10 and ismale&#x3D;1; 复制代码 你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。当然，这还不错，总比全表扫描要好。 然后呢？ 当然是判断其他条件是否满足。 在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。 而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 图 3无索引下推执行流程 和图 4索引下推执行流程，是这两个过程的执行流程图。 在图 3 和 4 这两个图里面，每一个虚线箭头表示回表一次。 图 3 中，在 (name,age) 索引里面我特意去掉了 age 的值，这个过程 InnoDB 并不会去看 age 的值，只是按顺序把“name 第一个字是’张’”的记录一条条取出来回表。因此，需要回表 4 次。 图 4 跟图 3 的区别是，InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。在我们的这个例子中，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。 小结今天这篇文章，我和你继续讨论了数据库索引的概念，包括了覆盖索引、前缀索引、索引下推。你可以看到，在满足语句需求的情况下， 尽量少地访问资源是数据库设计的重要原则之一。我们在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。 课后题实际上主键索引也是可以使用多个字段的。DBA 小吕在入职新公司的时候，就发现自己接手维护的库里面，有这么一个表，表结构定义类似这样的： CREATE TABLE &#96;geek&#96; ( &#96;a&#96; int(11) NOT NULL, &#96;b&#96; int(11) NOT NULL, &#96;c&#96; int(11) NOT NULL, &#96;d&#96; int(11) NOT NULL, PRIMARY KEY (&#96;a&#96;,&#96;b&#96;), KEY &#96;c&#96; (&#96;c&#96;), KEY &#96;ca&#96; (&#96;c&#96;,&#96;a&#96;), KEY &#96;cb&#96; (&#96;c&#96;,&#96;b&#96;) ) ENGINE&#x3D;InnoDB; 公司的同事告诉他说，由于历史原因，这个表需要 a、b 做联合主键，这个小吕理解了。 但是，学过本章内容的小吕又纳闷了，既然主键包含了 a、b 这两个字段，那意味着单独在字段 c 上创建一个索引，就已经包含了三个字段了呀，为什么要创建“ca”“cb”这两个索引？ 同事告诉他，是因为他们的业务里面有这样的两种语句： select * from geek where c&#x3D;N order by a limit 1; select * from geek where c&#x3D;N order by b limit 1; 我给你的问题是，这位同事的解释对吗，为了这两个查询模式，这两个索引是否都是必须的？为什么呢？ 表记录–a–|–b–|–c–|–d–1 2 3 d1 3 2 d1 4 3 d2 1 3 d2 2 2 d2 3 4 d主键 a，b 的聚簇索引组织顺序相当于 order by a,b ，也就是先按 a 排序，再按 b 排序，c 无序。 索引 ca 的组织是先按 c 排序，再按 a 排序，同时记录主键–c–|–a–|–主键部分b– （注意，这里不是 ab，而是只有 b）2 1 32 2 23 1 23 1 43 2 14 2 3这个跟索引 c 的数据是一模一样的。 索引 cb 的组织是先按 c 排序，在按 b 排序，同时记录主键–c–|–b–|–主键部分a– （同上）2 2 22 3 13 1 23 2 13 4 14 3 2 所以，结论是 ca 可以去掉，cb 需要保留。","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"}],"author":"ehzyil"},{"title":"Java线程池","slug":"2023/线程池","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/12/26/2023/线程池/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/%E7%BA%BF%E7%A8%8B%E6%B1%A0/","excerpt":"","text":"构造方法我们在构造线程池的时候，使用了ThreadPoolExecutor的构造方法： public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable> workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); &#125; 先来看看几个参数的含义： corePoolSize: 核心线程数 maximumPoolSize:允许的最大线程数（核心线程数+非核心线程数） workQueue:线程池任务队列 用来保存等待执行的任务的阻塞队列，常见阻塞队列有 ArrayBlockingQueue：一个基于数组结构的有界阻塞队列 LinkedBlockingQueue：基于链表结构的阻塞队列 SynchronousQueue：不存储元素的阻塞队列 PriorityBlockingQueue：具有优先级的无限阻塞队列 handler: 线程池饱和拒绝策略 JDK线程池框架提供了四种策略： 也可以根据自己的应用场景，实现RejectedExecutionHandler接口来自定义策略。 AbortPolicy：直接抛出异常，默认策略。 CallerRunsPolicy：用调用者所在线程来运行任务。 DiscardOldestPolicy：丢弃任务队列里最老的任务 DiscardPolicy:不处理，丢弃当前任务 上面四个是和线程池工作流程息息相关的参数，我们再来看看剩下三个参数。 keepAliveTime:非核心线程闲置下来最多存活的时间 unit：线程池中非核心线程保持存活的时间 threadFactory:创建一个新线程时使用的工厂，可以用来设定线程名等(不指定使用默认线程工厂) 线程池工作流程 向线程池提交任务的时候： 如果当前运行的线程少于核心线程数corePoolSize，则创建新线程来执行任务 如果运行的线程等于或多于核心线程数corePoolSize，则将任务加入任务队列workQueue 如果任务队列workQueue已满，创建新的线程来处理任务 如果创建新线程使当前总线程数超过最大线程数maximumPoolSize，任务将被拒绝，线程池拒绝策略handler执行 线程池工作源码分析提交线程（execute）线程池执行任务的方法如下： public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); //获取当前线程池的状态+线程个数变量的组合值 int c = ctl.get(); //1.如果正在运行线程数少于核心线程数 if (workerCountOf(c) &lt; corePoolSize) &#123; //开启新线程运行 if (addWorker(command, true)) return; c = ctl.get(); &#125; //2. 判断线程池是否处于运行状态，是则添加任务到阻塞队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; //二次检查 int recheck = ctl.get(); //如果当前线程池不是运行状态，则从队列中移除任务，并执行拒绝策略 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); //如若当前线程池为空，则添加一个新线程 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; //最后尝试添加线程，如若添加失败，执行拒绝策略 else if (!addWorker(command, false)) reject(command); &#125; 我们来看一下execute()的详细流程图： 新增线程 (addWorker)在execute方法代码里，有个关键的方法private boolean addWorker(Runnable firstTask, boolean core)，这个方法主要完成两部分工作：增加线程数、添加任务，并执行。 我们先来看第一部分增加线程数： retry: for (;;) &#123; int c &#x3D; ctl.get(); int rs &#x3D; runStateOf(c); &#x2F;&#x2F; 1.检查队列是否只在必要时为空（判断线程状态，且队列不为空） if (rs &gt;&#x3D; SHUTDOWN &amp;&amp; ! (rs &#x3D;&#x3D; SHUTDOWN &amp;&amp; firstTask &#x3D;&#x3D; null &amp;&amp; ! workQueue.isEmpty())) return false; &#x2F;&#x2F;2.循环CAS增加线程个数 for (;;) &#123; int wc &#x3D; workerCountOf(c); &#x2F;&#x2F;2.1 如果线程个数超限则返回 false if (wc &gt;&#x3D; CAPACITY || wc &gt;&#x3D; (core ? corePoolSize : maximumPoolSize)) return false; &#x2F;&#x2F;2.2 CAS方式增加线程个数，同时只有一个线程成功，成功跳出循环 if (compareAndIncrementWorkerCount(c)) break retry; &#x2F;&#x2F;2.3 CAS失败，看线程池状态是否变化，变化则跳到外层，尝试重新获取线程池状态，否则内层重新CAS c &#x3D; ctl.get(); &#x2F;&#x2F; Re-read ctl if (runStateOf(c) !&#x3D; rs) continue retry; &#125; &#125; &#x2F;&#x2F;3. 到这说明CAS成功了 boolean workerStarted &#x3D; false; boolean workerAdded &#x3D; false; 接着来看第二部分添加任务，并执行 Worker w &#x3D; null; try &#123; &#x2F;&#x2F;4.创建worker w &#x3D; new Worker(firstTask); final Thread t &#x3D; w.thread; if (t !&#x3D; null) &#123; &#x2F;&#x2F;4.1、加独占锁 ，为了实现workers同步，因为可能多个线程调用了线程池的excute方法 final ReentrantLock mainLock &#x3D; this.mainLock; mainLock.lock(); try &#123; &#x2F;&#x2F;4.2、重新检查线程池状态，以避免在获取锁前调用了shutdown接口 int rs &#x3D; runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs &#x3D;&#x3D; SHUTDOWN &amp;&amp; firstTask &#x3D;&#x3D; null)) &#123; if (t.isAlive()) &#x2F;&#x2F; precheck that t is startable throw new IllegalThreadStateException(); &#x2F;&#x2F;4.3添加任务 workers.add(w); int s &#x3D; workers.size(); if (s &gt; largestPoolSize) largestPoolSize &#x3D; s; workerAdded &#x3D; true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; &#x2F;&#x2F;4.4、添加成功之后启动任务 if (workerAdded) &#123; t.start(); workerStarted &#x3D; true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted; 我们来看一下整体的流程： 执行线程(runWorker)用户线程提交到线程池之后，由Worker执行，Worker是线程池内部一个继承AQS、实现Runnable接口的自定义类，它是具体承载任务的对象。 先看一下它的构造方法： Worker(Runnable firstTask) &#123; setState(-1); &#x2F;&#x2F; 在调用runWorker之前禁止中断 this.firstTask &#x3D; firstTask; this.thread &#x3D; getThreadFactory().newThread(this); &#x2F;&#x2F;创建一个线程 &#125; 在构造函数内 首先设置 state&#x3D;-1,现了简单不可重入独占锁，state&#x3D;0表示锁未被获取状态，state&#x3D;1表示锁已被获取状态，设置状态大小为-1，是为了避免线程在运行runWorker()方法之前被中断 firstTask记录该工作线程的第一个任务 thread是具体执行任务的线程 它的run方法直接调用runWorker，真正地执行线程就是在我们的runWorker 方法里： final void runWorker(Worker w) &#123; Thread wt &#x3D; Thread.currentThread(); Runnable task &#x3D; w.firstTask; w.firstTask &#x3D; null; w.unlock(); &#x2F;&#x2F; 允许中断 boolean completedAbruptly &#x3D; true; try &#123; &#x2F;&#x2F;获取当前任务，从队列中获取任务 while (task !&#x3D; null || (task &#x3D; getTask()) !&#x3D; null) &#123; w.lock(); ………… try &#123; &#x2F;&#x2F;执行任务前做一些类似统计之类的事情 beforeExecute(wt, task); Throwable thrown &#x3D; null; try &#123; &#x2F;&#x2F;执行任务 task.run(); &#125; catch (RuntimeException x) &#123; thrown &#x3D; x; throw x; &#125; catch (Error x) &#123; thrown &#x3D; x; throw x; &#125; catch (Throwable x) &#123; thrown &#x3D; x; throw new Error(x); &#125; finally &#123; &#x2F;&#x2F; 执行任务完毕后干一些些事情 afterExecute(task, thrown); &#125; &#125; finally &#123; task &#x3D; null; &#x2F;&#x2F; 统计当前Worker 完成了多少个任务 w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly &#x3D; false; &#125; finally &#123; &#x2F;&#x2F;执行清理工作 processWorkerExit(w, completedAbruptly); &#125; &#125; 代码看着多，其实砍掉枝蔓，最核心的点就是task.run()让线程跑起来。 获取任务(getTask)我们在上面的执行任务runWorker里看到，这么一句while (task != null || (task = getTask()) != null),执行的任务是要么当前传入的firstTask，或者还可以通过getTask()获取，这个getTask的核心目的就是从队列中获取任务。 private Runnable getTask() &#123; &#x2F;&#x2F;poll()方法是否超时 boolean timedOut &#x3D; false; &#x2F;&#x2F;循环获取 for (;;) &#123; int c &#x3D; ctl.get(); int rs &#x3D; runStateOf(c); &#x2F;&#x2F; 1.线程池未终止，且队列为空，返回null if (rs &gt;&#x3D; SHUTDOWN &amp;&amp; (rs &gt;&#x3D; STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; &#x2F;&#x2F;工作线程数 int wc &#x3D; workerCountOf(c); boolean timed &#x3D; allowCoreThreadTimeOut || wc &gt; corePoolSize; &#x2F;&#x2F;2.判断工作线程数是否超过最大线程数 &amp;&amp; 超时判断 &amp;&amp; 工作线程数大于0或队列为空 if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; &#x2F;&#x2F;从任务队列中获取线程 Runnable r &#x3D; timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); &#x2F;&#x2F;获取成功 if (r !&#x3D; null) return r; timedOut &#x3D; true; &#125; catch (InterruptedException retry) &#123; timedOut &#x3D; false; &#125; &#125; &#125; 总结一下，Worker执行任务的模型如下： 线程池的关闭线程池提供了 shutdown 和 shutdownNow 两个⽅法来关闭线程池。 shutdown() /** * 启动⼀次顺序关闭，在这次关闭中，执⾏器不再接受新任务，但会继续处理队列中的已存在任务。 * 当所有任务都完成后，线程池中的线程会逐渐退出。 */ public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; // ThreadPoolExecutor的主锁 mainLock.lock(); // 加锁以确保独占访问 try &#123; checkShutdownAccess(); // 检查是否有关闭的权限 advanceRunState(SHUTDOWN); // 将执⾏器的状态更新为SHUTDOWN interruptIdleWorkers(); // 中断所有闲置的⼯作线程 onShutdown(); // ScheduledThreadPoolExecutor中的挂钩⽅法，可供⼦类᯿写以进⾏额外操作 &#125; finally &#123; mainLock.unlock(); // ⽆论try块如何退出都要释放锁 &#125; tryTerminate(); // 如果条件允许，尝试终⽌执⾏器 &#125; 就是将线程池的状态修改为 SHUTDOWN，然后尝试打断空闲的线程（如何判断空闲，上⾯在说 Worker 继承 AQS 的时候说过），也就是在阻塞等待任务的线程 shutdownNow () &#x2F;** * 尝试停⽌所有正在执⾏的任务，停⽌处理等待的任务， * 并返回等待处理的任务列表。 * * @return 从未开始执⾏的任务列表 *&#x2F; public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; &#x2F;&#x2F; ⽤于存储未执⾏的任务的列表 final ReentrantLock mainLock &#x3D; this.mainLock; &#x2F;&#x2F; ThreadPoolExecutor的主锁 mainLock.lock(); &#x2F;&#x2F; 加锁以确保独占访问 try &#123; checkShutdownAccess(); &#x2F;&#x2F; 检查是否有关闭的权限 advanceRunState(STOP); &#x2F;&#x2F; 将执⾏器的状态更新为STOP interruptWorkers(); &#x2F;&#x2F; 中断所有⼯作线程 tasks &#x3D; drainQueue(); &#x2F;&#x2F; 清空队列并将结果放⼊任务列表中 &#125; finally &#123; mainLock.unlock(); &#x2F;&#x2F; ⽆论try块如何退出都要释放锁 &#125; tryTerminate(); &#x2F;&#x2F; 如果条件允许，尝试终⽌执⾏器 return tasks; &#x2F;&#x2F; 返回队列中未被执⾏的任务列表 &#125; 就是将线程池的状态修改为 STOP，然后尝试打断所有的线程，从阻塞队列中移除剩余的任务，这也是为什么 shutdownNow 不能执⾏剩余任务的原因。 所以也可以看出 shutdown ⽅法和 shutdownNow ⽅法的主要区别就是，shutdown 之后还能处理在队列中的任 务，shutdownNow 直接就将任务从队列中移除，线程池⾥的线程就不再处理了。 线程池的监控如果在系统中大量使用线程池，则有必要对线程池进行监控，方便在出现问题时，可以根据线程池的使用状况快速定位问题。 可以通过线程池提供的参数和方法来监控线程池： getActiveCount() ：线程池中正在执行任务的线程数量 getCompletedTaskCount() ：线程池已完成的任务数量，该值小于等于 taskCount getCorePoolSize() ：线程池的核心线程数量 getLargestPoolSize()：线程池曾经创建过的最大线程数量。通过这个数据可以知道线程池是否满过，也就是达到了 maximumPoolSize getMaximumPoolSize()：线程池的最大线程数量 getPoolSize() ：线程池当前的线程数量 getTaskCount() ：线程池已经执行的和未执行的任务总数 还可以通过扩展线程池来进行监控： 通过继承线程池来自定义线程池，重写线程池的beforeExecute、afterExecute和terminated方法， 也可以在任务执行前、执行后和线程池关闭前执行一些代码来进行监控。例如，监控任务的平均执行时间、最大执行时间和最小执行时间等。 小结到这，了解了execute和worker的一些流程，可以说其实ThreadPoolExecutor 的实现就是一个生产消费模型。 当用户添加任务到线程池时相当于生产者生产元素，workers 线程工作集中的线程直接执行任务或者从任务队列里面获取任务时则相当于消费者消费元素。 线程池生命周期线程池状态表示在ThreadPoolExecutor里定义了一些状态，同时利用高低位的方式，让ctl这个参数能够保存状态，又能保存线程数量，非常巧妙！[6] &#x2F;&#x2F;记录线程池状态和线程数量 private final AtomicInteger ctl &#x3D; new AtomicInteger(ctlOf(RUNNING, 0)); &#x2F;&#x2F;29 private static final int COUNT_BITS &#x3D; Integer.SIZE - 3; private static final int CAPACITY &#x3D; (1 &lt;&lt; COUNT_BITS) - 1; &#x2F;&#x2F; 线程池状态 private static final int RUNNING &#x3D; -1 &lt;&lt; COUNT_BITS; private static final int SHUTDOWN &#x3D; 0 &lt;&lt; COUNT_BITS; private static final int STOP &#x3D; 1 &lt;&lt; COUNT_BITS; private static final int TIDYING &#x3D; 2 &lt;&lt; COUNT_BITS; private static final int TERMINATED &#x3D; 3 &lt;&lt; COUNT_BITS; 高3位表示状态，低29位记录线程数量： 线程池状态流转线程池一共定义了五种状态，来看看这些状态是怎么流转的[6]： RUNNING：运行状态，接受新的任务并且处理队列中的任务。 SHUTDOWN：关闭状态(调用了 shutdown 方法)。不接受新任务，,但是要处理队列中的任务。 STOP：停止状态(调用了 shutdownNow 方法)。不接受新任务，也不处理队列中的任务，并且要中断正在处理的任务。 TIDYING：所有的任务都已终止了，workerCount 为 0，线程池进入该状态后会调terminated() 方法进入 TERMINATED 状态。 TERMINATED：终止状态，terminated() 方法调用结束后的状态。 Executors 构建线程池在上⾯的示例中，我们使⽤了 JDK 内部提供的 Executors ⼯具类来快速创建线程池。 1）固定线程数量的线程池：核⼼线程数与最⼤线程数相等 public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); &#125; 2）单个线程数量的线程池 public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); &#125; 3）接近⽆限⼤线程数量的线程池 public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); &#125; 4）带定时调度功能的线程池 public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize); &#125; 虽然JDK 提供了快速创建线程池的方法，但其实不推荐使用 Exeutors 来创建线程池，因为从上面构造线程池的代码可以看出，newFixedThreadPool 线程池由于使用了 LinkedBlockingOueue，队列的容量默认无限大，实际使用中出现任务过多时会导致内存溢出; newCachedThreadPool 线程池由于核心线程数无限大，当任务过多的时候会导致创建大量的线程，可能机器负载过高导致服务宕机。 如何合理的⾃定义线程池线程池大小 关于线程池的大小，并没有一个需要严格遵守的“金规铁律”，按照任务性质，大概可以分为CPU密集型任务、IO密集型任务和混合型任务。 CPU密集型任务：CPU密集型任务应配置尽可能小的线程，如配置Ncpu+1个线程的线程池。 IO密集型任务：IO密集型任务线程并不是一直在执行任务，则应配置尽可能多的线程，如2*Ncpu。 混合型任务：混合型任务可以按需拆分成CPU密集型任务和IO密集型任务。 当然，这个只是建议，实际上具体怎么配置，还要结合事前评估和测试、事中监控来确定一个大致的线程线程池大小。线程池大小也可以不用写死，使用动态配置的方式，以便调整。 线程工厂一般建议自定义线程工厂，构建线程的时候设置线程的名称，这样在查日志的时候就方便知道是哪个线程执行的代码。 import java.util.concurrent.ThreadFactory; import java.util.concurrent.atomic.AtomicInteger; /** * @author ehyzil * @Description * @create 2023-12-2023/12/26-19:05 */ public class MyThreadFactory implements ThreadFactory &#123; private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; /** * 构造函数传入我们想业务需要的线程名字threadName，方便发生异常是追溯 * @param threadName */ public MyThreadFactory(String threadName) &#123; SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); if (threadName == null || threadName.isEmpty())&#123; threadName = \"pool\"; &#125; namePrefix = threadName + poolNumber.getAndIncrement() + \"-thread-\"; &#125; @Override public Thread newThread(Runnable r) &#123; Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; &#125; &#125; //测试类 import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; /** * @author ehyzil * @Description * @create 2023-12-2023/12/26-19:06 */ public class Test &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(3, 5, 5, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;>(100), new MyThreadFactory(\"测试自建线程工厂\")); for (int i = 0; i &lt; 100; i++) &#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()); &#125; &#125;); &#125; &#125; &#125; 执行结果 测试自建线程工厂1-thread-1 测试自建线程工厂1-thread-2 测试自建线程工厂1-thread-3 测试自建线程工厂1-thread-2 有界队列一般需要设置有界队列的大小，比如 LinkedBlockingOueue 在构造的时候可以传入参数来限制队列中任务数据的大小，这样就不会因为无限往队列中扔任务导致系统的 oom。","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"},{"name":"并发","slug":"并发","permalink":"https://blog.ehzyil.xyz/tags/%E5%B9%B6%E5%8F%91/"}],"author":"ehzyil"},{"title":"记录需求中写的递归方法","slug":"2023/记录需求中写的递归方法","date":"2023-12-26T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/12/26/2023/记录需求中写的递归方法/","link":"","permalink":"https://blog.ehzyil.xyz/2023/12/26/2023/%E8%AE%B0%E5%BD%95%E9%9C%80%E6%B1%82%E4%B8%AD%E5%86%99%E7%9A%84%E9%80%92%E5%BD%92%E6%96%B9%E6%B3%95/","excerpt":"","text":"记录需求中写的递归方法递归遍历json数据，将其转换成 List&lt;Map&lt;String, Object&gt;&gt;列表public static List&lt;Map&lt;String, Object>> json2List(com.alibaba.fastjson.JSONArray array) &#123; // 创建一个空的列表 List&lt;Map&lt;String, Object>> list = new ArrayList&lt;>(); // 遍历json数组中的每个对象 for (Object obj : array) &#123; // 将对象转换为JSONObject com.alibaba.fastjson.JSONObject object = (com.alibaba.fastjson.JSONObject) obj; // 创建一个空的HashMap HashMap&lt;String, Object> map = new HashMap&lt;>(); // 获取对象中名为\"children\"的数组 com.alibaba.fastjson.JSONArray children = object.getJSONArray(\"children\"); // 如果\"children\"不为空，则将其转换为列表 if (children != null) &#123; List&lt;Map&lt;String, Object>> childrenList = json2List(children); map.put(\"children\", childrenList); &#125; // 将对象中的\"name\"、\"id\"、\"value\"放入map中 map.put(\"name\", object.getString(\"name\")); map.put(\"id\", object.getString(\"id\")); map.put(\"value\", object.getString(\"value\")); // 将map添加到列表中 list.add(map); &#125; // 返回最终的列表 return list; &#125; 递归调用方法处理传入的列表将复合条件的节点添加指定属性/** * 递归设置节点的打开状态 * @param list 待处理的节点列表 * @param target * @return 设置打开状态后的节点列表 */ public static List&lt;Map&lt;String, Object>> recursiveSetopen(List&lt;Map&lt;String, Object>> list, List&lt;String> target) &#123; for (Map&lt;String, Object> map : list) &#123; List&lt;Map&lt;String, Object>> children = (List&lt;Map&lt;String, Object>>) map.get(\"children\"); if (map.get(\"children\") != null) &#123; // 递归设置子节点的打开状态 recursiveSetOpen(children, target); String open = \"\"; for (Map&lt;String, Object> child : children) &#123; open = MapUtils.getString(child, \"open\"); if (StringUtils.isNotEmpty(open)) &#123; // 如果子节点已经打开，则将父节点设置为打开状态 Map&lt;String, Object> objectMap = list.get(list.indexOf(map)); objectMap.put(\"open\", true); list.set(list.indexOf(map), objectMap); break; &#125; &#125; &#125; else &#123; String name = map.get(\"name\").toString(); if (target.contains(name)) &#123; // 如果节点在目标列表中，则设置为打开状态和选中状态 map.put(\"open\", true); map.put(\"checked\", true); &#125; &#125; &#125; return list; &#125; JSON数据： &#123; \"beans\":[ &#123; \"name\":\"沟通技巧类\", \"value\":\"4\", \"id\":\"4\", \"children\":[ &#123; \"name\":\"沟通过于随意或使用不恰当语言\", \"value\":\"401\", \"id\":\"401\" &#125;, &#123; \"name\":\"语音、语调、语速运用不佳\", \"value\":\"402\", \"id\":\"402\" &#125; ] &#125;, &#123; \"name\":\"业务能力类\", \"value\":\"3\", \"id\":\"3\", \"children\":[ &#123; \"name\":\"业务解释或办理差错\", \"value\":\"309\", \"id\":\"309\" &#125;, &#123; \"name\":\"业务不熟练\", \"value\":\"305\", \"id\":\"305\" &#125;, &#123; \"name\":\"承诺未兑现\", \"value\":\"310\", \"id\":\"310\" &#125;, &#123; \"name\":\"未执行公司内部各类流程规范\", \"value\":\"311\", \"id\":\"311\" &#125; ] &#125;, &#123; \"name\":\"服务能力类\", \"value\":\"2\", \"id\":\"2\", \"children\":[ &#123; \"name\":\"抓不住客户重点\", \"value\":\"201\", \"id\":\"201\" &#125;, &#123; \"name\":\"机械化服务、服务不灵活\", \"value\":\"202\", \"id\":\"202\" &#125; ] &#125;, &#123; \"name\":\"服务意识类\", \"value\":\"1\", \"id\":\"1\", \"children\":[ &#123; \"name\":\"敷衍、搪塞、推诿客户\", \"value\":\"107\", \"id\":\"107\" &#125;, &#123; \"name\":\"反问、质问、指责客户\", \"value\":\"102\", \"id\":\"102\" &#125;, &#123; \"name\":\"抢话、插话\", \"value\":\"103\", \"id\":\"103\" &#125;, &#123; \"name\":\"强制转接或挂机\", \"value\":\"104\", \"id\":\"104\" &#125;, &#123; \"name\":\"出现不文明用语\", \"value\":\"105\", \"id\":\"105\" &#125;, &#123; \"name\":\"故意引导客户升级投诉\", \"value\":\"106\", \"id\":\"106\" &#125; ] &#125;, &#123; \"name\":\"服务禁忌类\", \"value\":\"0\", \"id\":\"0\", \"children\":[ &#123; \"name\":\"辱骂客户\", \"value\":\"001\", \"id\":\"001\" &#125;, &#123; \"name\":\"威胁恐吓客户\", \"value\":\"002\", \"id\":\"002\" &#125;, &#123; \"name\":\"恶意骚扰报复客户\", \"value\":\"007\", \"id\":\"007\" &#125;, &#123; \"name\":\"欺瞒、诱导客户\", \"value\":\"010\", \"id\":\"010\" &#125;, &#123; \"name\":\"与客户发生争吵言语过激攻击客户\", \"value\":\"008\", \"id\":\"008\" &#125;, &#123; \"name\":\"违反五条禁令\", \"value\":\"009\", \"id\":\"009\" &#125; ] &#125;, &#123; \"name\":\"无问题\", \"value\":\"5\", \"id\":\"5\" &#125; ] &#125; 递归寻找满足条件的节点-返回子节点到父节点的链路public static StringBuffer recursiveForAllName(List&lt;Map&lt;String, Object>> list, String target, StringBuffer result) &#123; if (CollectionUtils.isEmpty(list)) &#123; return result.append(target); &#125; for (Map&lt;String, Object> map : list) &#123; List&lt;Map&lt;String, Object>> children = (List&lt;Map&lt;String, Object>>) map.get(\"children\"); if (map.get(\"children\") != null) &#123; // Recursively set the open state of the child nodes recursiveForAllName(children, target, result); if (StringUtils.isNotBlank(result)) &#123; result.insert(0, map.get(\"name\") + \"_\"); break; &#125; else &#123; String name = map.get(\"name\").toString(); if (target.contains(name)) &#123; result.insert(0, name); break; &#125; &#125; &#125; &#125; return result; &#125; 调用方法 public static List&lt;Map&lt;String, Object>> handleExportData(List&lt;Map&lt;String, Object>> checkResults, List&lt;Map&lt;String, Object>> exportNeedModels) throws IOException &#123; // MODELS_NM ----> List&lt;Map&lt;String, Object>> control层获取 // problemTypeNm 问题判定 com.alibaba.fastjson.JSONObject problemTypeObj = getProblemTypeObj(); com.alibaba.fastjson.JSONArray beansArray = problemTypeObj.getJSONArray(\"beans\"); List&lt;Map&lt;String, Object>> beans = json2List(beansArray); for (Map&lt;String, Object> map : checkResults) &#123; String problemTypeNm = map.get(\"problemTypeNm\").toString(); if (StringUtils.isNotEmpty(problemTypeNm)) &#123; StringBuffer stringBuffer = new StringBuffer(); String[] split = problemTypeNm.split(\";\"); for (String singleProblemTypeNm : split) &#123; stringBuffer.append(recursiveForAllName(beans, singleProblemTypeNm, new StringBuffer())).append(\",\"); &#125; map.put(\"problemTypeNm\", stringBuffer.toString()); &#125; String checkComment = map.get(\"MODELS_NM\").toString(); if (StringUtils.isNotEmpty(checkComment)) &#123; StringBuffer stringBuffer = new StringBuffer(); String[] split = checkComment.split(\";\"); for (String singleModel : split) &#123; stringBuffer.append(recursiveForAllName(exportNeedModels, singleModel, new StringBuffer())).append(\";\"); &#125; map.put(\"MODELS_NM\", stringBuffer.toString()); &#125; &#125; return checkResults; &#125;","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"}],"author":"ehzyil"},{"title":"CentOS7同步时间","slug":"2023/CentOS 7同步时间","date":"2023-11-11T00:00:00.000Z","updated":"2024-06-17T01:04:53.975Z","comments":true,"path":"2023/11/11/2023/CentOS 7同步时间/","link":"","permalink":"https://blog.ehzyil.xyz/2023/11/11/2023/CentOS%207%E5%90%8C%E6%AD%A5%E6%97%B6%E9%97%B4/","excerpt":"","text":"问题描述The difference between the request time and the current time is too large。 由于minio部署在centos的docker容器中，虚拟机挂起再启动后时间不会同步，造成上述问题。 解决方法是同步虚拟机的时间。 设置时区（CentOS 7）先执行命令timedatectl status|grep &#39;Time zone&#39;查看当前时区，如果不是中国时区（Asia&#x2F;Shanghai），则需要先设置为中国时区，否则时区不同会存在时差。 #已经是Asia/Shanghai，则无需设置 [root@xiaoz shadowsocks]# timedatectl status|grep 'Time zone' Time zone: Asia/Shanghai (CST, +0800) 执行下面的命令设置时区 #设置硬件时钟调整为与本地时钟一致 timedatectl set-local-rtc 1 #设置时区为上海 timedatectl set-timezone Asia/Shanghai 使用ntpdate同步时间目前比较常用的做法就是使用ntpdate命令来同步时间，使用方法如下： #安装ntpdate yum -y install ntpdate #同步时间 ntpdate -u pool.ntp.org #同步完成后,date命令查看时间是否正确 date","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ehzyil.xyz/categories/Linux/"}],"tags":[{"name":"CentOS","slug":"CentOS","permalink":"https://blog.ehzyil.xyz/tags/CentOS/"}],"author":"ehzyil"},{"title":"在服务器上搭建Alist文件列表程序","slug":"2023/在服务器上搭建Alist文件列表程序","date":"2023-10-22T00:00:00.000Z","updated":"2024-06-17T01:04:53.991Z","comments":true,"path":"2023/10/22/2023/在服务器上搭建Alist文件列表程序/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/22/2023/%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E6%90%AD%E5%BB%BAAlist%E6%96%87%E4%BB%B6%E5%88%97%E8%A1%A8%E7%A8%8B%E5%BA%8F/","excerpt":"","text":"在我们使用网盘的时候，有时候需要搭建一个分享页。但是默认的网盘分享页有些麻烦。在这篇教程中，我们来和大家一起在服务器上使用shell脚本一键搭建Alist文件列表程序 准备材料 服务器一台 部署步骤 SSH进入服务器的控制台，执行以下命令 curl -fsSL &quot;https:&#x2F;&#x2F;alist.nn.ci&#x2F;v3.sh&quot; | bash -s install 2.脚本将会提示访问IP地址、管理员用户名及密码 3.配置服务器安全组的规则 4.如需要更新Alist，则使用以下命令 SHELL 复制成功curl -fsSL &quot;https:&#x2F;&#x2F;alist.nn.ci&#x2F;v3.sh&quot; | bash -s update 5.如需要卸载Alist，则使用以下命令 SHELL curl -fsSL &quot;https:&#x2F;&#x2F;alist.nn.ci&#x2F;v3.sh&quot; | bash -s update 配置Alisthttps://alist.nn.ci/zh/guide/drivers/aliyundrive.html","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"网盘","slug":"网盘","permalink":"https://blog.ehzyil.xyz/tags/%E7%BD%91%E7%9B%98/"}],"author":"ehzyil"},{"title":"小白初遇服务器","slug":"2023/小白初遇服务器","date":"2023-10-22T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/10/22/2023/小白初遇服务器/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/22/2023/%E5%B0%8F%E7%99%BD%E5%88%9D%E9%81%87%E6%9C%8D%E5%8A%A1%E5%99%A8/","excerpt":"","text":"前言白飘了阿里云服务器，创建实例后一脸懵逼，完全不知道在哪查看密码、选择的宝塔镜像怎么使用、在哪里打开等等，于是就有了本文，用来记录一些配置过程，方便下次创建服务器的时候直接copy配置，现在就开始玩一玩服务器吧，折腾起来！ 0.更新、安装必备软件yum update &amp;&amp; yum install -y wget vim #centos apt-get update &amp;&amp; apt-get install -y wget vim #dibian 1.安装宝塔面板流程本文是以阿里云服务器ECS为例，CentOS操作系统，手动安装宝塔Linux面板，大致流程为：先远程连接到云服务器，执行宝塔面板安装命令，安装完毕后保存好宝塔登录地址、用户名和密码等信息，然后在云服务器安全组开通宝塔面板端口号，最后登录到宝塔面板安装所需的运行环境。操作流程如下： 1、先创建云服务器ECS，云服务器配置选择； 2、远程连接到云服务器； 3、执行宝塔面板安装命令脚本； 4、保存宝塔面板登录地址、账号和密码； 5、在云服务器的安全组中开通宝塔面板所需端口号； 6、登录宝塔面板后台，并安装应用程序环境。 步骤一：阿里云服务器配置选择步骤二：远程连接登录到云服务器阿里云服务器支持多种远程连接方式，可以使用阿里云自带的Workbench远程连接方式，也可以使用第三方SSH远程连接软件如PuTTY、Xshell等。阿里云服务器网使用阿里云自带的远程连接方式： 首先登录到云服务器ECS管理控制台，左侧栏【实例与镜像】&gt;&gt;【实例】，找到目标云服务器ECS实例，然后点击右侧的【远程连接】。 步骤三：执行宝塔面板的安装命令登录到你的云服务器后，执行宝塔面板安装命令，阿里云服务器网使用的CentOS操作系统，命令如下： yum install -y wget &amp;&amp; wget -O install.sh https:&#x2F;&#x2F;download.bt.cn&#x2F;install&#x2F;install_6.0.sh &amp;&amp; sh install.sh ed8484bec 执行宝塔Linux面板安装命令后，会提示如下： Do you want to install Bt-Panel to the &#x2F;www directory now?(y&#x2F;n): y 保持默认，回复个字母“y”。 然后回车，系统会自动安装，大约1分钟左右会自动安装完成。 步骤四：宝塔面板登录地址、账号和密码宝塔面板自动安装完成后，会显示宝塔后台登录地址、username和password，如下： &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; BT-Panel default info! # 注意: 5.x系列Linux面板从2020年1月1日起终止维护，与技术支持，请考虑安装全新的7.x版本 宝塔官网: https:&#x2F;&#x2F;www.bt.cn &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; Bt-Panel: http:&#x2F;&#x2F;IP:8888 username: 08bnpksl password: 89b58de05321 Warning: If you cannot access the panel, release the following port (8888|888|80|443|20|21) in the security group [root@iZ2ze552628d2crm1m97ppZ ~]# bt default cd &#x2F;www&#x2F;server&#x2F;panel &amp;&amp; btpython tools.py panel testpasswd a 步骤五：在阿里云服务器控制台开通宝塔面板端口1、登录到ECS云服务器管理控制台 2、左侧栏找到【实例与镜像】&gt;&gt;【实例】，找到目标ECS实例，点击实例ID进入到实例详情页 3、切换到【安全组】页面，点击右侧【配置规则】，如下图： 关于阿里云安全组开通端口详细教程参考下方文档： 详细教程参考：https://help.aliyun.com/document_detail&#x2F;25471.html 配置成如下： 步骤六：登录到宝塔管理地址并安装LNMP环境登录后会自动弹出此界面，按需求选择软件并安装，也可以到软件管理去安装。 2.安装 Docker、Docker-compose国内机国内机安装 dockercurl -sSL https://get.daocloud.io/docker | sh 国内机安装 docker-composecurl -L https://get.daocloud.io/docker/compose/releases/download/v2.1.1/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 若无法使用 卸载旧版本的 Docker Compose： sudo rm /usr/local/bin/docker-compose 下载最新版本的 Docker Compose： sudo curl -L &quot;https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;2.27.0&#x2F;docker-compose-$(uname -s)-$(uname -m)&quot; -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose 应用执行权限：bash sudo chmod +x /usr/local/bin/docker-compose 卸载 dockersudo apt-get remove docker docker-engine rm -fr /var/lib/docker/ 海外服务器非大陆 Docker 安装BASHwget -qO- get.docker.com | bash 卸载 Dockersudo apt-get purge docker-ce docker-ce-cli containerd.io sudo rm -rf &#x2F;var&#x2F;lib&#x2F;docker sudo rm -rf &#x2F;var&#x2F;lib&#x2F;containerd 非大陆 Docker-compose 安装sudo curl -L &quot;https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.29.2&#x2F;docker-compose-$(uname -s)-$(uname -m)&quot; -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose sudo chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose 查看版本 先进入 docker-compose --version # /usr/local/bin/ 修改 Docker 配置（来自烧饼博客）以下配置会增加一段自定义内网 IPv6 地址，开启容器的 IPv6 功能，以及限制日志文件大小，防止 Docker 日志塞满硬盘（泪的教训）： cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF &#123; &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;20m&quot;, &quot;max-file&quot;: &quot;3&quot; &#125;, &quot;ipv6&quot;: true, &quot;fixed-cidr-v6&quot;: &quot;fd00:dead:beef:c0::&#x2F;80&quot;, &quot;experimental&quot;:true, &quot;ip6tables&quot;:true &#125; EOF 然后重启 Docker 服务： 解决目前DockerHub国内无法访问方法 解决目前Docker Hub国内无法访问方法汇总 - 我和你并没有不同 - 博客园 sudo mkdir -p &#x2F;etc&#x2F;docker sudo tee &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;-&#39;EOF&#39; &#123; &quot;registry-mirrors&quot;: [ &quot;https:&#x2F;&#x2F;docker.m.daocloud.io&quot;, &quot;https:&#x2F;&#x2F;dockerproxy.com&quot;, &quot;https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&quot;, &quot;https:&#x2F;&#x2F;docker.nju.edu.cn&quot; ] &#125; EOF sudo systemctl daemon-reload sudo systemctl restart docker systemctl restart docker 文件管理强烈建议专门给 Docker 的数据、配置文件新建一个文件夹， mkdir -p data&#x2F;docker_data 常用指令： 查看 Docker 安装版本等信息 docker version 启动 Docker 服务 systemctl start docker 查看 Docker 运行状态 systemctl status docker 将 Docker 服务加入开机自启动 systemctl enable docker Docker 项目卸载（包括卸载 Docker、docker-compose）docker 命令搭建的常用卸载方法docker ps docker stop 容器名字 cd ~ docker rm -f 容器名字 rm -rf 映射出来的路径 docker-compose 搭建的卸载方法cd &#x2F;root&#x2F;data&#x2F;docker_data&#x2F;xxx docker-compose down cd ~ rm -rf &#x2F;root&#x2F;data&#x2F;docker_data&#x2F;xxx # rm -rf 映射出来的路径 卸载 docker 本身yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine \\ docker-ce sudo rm -rf &#x2F;var&#x2F;lib&#x2F;docker sudo rm -rf &#x2F;var&#x2F;lib&#x2F;containerd 卸载 docker-composecd &#x2F;usr&#x2F;local&#x2F;bin&#x2F; rm -rf docker-compose 3.防火墙CentOS 关闭防火墙systemctl start supervisord systemctl disable firewalld systemctl stop firewalld 4.安装宝塔国际版 aapanel宝塔总是登录不上去于是就换成了dibian系统，从新开始 Debian : wget -O install.sh http:&#x2F;&#x2F;www.aapanel.com&#x2F;script&#x2F;install-ubuntu_6.0_en.sh &amp;&amp; bash install.sh forum aaPanel Internet Address: http://47.93.26.115:7800/7417c54faaPanel Internal Address: http://172.25.138.127:7800/7417c54 附上相关教程：宝塔面板 7.8 无缝转为宝塔国际版 aapanel，附宝塔 7.8 降级为宝塔 7.7 5.安装alist 普通版 docker run -d --restart&#x3D;always -v &#x2F;root&#x2F;data&#x2F;docker_data&#x2F;alist:&#x2F;opt&#x2F;alist&#x2F;data -p 5244:5244 --name&#x3D;&quot;alist&quot; xhofe&#x2F;alist:latest 预装了aria2版 docker run -d --restart=always -v /root/data/docker_data/alist:/opt/alist/data -p 5244:5244 -e PUID=0 -e PGID=0 -e UMASK=022 --name=\"alist\" xhofe/alist-aria2:latest 查看密码 docker logs alist 2f6e1555-20e5-492d-9e26-5e4ce90a0a45.id.repl.co 5.安装MySql1.拉取镜像 sudo docker pull mysql:8.0.23 2.安装 sudo docker run -p 3306:3306 --name mysql8 \\ --restart unless-stopped \\ -v /root/data/docker_data/mysql/mysql-files:/var/lib/mysql-files \\ -v /root/data/docker_data/mysql/conf:/etc/mysql \\ -v /root/data/docker_data/mysql/logs:/var/log/mysql \\ -v /root/data/docker_data/mysql/data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=666666 \\ -d mysql:8.0.23 3.进入容器内部修改root允许远程登录。 进入容器内部 docker exec -it mysql8 bash 登录mysql，并一次执行下列命令，允许远程访问 mysql -uroot -p use mysql; alter user 'root'@'%' identified with mysql_native_password by '9988174' flush privileges; select host,user,plugin,authentication_string from mysql.user 6.安装redis1.一般需要的配置文件 bind 127.0.0.1 #限制redis只能本地访问，若需要其他ip地址访问需要注释 protected-mode yes #是否开启保护模式，默认值为yes，开启后限制为本地访问，修改为no daemonize no #默认no，修改为yes会使docker使用配置文件方式启动redis失败，yes：以守护进程方式启动，可后台运行，除非kill进程 requirepass 666666 #redis密码 appendonly yes #默认yes，开启AOF模式持久化 databases 16 #数据库个数 dir ./ #redis数据库存放文件夹 创建将配置文件改为redis.conf #bind 127.0.0.1 protected-mode no daemonize no requirepass 9988174 appendonly no databases 16 dir ./ 2.拉取镜像 docker pull redis 3.以配置文件启动 docker run -p 6379:6379 --name redis -v /root/data/docker_data/redis/redis.conf:/etc/redis/redis.conf -v /root/data/docker_data/redis/data:/data -d redis redis-server /etc/redis/redis.conf --appendonly yes 4.一些命令 docker exec -it redis bash #进入容器内部 docker exec -ti redis redis-cli #执行容器内的redis auth xxx #登录 7.安装memosdocker run -d --name memos -p 5230:5230 -v &#x2F;home&#x2F;ehzyil&#x2F;data&#x2F;docker_data&#x2F;memos&#x2F;:&#x2F;var&#x2F;opt&#x2F;memos ghcr.io&#x2F;usememos&#x2F;memos:latest 8.安装 nginx-proxy-manager 新建docker-compose.yml文件 vi docker-compose.yml version: &quot;3&quot; services: app: image: &#39;jc21&#x2F;nginx-proxy-manager:latest&#39; restart: unless-stopped ports: - &#39;80:80&#39; - &#39;443:443&#39; - &#39;81:81&#39; volumes: - &#x2F;home&#x2F;ehzyil&#x2F;data&#x2F;docker_data&#x2F;ng&#x2F;data:&#x2F;data - &#x2F;home&#x2F;ehzyil&#x2F;data&#x2F;docker_data&#x2F;ng&#x2F;letsencrypt:&#x2F;etc&#x2F;letsencrypt 启动容器 docker-compose up -d 在确保云服务器防火墙已经放行80、81和443端口后即可访问服务器ip:81 进入ngingx_proxy_manager的web管理界面。默认密码为： Email: admin@example.com Password: changeme 9.安装rustdeskversion: &#39;3&#39; networks: rustdesk-net: external: false services: hbbs: container_name: hbbs ports: - 21115:21115 - 21116:21116 - 21116:21116&#x2F;udp - 21118:21118 image: rustdesk&#x2F;rustdesk-server:latest command: hbbs -r 520217.xyz:21117 volumes: - .&#x2F;rustdesk:&#x2F;root networks: - rustdesk-net depends_on: - hbbr restart: unless-stopped hbbr: container_name: hbbr ports: - 21117:21117 - 21119:21119 image: rustdesk&#x2F;rustdesk-server:latest command: hbbr volumes: - .&#x2F;rustdesk:&#x2F;root networks: - rustdesk-net restart: unless-stopped 打开防火墙的端口 21115、21116、21117、21118、21119 默认情况下，hbbs 监听 21115(tcp), 21116(tcp/udp), 21118(tcp)，hbbr 监听 21117(tcp), 21119(tcp)。 务必在防火墙开启这几个端口， **请注意 21116 同时要开启 TCP 和 UDP**。 其中 21115 是 hbbs 用作 NAT 类型测试，21116/UDP 是 hbbs 用作 ID 注册与心跳服务，21116/TCP 是 hbbs 用作 TCP 打洞与连接服务，21117 是 hbbr 用作中继服务，21118 和 21119 是为了支持网页客户端。 如果您不需要网页客户端（21118，21119）支持，对应端口可以不开。 卸载 cd &#x2F;data # 进入docker-compose所在的文件夹 docker compose down # 停止容器，此时不会删除映射到本地的数据 rm -rf &#x2F;data # 完全删除映射到本地的数据 加密 version: &#39;3&#39; networks: rustdesk-net: external: false services: hbbs: container_name: hbbs ports: - 21115:21115 - 21116:21116 - 21116:21116&#x2F;udp - 21118:21118 image: rustdesk&#x2F;rustdesk-server:latest command: hbbs -r 520217.xyz:21117 -k volumes: - .&#x2F;rustdesk:&#x2F;root networks: - rustdesk-net depends_on: - hbbr restart: unless-stopped hbbr: container_name: hbbr ports: - 21117:21117 - 21119:21119 image: rustdesk&#x2F;rustdesk-server:latest command: hbbr -k volumes: - .&#x2F;rustdesk:&#x2F;root networks: - rustdesk-net restart: unless-stopped 加密后无法使用 重新回复","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"https://blog.ehzyil.xyz/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"author":"ehzyil"},{"title":"自建rss聚合阅读工具","slug":"2023/自建rss聚合阅读工具","date":"2023-10-22T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/10/22/2023/自建rss聚合阅读工具/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/22/2023/%E8%87%AA%E5%BB%BArss%E8%81%9A%E5%90%88%E9%98%85%E8%AF%BB%E5%B7%A5%E5%85%B7/","excerpt":"自建rss聚合阅读工具，极其轻量化配合Render⚡️一键部署，还能绑定自定义域名","text":"自建rss聚合阅读工具，极其轻量化配合Render⚡️一键部署，还能绑定自定义域名 前置准备1.一个Render账号Render是一款可以白嫖的Paas提供商，不需要绑卡什么的，可以白嫖😄😄😄。 2.一个GitHub账号 GitHub是一个可以保存自己代码的地方，代码托管。 3.一个托管了域名的CloudFlare 账号(可选) CloudFlare 是一家全球知名的CDN服务商，并且提供了免费的 CDN 套餐，还不限流量，所以我们完全不需要花一分钱就能使用它的 CDN 服务。 开始搭建Fork源代码到自己的仓库项目名字：rss-reader https://github.com/srcrs/rss-reader 进入上方链接进入该源码的Github仓库 Fork到自己仓库。 因为当前的代码配置完全是原作者的，我们可以配置成自己喜欢的rss。具体如何修改README.md中都有教程自己配置。 修改方式有两种，可以自己把代码拉到本地修改完再commit或者直接在自己的代码库修改。 使用Render进行一键部署进入Render的Dashboard点击New，如下图所示 点击Web Service 绑定自己的Github后操作如下： 找到你要部署的分支： 填写部署的项目名称和部署方式，Docker 选择实例类型，方然是Free白嫖了 点击最下方的Create Web Service等待部署完成即可. 部署完成后可以点击蓝色连接即可访问。 至此该项目搭建完成。 自定义域名（可选）Render免费提供部署好的项目的域名🌹，看着太长不好记或者不喜欢可以更改成自己的域名。 进入Render进行以下操作 然后登录CloudFlare去你所要绑定域名的界面进行以下操作，红色框里的内容由上图复制。 红色框里的Type改成CNAME（懒得编辑图片了） 然后回到Render，点击Verify 显示如下，域名就绑定成功了 现在就可以从自己配置的域名访问了。","categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"rss","slug":"rss","permalink":"https://blog.ehzyil.xyz/tags/rss/"},{"name":"render","slug":"render","permalink":"https://blog.ehzyil.xyz/tags/render/"}],"author":"ehzyil"},{"title":"JVM学习笔记","slug":"2023/JVM学习笔记","date":"2023-10-19T00:00:00.000Z","updated":"2024-06-17T01:04:53.975Z","comments":true,"path":"2023/10/19/2023/JVM学习笔记/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/19/2023/JVM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"初始JVM什么是JVMJVM 全称是 Java Virtual Machine，中文译名 Java虚拟机。 JVM 本质上是一个运行在计算机上的程序，他的职责是运行Java字节码文件。 JVM 的功能解释和运行 对字节码文件中的指令实时的解释成机器码让计算机执行 内存管理 自动为对象、方法等分配内存 自动的垃圾回收机制，回收不再使用的对象 即时编译 ——主要是为了支持跨平台特性。 对热点代码进行优化提升执行效率 JVM提供了即时编译（Just-In-Time 简称JIT) 进行性能的优化，最终能达到接近C、C++语言的运行性能 甚至在特定场景下实现超越。 常见的JVM常见的JVM有HotSpot、GraalVM、OpenJ9等，另外DragonWell龙井JDK也 提供了一款功能增强版的JVM。其中使用最广泛的是HotSpot虚拟机。 常见的JVM: Java虚拟机规范 《Java虚拟机规范》由Oracle制定，内容主要包含了Java虚拟机在设计和实现时需要遵守的规范，主 要包含class字节码文件的定义、类和接口的加载和初始化、指令集等内容。 《Java虚拟机规范》是对虚拟机设计的要求，而不是对Java设计的要求，也就是说虚拟机可以运行在 其他的语言比如Groovy、Scala生成的class字节码文件之上。 官网地址：https://docs.oracle.com/javase/specs/index.htm 详解字节码文件Java内存区域垃圾回收","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"https://blog.ehzyil.xyz/tags/JVM/"}],"author":"ehzyil"},{"title":"LeetCode刷题笔记","slug":"数据结构与算法/LeetCode刷题笔记","date":"2023-10-19T00:00:00.000Z","updated":"2024-06-17T01:04:54.011Z","comments":true,"path":"2023/10/19/数据结构与算法/LeetCode刷题笔记/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/19/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/LeetCode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/","excerpt":"","text":"1、两数之和给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target 的那 两个 整数，并返回它们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。 你可以按任意顺序返回答案。 示例 1： 输入：nums &#x3D; [2,7,11,15], target &#x3D; 9 输出：[0,1] 解释：因为 nums[0] + nums[1] &#x3D;&#x3D; 9 ，返回 [0, 1] 。 示例 2： 输入：nums &#x3D; [3,2,4], target &#x3D; 6 输出：[1,2] 示例 3： 输入：nums &#x3D; [3,3], target &#x3D; 6 输出：[0,1] 提示： 2 &lt;= nums.length &lt;= 104 -109 &lt;= nums[i] &lt;= 109 -109 &lt;= target &lt;= 109 只会存在一个有效答案 解法一 ​ 双重for循环遍历获取两个元素判断两个元素相加是否满足目标值，返回索引值。 class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; // 创建一个长度为2的整型数组，用于存储结果 int[] arr = new int[2]; // 遍历给定的nums数组 for (int i = 0; i &lt; nums.length; i++) &#123; // 内层循环遍历索引范围为0到i之间的元素 for (int j = 0; j &lt; i; j++) &#123; // 如果当前元素nums[i]和nums[j]的和等于目标值target if (nums[i] + nums[j] == target) &#123; // 将当前的i和j作为结果存入数组arr中 arr[0] = i; arr[1] = j; &#125; &#125; &#125; return arr; &#125; &#125; 时间复杂度为O($n^{2}$) 解法二 循环遍历数组元素，计算出target减去arr[i]的值另一个数的值将其存入map，每次循环判断map中是否有可以满足条件的值，若有就返回，否则继续循环。 如何构建map？ map常用于存储键值对，就本题而言，我们可以把数组元素的值当做键，索引当做值。 class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; //map用与存储值-索引 若target-nums[i]的值(map的键在存在说明错在另一个值可以凑成target) // 这样只需要一个for循环 HashMap&lt;Integer, Integer> map = new HashMap&lt;Integer, Integer>(); int[] arr = new int[2]; for (int i = 0; i &lt; nums.length; i++) &#123; // 判断HashMap中是否存在键为target - nums[i]的元素 if (map.containsKey(target - nums[i])) &#123; // 如果存在，则将当前索引i和HashMap中对应的值作为结果存入数组arr中 arr[0] = i; arr[1] = map.get(target - nums[i]); // 跳出循环，结束查找 break; &#125; // 将当前元素nums[i]作为键，将当前索引i作为值，存入HashMap中 map.put(nums[i], i); &#125; return arr; &#125; &#125; 解法三 ​ 现将数组排序,使用二分法思想寻找两边界指向元素值的和于target比较，通过不断缩小边界来寻找目标值， class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Arrays.sort(nums); int left = 0, right = nums.length - 1; int[] arr = new int[2]; while (left &lt; right) &#123; int sum = nums[left] + nums[right]; if (sum &lt; target) &#123; left++; &#125; else if (sum > target) &#123; right--; &#125; else if (sum == target) &#123; arr[0] = left; arr[1] = right; return arr; &#125; &#125; return new int[]&#123;&#125;; &#125; &#125; 2、两数相加给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。 请你将两个数相加，并以相同形式返回一个表示和的链表。 你可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例 1： 输入：l1 &#x3D; [2,4,3], l2 &#x3D; [5,6,4] 输出：[7,0,8] 解释：342 + 465 &#x3D; 807. 示例 2： 输入：l1 &#x3D; [0], l2 &#x3D; [0] 输出：[0] 示例 3： 输入：l1 &#x3D; [9,9,9,9,9,9,9], l2 &#x3D; [9,9,9,9] 输出：[8,9,9,9,0,0,0,1] 提示： 每个链表中的节点数在范围 [1, 100] 内 0 &lt;= Node.val &lt;= 9 题目数据保证列表表示的数字不含前导零 解法一 class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode p1 = l1, p2 = l2; //创建一个头结点来作为头节点 可以避免处理初始的空指针情况 ListNode dummy = new ListNode(-1); //p作为指针移动 用来添加元素 ListNode p = dummy; //存放进位的数 int carry = 0; //当有链表不为空或者有进位的数位加入链表 while (p1 != null || p2 != null || carry > 0) &#123; int sum = carry; //加上p1的值并让p1指针后移 if (p1 != null) &#123; sum += p1.val; p1 = p1.next; &#125; //加上p2的值并让p1指针后移 if (p2 != null) &#123; sum += p2.val; p2 = p2.next; &#125; //模10 得到进位的数 carry = sum / 10; //除10 得到余数 sum = sum % 10; //将余数加入链表 链表指针后移 p.next = new ListNode(sum); p = p.next; &#125; return dummy.next; &#125; &#125; 3、无重复字符的最长子串给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。 示例 1: 输入: s &#x3D; &quot;abcabcbb&quot; 输出: 3 解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。 示例 2: 输入: s &#x3D; &quot;bbbbb&quot; 输出: 1 解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。 示例 3: 输入: s &#x3D; &quot;pwwkew&quot; 输出: 3 解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。 提示： 0 &lt;= s.length &lt;= 5 * 104 s 由英文字母、数字、符号和空格组成 解法一 public static int lengthOfLongestSubstring(String s) &#123; int res = 0; // 用于存储最大长度 for (int i = 0; i &lt; s.length(); i++) &#123; // 外层循环，遍历字符串的每个字符 boolean[] book = new boolean[300]; // 用于记录字符是否出现过的数组 for (int j = i; j > 0; j--) &#123; // 内层循环，从当前字符开始向前遍历 if (book[s.charAt(j)]) &#123; // 如果当前字符已经出现过，说明已经找到以当前字符为结尾的最长子串 break; // 结束内层循环 &#125; book[s.charAt(j)] = true; // 将当前字符标记为已出现 res = Math.max(res, i - j + 1); &#125; &#125; return res; // 返回最大长度 &#125; 内层for循环递减的原因: 为了找到以当前字符为结尾的最长子串。如果内层for循环递增，那么每次循环都是从字符串的开头开始，这样就无法找到以当前字符为结尾的最长子串。而递减的话，每次循环都是从当前字符开始，逐渐向前遍历，可以找到以当前字符为结尾的最长子串。 下列代码的意思是获取字符串j索引位置的值，因为是char类型当将其放入数组是会转换层int型比如 a(char)-&gt;99(int) book[s.charAt(j)] &#x3D; true; 解法二 我们知道，题目要我们求解的是一个区间，我们可以考虑设置两个指针，一个指向这个区间的开头，另外一个则指向这个区间的结尾。我们让头指针不动，向后扩展尾指针，直到遇见重复元素 (见下图) public static int lengthOfLongestSubstring(String s) &#123; int res = 0; // 用于存储最大长度 Map&lt;Character, Integer> window = new HashMap&lt;>(); //i是左指针，j是右指针 for (int i = 0, j = 0; j &lt; s.length(); j++) &#123; //若s.charAt(j)已在window中则移动i的值到第一次出现位置的右侧 if (window.containsKey(s.charAt(j))) &#123; i = Math.max(i, window.get(s.charAt(j)) + 1); &#125; res = Math.max(res, j - i + 1); window.put(s.charAt(j), j); &#125; return res; // 返回最大长度 &#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.ehzyil.xyz/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://blog.ehzyil.xyz/tags/Leetcode/"}],"author":"ehzyil"},{"title":"CentOS下的软件","slug":"2023/CentOS下的软件","date":"2023-10-17T00:00:00.000Z","updated":"2024-06-17T01:04:53.975Z","comments":true,"path":"2023/10/17/2023/CentOS下的软件/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/17/2023/CentOS%E4%B8%8B%E7%9A%84%E8%BD%AF%E4%BB%B6/","excerpt":"","text":"CentOS下MySQLMySQL8.0.27 安装[root@localhost ~]# uname -m x86_64 [root@localhost ~]# cat &#x2F;etc&#x2F;redhat-release CentOS Linux release 7.9.2009 (Core) #解压 tar -zxvf mysql-8.0.27-1.el7.x86_64.rpm-bundle.tar tar -xvf mysql-8.0.27-1.el7.x86_64.rpm-bundle.tar\\ #安装顺序 rpm -ivh mysql-community-common-8.0.27-1.el7.x86_64.rpm rpm -ivh mysql-community-client-plugins-8.0.27-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-8.0.27-1.el7.x86_64.rpm yum install openssl-devel -y rpm -ivh mysql-community-devel-8.0.27-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-compat-8.0.27-1.el7.x86_64.rpm rpm -ivh mysql-community-client-8.0.27-1.el7.x86_64.rpm yum install net-tools yum install -y perl-Module-Install.noarch rpm -ivh mysql-community-server-8.0.27-1.el7.x86_64.rpm MySQL8.0主从复制的配置&#x2F;&#x2F;主机 CREATE USER &#39;ehzyil&#39;@&#39;%&#39; IDENTIFIED BY &#39;Li021712.&#39;; GRANT REPLICATION SLAVE ON *.* TO &#39;ehzyil&#39;@&#39;%&#39;; &#x2F;&#x2F;从机 CHANGE REPLICATION SOURCE TO SOURCE_HOST&#x3D;&#39;192.168.154.141&#39;,SOURCE_PORT&#x3D;3306,SOURCE_USER&#x3D;&#39;ehzyil&#39;,SOURCE_PASSWORD&#x3D;&#39;Li021712.&#39;,SOURCE_LOG_FILE&#x3D;&#39;mysql-bin.000004&#39;,SOURCE_LOG_POS&#x3D;685; start slave; ​ 若查询slave状态，出现以下情况 mysql&gt; show slave status\\G; *************************** 1. row *************************** Slave_IO_State: Connecting to source Master_Host: 192.168.154.141 Master_User: ehzyil Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000004 Read_Master_Log_Pos: 685 Relay_Log_File: localhost-relay-bin.000001 Relay_Log_Pos: 4 Relay_Master_Log_File: mysql-bin.000004 Slave_IO_Running: Connecting Slave_SQL_Running: Yes ... Last_IO_Errno: 2061 Last_IO_Error: error connecting to master &#39;ehzyil@192.168.154.141:3306&#39; - retry-time: 60 retries: 1 message: Authentication plugin &#39;caching_sha2_password&#39; reported error: Authentication requires secure connection. ... 分析报错，应该是连接的用户配置有问题。 所以去主库mysql上查询当前用户信息 SELECT plugin FROM user where user &#x3D; ‘ehzyil’; 原来是主库repluser的plugin是caching_sha2_password 导致连接不上，修改为mysql_native_password即可解决。 ALTER USER ‘ehzyil‘@’%’ IDENTIFIED WITH mysql_native_password BY ‘Li021712.’; mysql&gt; use mysql; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql&gt; SELECT plugin FROM &#96;user&#96; where user &#x3D; &#39;ehzyil&#39;; +-----------------------+ | plugin | +-----------------------+ | caching_sha2_password | +-----------------------+ 1 row in set (0.00 sec) mysql&gt; ALTER USER &#39;ehzyil&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;Li021712.&#39;; Query OK, 0 rows affected (0.00 sec) 修改后，重新配置从库binlog位置后重启状态展示如下： Slave_IO_State: Waiting for source to send event Slave_IO_Running: Yes Slave_SQL_Running: Yes 看到以上三行配置相同，则主从连接成功。 MySQL清除主从复制关系mysql主从复制中，需要将主从复制关系清除，需要取消其从库角色。这可通过执行RESET SLAVE ALL清除从库的同步复制信息、包括连接信息和二进制文件名、位置。从库上执行这个命令后，使用show slave status将不会有输出。reset slave是各版本Mysql都有的功能，在stop slave之后使用。主要做：删除master.info和relay-log.info文件；删除所有的relay log（包括还没有应用完的日志），创建一个新的relay log文件；从Mysql 5.5开始，多了一个all参数。如果不加all参数，那么所有的连接信息仍然保留在内存中，包括主库地址、端口、用户、密码等。这样可以直接运行start slave命令而不必重新输入change master to命令，而运行show slave status也仍和没有运行reset slave一样，有正常的输出。但如果加了all参数，那么这些内存中的数据也会被清除掉，运行show slave status就输出为空了。 mysql&gt; stop slave; Query OK, 0 rows affected, 2 warnings (0.00 sec) mysql&gt; reset slave all; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql&gt; show slave status\\G Empty set, 1 warning (0.00 sec) nginxCentOS下Nginx的安装安装 nginx 编译需要依赖 gcc 环境: yum install gcc-c++ 安装两个安装包pcre和pcre-devel： yum install -y pcre pcre-devel 安装zlib，Nginx的各种模块中需要使用gzip压缩： yum install -y zlib zlib-devel 如果使用了 https，需要安装 OpenSSL 库。安装指令如下： yum install -y openssl openssl-devel 一键安装上面四个依赖（追求安装速度可直接用这条指令） yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel 进入usr&#x2F;local 文件路径(路径可自选)创建一个文件夹nginx保存使用wget下载的nginx压缩包 &#x2F;&#x2F;下载nginx压缩包 wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.24.0.tar.gz tar -zxvf nginx-1.24.0.tar.gz -C &#x2F;usr&#x2F;local&#x2F; cd &#x2F;usr&#x2F;local&#x2F;nginx-1.24.0&#x2F; &#x2F;&#x2F;创建一个文件夹 mkdir &#x2F;usr&#x2F;local&#x2F;nginx &#x2F;&#x2F;安装前检查工作 .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx &#x2F;&#x2F;安装 make &amp;&amp; make install &#x2F;&#x2F;文件目录树状图 nginx ├── conf &lt;-- Nginx配置文件 │ ├── fastcgi.conf │ ├── fastcgi.conf.default │ ├── fastcgi_params │ ├── fastcgi_params.default │ ├── koi-utf │ ├── koi-win │ ├── mime.types │ ├── mime.types.default │ ├── nginx.conf &lt;-- 这个文件我们经常操作 │ ├── nginx.conf.default │ ├── scgi_params │ ├── scgi_params.default │ ├── uwsgi_params │ ├── uwsgi_params.default │ └── win-utf ├── html &lt;-- 存放静态文件，我们后期部署项目，就要将静态文件放在这 │ ├── 50x.html │ └── index.html &lt;-- 提供的默认的页面 ├── logs └── sbin └── nginx &lt;-- 启动文件 配置环境变量 使用vim /etc/profile命令打开配置文件，并配置环境变量，保存并退出 之后重新加载配置文件，使用source /etc/profile命令，然后我们在任意位置输入nginx即可启动服务，nginx -s stop即可停止服务 Nginx常用命令 启动Nginx： nginx 启动Nginx服务器。 停止Nginx： nginx -s stop 停止正在运行的Nginx服务器。 重新加载Nginx配置： nginx -s reload 重新加载Nginx的配置文件，使新的配置生效，而无需停止服务器。 检查Nginx配置是否正确： nginx -t 检查Nginx的配置文件语法是否正确，如果有错误，它会显示错误信息。 查看Nginx版本：nginx -v 或 nginx -V 第一个命令会显示Nginx的版本号，第二个命令会显示编译时的详细配置信息。 查看Nginx进程状态： nginx -s status 显示Nginx进程的状态，包括正在运行的进程数量和其它相关信息。 重新打开日志文件： nginx -s reopen 重新打开Nginx的日志文件，可以用于实现日志文件的切割和归档。 CentOS 7CentOS 7上防火墙设置在CentOS 7上设置防火墙可以使用firewalld服务进行管理。 以下是在CentOS 7上设置防火墙的一些常用操作： 检查防火墙状态： sudo firewall-cmd --state 这将显示防火墙的当前状态，如果防火墙已启用，它会显示”running”。 查看防火墙规则： sudo firewall-cmd --list-all 这将显示防火墙的当前规则列表。 启动防火墙服务： sudo systemctl start firewalld 停止防火墙服务： sudo systemctl stop firewalld 设置防火墙开机启动： sudo systemctl enable firewalld 停用防火墙开机启动： sudo systemctl disable firewalld 开放端口： sudo firewall-cmd --zone&#x3D;public --add-port&#x3D;&lt;port&gt;&#x2F;tcp --permanent 将&lt;port&gt;替换为要开放的端口号。此命令会永久性地在防火墙中开放指定的端口。 移除已开放的端口： sudo firewall-cmd --zone&#x3D;public --remove-port&#x3D;&lt;port&gt;&#x2F;tcp --permanent 将&lt;port&gt;替换为要移除的端口号。此命令会永久性地从防火墙中移除指定的端口。 重新加载防火墙配置： sudo firewall-cmd --reload 在修改防火墙规则后，使用此命令重新加载配置，使更改生效。 10.验证规则是否生效： sudo firewall-cmd --zone&#x3D;public --list-all 检查规则列表以确认添加或删除的规则已正确应用。 查看yum已安装的包在linux下如何使用yum查看安装了哪些软件包 列出所有已安装的软件包yum list installed yum针对软件包操作常用命令： 1.使用 yum 查找软件包命令：yum search 2.列出所有可安装的软件包命令： yum list 3.列出所有可更新的软件包命令： yum list updates 4.列出所有已安装的软件包命令： yum list installed yum 语法yum [options] [command] [package ...] options 可选，选项包括 -h（帮助），-y（当安装过程提示选择全部为”yes”），-q（不显示安装的过程）等等。 command 要进行的操作。 package 操作的对象。 其它的例子就不列举了，这里说一下查看yum安装了哪些软件包 查看 yum 已安装 docker 的包 yum list installed docker 查看 yum 已安装 docker相关的包 yum list installed docker* [root@iZ2ze552628d2crm1m97ppZ ~]# yum list installed docker* 已加载插件：fastestmirror Loading mirror speeds from cached hostfile 已安装的软件包 docker-buildx-plugin.x86_64 0.11.2-1.el7 @docker-ce-stable docker-ce.x86_64 3:24.0.6-1.el7 @docker-ce-stable docker-ce-cli.x86_64 1:24.0.6-1.el7 @docker-ce-stable docker-ce-rootless-extras.x86_64 24.0.6-1.el7 @docker-ce-stable docker-compose-plugin.x86_64 2.21.0-1.el7 @docker-ce-stable [root@iZ2ze552628d2crm1m97ppZ ~]#","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ehzyil.xyz/categories/Linux/"}],"tags":[{"name":"CentOS","slug":"CentOS","permalink":"https://blog.ehzyil.xyz/tags/CentOS/"},{"name":"软件","slug":"软件","permalink":"https://blog.ehzyil.xyz/tags/%E8%BD%AF%E4%BB%B6/"}],"author":"ehzyil"},{"title":"SpringBoot项目中集成第三方登录功能","slug":"2023/SpringBoot项目中集成第三方登录功能","date":"2023-10-17T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/10/17/2023/SpringBoot项目中集成第三方登录功能/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/17/2023/SpringBoot%E9%A1%B9%E7%9B%AE%E4%B8%AD%E9%9B%86%E6%88%90%E7%AC%AC%E4%B8%89%E6%96%B9%E7%99%BB%E5%BD%95%E5%8A%9F%E8%83%BD/","excerpt":"","text":"TODO！！！","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"}],"author":"ehzyil"},{"title":"基于@ControllerAdvice实现全局异常处理","slug":"2023/基于@ControllerAdvice实现全局异常处理","date":"2023-10-17T00:00:00.000Z","updated":"2024-06-17T01:04:53.991Z","comments":true,"path":"2023/10/17/2023/基于@ControllerAdvice实现全局异常处理/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/17/2023/%E5%9F%BA%E4%BA%8E@ControllerAdvice%E5%AE%9E%E7%8E%B0%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/","excerpt":"","text":"RestControllerAdvice注解与全局异常处理1.前置知识@RestControllerAdvice 和 @ExceptionHandler 是 Spring 框架中用于处理全局异常的注解。 @RestControllerAdvice: @RestControllerAdvice 是一个组合注解，结合了 @ControllerAdvice 和 @ResponseBody。它用于全局处理控制器层（Controller）抛出的异常，可以将处理结果直接以 JSON 格式返回给客户端。 具体用途包括但不限于： 全局异常处理：捕获所有 Controller 层抛出的异常，统一进行处理，避免异常直接传递到客户端。 全局数据绑定：可以在这里对请求参数进行全局的预处理或校验。 全局数据预处理：可以在这里对返回的数据进行统一处理。 @ExceptionHandler: @ExceptionHandler 是一个局部异常处理的注解，它可以用在控制器层的方法上，用于处理指定类型的异常。 具体用途包括但不限于： 处理特定类型的异常：将指定类型的异常捕获，然后执行相应的处理逻辑。 提供自定义的异常处理逻辑：可以根据业务需求编写特定的异常处理代码。 2.举例 举例： @RestControllerAdvice public class GlobalExceptionHandler &#123; @ExceptionHandler(Exception.class) public ResponseEntity&lt;String> handleException(Exception e) &#123; return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(\"Internal Server Error\"); &#125; @ExceptionHandler(UserNotFoundException.class) public ResponseEntity&lt;String> handleUserNotFoundException(UserNotFoundException e) &#123; return ResponseEntity.status(HttpStatus.NOT_FOUND).body(\"User Not Found\"); &#125; &#125; 在上面的例子中，GlobalExceptionHandler 类使用 @RestControllerAdvice 注解来指示它是一个全局异常处理器。然后，使用 @ExceptionHandler 注解来定义处理特定类型异常的方法。例如，handleException 方法处理所有类型的异常，而 handleUserNotFoundException 方法只处理 UserNotFoundException 类型的异常。 总的来说，@RestControllerAdvice 和 @ExceptionHandler 的组合允许你在应用程序中建立一套全局的异常处理机制，提高了代码的可维护性和健壮性。 3.代码实现自定义业务异常类ServiceException package com.ehzyil.exception; import lombok.Getter; import static com.ehzyil.enums.StatusCodeEnum.FAIL; /** * 业务异常 **/ @Getter public final class ServiceException extends RuntimeException &#123; /** * 返回失败状态码 */ private Integer code = FAIL.getCode(); /** * 返回信息 */ private final String message; public ServiceException(String message) &#123; this.message = message; &#125; &#125; 全局异常处理类GlobalExceptionHandler.java package com.ehzyil.handler; import cn.dev33.satoken.exception.DisableServiceException; import cn.dev33.satoken.exception.NotLoginException; import cn.dev33.satoken.exception.NotPermissionException; import cn.dev33.satoken.exception.NotRoleException; import com.ehzyil.exception.ServiceException; import com.ehzyil.model.vo.Result; import org.springframework.web.bind.MethodArgumentNotValidException; import org.springframework.web.bind.annotation.ExceptionHandler; import org.springframework.web.bind.annotation.RestControllerAdvice; import java.util.Objects; import static com.ehzyil.enums.StatusCodeEnum.*; /** * @author ehyzil * @Description 全局异常处理 * @create 2023-09-2023/9/30-22:47 */ @RestControllerAdvice public class GlobalExceptionHandler &#123; /** * 处理业务异常 * * @param e * @return */ @ExceptionHandler(value = ServiceException.class) public Result&lt;?> handleServiceException(ServiceException e) &#123; return Result.fail(e.getMessage()); &#125; /** * 处理Assert异常 */ @ExceptionHandler(value = IllegalArgumentException.class) public Result&lt;?> handleIllegalArgumentException(IllegalArgumentException e) &#123; return Result.fail(e.getMessage()); &#125; /** * 处理参数校验异常 */ @ExceptionHandler(value = MethodArgumentNotValidException.class) public Result&lt;?> handleMethodArgumentNotValidException(MethodArgumentNotValidException e) &#123; return Result.fail(VALID_ERROR.getCode(), Objects.requireNonNull(e.getBindingResult().getFieldError()).getDefaultMessage()); &#125; /** * 处理权限不足 */ @ExceptionHandler(value = NotPermissionException.class) public Result&lt;?> handleNotPermissionException() &#123; return Result.fail(\"权限不足\"); &#125; /** * 处理账号封禁 */ @ExceptionHandler(value = DisableServiceException.class) public Result&lt;?> handleDisableServiceExceptionException() &#123; return Result.fail(\"此账号已被禁止访问服务\"); &#125; /** * 处理无此角色异常 */ @ExceptionHandler(value = NotRoleException.class) public Result&lt;?> handleNotRoleException() &#123; return Result.fail(\"权限不足\"); &#125; /** * 处理SaToken异常 */ @ExceptionHandler(value = NotLoginException.class) public Result&lt;?> handlerNotLoginException(NotLoginException nle) &#123; // 判断场景值，定制化异常信息 String message; if (nle.getType().equals(NotLoginException.NOT_TOKEN)) &#123; message = \"未提供token\"; &#125; else if (nle.getType().equals(NotLoginException.INVALID_TOKEN)) &#123; message = \"token无效\"; &#125; else if (nle.getType().equals(NotLoginException.TOKEN_TIMEOUT)) &#123; message = \"token已过期\"; &#125; else &#123; message = \"当前会话未登录\"; &#125; // 返回给前端 return Result.fail(UNAUTHORIZED.getCode(), message); &#125; /** * 处理系统异常 */ @ExceptionHandler(value = Exception.class) public Result&lt;?> handleSystemException() &#123; return Result.fail(SYSTEM_ERROR.getCode(), SYSTEM_ERROR.getMsg()); &#125; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"异常","slug":"异常","permalink":"https://blog.ehzyil.xyz/tags/%E5%BC%82%E5%B8%B8/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"}],"author":"ehzyil"},{"title":"基于AOP实现访客日志记录","slug":"2023/基于AOP实现访客日志记录","date":"2023-10-17T00:00:00.000Z","updated":"2024-06-17T01:04:53.991Z","comments":true,"path":"2023/10/17/2023/基于AOP实现访客日志记录/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/17/2023/%E5%9F%BA%E4%BA%8EAOP%E5%AE%9E%E7%8E%B0%E8%AE%BF%E5%AE%A2%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95/","excerpt":"","text":"基于AOP实现访客日志记录1.准备所需的日志表create table t_visit_log ( id int auto_increment comment 'id' primary key, page varchar(50) null comment '访问页面', ip_address varchar(50) null comment '访问ip', ip_source varchar(50) null comment '访问地址', os varchar(50) null comment '操作系统', browser varchar(50) null comment '浏览器', create_time datetime not null comment '访问时间' ); 2.pom依赖… 3.实体/** * 访问日志 */ @Data public class VisitLog &#123; /** * id */ @TableId(type = IdType.AUTO) private Integer id; /** * 访问页面 */ private String page; /** * 访问ip */ private String ipAddress; /** * 访问地址 */ private String ipSource; /** * 操作系统 */ private String os; /** * 浏览器 */ private String browser; /** * 访问时间 */ @TableField(fill = FieldFill.INSERT) private LocalDateTime createTime; &#125; 4.自定义日志注解/** * @author ehyzil * @Description 访问日志注解 * @create 2023-10-2023/10/1-20:37 */ @Documented @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface VisitLogger &#123; /** * * @return 访问页面 */ String value() default \"\"; &#125; 5.Spring实例获取工具类// 这是一个实用工具类，用于与Spring容器交互，获取Bean实例等功能。 public final class SpringUtils implements BeanFactoryPostProcessor &#123; private static ConfigurableListableBeanFactory beanFactory; @Override public void postProcessBeanFactory(@NotNull ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; SpringUtils.beanFactory = beanFactory; &#125; /** * 获取指定名称的对象 * @param name 对象的名称 * @return 获取到的对象实例 */ @SuppressWarnings(\"unchecked\") public static &lt;T> T getBean(String name) throws BeansException &#123; return (T) beanFactory.getBean(name); &#125; /** * 获取指定类型的对象 * @param clz 对象的类型 * @return 获取到的对象实例 */ public static &lt;T> T getBean(Class&lt;T> clz) throws BeansException &#123; return beanFactory.getBean(clz); &#125; /** * 获取指定名称的Bean的类型 * @param name Bean的名称 * @return Bean的类型 * @throws NoSuchBeanDefinitionException 如果找不到对应的Bean定义 */ public static Class&lt;?> getType(String name) throws NoSuchBeanDefinitionException &#123; return beanFactory.getType(name); &#125; &#125; 6.IP地址工具类package com.ehzyil.utils; import com.ehzyil.exception.ServiceException; import org.lionsoul.ip2region.xdb.Searcher; import org.springframework.core.io.ClassPathResource; import org.springframework.util.FileCopyUtils; import org.springframework.util.StringUtils; import javax.servlet.http.HttpServletRequest; import java.io.IOException; import java.io.InputStream; import java.net.InetAddress; import java.net.UnknownHostException; import java.util.Objects; /** * IP地址工具类 * * @author ehzyil */ @SuppressWarnings(\"all\") public class IpUtils &#123; private static Searcher searcher; static &#123; // 解决项目打包找不到ip2region.xdb try &#123; InputStream inputStream = new ClassPathResource(\"/ipdb/ip2region.xdb\").getInputStream(); //将 ip2region.db 转为 ByteArray byte[] cBuff = FileCopyUtils.copyToByteArray(inputStream); searcher = Searcher.newWithBuffer(cBuff); &#125; catch (IOException e) &#123; throw new ServiceException(\"ip2region.xdb加载失败\"); &#125; &#125; /** * 在Nginx等代理之后获取用户真实IP地址 */ public static String getIpAddress(HttpServletRequest request) &#123; String ip; try &#123; ip = request.getHeader(\"X-Real-IP\"); if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) &#123; ip = request.getHeader(\"x-forwarded-for\"); &#125; if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) &#123; ip = request.getHeader(\"Proxy-Client-IP\"); &#125; if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) &#123; ip = request.getHeader(\"WL-Proxy-Client-IP\"); &#125; if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) &#123; ip = request.getHeader(\"HTTP_CLIENT_IP\"); &#125; if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) &#123; ip = request.getHeader(\"HTTP_X_FORWARDED_FOR\"); &#125; if (ip == null || ip.length() == 0 || \"unknown\".equalsIgnoreCase(ip)) &#123; ip = request.getRemoteAddr(); if (\"127.0.0.1\".equals(ip) || \"0:0:0:0:0:0:0:1\".equals(ip)) &#123; //根据网卡取本机配置的IP InetAddress inet = null; try &#123; inet = InetAddress.getLocalHost(); &#125; catch (UnknownHostException e) &#123; throw new UnknownHostException(\"无法确定主机的IP地址\"); &#125; ip = inet.getHostAddress(); &#125; &#125; // 使用代理，则获取第一个IP地址 if (!StringUtils.hasText(ip) &amp;&amp; Objects.requireNonNull(ip).length() > 15) &#123; int idx = ip.indexOf(\",\"); if (idx > 0) &#123; ip = ip.substring(0, idx); &#125; &#125; &#125; catch (Exception e) &#123; ip = \"\"; &#125; return ip; &#125; /** * 根据ip从 ip2region.db 中获取地理位置 * * @param ip * @return */ public static String getIpSource(String ip) &#123; try &#123; String address = searcher.searchByStr(ip); if (StringUtils.hasText(address)) &#123; address = address.replace(\"|0\", \"\"); address = address.replace(\"0|\", \"\"); return address; &#125; return address; &#125; catch (Exception e) &#123; return \"\"; &#125; &#125; &#125; 7.线程池配置ThreadPoolProperties类是一个自定义的属性类，用于配置线程池的相关属性。 package com.ican.config.properties; import lombok.Data; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.context.annotation.Configuration; /** * 线程池参数 **/ @Data @Configuration @ConfigurationProperties(prefix = \"thread.pool\") public class ThreadPoolProperties &#123; /** * 核心线程池大小 */ private int corePoolSize; /** * 最大可创建的线程数 */ private int maxPoolSize; /** * 队列最大长度 */ private int queueCapacity; /** * 线程池维护线程所允许的空闲时间 */ private int keepAliveSeconds; &#125; 线程池配置 threadPoolTaskExecutor()方法，使用@Bean注解将其声明为一个Bean。该方法返回了一个ThreadPoolTaskExecutor对象，用于创建线程池。 /** * 线程池配置 **/ @Configuration public class ThreadPoolConfig &#123; @Autowired private ThreadPoolProperties threadPoolProperties; /** * 创建线程池 * * @return 线程池 */ @Bean public ThreadPoolTaskExecutor threadPoolTaskExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); // 核心线程池大小 executor.setCorePoolSize(threadPoolProperties.getCorePoolSize()); // 最大可创建的线程数 executor.setMaxPoolSize(threadPoolProperties.getMaxPoolSize()); // 等待队列最大长度 executor.setQueueCapacity(threadPoolProperties.getQueueCapacity()); // 线程池维护线程所允许的空闲时间 executor.setKeepAliveSeconds(threadPoolProperties.getKeepAliveSeconds()); // 线程池对拒绝任务(无线程可用)的处理策略 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); return executor; &#125; /** * 执行周期性或定时任务 */ @Bean(name = \"scheduledExecutorService\") protected ScheduledExecutorService scheduledExecutorService() &#123; return new ScheduledThreadPoolExecutor(threadPoolProperties.getCorePoolSize(), new BasicThreadFactory.Builder().namingPattern(\"schedule-pool-%d\").daemon(true).build(), new ThreadPoolExecutor.CallerRunsPolicy()) &#123; @Override protected void afterExecute(Runnable r, Throwable t) &#123; super.afterExecute(r, t); ThreadUtils.printException(r, t); &#125; &#125;; &#125; &#125; 使用了ThreadPoolExecutor.CallerRunsPolicy()，表示当线程池无法接受新任务时，将任务回退到调用者线程中执行。 8.异步任务配置package com.ehzyil.manager; import com.ehzyil.utils.SpringUtils; import com.ehzyil.utils.ThreadUtils; import java.util.TimerTask; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; /** * 异步任务管理器 * * @author ehzyil */ public class AsyncManager &#123; /** * 单例模式，确保类只有一个实例 */ private AsyncManager() &#123; &#125; /** * 饿汉式，在类加载的时候立刻进行实例化 */ private static final AsyncManager INSTANCE = new AsyncManager(); public static AsyncManager getInstance() &#123; return INSTANCE; &#125; /** * 异步操作任务调度线程池 */ private final ScheduledExecutorService executor = SpringUtils.getBean(\"scheduledExecutorService\"); /** * 执行任务 * * @param task 任务 */ public void execute(TimerTask task) &#123; executor.schedule(task, 10, TimeUnit.MILLISECONDS); &#125; /** * 停止任务线程池 */ public void shutdown() &#123; ThreadUtils.shutdownAndAwaitTermination(executor); &#125; &#125; 线程工具类ThreadUtils package com.ehzyil.utils; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.util.concurrent.*; /** * 线程工具类 * * @author ehzyil */ public class ThreadUtils &#123; private static final Logger logger = LoggerFactory.getLogger(ThreadUtils.class); private static final long OVERTIME = 120; /** * 停止线程池 * 先使用shutdown, 停止接收新任务并尝试完成所有已存在任务. * 如果超时, 则调用shutdownNow, 取消在workQueue中Pending的任务,并中断所有阻塞函数. * 如果仍然超時，則強制退出. * 另对在shutdown时线程本身被调用中断做了处理. */ public static void shutdownAndAwaitTermination(ExecutorService pool) &#123; if (pool != null &amp;&amp; !pool.isShutdown()) &#123; pool.shutdown(); try &#123; if (!pool.awaitTermination(OVERTIME, TimeUnit.SECONDS)) &#123; pool.shutdownNow(); if (!pool.awaitTermination(OVERTIME, TimeUnit.SECONDS)) &#123; logger.info(\"Pool did not terminate\"); &#125; &#125; &#125; catch (InterruptedException ie) &#123; pool.shutdownNow(); Thread.currentThread().interrupt(); &#125; &#125; &#125; /** * 打印线程异常信息 */ public static void printException(Runnable r, Throwable t) &#123; if (t == null &amp;&amp; r instanceof Future&lt;?>) &#123; try &#123; Future&lt;?> future = (Future&lt;?>) r; if (future.isDone()) &#123; future.get(); &#125; &#125; catch (CancellationException ce) &#123; t = ce; &#125; catch (ExecutionException ee) &#123; t = ee.getCause(); &#125; catch (InterruptedException ie) &#123; Thread.currentThread().interrupt(); &#125; &#125; if (t != null) &#123; logger.error(t.getMessage(), t); &#125; &#125; &#125; 9.任务工厂配置/** * 异步工厂（产生任务用） */ public class AsyncFactory &#123; /** * 记录访问日志 * * @param visitLog 访问日志信息 * @return 任务task */ public static TimerTask recordVisit(VisitLog visitLog) &#123; return new TimerTask() &#123; @Override public void run() &#123; SpringUtils.getBean(VisitLogService.class).saveVisitLog(visitLog); &#125; &#125;; &#125; &#125; 核心切面配置/** * AOP记录访问日志 **/ @Aspect @Component public class VisitLogAspect &#123; @Pointcut(\"@annotation(com.ehzyil.annotation.VisitLogger)\") public void visitLogPointCut() &#123; &#125; /** * 连接点正常返回通知，拦截用户操作日志，正常执行完成后执行， 如果连接点抛出异常，则不会执行 * * @param joinPoint 切面方法的信息 * @param result 返回结果 */ @AfterReturning(value = \"visitLogPointCut()\", returning = \"result\") public void doAfterReturning(JoinPoint joinPoint, Object result) &#123; // 从切面织入点处通过反射机制获取织入点处的方法 MethodSignature signature = (MethodSignature) joinPoint.getSignature(); // 获取切入点所在的方法 Method method = signature.getMethod(); // 获取操作 VisitLogger visitLogger = method.getAnnotation(VisitLogger.class); // 获取request ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = Objects.requireNonNull(attributes).getRequest(); VisitLog visitLog = new VisitLog(); String ipAddress = IpUtils.getIpAddress(request); String ipSource = IpUtils.getIpSource(ipAddress); // 解析browser和os Map&lt;String, String> userAgentMap = UserAgentUtils.parseOsAndBrowser(request.getHeader(\"User-Agent\")); visitLog.setIpAddress(ipAddress); visitLog.setIpSource(ipSource); visitLog.setOs(userAgentMap.get(\"os\")); visitLog.setBrowser(userAgentMap.get(\"browser\")); visitLog.setPage(visitLogger.value()); // 保存到数据库 AsyncManager.getInstance().execute(AsyncFactory.recordVisit(visitLog)); &#125; &#125; 日志Service层/** * 访问业务接口 */ public interface VisitLogService extends IService&lt;VisitLog> &#123; /** * 保存访问日志 * * @param visitLog 访问日志信息 */ void saveVisitLog(VisitLog visitLog); &#125; @Service public class VisitLogServiceImpl extends ServiceImpl&lt;VisitLogMapper, VisitLog> implements VisitLogService &#123; @Autowired private VisitLogMapper visitLogMapper; @Override public void saveVisitLog(VisitLog visitLog) &#123; // 保存访问日志 visitLogMapper.insert(visitLog); &#125; 控制器测试/** * 查看首页文章列表 * * @return &#123;@link Result&lt;ArticleHomeVO>&#125; */ @VisitLogger(value = \"首页\") @ApiOperation(value = \"查看首页文章列表\") @GetMapping(\"/article/list\") public Result&lt;PageResult&lt;ArticleHomeVO>> listArticleHomeVO() &#123; return Result.success(articleService.listArticleHomeVO()); &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"},{"name":"AOP","slug":"AOP","permalink":"https://blog.ehzyil.xyz/tags/AOP/"},{"name":"日志","slug":"日志","permalink":"https://blog.ehzyil.xyz/tags/%E6%97%A5%E5%BF%97/"}],"author":"ehzyil"},{"title":"基于策略模式实现图片上传","slug":"2023/基于策略模式实现图片上传","date":"2023-10-17T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/10/17/2023/基于策略模式实现图片上传/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/17/2023/%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%E5%AE%9E%E7%8E%B0%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0/","excerpt":"","text":"基于策略模式实现图片上传1.前置准备-策略接口的编写新建一个名称为 strategy 的文件夹（在代码规范中，使用设计模式要明确的体现出来，便于后期维护） public interface UploadStrategy &#123; /** * 上传文件 * * @param file 文件 * @param path 上传路径 * @return &#123;@link String&#125; 文件地址 */ String uploadFile(MultipartFile file, String path); &#125; 完善配置文件 在编写对象存储实现类之前，我门会发现一个问题。我们需要去对应的云服务厂商开通对象存储服务，然后获取到accessKey、accessKeySecret、endpoint、bucket、domainUrl等必须的参数。因为这些信息基本是不会发生改变，所以我们可以将这些信息存储在配置文件中。除此之外我们还需要对文件上传进行配置，设置为最大文件为100MB spring: servlet: multipart: max-file-size: 100MB max-request-size: 100MB # 文件上传策略 local、oss、cos upload: strategy: qiniu local: # nginx映射本地文件路径 url: https://你的文件上传子域名/ # 本地文件存储路径 path: /usr/local/upload/ # oss存储 oss: url: http://Bucket域名/ endpoint: OSS配置endpoint bucketName: OSS配置bucketName accessKeyId: OSS配置accessKeyId accesskeySecret: OSS配置accesskeySecret # cos存储 cos: url: https://Bucket域名/ secretId: COS配置secretId secretKey: COS配置secretKey region: COS配置region bucketName: COS配置bucketName # qiniu存储(七牛云) qiniu: url: http://s14jjz3r5.hb-bkt.clouddn.com/ # 访问域名(给存储桶绑定的二级域名) bucketName: ehzyil-blog # 空间名称bucketName region: huabei # 存储区域 华南(huanan) 华北(huabei)... accessKey: CSOFmR7_70ThEOZqlgTdbe2HnvOjbjPdCHt8CZrb #accessKey secretKey: _D-S6V1rBT2jjnZrX-Z_s30Aw6SNO1ydOFYHnUoC #secretKey 引入自定义配置依赖 以及 云服务依赖 &lt;!-- 腾讯云cos --> &lt;dependency> &lt;groupId>com.qcloud&lt;/groupId> &lt;artifactId>cos_api&lt;/artifactId> &lt;version>$&#123;cos.version&#125;&lt;/version> &lt;/dependency> &lt;!--七牛云qiniu--> &lt;dependency> &lt;groupId>com.qiniu&lt;/groupId> &lt;artifactId>qiniu-java-sdk&lt;/artifactId> &lt;version>[7.2.0, 7.2.99]&lt;/version> &lt;/dependency> &lt;!-- 阿里云oss --> &lt;dependency> &lt;groupId>com.aliyun.oss&lt;/groupId> &lt;artifactId>aliyun-sdk-oss&lt;/artifactId> &lt;version>$&#123;oss.version&#125;&lt;/version> &lt;/dependency> 编写properties实体类，通过@ConfigurationProperties()注解可以将配置文件中的内容读取到实体类中。 @Data @Configuration @ConfigurationProperties(prefix = \"upload.oss\") public class OssProperties &#123; /** * oss域名 */ private String url; /** * 终点 */ private String endpoint; /** * 访问密钥id */ private String accessKeyId; /** * 访问密钥密码 */ private String accessKeySecret; /** * bucket名称 */ private String bucketName; &#125; @Data @Configuration @ConfigurationProperties(prefix = \"upload.qiniu\") public class QiniuProperties &#123; private String url;//url 或者 域名 private String bucketName; //存储桶名字 private String region;//区域 如huanan hubei private String accessKey;//accessKey private String secretKey;//secretKey /** * 配置空间的存储区域 */ @Bean public com.qiniu.storage.Configuration qiNiuConfig() &#123; switch (region) &#123; case \"huadong\": return new com.qiniu.storage.Configuration(Zone.huadong()); case \"huabei\": return new com.qiniu.storage.Configuration(Zone.huabei()); case \"huanan\": return new com.qiniu.storage.Configuration(Zone.huanan()); case \"beimei\": return new com.qiniu.storage.Configuration(Zone.beimei()); default: throw new RuntimeException(\"存储区域配置错误\"); &#125; &#125; /** * 构建一个七牛上传工具实例 */ @Bean public UploadManager uploadManager() &#123; return new UploadManager(qiNiuConfig()); &#125; /** * 认证信息实例 */ @Bean public Auth auth() &#123; return Auth.create(accessKey, secretKey); &#125; /** * 构建七牛空间管理实例 */ @Bean public BucketManager bucketManager() &#123; return new BucketManager(auth(), qiNiuConfig()); &#125; &#125; 本地上传配置信息完善 本地上传目前不需要进行配置，项目上线可以进行域名配置域名配制 上传策略枚举 @Getter @AllArgsConstructor public enum UploadModeEnum &#123; /** * 本地 */ LOCAL(\"local\", \"localUploadStrategyImpl\"), /** * oss */ OSS(\"oss\", \"ossUploadStrategyImpl\"), /** * cos */ COS(\"cos\", \"cosUploadStrategyImpl\"), /** * qiniu */ QINIU(\"qiniu\", \"qiniuUploadStrategyImpl\"); /** * 模式 */ private final String mode; /** * 策略 */ private final String strategy; /** * 获取策略 * * @param mode 模式 * @return 搜索策略 */ public static String getStrategy(String mode) &#123; for (UploadModeEnum value : UploadModeEnum.values()) &#123; if (value.getMode().equals(mode)) &#123; return value.getStrategy(); &#125; &#125; return null; &#125; &#125; 2.策略实现类内部实现-模板设计模式我们在进行具体文件上传策略实现之前总结一下所涉及到的功能。 初始化客户端 文件是否已经存在 文件上传 获取访问路径 我们会发现无论是通过哪个平台进行文件的上传，基本上都会使用到上述的步骤，也就是说都会使用到上述的方法。所以在这里我们定义一个抽象类来规定具体所需要使用的方法，然后各个具体实现来继承我们的抽象类即可。 public abstract class AbstractUploadStrategyImpl implements UploadStrategy &#123; @Override public String uploadFile(MultipartFile file, String path) &#123; try &#123; // 判断文件大小是否超过2MB（2MB=2*1024*1024 bytes） if (file.getSize() > 2 * 1024 * 1024) &#123; // 抛出文件大小超过限制的异常 throw new ServiceException(\"文件大小不能超出2MB！\"); &#125; //获取文件md5值 String md5 = FileUtils.getMd5(file.getInputStream()); //获取拓展名 String extName = FileUtils.getExtension(file); // 重新生成文件名 String fileName = md5 + \".\" + extName; //region 初始化 initClient(); //endregion // 判断文件是否已存在 if (!exists(path + fileName)) &#123; // 不存在则继续上传 upload(path, fileName, file.getInputStream()); &#125; // 返回文件访问路径 String url=getFileAccessUrl(path + fileName); uploadToDB(file, path, md5, extName, url); return url; &#125; catch (Exception e) &#123; e.printStackTrace(); throw new ServiceException(\"文件上传失败\"); &#125; &#125; /** * 初始化客户端 */ public abstract void initClient(); /** * 判断文件是否存在 * * @param filePath 文件路径 * @return &#123;@link Boolean&#125; */ public abstract Boolean exists(String filePath); /** * 上传 * * @param path 路径 * @param fileName 文件名 * @param inputStream 输入流 * @throws IOException io异常 */ public abstract void upload(String path, String fileName, InputStream inputStream) throws IOException; /** * 获取文件访问url * * @param filePath 文件路径 * @return &#123;@link String&#125; 文件url */ public abstract String getFileAccessUrl(String filePath); public abstract void uploadToDB(MultipartFile file, String path, String md5, String extName, String url); &#125; 3.Oss上传策略具体实现Oss上传策略具体实现 @Slf4j @Service(\"ossUploadStrategyImpl\") public class OssUploadStrategyImpl extends AbstractUploadStrategyImpl &#123; @Autowired private OssProperties ossProperties; @Autowired @Lazy private IBlogFileService blogFileService; @Override public void initClient() &#123; &#125; @Override public Boolean exists(String filePath) &#123; return getOssClient().doesObjectExist(ossProperties.getBucketName(), filePath); &#125; @Override public void upload(String path, String fileName, InputStream inputStream) &#123; OSS ossClient = getOssClient(); try &#123; // 调用oss方法上传 ossClient.putObject(ossProperties.getBucketName(), path + fileName, inputStream); &#125; catch (OSSException oe) &#123; log.error(\"Error Message:\" + oe.getErrorMessage()); log.error(\"Error Code:\" + oe.getErrorCode()); log.info(\"Request ID:\" + oe.getRequestId()); log.info(\"Host ID:\" + oe.getHostId()); &#125; catch (ClientException ce) &#123; log.error(\"Caught an ClientException, Error Message:\" + ce.getMessage()); &#125; finally &#123; if (ossClient != null) &#123; ossClient.shutdown(); &#125; &#125; &#125; @Override public String getFileAccessUrl(String filePath) &#123; return ossProperties.getUrl() + filePath; &#125; @Override public void uploadToDB(MultipartFile file, String path, String md5, String extName, String url) &#123; blogFileService.uploadToDB(file,path,md5,extName,url); &#125; /** * 获取ossClient * * @return &#123;@link OSS&#125; ossClient */ private OSS getOssClient() &#123; return new OSSClientBuilder().build(ossProperties.getEndpoint(), ossProperties.getAccessKeyId(), ossProperties.getAccessKeySecret()); &#125; &#125; Cos上传策略具体实现 @Slf4j @Service(\"cosUploadStrategyImpl\") public class CosUploadStrategyImpl extends AbstractUploadStrategyImpl &#123; @Autowired private CosProperties cosProperties; @Autowired @Lazy private IBlogFileService blogFileService; @Override public void initClient() &#123; &#125; @Override public Boolean exists(String filePath) &#123; return getCosClient().doesObjectExist(cosProperties.getBucketName(), filePath); &#125; @Override public void upload(String path, String fileName, InputStream inputStream) &#123; COSClient cosClient = getCosClient(); try &#123; ObjectMetadata objectMetadata = new ObjectMetadata(); // 上传的流如果能够获取准确的流长度，则推荐一定填写 content-length objectMetadata.setContentLength(inputStream.available()); // 调用cos方法上传 cosClient.putObject(cosProperties.getBucketName(), path + fileName, inputStream, objectMetadata); &#125; catch (CosServiceException e) &#123; log.error(\"Error Message:\" + e.getErrorMessage()); log.error(\"Error Code:\" + e.getErrorCode()); log.info(\"Request ID:\" + e.getRequestId()); &#125; catch (CosClientException e) &#123; log.error(\"Caught an CosClientException, Error Message:\" + e.getMessage()); &#125; catch (IOException e) &#123; log.error(\"Caught an IOException, Error Message:\" + e.getMessage()); &#125; finally &#123; cosClient.shutdown(); &#125; &#125; @Override public String getFileAccessUrl(String filePath) &#123; return cosProperties.getUrl() + filePath; &#125; @Override public void uploadToDB(MultipartFile file, String path, String md5, String extName, String url) &#123; blogFileService.uploadToDB(file,path,md5,extName,url); &#125; /** * 获取cosClient * * @return &#123;@link COSClient&#125; cosClient */ private COSClient getCosClient() &#123; // 1 初始化用户身份信息（secretId, secretKey）。 COSCredentials cred = new BasicCOSCredentials(cosProperties.getSecretId(), cosProperties.getSecretKey()); // 2 设置 bucket 的地域, COS 地域的简称请参照 https://cloud.tencent.com/document/product/436/6224 Region region = new Region(cosProperties.getRegion()); ClientConfig clientConfig = new ClientConfig(region); // 这里建议设置使用 https 协议 // 从 5.6.54 版本开始，默认使用了 https clientConfig.setHttpProtocol(HttpProtocol.https); // 3 生成 cos 客户端。 return new COSClient(cred, clientConfig); &#125; &#125; 本地上传策略具体实现-在LocalUploadStrategyImpl实现文件上传至本地 @Slf4j @Getter @Setter @RequiredArgsConstructor @Service(\"localUploadStrategyImpl\") public class LocalUploadStrategyImpl extends AbstractUploadStrategyImpl &#123; /** * 访问url */ @Value(\"$&#123;upload.local.url&#125;\") private String localUrl; /** * 本地路径 */ @Value(\"$&#123;upload.local.path&#125;\") private String localPath; /** * 本地项目端口 */ @Value(\"$&#123;server.port&#125;\") private Integer port; @Autowired @Lazy private IBlogFileService blogFileService; //TODO 本地上传待优化 @Override public void initClient() &#123; //win 本地 // try &#123; // localPath = ResourceUtils.getURL(\"classpath:\").getPath() + \"static/imgs/\"; // &#125; catch (FileNotFoundException e) &#123; // e.printStackTrace(); // throw new ServiceException(\"文件不存在\"); // &#125; // 判断目录是否存在 File directory = new File(localPath); if (!directory.exists()) &#123; if (directory.mkdirs()) &#123; throw new ServiceException(\"创建目录失败\"); &#125; &#125; &#125; @Override public Boolean exists(String filePath) &#123; return new File(localPath + filePath).exists(); &#125; @Override public void upload(String path, String fileName, InputStream inputStream) throws IOException &#123; // 判断上传目录是否存在 File directory = new File(localPath + path); if (!directory.exists()) &#123; if (!directory.mkdirs()) &#123; throw new ServiceException(\"创建目录失败\"); &#125; &#125; // 写入文件 File file = new File(localPath + path + fileName); if (file.createNewFile()) &#123; //使用缓冲流写入本地 try (BufferedInputStream bis = new BufferedInputStream(inputStream); BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(file))) &#123; byte[] bytes = new byte[4096]; int length; while ((length = bis.read(bytes)) != -1) &#123; bos.write(bytes, 0, length); &#125; &#125; &#125; &#125; @Override public String getFileAccessUrl(String filePath) &#123; return localPath + filePath; &#125; @Override public void uploadToDB(MultipartFile file, String path, String md5, String extName, String url) &#123; blogFileService.uploadToDB(file,path,md5,extName,url); &#125; &#125; 4.策略上下文实现我们通过策略上下文来选择使用哪种上传方式。注意点： 当Map集合的Value为接口类型时，Spring会自动对Map集合进行注入。 ​ 其中map集合的key为接口对应实现类的BeanName​ 其中map集合的vlaue为接口对应实现类的实例 其中传入的uploadServiceName就是对应策略类所规定的的BeanName，这里的BeanName就作为选择的条件。 @Service public class UploadStrategyContext &#123; /** * 上传模式 */ @Value(\"$&#123;upload.strategy&#125;\") private String uploadStrategy; @Autowired private Map&lt;String, UploadStrategy> uploadStrategyMap; /** * 上传文件 * * @param file 文件 * @param path 路径 * @return &#123;@link String&#125; 文件地址 */ public String executeUploadStrategy(MultipartFile file, String path) &#123; return uploadStrategyMap.get(getStrategy(uploadStrategy)).uploadFile(file, path); &#125; &#125; 5.不同策略上传测试@RestController @RequiredArgsConstructor public class UploadController &#123; private final UploadStrategyContext uploadStrategyContext; @PostMapping(\"/upload\") public ResponseResult&lt;?> upload(MultipartFile file) &#123; return ResponseResult.success(\"文件上传成功！\",uploadStrategyContext.executeUploadStrategy(file,\"/blog/avatar\",\"cosUploadServiceImpl\")); &#125; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"},{"name":"图片上传","slug":"图片上传","permalink":"https://blog.ehzyil.xyz/tags/%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0/"}],"author":"ehzyil"},{"title":"异常之doesn't have a default value","slug":"2023/异常之doesn't have a default value","date":"2023-10-17T00:00:00.000Z","updated":"2024-06-17T01:04:53.995Z","comments":true,"path":"2023/10/17/2023/异常之doesn't have a default value/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/17/2023/%E5%BC%82%E5%B8%B8%E4%B9%8Bdoesn't%20have%20a%20default%20value/","excerpt":"","text":"MySQL报错Field ‘user_id’ doesn’t have a default value一、问题 执行sql语句时，报错Field ‘user_id’ doesn’t have a default value； ![image-20231017122452384](.&#x2F;images&#x2F;异常之doesn’t have a default value&#x2F;image-20231017122452384.png) 二、解决 1、表结构 CREATE TABLE `sys_user_role` ( `user_id` bigint NOT NULL COMMENT '用户ID', `role_id` bigint NOT NULL COMMENT '角色ID', ) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3 COMMENT='用户和角色关联表' 2、执行的sql INSERT INTO sys_user_role ( role_id ) VALUES ( ? ) 在插入信息时只插入了’role_id’未插入’user_id’,sql中没有字段且 PRIMARY KEY (user_id,role_id） 因此插入时报异常. 经查找实体类user_role 发现在userId上添加了如下注解 @TableId(\"user_id\")//相当于 @TableId(\"user_id\") 导致无法获取User实体的userId字段。 解决方案 去掉@TableId注解 将注解改为：@TableId(value &#x3D; “user_id”, type &#x3D; IdType.INPUT) 三、延伸 @TableId注解 描述：主键注解 使用位置：实体类主键字段 @TableName(\"sys_user\") public class User &#123; @TableId private Long id; private String name; private Integer age; private String email; &#125; 属性 类型 必须指定 默认值 描述 value String 否 “” 主键字段名 type Enum 否 IdType.NONE 指定主键类型 IdType 值 描述 AUTO 数据库 ID 自增 NONE 无状态，该类型为未设置主键类型（注解里等于跟随全局，全局里约等于 INPUT） INPUT insert 前自行 set 主键值 ASSIGN_ID 分配 ID(主键类型为 Number(Long 和 Integer)或 String)(since 3.3.0),使用接口IdentifierGenerator的方法nextId(默认实现类为DefaultIdentifierGenerator雪花算法) ASSIGN_UUID 分配 UUID,主键类型为 String(since 3.3.0),使用接口IdentifierGenerator的方法nextUUID(默认 default 方法) ID_WORKER 分布式全局唯一 ID 长整型类型(please use ASSIGN_ID) UUID 32 位 UUID 字符串(please use ASSIGN_UUID) ID_WORKER_STR 分布式全局唯一 ID 字符串类型(please use ASSIGN_ID)","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"异常","slug":"异常","permalink":"https://blog.ehzyil.xyz/tags/%E5%BC%82%E5%B8%B8/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"}],"author":"ehzyil"},{"title":"Spring Boot整合Quartz实现动态定时任务","slug":"2023/Spring Boot整合Quartz实现动态定时任务","date":"2023-10-16T20:22:46.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/10/16/2023/Spring Boot整合Quartz实现动态定时任务/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/16/2023/Spring%20Boot%E6%95%B4%E5%90%88Quartz%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"本文目的是Spring Boot整合Quartz实现动态任务配置和实现定时清理日志。 Spring Boot整合Quartz 详见 Quartz 使用教程 前置准备/** * 定时任务 */ @Data @Builder @NoArgsConstructor @AllArgsConstructor public class Task &#123; /** * 任务id */ @TableId(type = IdType.AUTO) private Integer id; /** * 任务名称 */ private String taskName; /** * 任务组名 */ private String taskGroup; /** * 调用目标 */ private String invokeTarget; /** * cron执行表达式 */ private String cronExpression; /** * 计划执行错误策略 (1立即执行 2执行一次 3放弃执行) */ private Integer misfirePolicy; /** * 是否并发执行 (0否 1是) */ private Integer concurrent; /** * 任务状态 (0运行 1暂停) */ private Integer status; /** * 任务备注信息 */ private String remark; /** * 创建时间 */ @TableField(fill = FieldFill.INSERT) private LocalDateTime createTime; /** * 更新时间 */ @TableField(fill = FieldFill.UPDATE) private LocalDateTime updateTime; &#125; SpringUtils工具类，用于获取bean。 @Component public final class SpringUtils implements BeanFactoryPostProcessor &#123; private static ConfigurableListableBeanFactory beanFactory; @Override public void postProcessBeanFactory(@NotNull ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; SpringUtils.beanFactory = beanFactory; &#125; /** * 获取对象 */ @SuppressWarnings(\"unchecked\") public static &lt;T> T getBean(String name) throws BeansException &#123; return (T) beanFactory.getBean(name); &#125; /** * 获取类型为requiredType的对象 */ public static &lt;T> T getBean(Class&lt;T> clz) throws BeansException &#123; return beanFactory.getBean(clz); &#125; public static Class&lt;?> getType(String name) throws NoSuchBeanDefinitionException &#123; return beanFactory.getType(name); &#125; &#125; 创建定时任务工具类 /** * 定时任务工具类 * * @author ican */ public class ScheduleUtils &#123; /** * 得到quartz任务类 * * @param task 执行计划 * @return 具体执行任务类 */ private static Class&lt;? extends Job> getQuartzJobClass(Task task) &#123; boolean isConcurrent = TRUE.equals(task.getConcurrent()); return isConcurrent ? QuartzJobExecution.class : QuartzDisallowConcurrentExecution.class; &#125; /** * 构建任务触发对象 */ public static TriggerKey getTriggerKey(Integer taskId, String taskGroup) &#123; return TriggerKey.triggerKey(ScheduleConstant.TASK_CLASS_NAME + taskId, taskGroup); &#125; /** * 构建任务键对象 */ public static JobKey getJobKey(Integer taskId, String taskGroup) &#123; return JobKey.jobKey(ScheduleConstant.TASK_CLASS_NAME + taskId, taskGroup); &#125; /** * 创建定时任务 */ public static void createScheduleJob(Scheduler scheduler, Task task) &#123; try &#123; Class&lt;? extends Job> jobClass = getQuartzJobClass(task); // 构建task信息 Integer taskId = task.getId(); String taskGroup = task.getTaskGroup(); JobDetail jobDetail = JobBuilder.newJob(jobClass).withIdentity(getJobKey(taskId, taskGroup)).build(); // 表达式调度构建器 CronScheduleBuilder cronScheduleBuilder = CronScheduleBuilder.cronSchedule(task.getCronExpression()); cronScheduleBuilder = handleCronScheduleMisfirePolicy(task, cronScheduleBuilder); // 按新的cronExpression表达式构建一个新的trigger CronTrigger trigger = TriggerBuilder.newTrigger().withIdentity(getTriggerKey(taskId, taskGroup)) .withSchedule(cronScheduleBuilder).build(); // 放入参数，运行时的方法可以获取 jobDetail.getJobDataMap().put(ScheduleConstant.TASK_PROPERTIES, task); // 判断是否存在 if (scheduler.checkExists(getJobKey(taskId, taskGroup))) &#123; // 防止创建时存在数据问题 先移除，然后在执行创建操作 scheduler.deleteJob(getJobKey(taskId, taskGroup)); &#125; scheduler.scheduleJob(jobDetail, trigger); // 暂停任务 if (task.getStatus().equals(TaskStatusEnum.PAUSE.getStatus())) &#123; scheduler.pauseJob(ScheduleUtils.getJobKey(taskId, taskGroup)); &#125; &#125; catch (ServiceException | SchedulerException e) &#123; throw new ServiceException(e.getMessage()); &#125; &#125; /** * 设置定时任务策略 */ public static CronScheduleBuilder handleCronScheduleMisfirePolicy(Task task, CronScheduleBuilder cb) throws ServiceException &#123; switch (task.getMisfirePolicy()) &#123; case ScheduleConstant.MISFIRE_DEFAULT: return cb; case ScheduleConstant.MISFIRE_IGNORE_MISFIRES: return cb.withMisfireHandlingInstructionIgnoreMisfires(); case ScheduleConstant.MISFIRE_FIRE_AND_PROCEED: return cb.withMisfireHandlingInstructionFireAndProceed(); case ScheduleConstant.MISFIRE_DO_NOTHING: return cb.withMisfireHandlingInstructionDoNothing(); default: throw new ServiceException(\"The task misfire policy '\" + task.getMisfirePolicy() + \"' cannot be used in cron schedule tasks\"); &#125; &#125; &#125; 任务执行工具 /** * 任务执行工具 */ public class TaskInvokeUtils &#123; /** * 执行方法 * * @param task 系统任务 */ public static void invokeMethod(Task task) throws Exception &#123; String invokeTarget = task.getInvokeTarget(); String beanName = getBeanName(invokeTarget); String methodName = getMethodName(invokeTarget); List&lt;Object[]> methodParams = getMethodParams(invokeTarget); if (!isValidClassName(beanName)) &#123; Object bean = SpringUtils.getBean(beanName); invokeMethod(bean, methodName, methodParams); &#125; else &#123; Object bean = Class.forName(beanName).getDeclaredConstructor().newInstance(); invokeMethod(bean, methodName, methodParams); &#125; &#125; /** * 调用任务方法 * * @param bean 目标对象 * @param methodName 方法名称 * @param methodParams 方法参数 */ private static void invokeMethod(Object bean, String methodName, List&lt;Object[]> methodParams) throws NoSuchMethodException, SecurityException, IllegalAccessException, IllegalArgumentException, InvocationTargetException &#123; if (methodParams != null &amp;&amp; methodParams.size() > 0) &#123; Method method = bean.getClass().getDeclaredMethod(methodName, getMethodParamsType(methodParams)); method.invoke(bean, getMethodParamsValue(methodParams)); &#125; else &#123; Method method = bean.getClass().getDeclaredMethod(methodName); method.invoke(bean); &#125; &#125; /** * 校验是否为为class包名 * * @param invokeTarget 调用目标 * @return true是 false否 */ public static boolean isValidClassName(String invokeTarget) &#123; return StringUtils.countMatches(invokeTarget, \".\") > 1; &#125; /** * 获取bean名称 * * @param invokeTarget 调用目标 * @return bean名称 */ public static String getBeanName(String invokeTarget) &#123; String beanName = StringUtils.substringBefore(invokeTarget, \"(\"); return StringUtils.substringBeforeLast(beanName, \".\"); &#125; /** * 获取bean方法 * * @param invokeTarget 调用目标 * @return method方法 */ public static String getMethodName(String invokeTarget) &#123; String methodName = StringUtils.substringBefore(invokeTarget, \"(\"); return StringUtils.substringAfterLast(methodName, \".\"); &#125; /** * 获取method方法参数相关列表 * * @param invokeTarget 调用目标 * @return method方法相关参数列表 */ public static List&lt;Object[]> getMethodParams(String invokeTarget) &#123; String methodStr = StringUtils.substringBetween(invokeTarget, \"(\", \")\"); if (StringUtils.isEmpty(methodStr)) &#123; return null; &#125; String[] methodParams = methodStr.split(\",(?=([^\\\"']*[\\\"'][^\\\"']*[\\\"'])*[^\\\"']*$)\"); List&lt;Object[]> clazz = new LinkedList&lt;>(); for (String methodParam : methodParams) &#123; String str = StringUtils.trimToEmpty(methodParam); // String字符串类型，以'或\"开头 if (StringUtils.startsWithAny(str, \"'\", \"\\\"\")) &#123; clazz.add(new Object[]&#123;StringUtils.substring(str, 1, str.length() - 1), String.class&#125;); &#125; // boolean布尔类型，等于true或者false else if (\"true\".equalsIgnoreCase(str) || \"false\".equalsIgnoreCase(str)) &#123; clazz.add(new Object[]&#123;Boolean.valueOf(str), Boolean.class&#125;); &#125; // long长整形，以L结尾 else if (StringUtils.endsWith(str, \"L\")) &#123; clazz.add(new Object[]&#123;Long.valueOf(StringUtils.substring(str, 0, str.length() - 1)), Long.class&#125;); &#125; // double浮点类型，以D结尾 else if (StringUtils.endsWith(str, \"D\")) &#123; clazz.add(new Object[]&#123;Double.valueOf(StringUtils.substring(str, 0, str.length() - 1)), Double.class&#125;); &#125; // 其他类型归类为整形 else &#123; clazz.add(new Object[]&#123;Integer.valueOf(str), Integer.class&#125;); &#125; &#125; return clazz; &#125; /** * 获取参数类型 * * @param methodParams 参数相关列表 * @return 参数类型列表 */ public static Class&lt;?>[] getMethodParamsType(List&lt;Object[]> methodParams) &#123; Class&lt;?>[] clazz = new Class&lt;?>[methodParams.size()]; int index = 0; for (Object[] os : methodParams) &#123; clazz[index] = (Class&lt;?>) os[1]; index++; &#125; return clazz; &#125; /** * 获取参数值 * * @param methodParams 参数相关列表 * @return 参数值列表 */ public static Object[] getMethodParamsValue(List&lt;Object[]> methodParams) &#123; Object[] clazz = new Object[methodParams.size()]; int index = 0; for (Object[] os : methodParams) &#123; clazz[index] = os[0]; index++; &#125; return clazz; &#125; &#125; cron表达式工具类 /** * cron表达式工具类 */ public class CronUtils &#123; /** * 返回一个布尔值代表一个给定的Cron表达式的有效性 * * @param cronExpression Cron表达式 * @return boolean 表达式是否有效 */ public static boolean isValid(String cronExpression) &#123; return CronExpression.isValidExpression(cronExpression); &#125; /** * 返回下一个执行时间根据给定的Cron表达式 * * @param cronExpression Cron表达式 * @return Date 下次Cron表达式执行时间 */ public static Date getNextExecution(String cronExpression) &#123; try &#123; CronExpression cron = new CronExpression(cronExpression); return cron.getNextValidTimeAfter(new Date(System.currentTimeMillis())); &#125; catch (ParseException e) &#123; throw new IllegalArgumentException(e.getMessage()); &#125; &#125; &#125; 定时任务的配置与实现基本配置1.创建抽象类：AbstractQuartzJob public abstract class AbstractQuartzJob implements Job &#123; private static final Logger log = LoggerFactory.getLogger(AbstractQuartzJob.class); //线程本地变量 用于存储执行开始时间 private static ThreadLocal&lt;Date> threadLocal = new ThreadLocal&lt;>(); @Override public void execute(JobExecutionContext context) &#123; Task task = new Task(); BeanUtils.copyProperties(context.getMergedJobDataMap().get(ScheduleConstant.TASK_PROPERTIES), task); try &#123; before(context, task); doExecute(context, task); after(context, task, null); &#125; catch (Exception e) &#123; log.error(\"任务执行异常 - ：\", e); after(context, task, e); &#125; &#125; /** * 执行后 * * @param context 工作执行上下文对象 * @param task 系统计划任务 */ private void after(JobExecutionContext context, Task task, Exception e) &#123; //获取任务开始时间 Date startTime = threadLocal.get(); threadLocal.remove(); //记录任务日志 final TaskLog taskLog = new TaskLog(); taskLog.setTaskName(task.getTaskName()); taskLog.setTaskGroup(task.getTaskGroup()); taskLog.setInvokeTarget(task.getInvokeTarget()); long runTime = new Date().getTime() - startTime.getTime(); taskLog.setTaskMessage(taskLog.getTaskName() + \" 总共耗时：\" + runTime + \"毫秒\"); //处理异常情况 if (e != null) &#123; taskLog.setStatus(FALSE); //截取异常信息 String errorMsg = StringUtils.substring(e.getMessage(), 0, 2000); taskLog.setErrorInfo(errorMsg); &#125; else &#123; //正常情况 taskLog.setStatus(TRUE); &#125; // 写入数据库当中 SpringUtils.getBean(TaskLogMapper.class).insert(taskLog); &#125; /** * 执行方法，由子类重载 * * @param context 工作执行上下文对象 * @param task 系统计划任务 * @throws Exception 执行过程中的异常 */ protected abstract void doExecute(JobExecutionContext context, Task task) throws Exception; /** * 执行前 * * @param context 工作执行上下文对象 * @param task 系统计划任务 */ private void before(JobExecutionContext context, Task task) &#123; threadLocal.set(new Date()); &#125; &#125; 该抽象类为调用定时任务的模板类，子类通过重载来实现支持并发的任务和默认的任务（非并发）。 其中下列的before()方法用于记录任务开始的执行时间并存入线程局部变量中。 after()方法用来记录任务的执行日志。 /** * 执行前 * * @param context 工作执行上下文对象 * @param task 系统计划任务 */ private void before(JobExecutionContext context, Task task) &#123; ... &#125; /** * 执行后 * * @param context 工作执行上下文对象 * @param task 系统计划任务 */ private void after(JobExecutionContext context, Task task, Exception e) &#123; ... &#125; 抽象类AbstractQuartzJob的实现类 /** * 定时任务处理（禁止并发执行） */ @DisallowConcurrentExecution public class QuartzDisallowConcurrentExecution extends AbstractQuartzJob &#123; @Override protected void doExecute(JobExecutionContext context, Task task) throws Exception &#123; TaskInvokeUtils.invokeMethod(task); &#125; &#125; /** * 定时任务处理（允许并发执行） */ public class QuartzJobExecution extends AbstractQuartzJob &#123; @Override protected void doExecute(JobExecutionContext context, Task task) throws Exception &#123; TaskInvokeUtils.invokeMethod(task); &#125; &#125; 创建执行定时任务类：TimedTask,其中clear用于清除博客访问记录，clearVistiLog()用于 清除一周前的访问日志，clear() 用于执行测试任务。 /** * 执行定时任务 */ @SuppressWarnings(value = \"all\") @Component(\"timedTask\") public class TimedTask &#123; @Autowired private RedisService redisService; @Autowired private VisitLogMapper visitLogMapper; /** * 清除博客访问记录 */ public void clear() &#123; redisService.deleteObject(UNIQUE_VISITOR); &#125; /** * 测试任务 */ public void test() &#123; System.out.println(\"测试任务\"); &#125; /** * 清除一周前的访问日志 */ public void clearVistiLog() &#123; DateTime endTime = DateUtil.beginOfDay(DateUtil.offsetDay(new Date(), -7)); visitLogMapper.deleteVisitLog(endTime); &#125; &#125; 至此只需要实现定时任务的调用即可。由于该项目定时任务技术动态配置和接口接口调用，因此使用手动调用接口实现。 重点1.创建定时任务与调度器 Scheduler 通过查找t_task表查询所有任务 通过ScheduleUtils.createScheduleJob(scheduler, task)创建定时任务 @Autowired private Scheduler scheduler; @Autowired private TaskMapper taskMapper; /** * 项目启动时，初始化定时器 主要是防止手动修改数据库导致未同步到定时任务处理 * 注：不能手动修改数据库ID和任务组名，否则会导致脏数据 */ @PostConstruct public void init() throws SchedulerException &#123; scheduler.clear(); List&lt;Task> taskList = taskMapper.selectList(null); for (Task task : taskList) &#123; // 创建定时任务 ScheduleUtils.createScheduleJob(scheduler, task); &#125; &#125; 2.创建JobDetail实例与构建Trigger实例 public static void createScheduleJob(Scheduler scheduler, Task task) &#123; try &#123; Class&lt;? extends Job> jobClass = getQuartzJobClass(task); // 构建task信息 Integer taskId = task.getId(); String taskGroup = task.getTaskGroup(); JobDetail jobDetail = JobBuilder.newJob(jobClass).withIdentity(getJobKey(taskId, taskGroup)).build(); // 表达式调度构建器 CronScheduleBuilder cronScheduleBuilder = CronScheduleBuilder.cronSchedule(task.getCronExpression()); cronScheduleBuilder = handleCronScheduleMisfirePolicy(task, cronScheduleBuilder); // 按新的cronExpression表达式构建一个新的trigger CronTrigger trigger = TriggerBuilder.newTrigger().withIdentity(getTriggerKey(taskId, taskGroup)) .withSchedule(cronScheduleBuilder).build(); // 放入参数，运行时的方法可以获取 jobDetail.getJobDataMap().put(ScheduleConstant.TASK_PROPERTIES, task); // 判断是否存在 if (scheduler.checkExists(getJobKey(taskId, taskGroup))) &#123; // 防止创建时存在数据问题 先移除，然后在执行创建操作 scheduler.deleteJob(getJobKey(taskId, taskGroup)); &#125; scheduler.scheduleJob(jobDetail, trigger); // 暂停任务 if (task.getStatus().equals(TaskStatusEnum.PAUSE.getStatus())) &#123; scheduler.pauseJob(ScheduleUtils.getJobKey(taskId, taskGroup)); &#125; &#125; catch (ServiceException | SchedulerException e) &#123; throw new ServiceException(e.getMessage()); &#125; &#125; 3.执行，开启调度器 public void runTask(TaskRunDTO taskRun) &#123; Integer taskId = taskRun.getId(); String taskGroup = taskRun.getTaskGroup(); // 查询定时任务 Task task = taskMapper.selectById(taskRun.getId()); // 设置参数 JobDataMap dataMap = new JobDataMap(); dataMap.put(ScheduleConstant.TASK_PROPERTIES, task); try &#123; scheduler.triggerJob(ScheduleUtils.getJobKey(taskId, taskGroup), dataMap); &#125; catch (SchedulerException e) &#123; throw new ServiceException(\"执行定时任务异常\"); &#125; &#125; 业务层服务类ITaskService public interface ITaskService extends IService&lt;Task> &#123; /** * 查看定时任务列表 * * @param condition 条件 * @return 定时任务列表 */ PageResult&lt;TaskBackVO> listTaskBackVO(ConditionDTO condition); /** * 添加定时任务 * * @param task 定时任务 */ void addTask(TaskDTO task); /** * 修改定时任务 * * @param task 定时任务信息 */ void updateTask(TaskDTO task); /** * 删除定时任务 * * @param taskIdList 定时任务id集合 */ void deleteTask(List&lt;Integer> taskIdList); /** * 修改定时任务状态 * * @param taskStatus 定时任务状态 */ void updateTaskStatus(StatusDTO taskStatus); /** * 定时任务立即执行一次 * * @param taskRun 定时任务 */ void runTask(TaskRunDTO taskRun); &#125; 业务层实现实现类TaskServiceImpl @Service public class TaskServiceImpl extends ServiceImpl&lt;TaskMapper, Task> implements ITaskService &#123; @Autowired private Scheduler scheduler; @Autowired private TaskMapper taskMapper; /** * 项目启动时，初始化定时器 主要是防止手动修改数据库导致未同步到定时任务处理 * 注：不能手动修改数据库ID和任务组名，否则会导致脏数据 */ @PostConstruct public void init() throws SchedulerException &#123; scheduler.clear(); List&lt;Task> taskList = taskMapper.selectList(null); for (Task task : taskList) &#123; // 创建定时任务 ScheduleUtils.createScheduleJob(scheduler, task); &#125; &#125; @Override public PageResult&lt;TaskBackVO> listTaskBackVO(ConditionDTO condition) &#123; // 查询定时任务数量 Long count = taskMapper.countTaskBackVO(condition); if (count == 0) &#123; return new PageResult&lt;>(); &#125; // 分页查询定时任务列表 List&lt;TaskBackVO> taskBackVOList = taskMapper.selectTaskBackVO(PageUtils.getLimit(), PageUtils.getSize(), condition); taskBackVOList.forEach(item -> &#123; if (StringUtils.isNotEmpty(item.getCronExpression())) &#123; Date nextExecution = CronUtils.getNextExecution(item.getCronExpression()); item.setNextValidTime(nextExecution); &#125; else &#123; item.setNextValidTime(null); &#125; &#125;); return new PageResult&lt;>(taskBackVOList, count); &#125; @Override public void addTask(TaskDTO task) &#123; Assert.isTrue(CronUtils.isValid(task.getCronExpression()), \"Cron表达式无效\"); Task newTask = BeanCopyUtils.copyBean(task, Task.class); // 新增定时任务 int row = taskMapper.insert(newTask); // 创建定时任务 if (row > 0) &#123; ScheduleUtils.createScheduleJob(scheduler, newTask); &#125; &#125; @Override public void updateTask(TaskDTO task) &#123; Assert.isTrue(CronUtils.isValid(task.getCronExpression()), \"Cron表达式无效\"); Task existTask = taskMapper.selectById(task.getId()); Task newTask = BeanCopyUtils.copyBean(task, Task.class); // 更新定时任务 int row = taskMapper.updateById(newTask); if (row > 0) &#123; try &#123; updateSchedulerJob(newTask, existTask.getTaskGroup()); &#125; catch (SchedulerException e) &#123; throw new ServiceException(\"更新定时任务异常\"); &#125; &#125; &#125; @Override public void deleteTask(List&lt;Integer> taskIdList) &#123; List&lt;Task> taskList = taskMapper.selectList(new LambdaQueryWrapper&lt;Task>().select(Task::getId, Task::getTaskGroup).in(Task::getId, taskIdList)); // 删除定时任务 int row = taskMapper.delete(new LambdaQueryWrapper&lt;Task>().in(Task::getId, taskIdList)); if (row > 0) &#123; taskList.forEach(task -> &#123; try &#123; scheduler.deleteJob(ScheduleUtils.getJobKey(task.getId(), task.getTaskGroup())); &#125; catch (SchedulerException e) &#123; throw new ServiceException(\"删除定时任务异常\"); &#125; &#125;); &#125; &#125; @Override public void updateTaskStatus(StatusDTO taskStatus) &#123; Task task = taskMapper.selectById(taskStatus.getId()); // 相同不用更新 if (task.getStatus().equals(taskStatus.getStatus())) &#123; return; &#125; // 更新数据库中的定时任务状态 Task newTask = Task.builder().id(taskStatus.getId()).status(taskStatus.getStatus()).build(); int row = taskMapper.updateById(newTask); // 获取定时任务状态、id、任务组 Integer status = taskStatus.getStatus(); Integer taskId = task.getId(); String taskGroup = task.getTaskGroup(); if (row > 0) &#123; // 更新定时任务 try &#123; if (TaskStatusEnum.RUNNING.getStatus().equals(status)) &#123; scheduler.resumeJob(ScheduleUtils.getJobKey(taskId, taskGroup)); &#125; if (TaskStatusEnum.PAUSE.getStatus().equals(status)) &#123; scheduler.pauseJob(ScheduleUtils.getJobKey(taskId, taskGroup)); &#125; &#125; catch (SchedulerException e) &#123; throw new ServiceException(\"更新定时任务状态异常\"); &#125; &#125; &#125; @Override public void runTask(TaskRunDTO taskRun) &#123; Integer taskId = taskRun.getId(); String taskGroup = taskRun.getTaskGroup(); // 查询定时任务 Task task = taskMapper.selectById(taskRun.getId()); // 设置参数 JobDataMap dataMap = new JobDataMap(); dataMap.put(ScheduleConstant.TASK_PROPERTIES, task); try &#123; scheduler.triggerJob(ScheduleUtils.getJobKey(taskId, taskGroup), dataMap); &#125; catch (SchedulerException e) &#123; throw new ServiceException(\"执行定时任务异常\"); &#125; &#125; /** * 更新任务 * * @param task 任务对象 * @param taskGroup 任务组名 */ public void updateSchedulerJob(Task task, String taskGroup) throws SchedulerException &#123; Integer taskId = task.getId(); // 判断是否存在 JobKey jobKey = ScheduleUtils.getJobKey(taskId, taskGroup); if (scheduler.checkExists(jobKey)) &#123; // 防止创建时存在数据问题 先移除，然后在执行创建操作 scheduler.deleteJob(jobKey); &#125; ScheduleUtils.createScheduleJob(scheduler, task); &#125; &#125; 接口层创建定时任务接口TaskController。 @Api(tags = \"定时任务模块\") @RestController public class TaskController &#123; @Autowired private ITaskService taskService; /** * 查看定时任务列表 * * @param condition 条件 * @return &#123;@link TaskBackVO&#125; 定时任务列表 */ @ApiOperation(\"查看定时任务列表\") @SaCheckPermission(\"monitor:task:list\") @GetMapping(\"/admin/task/list\") public Result&lt;PageResult&lt;TaskBackVO>> listTaskBackVO(ConditionDTO condition) &#123; return Result.success(taskService.listTaskBackVO(condition)); &#125; /** * 添加定时任务 * * @param task 定时任务信息 * @return &#123;@link Result&lt;>&#125; */ @OptLogger(value = ADD) @ApiOperation(\"添加定时任务\") @SaCheckPermission(\"monitor:task:add\") @PostMapping(\"/admin/task/add\") public Result&lt;?> addTask(@Validated @RequestBody TaskDTO task) &#123; taskService.addTask(task); return Result.success(); &#125; /** * 修改定时任务 * * @param task 定时任务信息 * @return &#123;@link Result&lt;>&#125; */ @OptLogger(value = UPDATE) @ApiOperation(\"修改定时任务\") @SaCheckPermission(value = \"monitor:task:update\") @PutMapping(\"/admin/task/update\") public Result&lt;?> updateTask(@Validated @RequestBody TaskDTO task) &#123; taskService.updateTask(task); return Result.success(); &#125; /** * 删除定时任务 * * @param taskIdList 定时任务id集合 * @return &#123;@link Result&lt;>&#125; */ @OptLogger(value = DELETE) @ApiOperation(\"删除定时任务\") @SaCheckPermission(\"monitor:task:delete\") @DeleteMapping(\"/admin/task/delete\") public Result&lt;?> deleteTask(@RequestBody List&lt;Integer> taskIdList) &#123; taskService.deleteTask(taskIdList); return Result.success(); &#125; /** * 修改定时任务状态 * * @param taskStatus 定时任务状态 * @return &#123;@link Result&lt;>&#125; */ @OptLogger(value = UPDATE) @ApiOperation(\"修改定时任务状态\") @SaCheckPermission(value = &#123;\"monitor:task:update\", \"monitor:task:status\"&#125;, mode = SaMode.OR) @PutMapping(\"/admin/task/changeStatus\") public Result&lt;?> updateTaskStatus(@Validated @RequestBody StatusDTO taskStatus) &#123; taskService.updateTaskStatus(taskStatus); return Result.success(); &#125; /** * 执行定时任务 * * @param taskRun 运行定时任务 * @return &#123;@link Result&lt;>&#125; */ @OptLogger(value = UPDATE) @ApiOperation(\"执行定时任务\") @SaCheckPermission(\"monitor:task:run\") @PutMapping(\"/admin/task/run\") public Result&lt;?> runTask(@RequestBody TaskRunDTO taskRun) &#123; taskService.runTask(taskRun); return Result.success(); &#125; &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Quartz","slug":"Quartz","permalink":"https://blog.ehzyil.xyz/tags/Quartz/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"}],"author":"ehzyil"},{"title":"Git常用操作","slug":"2023/Git常用操作","date":"2023-10-15T18:19:34.000Z","updated":"2024-06-17T01:04:53.975Z","comments":true,"path":"2023/10/15/2023/Git常用操作/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/15/2023/Git%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Git拉取远程分支到本地以https://github.com/houqingying/ChatViewer/tree/main该仓库为例，拉取frontend分支的代码到ChatViewer-frontend文件夹 步骤： 1、新建一个空文件，文件名为ChatViewer-frontend 2、初始化 git init ehZyiL@DESKTOP-1H567GC MINGW64 /d/Study/Java/Code/ChatViewer/ChatViewer-frontend $ git init Initialized empty Git repository in D:/Study/Java/Code/ChatViewer/ChatViewer-frontend/.git/ 3、自己要与origin master建立连接（下划线为远程仓库链接） git remote add origin &#103;&#x69;&#116;&#x40;&#103;&#105;&#116;&#104;&#117;&#98;&#46;&#x63;&#111;&#x6d;:XXXX&#x2F;xxx.git 远程仓库链接在这里，如下图红色框内所示的链接： 输入命令： 4、把远程分支拉到本地 git fetch origin frontend（frontend为远程仓库的分支名） 下图的Branches界面下的均为可使用的分支名 下面拉取远程的frontend分支，命令： ehZyiL@DESKTOP-1H567GC MINGW64 /d/Study/Java/Code/ChatViewer/ChatViewer-frontend (master) $ git remote add origin git@github.com:houqingying/ChatViewer.git ehZyiL@DESKTOP-1H567GC MINGW64 /d/Study/Java/Code/ChatViewer/ChatViewer-frontend (master) $ git fetch origin frontend remote: Enumerating objects: 373, done. remote: Counting objects: 100% (373/373), done. remote: Compressing objects: 100% (331/331), done. remote: Total 373 (delta 26), reused 358 (delta 21), pack-reused 0 Receiving objects: 100% (373/373), 2.66 MiB | 1.04 MiB/s, done. Resolving deltas: 100% (26/26), done. From github.com:houqingying/ChatViewer * branch frontend -> FETCH_HEAD * [new branch] frontend -> origin/frontend 5、切换到该分支 git checkout origin&#x2F;frontend(远程分支名称) 命令： ehZyiL@DESKTOP-1H567GC MINGW64 /d/Study/Java/Code/ChatViewer/ChatViewer-frontend (master) $ git checkout origin/frontend Note: switching to 'origin/frontend'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -c with the switch command. Example: git switch -c &lt;new-branch-name> Or undo this operation with: git switch - Turn off this advice by setting config variable advice.detachedHead to false HEAD is now at f299acd Update README.md ehZyiL@DESKTOP-1H567GC MINGW64 /d/Study/Java/Code/ChatViewer/ChatViewer-frontend ((f299acd...)) $ 可以看到文件夹中已有frontend仓库的文件。 至此，git拉取远程分支到本地已完成。 若需要在本地创建分支并切换到该分支可以执行以下命令(以main分支为例) git checkout -b main(本地新建分支名称) origin&#x2F;main(远程分支名称) ehZyiL@DESKTOP-1H567GC MINGW64 /d/Study/Java/Code/ChatViewer/ChatViewer-admin (master) $ git fetch origin main remote: Enumerating objects: 148, done. remote: Counting objects: 100% (148/148), done. remote: Compressing objects: 100% (121/121), done. remote: Total 148 (delta 23), reused 132 (delta 17), pack-reused 0 Receiving objects: 100% (148/148), 91.02 KiB | 105.00 KiB/s, done. Resolving deltas: 100% (23/23), done. From github.com:houqingying/ChatViewer * branch main -> FETCH_HEAD * [new branch] main -> origin/main ehZyiL@DESKTOP-1H567GC MINGW64 /d/Study/Java/Code/ChatViewer/ChatViewer-admin (master) $ git checkout -b main origin/main Switched to a new branch 'main' branch 'main' set up to track 'origin/main'. ehZyiL@DESKTOP-1H567GC MINGW64 /d/Study/Java/Code/ChatViewer/ChatViewer-admin (main) $ git branch * main 6、拉取远程分支上的最新内容到本地 从远程仓库（origin）的主分支（frontend）拉取最新的代码更新到本地仓库。 git pull origin frontend(远程分支名称) 命令： git pull origin frontend Git提交全部代码 git add . git add xx命令可以将xx文件添加到暂存区，如果有很多改动可以通过 git add -A .来一次添加所有改变的文件。注意 -A 选项后面还有一个句点。 git add -A表示添加所有内容， git add . 表示添加新文件和编辑过的文件不包括删除的文件; git add -u 表示添加编辑或者删除的文件，不包括新添加的文件 git commit -m “提交注释” git push origin 分支名称，一般使用：git push origin master Git删除分支// 删除本地分支 git branch -d localBranchName // 删除远程分支 git push origin --delete remoteBranchName Git 操作——如何删除本地分支和远程分支 Git 如何退出merging如果你想在合并时退出，你可以使用 git merge --abort 来取消合并操作。这会使 Git 回到未合并之前的状态。如果您已经提交了合并，那么可以使用 git reset --hard HEAD 来撤销合并。这将丢弃合并的提交，并回滚到未合并之前的状态。 使用idea 把一个git分支的部分提交记录合并到另一个git分支上 Git cherry-pick合并特定的提交git cherry-pick [&lt;options>] &lt;commit-ish>... 常用options: --quit 退出当前的chery-pick序列 --continue 继续当前的chery-pick序列 --abort 取消当前的chery-pick序列，恢复当前分支 -n, --no-commit 不自动提交 -e, --edit 编辑提交信息 git cherry-pick的使用 Git stash 暂存代码 当我们在某分支开发功能的时候，功能开发到一半，业务方突然传来一个噩耗，有一个bug需要紧急处理一下，优先级为0，这时我们该怎么办呢？？？ 提交我们的修改–这样会产生一次无意义的提交 放弃我们写的代码–我们并不傻 那我们该怎么办呢？ 使用stash命令将修改还未提交的代码存储到缓存区 转载git-命令行-使用 git stash 暂存代码 解决方案： //使用stash命令将修改还未提交的代码存储到缓存区 git stash //切换到需要解决bug的分支 git checkout &lt;分支名> //修改完bug后回到当前开发分支，取出缓存中的代码 git stash pop 查看修改那么我们确定你操作成功了呢？ git stash show &#x2F;&#x2F;查看刚才暂存的修改 我们的修改存储到什么位置了?当我们使用 git init给项目添加版本控制的时候，会在项目路径下生成一个 .git 隐藏文件夹。.git 中存储着版本管理的所有信息。 .git&#x2F;refs&#x2F;stash 中，存储的是最后一个 stash 对应的节点指针 在 .git&#x2F;log&#x2F;refs&#x2F;stash 中可以看到我们全部的 stash 记录信息 多个 stash 的情况我们该怎么办？存储多个stash &#x2F;&#x2F;提交多个stash，我们需要区分每次提交的内容，这样我们可以给每次提交起一个名字来区分 git stash save &lt;名称&gt; 多个stash取出方式 git stash pop &#x2F;&#x2F;取出最近一次暂存并删除记录列表中对应记录 因为 git stash pop 是弹出栈顶的一个 stash ，也就是最后一次存储的 stash。在存储多个stash 时，想取出非栈顶的一个的情况下，是不适用的。 这个时候要使用： git stash list &#x2F;&#x2F;查看暂存区的所有暂存修改 $&gt;git stash list stash@&#123;0&#125;: WIP on book: 51bea1d... fixed images stash@&#123;1&#125;: WIP on master: 9705ae6... changed the browse code to the official repo git stash apply stash@&#123;X&#125; &#x2F;&#x2F;取出相应的暂存 git stash drop stash@&#123;X&#125; &#x2F;&#x2F;将记录列表中取出的对应暂存记录删除 清空stash栈 git stash clear Git commit之后，想撤销commit 原文 写完代码后，我们一般这样 git add . &#x2F;&#x2F;添加所有文件 git commit -m “本功能全部完成” 执行完commit后，想撤回commit，怎么办？ 答案： git reset --soft HEAD^ 这样就成功的撤销了你的commit 注意，仅仅是撤回commit操作，您写的代码仍然保留。 –mixed 意思是：不删除工作空间改动代码，撤销commit，并且撤销git add . 操作 这个为默认参数,git reset –mixed HEAD^ 和 git reset HEAD^ 效果是一样的。 –soft 不删除工作空间改动代码，撤销commit，不撤销git add . –hard 删除工作空间改动代码，撤销commit，撤销git add . 注意完成这个操作后，就恢复到了上一次的commit状态。 Git丢弃本地修改的所有文件（新增、删除、修改） refer git丢弃本地修改的所有文件（新增、删除、修改）_git怎么将已经提交的新增修改全部丢失-CSDN博客 本地修改了许多文件，其中有些是新增的，因为开发需要这些都不要了，想要丢弃掉，可以使用如下命令： git checkout . #本地所有修改的。没有的提交的，都返回到原来的状态 git stash #把所有没有提交的修改暂存到stash里面。可用git stash pop回复。 git reset --hard HASH #返回到某个节点，不保留修改，已有的改动会丢失。 git reset --soft HASH #返回到某个节点, 保留修改，已有的改动会保留，在未提交中，git status或git diff可看。 git clean -df #返回到某个节点，（未跟踪文件的删除） git clean 参数 -n 不实际删除，只是进行演练，展示将要进行的操作，有哪些文件将要被删除。（可先使用该命令参数，然后再决定是否执行） -f 删除文件 -i 显示将要删除的文件 -d 递归删除目录及文件（未跟踪的） -q 仅显示错误，成功删除的文件不显示 注： git reset 删除的是已跟踪的文件，将已commit的回退。 git clean 删除的是未跟踪的文件 也可以使用： git clean -nxdf（查看要删除的文件及目录，确认无误后再使用下面的命令进行删除） git checkout . &amp;&amp; git clean -xdf","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"git","slug":"git","permalink":"https://blog.ehzyil.xyz/tags/git/"}],"author":"ehzyil"},{"title":"Quartz 使用教程","slug":"2023/Quartz使用教程","date":"2023-10-15T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/10/15/2023/Quartz使用教程/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/15/2023/Quartz%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/","excerpt":"","text":"Quartz 介绍Quartz 是目前 Java 领域应用最为广泛的任务调度框架之一，目前很多流行的分布式调度框架，例如 xxl-job 都是基于它衍生出来的，所以了解和掌握 Quartz 的使用，对于平时完成一些需要定时调度的工作会有帮助。 Quartz类似于java.util.Timer。但是相较于Timer， Quartz增加了很多功能： 持久性作业 - 就是保持调度定时的状态; 作业管理 - 对调度作业进行有效的管理; 官方文档： http://www.quartz-scheduler.org/documentation/ http://www.quartz-scheduler.org/ap 快速开始我们通过一个最简单的示例，先快速上手 Quartz 最基本的用法，然后再逐步讲解 Quartz 每个模块的功能点 第一步：添加依赖在 pom.xml 文件添加 Quart 依赖： &lt;!-- 引入 quartz 基础依赖：可取当前最新版本 --> &lt;dependency> &lt;groupId>org.quartz-scheduler&lt;/groupId> &lt;artifactId>quartz&lt;/artifactId> &lt;version>2.3.2&lt;/version> &lt;/dependency> &lt;!-- 引入 quartz 所需的日志依赖：可取当前最新版本 --> &lt;dependency> &lt;groupId>org.slf4j&lt;/groupId> &lt;artifactId>slf4j-api&lt;/artifactId> &lt;version>1.7.26&lt;/version> &lt;scope>compile&lt;/scope> &lt;/dependency> &lt;dependency> &lt;groupId>org.slf4j&lt;/groupId> &lt;artifactId>slf4j-simple&lt;/artifactId> &lt;version>1.7.26&lt;/version> &lt;scope>compile&lt;/scope> &lt;/dependency> 在SpringBoot项目中只需引入spring-boot-starter-quartz即可 &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-quartz&lt;/artifactId> &lt;/dependency> 第二步：配置文件在项目的 classpath 路径下创建 Quartz 默认的 quartz.properties 配置文件，它看起来像这样： # 调度程序的名称 org.quartz.scheduler.instanceName = MyScheduler # 线程数量 org.quartz.threadPool.threadCount = 3 # 内存数据库（推荐刚上手时使用） org.quartz.jobStore.class = org.quartz.simpl.RAMJobStore 第三步：定义任务类实现 Job 接口，然后在覆盖的 execute 函数内定义任务逻辑，如下： package com.ehzyil.quartzStudy.test; import org.quartz.Job; import org.quartz.JobExecutionContext; public class HelloJob implements Job &#123; @Override public void execute(JobExecutionContext context) &#123; System.out.println(\"hello quartz!\"); &#125; &#125; 第四步：任务调度我们简单的使用 main() 方法即可运行 Quartz 任务调度示例： package com.ehzyil.quartzStudy.test; import org.quartz.*; import org.quartz.impl.StdSchedulerFactory; public class QuartzTest &#123; public static void main(String[] args) &#123; try &#123; // 获取默认的调度器实例 Scheduler scheduler = StdSchedulerFactory.getDefaultScheduler(); // 打开调度器 scheduler.start(); // 定义一个简单的任务 JobDetail job = JobBuilder.newJob(HelloJob.class) .withIdentity(\"job11\", \"group1\") .build(); // 定义一个简单的触发器: 每隔 1 秒执行 1 次，任务永不停止 SimpleTrigger trigger = TriggerBuilder.newTrigger() .withIdentity(\"trigger1\", \"group1\") .startNow() .withSchedule(SimpleScheduleBuilder .simpleSchedule() .withIntervalInSeconds(1) .repeatForever() ).build(); // 开始调度任务 scheduler.scheduleJob(job, trigger); // 等待任务执行一些时间 Thread.sleep(3000); // 关闭调度器 scheduler.shutdown(); &#125; catch (Exception se) &#123; se.printStackTrace(); &#125; &#125; &#125; 最后控制台会输出任务运行的全过程，然后关闭进程，如下： 21:54:58.178 [main] INFO org.quartz.impl.StdSchedulerFactory - Using default implementation for ThreadExecutor 21:54:58.182 [main] INFO org.quartz.simpl.SimpleThreadPool - Job execution threads will use class loader of thread: main 21:54:58.194 [main] INFO org.quartz.core.SchedulerSignalerImpl - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl 21:54:58.194 [main] INFO org.quartz.core.QuartzScheduler - Quartz Scheduler v.2.3.2 created. 21:54:58.198 [main] INFO org.quartz.simpl.RAMJobStore - RAMJobStore initialized. 21:54:58.198 [main] INFO org.quartz.core.QuartzScheduler - Scheduler meta-data: Quartz Scheduler (v2.3.2) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED' Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally. NOT STARTED. Currently in standby mode. Number of jobs executed: 0 Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads. Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered. 21:54:58.198 [main] INFO org.quartz.impl.StdSchedulerFactory - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties' 21:54:58.198 [main] INFO org.quartz.impl.StdSchedulerFactory - Quartz scheduler version: 2.3.2 21:54:58.198 [main] INFO org.quartz.core.QuartzScheduler - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started. 21:54:58.198 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.core.QuartzSchedulerThread - batch acquisition of 0 triggers 21:54:58.238 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.core.QuartzSchedulerThread - batch acquisition of 1 triggers 21:54:58.238 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.simpl.PropertySettingJobFactory - Producing instance of Job 'group1.job11', class=com.ehzyil.quartzStudy.test.HelloJob 21:54:58.242 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.core.QuartzSchedulerThread - batch acquisition of 1 triggers 21:54:58.242 [DefaultQuartzScheduler_Worker-1] DEBUG org.quartz.core.JobRunShell - Calling execute on job group1.job11 hello phoenix 21:54:59.203 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.simpl.PropertySettingJobFactory - Producing instance of Job 'group1.job11', class=com.ehzyil.quartzStudy.test.HelloJob 21:54:59.204 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.core.QuartzSchedulerThread - batch acquisition of 1 triggers 21:54:59.204 [DefaultQuartzScheduler_Worker-2] DEBUG org.quartz.core.JobRunShell - Calling execute on job group1.job11 hello phoenix 21:55:00.209 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.simpl.PropertySettingJobFactory - Producing instance of Job 'group1.job11', class=com.ehzyil.quartzStudy.test.HelloJob 21:55:00.209 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.core.QuartzSchedulerThread - batch acquisition of 1 triggers 21:55:00.209 [DefaultQuartzScheduler_Worker-3] DEBUG org.quartz.core.JobRunShell - Calling execute on job group1.job11 hello phoenix 21:55:01.213 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.simpl.PropertySettingJobFactory - Producing instance of Job 'group1.job11', class=com.ehzyil.quartzStudy.test.HelloJob 21:55:01.213 [DefaultQuartzScheduler_QuartzSchedulerThread] DEBUG org.quartz.core.QuartzSchedulerThread - batch acquisition of 1 triggers 21:55:01.213 [DefaultQuartzScheduler_Worker-4] DEBUG org.quartz.core.JobRunShell - Calling execute on job group1.job11 hello phoenix 21:55:01.243 [main] INFO org.quartz.core.QuartzScheduler - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED shutting down. 21:55:01.243 [main] INFO org.quartz.core.QuartzScheduler - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED paused. 21:55:01.243 [main] DEBUG org.quartz.simpl.SimpleThreadPool - Shutting down threadpool... 21:55:01.243 [main] DEBUG org.quartz.simpl.SimpleThreadPool - Shutdown of threadpool complete. 21:55:01.243 [main] INFO org.quartz.core.QuartzScheduler - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED shutdown complete. 可以看到，到这里一个最基本简单的 Quartz 使用示例基本就 OK 了，接下来介绍更多核心概念和深入使用的场景。 核心概念触发器和作业基本概念掌握 Quartz 之前，先来了解它框架的 3 个核心组件和概念，如下： Schduler：调度器，主要用于管理作业（JobDetail），触发器（Trigger） JobDetail：作业实例，内部包含作业运行的具体逻辑 Trigger：触发器实例，内部包含作业执行的实践计划 主要关系如下： 工作流程如上所示，使用 Quartz 的工作流程也很简单，大致如下： 首先基于 Job 接口定义你的作业 JobDetail 实例和触发器 Trigger 实例对象 将定义的作业和触发器实例对象通过调度器 scheduleJob，开始调度执行 调度器启动工作线程开始执行 JobDetail 实例的 execute 方法内容 任务运行时所需信息通过，JobExecutionContext 对象传递到工作线程，也可以在多个工作线程中跨线程传递 示意图： 唯一标识关于创建 JobDetail 作业实例和 Trigger 触发器的几个注意事项： 创建作业和触发器都需要通过（JobKey 和 TriggerKey + Group）组合创建唯一标识 你可以通过唯一标识在 Schduler 中获取作业对象，并且管理和维护他们 引入 Group 标识的目的也是了更好的让你管理作业环境：例如：通过不同的 Group 来区分：【测试作业，生产作业】等 JobDetail 的更多细节通过示例可以看到，定义和使用 Job 都非常简单，但是如果要深入使用，你可能需要了解关于 Job 的更多细节 先看看 Quartz 对于 JobDetail 的处理策略： 每次执行任务都会创建一个新的 JobDetail 实例对象，意味每次执行的 JobDetail 都是新对象，JobDetail 对象也是无状态的 JobDetail 实例对象任务完成后 （execute 方法），调度器 Schduler 会将作业实例对象删除，然后进行垃圾回收 JobDetail 实例之间的状态数据，只能通过 JobExecutionContext（实际上是 JobDataMap） 进行跨作业传递 为什么设计成JobDetail + Job，不直接使用Job？ JobDetail 定义的是任务数据，而真正的执行逻辑是在Job中。 这是因为任务是有可能并发执行，如果Scheduler直接使用Job，就会存在对同一个Job实例并发访问的问题。 而JobDetail &amp; Job 方式，Sheduler每次执行，都会根据JobDetail创建一个新的Job实例，这样就可以 规避并发访问 的问题。 JobDataMapjobDataMap 的使用主要分 2 步： 1：在 execute() 函数内，使用 jobDataMap 获取数据 public class HelloJob implements Job &#123; @Override public void execute(JobExecutionContext context) &#123; // 通过 JobDataMap 对象，可以在作业的执行逻辑中，获取参数 JobDataMap jobDataMap = context.getJobDetail().getJobDataMap(); String name = jobDataMap.getString(\"name\"); System.out.println(\"hello \" + name); &#125; &#125; 2：jobDataMao 添加参数 // 定义作业时，通过 usingJobData 将参数放入 JobDataMap JobDetail job = JobBuilder.newJob(HelloJob.class) .withIdentity(\"job11\", \"group1\") .usingJobData(\"name\", \"phoenix\") .build(); 最后运行效果如下： [main] INFO org.quartz.core.QuartzScheduler - Scheduler MyScheduler_$_NON_CLUSTERED started. hello phoenix [main] INFO org.quartz.core.QuartzScheduler - Scheduler MyScheduler_$_NON_CLUSTERED shutting down. # .... 关于 JobDataMap 的使用，需要关注以下的注意事项： 虽然 JobDataMap 可以传递任意类型的数据，对象的反序列化在版本迭代中容易遇到类版本控制的问题 如果从长远的安全性考虑，尽可能的将 jobDataMap 设置为只允许存放基本类型和字符串（通过 jobStore.useProperties 设置） Quartz 会自动通过 Job 类的 setter 方法和 JobDataMap 主键匹配，帮助你自动注入属性到 Job 类中 并发性和持久化Quartz定时任务默认都是并发执行的，不会等待上一次任务执行完毕，只要间隔时间到就会执行, 如果定时任执行太长，会长时间占用资源，导致其它任务堵塞。 Quartz 对于 Job 提供几个注释，合理的使用可以更好的控制 Quartz 的调度行为，具体如下： @DisallowConcurrentExecution：添加到 Job 类中，告诉 Job 防止相同定义的任务并发执行，例如：任务 A 实例未完成任务，则任务 B 实例不会开始执行（Quartz 默认策略是不会等待，启用新线程并发调度） @PersistJobDataAfterExecution：添加到 Job 类中，默认情况下 Job 作业运行逻辑不会影响到 JobDataMap （既每个 JobDetail 拿到的都是初始化的 JobDataMap 内容），开启该注解后，Job 的 execute() 方法完成后，对于 JobDataMap 的更新，将会被持久化到 JobDataMap 中，从而供其他的 JobDetail 使用，这对于任务 B 依赖任务 A 的运行结果的场景下，非常有用，所以强烈建议和 @DisallowConcurrentExecution 注解一起使用，会让任务运行结果更加符合预期 Trigger 的更多细节和 Job 类似，触发器的定义和使用也非常简单，但是如果想充分的利用它来工作，你还需要了解关于触发器的更多细节 在 Quartz 中 Trigger 触发器有很多种类型，但是他们都有几个共同的属性，如下： startTime：触发器首次生效的时间 endTime：触发器失效时间 以上共同属性的值都是 java.util.Date 对象 关于 Trigger 的其他几个概念： Priority 优先权：当调度器遇到多个同时执行的 Trigger 时候，会根据优先权大小排序，然后先后调度 Misfire 错过触发：Trigger 达到触发时间，但因为外部原因无法执行，Trigger 开始计算 Misfire 时间 常见的外部原因有哪些？例如：调度程序被关闭，线程池无可用工作线程等 Calendar 日历（不是 java.util.calendar 对象）：用于排除执行日期非常有用 例如：定义一个每天 9 点执行 Trigger ，但是排除所有法定节假日 示例: Calendar instance = Calendar.getInstance(); Date startTime = instance.getTime(); instance.add(Calendar.MINUTE, 1); Date endTime = instance.getTime(); // 3.构建Trigger实例 Trigger trigger = TriggerBuilder.newTrigger() .withIdentity(\"trigger1\", \"group1\") // 开始时间 .startAt(startTime) // 结束时间 .endAt(endTime) .build(); SimpleTriggerSimpleTrigger 是适用于大多数场景的触发器，它可以指定特定时间，重复间隔，重复次数等简单场景，它主要设定参数如下： 开始时间 结束时间 重复次数 间隔时间 具体的 API 可以参考 Quartz 的 Java Doc 文档，这里就不赘述了 misfire 处理策略： 我们上面说过 Quartz Misfire 的概念，从源码 SimpleScheduleBuilder 类中可以看到 MISFIRE_INSTRUCTION_SMART_POLICY 是默认的触发策略，但是也我们也可以在创建 Trigger 时候设置我们期望的错过触发策略，如下： SimpleTrigger trigger = TriggerBuilder.newTrigger() .withIdentity(\"trigger1\", \"group1\") .withSchedule(SimpleScheduleBuilder .simpleSchedule() .withIntervalInSeconds(1) .repeatForever() // misfireInstruction = SimpleTrigger.MISFIRE_INSTRUCTION_FIRE_NOW; .withMisfireHandlingInstructionFireNow() ).build(); 在 SimpleTrigger 类中的常量可以看到所有错过触发（misfire）处理逻辑： MISFIRE_INSTRUCTION_FIRE_NOW MISFIRE_INSTRUCTION_RESCHEDULE_NOW_WITH_EXISTING_REPEAT_COUNT MISFIRE_INSTRUCTION_RESCHEDULE_NOW_WITH_REMAINING_REPEAT_COUNT MISFIRE_INSTRUCTION_RESCHEDULE_NEXT_WITH_REMAINING_COUNT MISFIRE_INSTRUCTION_RESCHEDULE_NEXT_WITH_EXISTING_COUNT 关于 misfire 的具体的行为，可以查阅 Quartz 的 Java Doc 文档 CronTrigger相比 SimpleTrigger 可以指定更为复杂的执行计划，CRON 是来自 UNIX 基于时间的任务管理系统，相关内容就不再展开，可以参阅 Cron - (wikipedia.org) 文档进一步了解， Cron 也有类似 SimpleTrigger 的相同属性，设置效果如下： startTime：触发器首次生效的时间 endTime：触发器失效时间 看看 CronTrigger 的使用示例： CronTrigger cronTrigger = TriggerBuilder.newTrigger() .withIdentity(\"trigger1\", \"group1\") .withSchedule(CronScheduleBuilder .cronSchedule(\"0 0/2 8-17 * * ?\") .withMisfireHandlingInstructionFireAndProceed() ) .build(); scheduler.scheduleJob(job, cronTrigger); 上述代码完成以下几件事情： 创建 Cron 表达式：每天上午 8 点到下午 5 点之间每隔一分钟触发一次 指定 MISFIRE_INSTRUCTION_FIRE_NOW 为 CronTrigger 的处理策略 通过 Schduler 对任务开始进行调度 CronTrigger Misfire 策略定义在 CronTrigger 常量中，可以在 Java Doc 文档中查看其具体的行为 Cron 表达式在线生成，反解析的工具：http://www.matools.com/cron Linstener 监听器JobStore 作业存储集群模式配置集群注意事项SpringBoot集成QuartzQuartz 整合 Springboot 非常普遍的场景，整合 Spring 可以带来好处： 更加简洁的配置，开箱即用 和 Spring 的 IOC 容器融合，使用更便捷 添加依赖可以在现有项目上添加 springboot 官方提供的 starter-quartz 依赖，如下： &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-quartz&lt;/artifactId> &lt;/dependency> 因为后面需要写接口测试，因此还需添加 SpringBoot关于web的依赖 &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> 启动 Springboot 会发现，无需任何配置就已经整合 Quartz 模块了： 2023-10-15 22:02:04.654 INFO 20840 --- [ main] org.quartz.impl.StdSchedulerFactory : Quartz scheduler &#39;quartzScheduler&#39; initialized from an externally provided properties instance. 2023-10-15 22:02:04.654 INFO 20840 --- [ main] org.quartz.impl.StdSchedulerFactory : Quartz scheduler version: 2.3.2 2023-10-15 22:02:04.654 INFO 20840 --- [ main] org.quartz.core.QuartzScheduler : JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@4dc8caa7 2023-10-15 22:02:04.686 INFO 20840 --- [ main] o.s.s.quartz.SchedulerFactoryBean : Starting Quartz Scheduler now 2023-10-15 22:02:04.686 INFO 20840 --- [ main] org.quartz.core.QuartzScheduler : Scheduler quartzScheduler_$_NON_CLUSTERED started. 2023-10-15 22:02:04.694 INFO 20840 --- [ main] c.e.quartzStudy.QuartzStudyApplication : Started QuartzStudyApplication in 0.768 seconds (JVM running for 1.504) 使用示例现在基于整合模式实现刚才的 Demo 示例，首先定义任务，这里不再是实现 Job 类： public class HelloJob extends QuartzJobBean &#123; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; JobDataMap jobDataMap = context.getJobDetail().getJobDataMap(); String name = jobDataMap.getString(\"name\"); System.out.println(\"Hello :\" + name); &#125; &#125; 这里实现由 Springboot 提供的 QuartzJobBean，实现 executerInternal() 方法，这是一个经过 Spring 容器包装后的任务类，可以在任务类使用 Spring 容器的实例 在 Demo 示例里面，我们调度启动都是在 Main 方法启动，在本地测试没有问题，但在生产环境就不建议了，和 springboot 整合后关于任务执行，现在可以有 2 中选项： 在控制层 Controller 提供接口，手动接收任务指定 监听 Spring 容器，在容器启动后，自动加载任务，并且注册为 Bean 手动执行我们先看看第一种实现方式，我们创建控制器，然后接收参数，创建任务，如下： @RestController public class HelloController &#123; @Autowired private Scheduler scheduler; @GetMapping(\"/hello\") public void helloJob(String name) throws SchedulerException &#123; // 定义一个的任务 JobDetail job = JobBuilder.newJob(HelloJob.class) .withIdentity(\"job11\", \"group1\") .usingJobData(\"name\", name) .build(); // 定义一个简单的触发器: 每隔 1 秒执行 1 次，任务永不停止 SimpleTrigger trigger = TriggerBuilder.newTrigger() .withIdentity(\"trigger1\", \"group1\") .withSchedule(SimpleScheduleBuilder .simpleSchedule() .withIntervalInSeconds(1) .repeatForever() ).build(); // 开始调度 scheduler.scheduleJob(job, trigger); &#125; &#125; 然后启动服务器，使用http-request访问接口传入参数： GET http://localhost:8080/hello?name=phoenix 然后控制台会输出： 2023-10-15 22:15:38.623 INFO 8060 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring DispatcherServlet 'dispatcherServlet' 2023-10-15 22:15:38.623 INFO 8060 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Initializing Servlet 'dispatcherServlet' 2023-10-15 22:15:38.623 INFO 8060 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : Completed initialization in 0 ms Hello :phoenix Hello :phoenix Hello :phoenix Hello :phoenix .... 自动执行将 JobDetail 注册 Bean，任务就会随 Spring 启动自动触发执行，这对于需要随程序启动执行的作业非常有效，配置如下： 先创建一个配置类： @Configuration public class QuartzConfig &#123; @Bean public JobDetail jobDetail() &#123; JobDetail job = JobBuilder.newJob(HelloJob.class) .withIdentity(\"job11\", \"group1\") .usingJobData(\"name\", \"springboot\") .storeDurably() .build(); return job; &#125; @Bean public Trigger trigger() &#123; SimpleTrigger trigger = TriggerBuilder.newTrigger() .forJob(jobDetail()) .withIdentity(\"trigger1\", \"group1\") .withSchedule(SimpleScheduleBuilder .simpleSchedule() .withIntervalInSeconds(1) .repeatForever() ).build(); return trigger; &#125; &#125; 然后在 springboot 启动后，任务就自动执行： 2023-10-15 22:16:50.481 INFO 21208 --- [ main] c.e.quartzStudy.QuartzStudyApplication : Started QuartzStudyApplication in 1.349 seconds (JVM running for 2.11) Hello :springboot Hello :springboot Hello :springboot ... 工具类 Folding ScheduleUtils 调度工具类 public class ScheduleUtils &#123; /** * 得到quartz任务类 * * @param job 执行计划 * @return 具体执行任务类 */ private static Class&lt;? extends Job> getQuartzJobClass(QuartzJob job) &#123; boolean isConcurrent = \"0\".equals(job.getConcurrent()); return isConcurrent ? QuartzJobExecution.class : QuartzDisallowConcurrentExecution.class; &#125; /** * 构建任务触发对象 */ public static TriggerKey getTriggerKey(Long jobId, String jobGroup) &#123; return TriggerKey.triggerKey(ScheduleConstants.TASK_CLASS_NAME + jobId, jobGroup); &#125; /** * 构建任务键对象 */ public static JobKey getJobKey(Long jobId, String jobGroup) &#123; return JobKey.jobKey(ScheduleConstants.TASK_CLASS_NAME + jobId, jobGroup); &#125; /** * 创建定时任务 */ public static void createScheduleJob(Scheduler scheduler, QuartzJob job) throws Exception &#123; Class&lt;? extends Job> jobClass = getQuartzJobClass(job); // 构建job信息 Long jobId = job.getJobId(); String jobGroup = job.getJobGroup(); JobDetail jobDetail = JobBuilder.newJob(jobClass).withIdentity(getJobKey(jobId, jobGroup)).build(); // 表达式调度构建器 CronScheduleBuilder cronScheduleBuilder = CronScheduleBuilder.cronSchedule(job.getCronExpression()); cronScheduleBuilder = handleCronScheduleMisfirePolicy(job, cronScheduleBuilder); // 按新的cronExpression表达式构建一个新的trigger CronTrigger trigger = TriggerBuilder.newTrigger().withIdentity(getTriggerKey(jobId, jobGroup)) .withSchedule(cronScheduleBuilder).build(); // 放入参数，运行时的方法可以获取 jobDetail.getJobDataMap().put(ScheduleConstants.TASK_PROPERTIES, job); // 判断是否存在 if (scheduler.checkExists(getJobKey(jobId, jobGroup))) &#123; // 防止创建时存在数据问题 先移除，然后在执行创建操作 scheduler.deleteJob(getJobKey(jobId, jobGroup)); &#125; scheduler.scheduleJob(jobDetail, trigger); // 暂停任务 if (job.getStatus().equals(ScheduleConstants.Status.PAUSE.getValue())) &#123; scheduler.pauseJob(ScheduleUtils.getJobKey(jobId, jobGroup)); &#125; &#125; /** * 设置定时任务策略 */ public static CronScheduleBuilder handleCronScheduleMisfirePolicy(QuartzJob job, CronScheduleBuilder cb) throws Exception &#123; switch (job.getMisfirePolicy()) &#123; case ScheduleConstants.MISFIRE_DEFAULT: return cb; case ScheduleConstants.MISFIRE_IGNORE_MISFIRES: return cb.withMisfireHandlingInstructionIgnoreMisfires(); case ScheduleConstants.MISFIRE_FIRE_AND_PROCEED: return cb.withMisfireHandlingInstructionFireAndProceed(); case ScheduleConstants.MISFIRE_DO_NOTHING: return cb.withMisfireHandlingInstructionDoNothing(); default: throw new Exception(\"The task misfire policy '\" + job.getMisfirePolicy() + \"' cannot be used in cron schedule tasks\"); &#125; &#125; &#125; Folding AbstractQuartzJob 抽象任务 public abstract class AbstractQuartzJob implements Job &#123; private static final Logger log = LoggerFactory.getLogger(AbstractQuartzJob.class); /** * 线程本地变量 */ private static ThreadLocal&lt;Date> threadLocal = new ThreadLocal&lt;>(); @Override public void execute(JobExecutionContext context) throws JobExecutionException &#123; QuartzJob job = new QuartzJob(); BeanUtils.copyBeanProp(job, context.getMergedJobDataMap().get(ScheduleConstants.TASK_PROPERTIES)); try &#123; before(context, job); if (job != null) &#123; doExecute(context, job); &#125; after(context, job, null); &#125; catch (Exception e) &#123; log.error(\"任务执行异常 - ：\", e); after(context, job, e); &#125; &#125; /** * 执行前 * * @param context 工作执行上下文对象 * @param job 系统计划任务 */ protected void before(JobExecutionContext context, QuartzJob job) &#123; threadLocal.set(new Date()); &#125; /** * 执行后 * * @param context 工作执行上下文对象 * @param sysJob 系统计划任务 */ protected void after(JobExecutionContext context, QuartzJob sysJob, Exception e) &#123; &#125; /** * 执行方法，由子类重载 * * @param context 工作执行上下文对象 * @param job 系统计划任务 * @throws Exception 执行过程中的异常 */ protected abstract void doExecute(JobExecutionContext context, QuartzJob job) throws Exception; &#125;这个类将原本 execute 方法执行的任务，下放到了子类重载的 doExecute 方法中同时准备实现，分了允许并发和不允许并发，差别就是一个注解public class QuartzJobExecution extends AbstractQuartzJob &#123; @Override protected void doExecute(JobExecutionContext context, QuartzJob job) throws Exception &#123; JobInvokeUtil.invokeMethod(job); &#125; &#125; @DisallowConcurrentExecution public class QuartzDisallowConcurrentExecution extends AbstractQuartzJob &#123; @Override protected void doExecute(JobExecutionContext context, QuartzJob job) throws Exception &#123; JobInvokeUtil.invokeMethod(job); &#125; &#125; Folding JobInvokeUtil :方法调用 最后由 通过反射，进行实际的方法调用public class JobInvokeUtil &#123; /** * 执行方法 * * @param job 系统任务 */ public static void invokeMethod(QuartzJob job) throws Exception &#123; String invokeTarget = job.getInvokeTarget(); String beanName = getBeanName(invokeTarget); String methodName = getMethodName(invokeTarget); List&lt;Object[]> methodParams = getMethodParams(invokeTarget); if (!isValidClassName(beanName)) &#123; Object bean = SpringUtils.getBean(beanName); invokeMethod(bean, methodName, methodParams); &#125; else &#123; Object bean = Class.forName(beanName).newInstance(); invokeMethod(bean, methodName, methodParams); &#125; &#125; /** * 调用任务方法 * * @param bean 目标对象 * @param methodName 方法名称 * @param methodParams 方法参数 */ private static void invokeMethod(Object bean, String methodName, List&lt;Object[]> methodParams) throws NoSuchMethodException, SecurityException, IllegalAccessException, IllegalArgumentException, InvocationTargetException &#123; if (StringUtils.isNotNull(methodParams) &amp;&amp; methodParams.size() > 0) &#123; Method method = bean.getClass().getDeclaredMethod(methodName, getMethodParamsType(methodParams)); method.invoke(bean, getMethodParamsValue(methodParams)); &#125; else &#123; Method method = bean.getClass().getDeclaredMethod(methodName); method.invoke(bean); &#125; &#125; /** * 校验是否为为class包名 * * @param invokeTarget 名称 * @return true是 false否 */ public static boolean isValidClassName(String invokeTarget) &#123; return StringUtils.countMatches(invokeTarget, \".\") > 1; &#125; /** * 获取bean名称 * * @param invokeTarget 目标字符串 * @return bean名称 */ public static String getBeanName(String invokeTarget) &#123; String beanName = StringUtils.substringBefore(invokeTarget, \"(\"); return StringUtils.substringBeforeLast(beanName, \".\"); &#125; /** * 获取bean方法 * * @param invokeTarget 目标字符串 * @return method方法 */ public static String getMethodName(String invokeTarget) &#123; String methodName = StringUtils.substringBefore(invokeTarget, \"(\"); return StringUtils.substringAfterLast(methodName, \".\"); &#125; /** * 获取method方法参数相关列表 * * @param invokeTarget 目标字符串 * @return method方法相关参数列表 */ public static List&lt;Object[]> getMethodParams(String invokeTarget) &#123; String methodStr = StringUtils.substringBetween(invokeTarget, \"(\", \")\"); if (StringUtils.isEmpty(methodStr)) &#123; return null; &#125; String[] methodParams = methodStr.split(\",\"); List&lt;Object[]> classs = new LinkedList&lt;>(); for (int i = 0; i &lt; methodParams.length; i++) &#123; String str = StringUtils.trimToEmpty(methodParams[i]); // String字符串类型，包含' if (StringUtils.contains(str, \"'\")) &#123; classs.add(new Object[]&#123;StringUtils.replace(str, \"'\", \"\"), String.class&#125;); &#125; // boolean布尔类型，等于true或者false else if (StringUtils.equals(str, \"true\") || StringUtils.equalsIgnoreCase(str, \"false\")) &#123; classs.add(new Object[]&#123;Boolean.valueOf(str), Boolean.class&#125;); &#125; // long长整形，包含L else if (StringUtils.containsIgnoreCase(str, \"L\")) &#123; classs.add(new Object[]&#123;Long.valueOf(StringUtils.replaceIgnoreCase(str, \"L\", \"\")), Long.class&#125;); &#125; // double浮点类型，包含D else if (StringUtils.containsIgnoreCase(str, \"D\")) &#123; classs.add(new Object[]&#123;Double.valueOf(StringUtils.replaceIgnoreCase(str, \"D\", \"\")), Double.class&#125;); &#125; // 其他类型归类为整形 else &#123; classs.add(new Object[]&#123;Integer.valueOf(str), Integer.class&#125;); &#125; &#125; return classs; &#125; /** * 获取参数类型 * * @param methodParams 参数相关列表 * @return 参数类型列表 */ public static Class&lt;?>[] getMethodParamsType(List&lt;Object[]> methodParams) &#123; Class&lt;?>[] classs = new Class&lt;?>[methodParams.size()]; int index = 0; for (Object[] os : methodParams) &#123; classs[index] = (Class&lt;?>) os[1]; index++; &#125; return classs; &#125; /** * 获取参数值 * * @param methodParams 参数相关列表 * @return 参数值列表 */ public static Object[] getMethodParamsValue(List&lt;Object[]> methodParams) &#123; Object[] classs = new Object[methodParams.size()]; int index = 0; for (Object[] os : methodParams) &#123; classs[index] = (Object) os[0]; index++; &#125; return classs; &#125; &#125; Folding cron表达式工具类 public class CronUtils &#123; /** * 返回一个布尔值代表一个给定的Cron表达式的有效性 * * @param cronExpression Cron表达式 * @return boolean 表达式是否有效 */ public static boolean isValid(String cronExpression) &#123; return CronExpression.isValidExpression(cronExpression); &#125; /** * 返回下一个执行时间根据给定的Cron表达式 * * @param cronExpression Cron表达式 * @return Date 下次Cron表达式执行时间 */ public static Date getNextExecution(String cronExpression) &#123; try &#123; CronExpression cron = new CronExpression(cronExpression); return cron.getNextValidTimeAfter(new Date(System.currentTimeMillis())); &#125; catch (ParseException e) &#123; throw new IllegalArgumentException(e.getMessage()); &#125; &#125; &#125; 参考文章 [1] Quartz 使用教程 [2] 任务调度框架 Quartz 用法指南「超详细」","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Quartz","slug":"Quartz","permalink":"https://blog.ehzyil.xyz/tags/Quartz/"}],"author":"ehzyil"},{"title":"IntelliJ IDEA创建JSP项目","slug":"2023/IntelliJ IDEA创建JSP项目","date":"2023-10-12T00:00:00.000Z","updated":"2024-06-17T01:04:53.975Z","comments":true,"path":"2023/10/12/2023/IntelliJ IDEA创建JSP项目/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/12/2023/IntelliJ%20IDEA%E5%88%9B%E5%BB%BAJSP%E9%A1%B9%E7%9B%AE/","excerpt":"","text":"创建项目在IntelliJ IDEA中创建jsp项目的步骤如下: 1.点击”Create New Project” 2.选择“Java Enterprise”并将配置设置为箭头所指内容后点击”Next” 输入项目名称并选择存储位置，选择Web Application，选择要使用的Web框架（若为空，自行去下载Tomcat并配置），选择构建系统，选择JDK版本 3.若不选择其他依赖项默认只有“Servlet”依赖，点击Create创建项目 4.创建后的JSP项目的结构如下： ├─.idea │ ├─artifacts │ └─libraries ├─.mvn │ └─wrapper ├─src │ ├─main │ │ ├─java │ │ │ └─com │ │ │ └─example │ │ │ └─practicaltraining │ │ ├─resources │ │ └─webapp │ │ └─WEB-INF │ └─test │ ├─java │ └─resources └─target ├─classes │ └─com │ └─example │ └─practicaltraining ├─generated-sources │ └─annotations └─practicalTraining-1.0-SNAPSHOT ├─META-INF └─WEB-INF └─classes └─com └─example └─practicaltraining 5.点击”Run”即可运行,项目默认创建的有一个”&#x2F;helloServlet”。项目运行后点击即可响应”Hello Servlet”。 更改默认跳转页面1.进入src-&gt;main-&gt;webapp-&gt;WEB-INF下的web.xml文件。 2.更改添加&lt;welcome-file-list&gt;和 &lt;welcome-file&gt;标签并用 &lt;welcome-file&gt;&lt;/welcome-file&gt;包裹启动后要跳转的页面名，该项目默认跳转的页面为login.jsp。 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\" version=\"4.0\"> &lt;welcome-file-list> &lt;welcome-file>login.jsp&lt;/welcome-file> &lt;/welcome-file-list> &lt;/web-app>","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"IntelliJ IDEA","slug":"IntelliJ-IDEA","permalink":"https://blog.ehzyil.xyz/tags/IntelliJ-IDEA/"},{"name":"JSP","slug":"JSP","permalink":"https://blog.ehzyil.xyz/tags/JSP/"}],"author":"ehzyil"},{"title":"刷题","slug":"数据结构与算法/刷题","date":"2023-10-12T00:00:00.000Z","updated":"2024-06-17T01:04:54.019Z","comments":true,"path":"2023/10/12/数据结构与算法/刷题/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%88%B7%E9%A2%98/","excerpt":"","text":"二分查找 69. x 的平方根 - 力扣（LeetCode） 69. x 的平方根 - 力扣（LeetCode）给你一个非负整数 x ，计算并返回 x 的 算术平方根 。 由于返回类型是整数，结果只保留 整数部分 ，小数部分将被 舍去 。 注意：不允许使用任何内置指数函数和算符，例如 pow(x, 0.5) 或者 x ** 0.5 。 示例 1： 输入：x &#x3D; 4 输出：2 示例 2： 输入：x &#x3D; 8 输出：2 解释：8 的算术平方根是 2.82842..., 由于返回类型是整数，小数部分将被舍去。 提示： 0 &lt;= x &lt;= 231 - 1 答案： public int mySqrt(int x) &#123; //处理特殊情况 if (x == 0) &#123; return 0; &#125; if (x == 1) &#123; return 1; &#125; int i = 0, j = x / 2 + 1; //使用平衡版二分法寻找比 中间数平方小的 while (1 &lt; j - i) &#123; int mid = (i + j) >>> 1; //求m的平方时会出现大于int值的情况，此时需要转为long进行计算 if ((long) mid * mid > x) &#123; j = mid; &#125; else &#123; i = mid; &#125; &#125; return i; &#125; E01. 二分查找-力扣 704 题要点：减而治之，可以用递归或非递归实现 给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target ，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1 例如 输入: nums = [-1,0,3,5,9,12], target = 9 输出: 4 解释: 9 出现在 nums 中并且下标为 4 输入: nums = [-1,0,3,5,9,12], target = 2 输出: -1 解释: 2 不存在 nums 中因此返回 -1 参考答案：可以用讲过的任意一种二分求解 static int binarySearch(int[] a, int target) &#123; int i = 0, j = a.length; while (1 &lt; j - i) &#123; int mid = (i + j) >>> 1; if (a[mid] &lt; target) &#123; i = mid + 1; &#125; else &#123; j = mid - 1; &#125; &#125; return a[i] == target ? i : -1; &#125; E02. 搜索插入位置-力扣 35 题要点：理解谁代表插入位置 给定一个排序数组和一个目标值 在数组中找到目标值，并返回其索引 如果目标值不存在于数组中，返回它将会被按顺序插入的位置 例如 输入: nums &#x3D; [1,3,5,6], target &#x3D; 5 输出: 2 输入: nums &#x3D; [1,3,5,6], target &#x3D; 2 输出: 1 输入: nums &#x3D; [1,3,5,6], target &#x3D; 7 输出: 4 参考答案1：用二分查找基础版代码改写，基础版中，找到返回 m，没找到 i 代表插入点，因此有 public int searchInsert(int[] a, int target) &#123; int i = 0, j = a.length - 1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m - 1; &#125; else if (a[m] &lt; target) &#123; i = m + 1; &#125; else &#123; return m; &#125; &#125; return i; // 原始 return -1 &#125; 参考答案2：用二分查找平衡版改写，平衡版中 如果 target &#x3D;&#x3D; a[i] 返回 i 表示找到 如果 target &lt; a[i]，例如 target &#x3D; 2，a[i] &#x3D; 3，这时就应该在 i 位置插入 2 如果 a[i] &lt; target，例如 a[i] &#x3D; 3，target &#x3D; 4，这时就应该在 i+1 位置插入 4 public static int searchInsert(int[] a, int target) &#123; int i = 0, j = a.length; while (1 &lt; j - i) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m; &#125; else &#123; i = m; &#125; &#125; return (target &lt;= a[i]) ? i : i + 1; // 原始 (target == a[i]) ? i : -1; &#125; 参考答案3：用 leftmost 版本解，返回值即为插入位置（并能处理元素重复的情况） public int searchInsert(int[] a, int target) &#123; int i = 0, j = a.length - 1; while(i &lt;= j) &#123; int m = (i + j) >>> 1; if(target &lt;= a[m]) &#123; j = m - 1; &#125; else &#123; i = m + 1; &#125; &#125; return i; &#125; E03. 搜索开始结束位置-力扣 34 题给你一个按照非递减顺序排列的整数数组 nums，和一个目标值 target。请你找出给定目标值在数组中的开始位置和结束位置。 如果数组中不存在目标值 target，返回 [-1, -1]。 你必须设计并实现时间复杂度为 O(log n) 的算法解决此问题 例如 输入：nums &#x3D; [5,7,7,8,8,10], target &#x3D; 8 输出：[3,4] 输入：nums &#x3D; [5,7,7,8,8,10], target &#x3D; 6 输出：[-1,-1] 输入：nums &#x3D; [], target &#x3D; 0 输出：[-1,-1] 参考答案 public static int left(int[] a, int target) &#123; int i = 0, j = a.length - 1; int candidate = -1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m - 1; &#125; else if (a[m] &lt; target) &#123; i = m + 1; &#125; else &#123; candidate = m; j = m - 1; &#125; &#125; return candidate; &#125; public static int right(int[] a, int target) &#123; int i = 0, j = a.length - 1; int candidate = -1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m - 1; &#125; else if (a[m] &lt; target) &#123; i = m + 1; &#125; else &#123; candidate = m; i = m + 1; &#125; &#125; return candidate; &#125; public static int[] searchRange(int[] nums, int target) &#123; int x = left(nums, target); if(x == -1) &#123; return new int[] &#123;-1, -1&#125;; &#125; else &#123; return new int[] &#123;x, right(nums, target)&#125;; &#125; &#125; 递归 - single recursionE03. 二分查找class Solution &#123; public static int recursion(int[] a, int target, int i, int j) &#123; //跳出递归条件 if (i > j) &#123; return -1; &#125; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; return recursion(a, target, i, m - 1); &#125; else if (a[m] &lt; target) &#123; return recursion(a, target, m + 1, j); &#125; else &#123; return m; &#125; &#125; public int search(int[] nums, int target) &#123; return recursion(nums, target, 0, nums.length - 1); &#125; &#125; E04. 冒泡排序public static void main(String[] args) &#123; int[] a = &#123;3, 2, 6, 1, 5, 4, 7&#125;; bubble(a, 0, a.length - 1); System.out.println(Arrays.toString(a)); &#125; /** * 递归冒泡排序 * &lt;ul> * &lt;li>将数组划分成两部分 [0 .. j] [j+1 .. a.length-1]&lt;/li> * &lt;li>左边 [0 .. j] 是未排序部分&lt;/li> * &lt;li>右边 [j+1 .. a.length-1] 是已排序部分&lt;/li> * &lt;li>未排序区间内, 相邻的两个元素比较, 如果前一个大于后一个, 则交换位置&lt;/li> * &lt;/ul> */ private static void bubble(int[] a, int low, int high) &#123; //终止条件 左右边界相等 if (low == high) &#123; return; &#125; //每一轮排序 重置j的值 int j = low; //for循环用来排序 for (int i = low; i &lt; high; i++) &#123; if (a[i] > a[i + 1]) &#123; //满足左比右大 交换顺序 int temp=a[i]; a[i]=a[i+1]; a[i+1]=temp; //j用来记录已排序的右边届 j右边的已排序完成 j=i; &#125; &#125; //递归进入下一轮 bubble(a,low,j); &#125; low 与 high 为排序范围 j 表示的是未排序的边界，下一次递归时的 high 发生交换，意味着有无序情况 最后一次交换（以后没有无序）时，左侧 i 仍是无序，右侧 i+1 已然有序 E05. 插入排序版本v1 public static void main(String[] args) &#123; int[] a = &#123;3, 2, 6, 1, 5, 7, 4&#125;; insertion(a, 1); System.out.println(Arrays.toString(a)); &#125; private static void insertion(int[] a, int low) &#123; // 结束条件 low 是已排序和未排序的边界，当low等于数组长度的时候说明全部排序完成 if (low == a.length) return; // low所指下向的值 int t = a[low]; // 已排序区域指针 int i = low - 1; // 通过循环将符合条件的值右移 i>=0来处理只有一张牌的情况 即i=-1 while (i >= 0 &amp;&amp; a[i] > t) &#123; a[i + 1] = a[i]; // 空出插入位置 i--; &#125; // 找到了插入的位置 // 若i+1==low 说明没有移动 if (i + 1 != low) &#123; a[i + 1] = t; &#125; // 递归 insertion(a, low + 1); &#125; 已排序区域：[0 .. i .. low-1] 未排序区域：[low .. high] 版本v2 public static void main(String[] args) &#123; int[] a = &#123;3, 2, 6, 1, 5, 7, 4&#125;; insertion(a, 1, a.length - 1); System.out.println(Arrays.toString(a)); &#125; private static void insertion(int[] a, int low, int high) &#123; if (low > high) &#123; return; &#125; int t = a[low]; int i = low - 1; while (i >= 0 &amp;&amp; a[i] > t) &#123; a[i + 1] = a[i]; i--; &#125; if(i + 1 != low) &#123; a[i + 1] = t; &#125; insertion(a, low + 1, high); &#125; 第一块代码是只考虑 low 边界的情况，参考以上代码，理解 low-1 .. high 范围内的处理方法 扩展：利用二分查找 leftmost 版本，改进寻找插入位置的代码 版本v3 另一种插入排序的实现,缺点: 赋值次数较多private static void insertion2(int[] a, int low) &#123; if (low == a.length) &#123; return; &#125; int i = low - 1; while (i >= 0 &amp;&amp; a[i] > a[i + 1]) &#123; int t = a[i]; a[i] = a[i + 1]; a[i + 1] = t; i--; &#125; insertion(a, low + 1); &#125; E06. 约瑟夫问题[^16]待完成！！！ 递归 - multi recursionE02. 汉诺塔[^13]Tower of Hanoi，是一个源于印度古老传说：大梵天创建世界时做了三根金刚石柱，在一根柱子从下往上按大小顺序摞着 64 片黄金圆盘，大梵天命令婆罗门把圆盘重新摆放在另一根柱子上，并且规定 一次只能移动一个圆盘 小圆盘上不能放大圆盘 下面的动图演示了4片圆盘的移动方法 使用程序代码模拟圆盘的移动过程，并估算出时间复杂度 思路 假设每根柱子标号 a，b，c，每个圆盘用 1，2，3 … 表示其大小，圆盘初始在 a，要移动到的目标是 c 如果只有一个圆盘，此时是最小问题，可以直接求解 移动圆盘1 $a \\mapsto c$ 如果有两个圆盘，那么 圆盘1 $a \\mapsto b$ 圆盘2 $a \\mapsto c$ 圆盘1 $b \\mapsto c$ 如果有三个圆盘，那么 圆盘12 $a \\mapsto b$ 圆盘3 $a \\mapsto c$ 圆盘12 $b \\mapsto c$ 如果有四个圆盘，那么 圆盘 123 $a \\mapsto b$ 圆盘4 $a \\mapsto c$ 圆盘 123 $b \\mapsto c$ 题解 public class E02HanoiTower &#123; static LinkedList&lt;Integer> a = new LinkedList&lt;>(); static LinkedList&lt;Integer> b = new LinkedList&lt;>(); static LinkedList&lt;Integer> c = new LinkedList&lt;>(); public static void main(String[] args) &#123; StopWatch sw = new StopWatch(); int n = 4; init(n); sw.start(\"n=\"+n); move(n,a,b,c); sw.stop(); print(); System.out.println(sw.prettyPrint()); &#125; /** * &lt;h3>移动圆盘&lt;/h3> * * @param n 圆盘个数 * @param a 由 * @param b 借 * @param c 至 */ static void move(int n, LinkedList&lt;Integer> a, LinkedList&lt;Integer> b, LinkedList&lt;Integer> c) &#123; //终止条件 n==0 if (n == 0) return; //将n-1个从a借助c移动到b move(n - 1, a, c, b); //将最后一个在a的盘子移动到c c.addLast(a.removeLast()); // print(); //将n-1个从b借助a移动到c move(n - 1, b,a, c); &#125; static void init(int n) &#123; for (int i = 1; i &lt;= n; i++) &#123; a.add(i); &#125; &#125; //打印列表 private static void print() &#123; System.out.println(\"******************************\"); System.out.println(a); System.out.println(b); System.out.println(c); &#125; &#125; E03. 杨辉三角[^6] 分析 把它斜着看 1 1 1 1 2 1 1 3 3 1 1 4 6 4 1 1 8 1 1 6 (n-i-1)*2 1 2 1 4 1 3 3 1 2 1 4 6 4 1 0 行 $i$，列 $j$，那么 $[i][j]$ 的取值应为 $[i-1][j-1] + [i-1][j]$ 当 $j&#x3D;0$ 或 $i&#x3D;j$ 时，$[i][j]$ 取值为 $1$ 题解 public static void print(int n) &#123; for (int i = 0; i &lt; n; i++) &#123; //打印字符串 if (i &lt; n - 1) &#123; System.out.printf(\"%\" + 2 * (n - 1 - i) + \"s\", \" \"); &#125; //printSpace((n - i - 1) * 4); for (int j = 0; j &lt; i + 1; j++) &#123; System.out.printf(\"%-4d\", element(i, j)); &#125; //换行 System.out.println(); &#125; &#125; //递归函数 public static int element(int i, int j) &#123; if (j == 0 || i == j) &#123; return 1; &#125; return element(i - 1, j - 1) + element(i - 1, j); &#125; public static void printSpace(int n) &#123; for (int i = 0; i &lt; n; i++) &#123; System.out.print(\" \"); &#125; &#125; 优化 优化1是 multiple recursion，因此很多递归调用是重复的，例如recursion(3, 1) 分解为recursion(2, 0) + recursion(2, 1)而 recursion(3, 2) 分解为recursion(2, 1) + recursion(2, 2)这里 recursion(2, 1) 就重复调用了，事实上它会重复很多次，可以用 static AtomicInteger counter &#x3D; new AtomicInteger(0) 来查看递归函数的调用总次数事实上，可以用 memoization 来进行优化：public static void print1(int n) &#123; int[][] triangle = new int[n][]; for (int i = 0; i &lt; n; i++) &#123; // 打印空格 triangle[i] = new int[i + 1]; for (int j = 0; j &lt;= i; j++) &#123; System.out.printf(\"%-4d\", element1(triangle, i, j)); &#125; System.out.println(); &#125; &#125; public static int element1(int[][] triangle, int i, int j) &#123; if (triangle[i][j] > 0) &#123; return triangle[i][j]; &#125; if (j == 0 || i == j) &#123; triangle[i][j] = 1; return triangle[i][j]; &#125; triangle[i][j] = element1(triangle, i - 1, j - 1) + element1(triangle, i - 1, j); return triangle[i][j]; &#125;将数组作为递归函数内可以访问的遍历，如果 $triangle[i][j]$ 已经有值，说明该元素已经被之前的递归函数计算过，就不必重复计算了优化2public static void print2(int n) &#123; int[] row = new int[n]; for (int i = 0; i &lt; n; i++) &#123; // 打印空格 createRow(row, i); for (int j = 0; j &lt;= i; j++) &#123; System.out.printf(\"%-4d\", row[j]); &#125; System.out.println(); &#125; &#125; private static void createRow(int[] row, int i) &#123; if (i == 0) &#123; row[0] = 1; return; &#125; for (int j = i; j > 0; j--) &#123; row[j] = row[j - 1] + row[j]; &#125; &#125;注意：还可以通过每一行的前一项计算出下一项，不必借助上一行，这与杨辉三角的另一个特性有关，暂不展开了 链表E01. 反转单向链表-力扣 206 题对应力扣题目 206. 反转链表 - 力扣（LeetCode） 输入：head &#x3D; [1,2,3,4,5] 输出：[5,4,3,2,1] 输入：[1,2] 输出：[2,1] 输入：[] 输出：[] 方法1 构造一个新链表，从旧链表依次拿到每个节点，创建新节点添加至新链表头部，完成后新链表即是倒序的 public ListNode reverseList(ListNode o1) &#123; ListNode n1 = null; ListNode p = o1; while (p != null) &#123; //null-> 1->null -> 2,1->null n1 = new ListNode(p.val, n1); p = p.next; &#125; return n1; &#125; 评价：简单直白，就是得新创建节点对象 方法2 与方法1 类似，构造一个新链表，从旧链表头部移除节点，添加到新链表头部，完成后新链表即是倒序的，区别在于原题目未提供节点外层的容器类，这里提供一个，另外一个区别是并不去构造新节点 static class List &#123; ListNode head; public List(ListNode head) &#123; this.head = head; &#125; public ListNode removeFirst()&#123; ListNode first = head; if (first != null) &#123; head = first.next; &#125; return first; &#125; public void addFirst(ListNode first) &#123; first.next = head; head = first; &#125; &#125; 代码 public ListNode reverseList(ListNode head) &#123; List list1 = new List(head); List list2 = new List(null); ListNode first; while ((first = list1.removeFirst()) != null) &#123; list2.addFirst(first); &#125; return list2.head; &#125; 评价：更加面向对象，如果实际写代码而非刷题，更多会这么做 方法3 递归，在归时让 $5 \\rightarrow 4$，$4 \\rightarrow 3$ … 首先，写一个递归方法，返回值用来拿到最后一个节点 public ListNode reverseList(ListNode p) &#123; if (p == null || p.next == null) &#123; // 不足两个节点 return p; // 最后一个节点 &#125; ListNode last = reverseList(p.next); return last; &#125; 注意1：递归终止条件是 curr.next &#x3D;&#x3D; null，目的是到最后一个节点就结束递归，与之前递归遍历不一样 注意2：需要考虑空链表即 p &#x3D;&#x3D; null 的情况 可以先测试一下 ListNode o5 = new ListNode(5, null); ListNode o4 = new ListNode(4, o5); ListNode o3 = new ListNode(3, o4); ListNode o2 = new ListNode(2, o3); ListNode o1 = new ListNode(1, o2); ListNode n1 = new E01Leetcode206().reverseList(o1); System.out.println(n1); 会打印 [5] 下面为伪码调用过程，假设节点分别是 $1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow null$，先忽略返回值 reverseList(ListNode p = 1) &#123; reverseList(ListNode p = 2) &#123; reverseList(ListNode p = 3) &#123; reverseList(ListNode p = 4) &#123; reverseList(ListNode p = 5) &#123; if (p == null || p.next == null) &#123; return p; // 返回5 &#125; &#125; // 此时p是4, p.next是5 &#125; // 此时p是3, p.next是4 &#125; // 此时p是2, p.next是3 &#125; // 此时p是1, p.next是2 &#125; 接下来，从 p &#x3D; 4 开始，要让 $5 \\rightarrow 4$，$4 \\rightarrow 3$ … reverseList(ListNode p = 1) &#123; reverseList(ListNode p = 2) &#123; reverseList(ListNode p = 3) &#123; reverseList(ListNode p = 4) &#123; reverseList(ListNode p = 5) &#123; if (p == null || p.next == null) &#123; return p; // 返回5 &#125; &#125; // 此时p是4, p.next是5, 此时p.next.next=null,要让5指向4,代码写成 p.next.next=p // 还要注意4要指向 null, 否则就死链了（4&lt;==>5即4.next=5，5.next=4） &#125; // 此时p是3, p.next是4,p.next.next=p 4->3 &#125; // 此时p是2, p.next是3 &#125; // 此时p是1, p.next是2 &#125; 最终代码为： ListNode reverseList(ListNode head) &#123; // 如果链表为空或只有一个节点，则直接返回原链表 if (head == null || head.next == null) &#123; return head; &#125; // 递归调用反转链表方法，将当前节点的下一个节点作为参数传入 ListNode last = reverse(head.next); // 将当前节点的下一个节点的下一个节点指向当前节点，实现反转 head.next.next = head; // 将当前节点的下一个节点设为null，断开原链表与反转后链表的连接 head.next = null; // 返回反转后的链表头节点 return last; &#125; Q：为啥不能在递的过程中倒序？ A：比如 $ 1 \\rightarrow 2 \\rightarrow 3 $ 如果递的过程中让 $2 \\rightarrow 1$ 那么此时 $2 \\rightarrow 3$ 就被覆盖，不知道接下来递给谁 而归的时候让 $3 \\rightarrow 2$ 不会影响上一层的 $1 \\rightarrow 2$ 评价：单向链表没有 prev 指针，但利用递归的特性【记住了】链表每次调用时相邻两个节点是谁 未实现的方法 方法4从链表每次拿到第二个节点，将其从链表断开，插入头部，直至它为 null 结束设置指针 o1(旧头)、n1(新头)、o2(旧老二)，分别指向第一，第一，第二节点$\\frac{n1 \\ o1}{1} \\rightarrow \\frac{o2}{2} \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow null$将 o2 节点从链表断开，即 o1 节点指向第三节点$ \\frac{n1 \\ o1}{1} \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow null$ ，$\\frac{o2}{2}$o2 节点链入链表头部，即$\\frac{o2}{2} \\rightarrow \\frac{n1 \\ o1}{1} \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow null$n1 指向 o2$\\frac{n1 \\ o2}{2} \\rightarrow \\frac{o1}{1} \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow null$o2 指向 o1 的下一个节点，即$\\frac{n1}{2} \\rightarrow \\frac{o1}{1} \\rightarrow \\frac{o2}{3} \\rightarrow 4 \\rightarrow 5 \\rightarrow null$重复以上 $2\\sim5$ 步，直到 o2 指向 null还应当考虑边界条件，即链表中不满两个元素时，无需走以上逻辑参考答案public ListNode reverseList(ListNode o1) &#123; if (o1 == null || o1.next == null) &#123; // 不足两个节点 return o1; &#125; ListNode o2 = o1.next; ListNode n1 = o1; while (o2 != null) &#123; o1.next = o2.next; o2.next = n1; n1 = o2; o2 = o1.next; &#125; return n1; &#125;方法5要点：把链表分成两部分，思路就是不断从链表2的头，往链表1的头搬移n1 指向 null，代表新链表一开始没有元素，o1 指向原链表的首节点$\\frac{n1}{null}$，$\\frac{o1}{1} \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow null$开始循环，o2 指向原链表次节点$\\frac{n1}{null}$，$\\frac{o1}{1} \\rightarrow \\frac{o2}{2} \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow null$搬移$\\frac{o1}{1} \\rightarrow \\frac{n1}{null}$ ， $\\frac{o2}{2} \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow null$指针复位$\\frac{n1}{1} \\rightarrow null$ ， $\\frac{o1 \\ o2}{2} \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow null$重复 $2\\sim4$ 步当 o1 &#x3D; null 时退出循环参考答案public ListNode reverseList(ListNode o1) &#123; if (o1 == null || o1.next == null) &#123; return o1; &#125; ListNode n1 = null; while (o1 != null) &#123; ListNode o2 = o1.next; o1.next = n1; n1 = o1; o1 = o2; &#125; return n1; &#125;评价：本质上与方法2 相同，只是方法2更为面向对象题例如输入：head &#x3D; [1,2,6,3,6], val &#x3D; 6 输出：[1,2,3] 输入：head &#x3D; [], val &#x3D; 1 输出：[] 输入：head &#x3D; [7,7,7,7], val &#x3D; 7 输出：[] E03. 删除倒数节点-力扣 19 题例如 输入：head &#x3D; [1,2,3,4,5], n &#x3D; 2 输出：[1,2,3,5] 输入：head &#x3D; [1], n &#x3D; 1 输出：[] 输入：head &#x3D; [1,2], n &#x3D; 1 输出：[1] 另外题目提示 链表至少一个节点 n 只会在合理范围 解法一 ​ 写一个递归方法，得到倒数的次序。 static int recursion(ListNode p, int n) &#123; if (p == null) &#123; return 0; &#125; int nth = recursion(p.next, n); System.out.println(\"nth:\"+nth+\"\\t p:\"+p.toString()); if (nth == n) &#123; p.next = p.next.next; &#125; return nth + 1; &#125; nth是倒数的次序 p为当前节点 nth:0 p:[5] nth:1 p:[4,5] nth:2 p:[3,4,5] nth:3 p:[2,3,4,5] nth:4 p:[1,2,4,5] 当nth&#x3D;&#x3D;n时 p为要删除节点的上一个，删除节点即可。 但上述代码有一个问题，就是若删除的是第一个节点，它没有上一个节点，因此可以加一个哨兵来解决 public ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode sentinel = new ListNode(-1, head); recursion(sentinel, n); return sentinel.next; &#125; public int recursion(ListNode p, int n) &#123; if (p == null) &#123; return 0; &#125; int nth = recursion(p.next, n); if (nth == n) &#123; p.next = p.next.next; &#125; return nth + 1; &#125; 解法二 快慢指针，p1 指向待删节点的上一个，p2 先走 n + 1 步 i=0 p2 s -> 1 -> 2 -> 3 -> 4 -> 5 -> null i=1 p2 s -> 1 -> 2 -> 3 -> 4 -> 5 -> null i=2 p2 s -> 1 -> 2 -> 3 -> 4 -> 5 -> null i=3 从此开始 p1 p2 依次向右平移, 直到 p2 移动到末尾 p1 p2 s -> 1 -> 2 -> 3 -> 4 -> 5 -> null p1 p2 s -> 1 -> 2 -> 3 -> 4 -> 5 -> null 代码： public static ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode s = new ListNode(-1, head); ListNode p1 = s; ListNode p2 = s; for (int i = 0; i &lt; n + 1; i++) &#123; p2 = p2.next; &#125; while (p2 != null) &#123; p1 = p1.next; p2 = p2.next; &#125; p1.next = p1.next.next; return s.next; &#125; 这个方法中，使用了两个指针p1和p2来定位要删除的节点。首先，p2指针先向后移动n+1步，使得p1和p2之间的距离为n+1。然后，同时移动p1和p2指针，直到p2指针到达链表末尾。这样，p1指针就指向了要删除节点的前一个节点。 接下来，将p1的next指针指向要删除节点的下一个节点，即完成了删除操作。最后，返回头节点的next指针，即为删除节点后的链表。 由于p2指针先向后移动了n+1步，所以p1指针和p2指针之间的距离为n+1。因此，当p2指针到达链表末尾时，p1指针正好指向要删除节点的前一个节点。这样，通过移动p1指针的next指针，就可以删除要删除的节点。 E04. 有序链表去重-力扣 83 题例如 输入：head &#x3D; [1,1,2] 输出：[1,2] 输入：head &#x3D; [1,1,2,3,3] 输出：[1,2,3] 注意：重复元素保留一个 E05. 有序链表去重-力扣 82 题例如 输入：head &#x3D; [1,2,3,3,4,4,5] 输出：[1,2,5] 输入：head &#x3D; [1,1,1,2,3] 输出：[2,3] 注意：重复元素一个不留 E06. 合并有序链表-力扣 21 题例 输入：l1 &#x3D; [1,2,4], l2 &#x3D; [1,3,4] 输出：[1,1,2,3,4,4] 输入：l1 &#x3D; [], l2 &#x3D; [] 输出：[] 输入：l1 &#x3D; [], l2 &#x3D; [0] 输出：[0] E07. 合并多个有序链表-力扣 23 题例 输入：lists &#x3D; [[1,4,5],[1,3,4],[2,6]] 输出：[1,1,2,3,4,4,5,6] 解释：链表数组如下： [ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6 ] 将它们合并到一个有序链表中得到。 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 E08. 查找链表中间节点-力扣 876 题例如 输入：[1,2,3,4,5] 输出：此列表中的结点 3 (序列化形式：[3,4,5]) 输入：[1,2,3,4,5,6] 输出：此列表中的结点 4 (序列化形式：[4,5,6]) 偶数节点时，中间点是靠右的那个 数组E01. 合并有序数组将数组内两个区间内的有序元素合并 例 [1, 5, 6, 2, 4, 10, 11] 可以视作两个有序区间 [1, 5, 6] 和 [2, 4, 10, 11] 合并后，结果仍存储于原有空间 [1, 2, 4, 5, 6, 10, 11] 队列栈双端队列优先级队列堆二叉树","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.ehzyil.xyz/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://blog.ehzyil.xyz/tags/Leetcode/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.ehzyil.xyz/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"author":"ehzyil"},{"title":"二分查找","slug":"数据结构与算法/二分查找","date":"2023-10-11T00:00:00.000Z","updated":"2024-06-17T01:04:54.019Z","comments":true,"path":"2023/10/11/数据结构与算法/二分查找/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/","excerpt":"","text":"二分查找基础版（左闭右闭型）需求：在有序数组 $A$ 内，查找值 $target$ 如果找到返回索引 如果找不到返回 $-1$ 算法描述 前提 给定一个内含 $n$ 个元素的有序数组 $A$，满足 $A_{0}\\leq A_{1}\\leq A_{2}\\leq \\cdots \\leq A_{n-1}$，一个待查值 $target$ 1 设置 $i&#x3D;0$(left)，$j&#x3D;n-1$(right) 2 如果 $i \\gt j$，结束查找，没找到 3 设置 $m &#x3D; floor(\\frac {i+j}{2})$ ，$m$ 为中间索引，$floor$ 是向下取整（$\\leq \\frac {i+j}{2}$ 的最小整数） 4 如果 $target &lt; A_{m}$ 设置 $j &#x3D; m - 1$，跳到第2步 5 如果 $A_{m} &lt; target$ 设置 $i &#x3D; m + 1$，跳到第2步 6 如果 $A_{m} &#x3D; target$，结束查找，找到了 java 实现 public static int binarySearch(int[] a, int target) &#123; int i = 0, j = a.length - 1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; // 在左边 j = m - 1; &#125; else if (a[m] &lt; target) &#123; // 在右边 i = m + 1; &#125; else &#123; return m; &#125; &#125; return -1; &#125; $i,j$ 对应着搜索区间 $[0,a.length-1]$（注意是闭合的区间），$i&lt;&#x3D;j$ 意味着搜索区间内还有未比较的元素，$i,j$ 指向的元素也可能是比较的目标 思考：如果不加 $i&#x3D;j$ 行不行？ 回答：不行，因为这意味着 $i,j$ 指向的元素会漏过比较 $m$ 对应着中间位置，中间位置左边和右边的元素可能不相等（差一个），不会影响结果 如果某次未找到，那么缩小后的区间内不包含 $m$ 二分查找改变版（左闭右开）另一种写法 public static int binarySearch(int[] a, int target) &#123; int i = 0, j = a.length; while (i &lt; j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; // 在左边 j = m; &#125; else if (a[m] &lt; target) &#123; // 在右边 i = m + 1; &#125; else &#123; return m; &#125; &#125; return -1; &#125; $i,j$ 对应着搜索区间 $[0,a.length)$（注意是左闭右开的区间），$i&lt;j$ 意味着搜索区间内还有未比较的元素，$j$ 指向的一定不是查找目标 思考：为啥这次不加 $i&#x3D;j$ 的条件了？ 回答：这回 $j$ 指向的不是查找目标，如果还加 $i&#x3D;j$ 条件，就意味着 $j$ 指向的还会再次比较，找不到时，会死循环 如果某次要缩小右边界，那么 $j&#x3D;m$，因为此时的 $m$ 已经不是查找目标了 若只添加 $i&#x3D;j$ 的条件但为修改 j = m - 1; -&gt; j = m;运行以下代码时: int search = Solution.binarySearch(new int[]&#123;-1, 0, 3, 5, 9, -1&#125;, 3);//search为-1 //第一轮循环 public static int binarySearch(int[] a, int target) &#123; int i = 0, j = a.length; while (i &lt; j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; // 在左边 j = m - 1; &#125; else if (a[m] &lt; target) &#123; // 在右边 i = m + 1; &#125; else &#123; return m; &#125; &#125; return -1; &#125; 循环次数 i j mid target a[m] 操作 1 0 6 3 3 0 target &lt; a[m] 1 0 6 3 3 0 target &lt; a[m] 2 0 2 1 3 5 a[m] &lt; target 3 2 2 2 不满足i &lt; j返回-1 二分查找性能下面分析二分查找算法的性能 时间复杂度 最坏情况：$O(\\log n)$ 最好情况：如果待查找元素恰好在数组中央，只需要循环一次 $O(1)$ 空间复杂度 需要常数个指针 $i,j,m$，因此额外占用的空间是 $O(1)$ 二分查找平衡版public static int binarySearchBalance(int[] a, int target) &#123; int i = 0, j = a.length; //1 &lt; j - i ==> i &lt; j - 1 // 即当i个j之间有1个以上元素时满足条件 // 否则跳出循环 此时只剩下一个元素 while (1 &lt; j - i) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; // right j = m; &#125; else if (a[m] &lt; target) &#123; // left i = m; &#125; else &#123; return m; &#125; &#125; //判断剩下的那个元素是否是查询元素 不是则返回 -1 return a[i] == target ? i : -1; &#125; 上述代码中可以优化的是将目标元素小于中间元素的情况和目标元素大于中间元素的情况合并为一个条件。在除了目标元素小于中间元素的情况外，都将i更新为m。 思路是，如果目标元素不小于中间元素，那么它要么等于中间元素，要么大于中间元素。在这两种情况下，更新i为m仍然可以保持目标元素在搜索范围内。优化后： public static int binarySearchBalance(int[] a, int target) &#123; int i = 0, j = a.length; while (1 &lt; j - i) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m; &#125; else &#123; i = m; &#125; &#125; return (a[i] == target) ? i : -1; &#125; 思想： 左闭右开的区间，$i$ 指向的可能是目标，而 $j$ 指向的不是目标 不奢望循环内通过 $m$ 找出目标, 缩小区间直至剩 1 个, 剩下的这个可能就是要找的（通过 $i$） $j - i &gt; 1$ 的含义是，在范围内待比较的元素个数 &gt; 1 改变 $i$ 边界时，它指向的可能是目标，因此不能 $m+1$ 循环内的平均比较次数减少了 时间复杂度 $\\Theta(log(n))$ 二分查找 Java 版private static int binarySearch0(int[] a, int fromIndex, int toIndex, int key) &#123; int low = fromIndex; int high = toIndex - 1; while (low &lt;= high) &#123; int mid = (low + high) >>> 1; int midVal = a[mid]; if (midVal &lt; key) low = mid + 1; else if (midVal > key) high = mid - 1; else return mid; // key found &#125; return -(low + 1); // key not found. &#125; 例如 $[1,3,5,6]$ 要插入 $2$ 那么就是找到一个位置，这个位置左侧元素都比它小 等循环结束，若没找到，low 左侧元素肯定都比 target 小，因此 low 即插入点 插入点取负是为了与找到情况区分 -1 是为了把索引 0 位置的插入点与找到的情况进行区分 若未找到 可以获取插入元素的位置(不是索引)即为-result+1 Leftmost 与 Rightmost有时我们希望返回的是最左侧的重复元素，如果用 Basic 二分查找 对于数组 $[1, 2, 3, 4, 4, 5, 6, 7]$，查找元素4，结果是索引3 对于数组 $[1, 2, 4, 4, 4, 5, 6, 7]$，查找元素4，结果也是索引3，并不是最左侧的元素 public static int binarySearchLeftmost1(int[] a, int target) &#123; int i = 0, j = a.length - 1; int candidate = -1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m - 1; &#125; else if (a[m] &lt; target) &#123; i = m + 1; &#125; else &#123; candidate = m; // 记录候选位置 j = m - 1; // 继续向左 &#125; &#125; return candidate; &#125; 如果希望返回的是最右侧元素 public static int binarySearchRightmost1(int[] a, int target) &#123; int i = 0, j = a.length - 1; int candidate = -1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m - 1; &#125; else if (a[m] &lt; target) &#123; i = m + 1; &#125; else &#123; candidate = m; // 记录候选位置 i = m + 1; // 继续向右 &#125; &#125; return candidate; &#125; 应用 对于 Leftmost 与 Rightmost，可以返回一个比 -1 更有用的值 Leftmost 改为 public static int binarySearchLeftmost(int[] a, int target) &#123; int i = 0, j = a.length - 1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt;= a[m]) &#123; j = m - 1; &#125; else &#123; i = m + 1; &#125; &#125; return i; &#125; leftmost 返回值的另一层含义：$\\lt target$ 的元素个数 小于等于中间值，都要向左找 Rightmost 改为 public static int binarySearchRightmost(int[] a, int target) &#123; int i = 0, j = a.length - 1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m - 1; &#125; else &#123; i = m + 1; &#125; &#125; return i - 1; &#125; 大于等于中间值，都要向右找 几个名词 范围查询： 查询 $x \\lt 4$，$0 .. leftmost(4) - 1$ 查询 $x \\leq 4$，$0 .. rightmost(4)$ 查询 $4 \\lt x$，$rightmost(4) + 1 .. \\infty $ 查询 $4 \\leq x$， $leftmost(4) .. \\infty$ 查询 $4 \\leq x \\leq 7$，$leftmost(4) .. rightmost(7)$ 查询 $4 \\lt x \\lt 7$，$rightmost(4)+1 .. leftmost(7)-1$ 求排名：$leftmost(target) + 1$ $target$ 可以不存在，如：$leftmost(5)+1 &#x3D; 6$ $target$ 也可以存在，如：$leftmost(4)+1 &#x3D; 3$ 求前任（predecessor）：$leftmost(target) - 1$ $leftmost(3) - 1 &#x3D; 1$，前任 $a_1 &#x3D; 2$ $leftmost(4) - 1 &#x3D; 1$，前任 $a_1 &#x3D; 2$ 求后任（successor）：$rightmost(target)+1$ $rightmost(5) + 1 &#x3D; 5$，后任 $a_5 &#x3D; 7$ $rightmost(4) + 1 &#x3D; 5$，后任 $a_5 &#x3D; 7$ 求最近邻居： 前任和后任距离更近者 完整代码如下： 查看代码 public class BinarySearch &#123; /** * &lt;h3>二分查找基础版&lt;/h3> * * &lt;ol> * &lt;li>i, j, m 指针都可能是查找目标&lt;/li> * &lt;li>因为 1. i > j 时表示区域内没有要找的了&lt;/li> * &lt;li>每次改变 i, j 边界时, m 已经比较过不是目标, 因此分别 m+1 m-1&lt;/li> * &lt;li>向左查找, 比较次数少, 向右查找, 比较次数多&lt;/li> * &lt;/ol> * * @param a 待查找的升序数组 * @param target 待查找的目标值 * @return &lt;p>找到则返回索引&lt;/p> * &lt;p>找不到返回 -1&lt;/p> */ public static int binarySearchBasic(int[] a, int target) &#123; int i = 0, j = a.length - 1; // 设置指针和初值 // L 次 元素在最左边 L 次， 元素在最右边 2*L 次 while (i &lt;= j) &#123; // i~j 范围内有东西 int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; // 目标在左边 j = m - 1; &#125; else if (a[m] &lt; target) &#123; // 目标在右边 i = m + 1; &#125; else &#123; // 找到了 return m; &#125; &#125; return -1; &#125; /* 1 [2,3,4,5] 5 右侧没找到更差 int i = 0, j = a.length - 1; 2 return -1; 1 元素个数 循环次数 4-7 3 floor(log_2(4)) = 2+1 8-15 4 floor(log_2(8)) = 3+1 16-31 5 floor(log_2(16)) = 4+1 32-63 6 floor(log_2(32)) = 5+1 ... ... 循环次数L = floor(log_2(n)) + 1 i &lt;= j L+1 int m = (i + j) >>> 1; L target &lt; a[m] L a[m] &lt; target L i = m + 1; L (floor(log_2(n)) + 1) * 5 + 4 (3) * 5 + 4 = 19*t (10 + 1) * 5 + 4 = 59*t */ /* 问题1: 为什么是 i&lt;=j 意味着区间内有未比较的元素, 而不是 i&lt;j ? i==j 意味着 i,j 它们指向的元素也会参与比较 i&lt;j 只意味着 m 指向的元素参与比较 问题2: (i + j) / 2 有没有问题? 问题3: 都写成小于号有啥好处? */ /** * &lt;h3>二分查找改动版&lt;/h3> * * &lt;ol> * &lt;li>i, m 指针可能是查找目标&lt;/li> * &lt;li>j 指针不可能是查找目标&lt;/li> * &lt;li>因为 1. 2. i >= j 时表示区域内没有要找的了&lt;/li> * &lt;li>改变 i 边界时, m 已经比较过不是目标, 因此需要 i=m+1&lt;/li> * &lt;li>改变 j 边界时, m 已经比较过不是目标, 同时因为 2. 所以 j=m&lt;/li> * &lt;/ol> * * @param a 待查找的升序数组 * @param target 待查找的目标值 * @return &lt;p>找到则返回索引&lt;/p> * &lt;p>找不到返回 -1&lt;/p> */ public static int binarySearchAlternative(int[] a, int target) &#123; int i = 0, j = a.length; // 第一处 while (i &lt; j) &#123; // 第二处 int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m; // 第三处 &#125; else if (a[m] &lt; target) &#123; i = m + 1; &#125; else &#123; return m; &#125; &#125; return -1; &#125; /** * &lt;h3>二分查找平衡版&lt;/h3> * * &lt;ol> * &lt;li>不奢望循环内通过 m 找出目标, 缩小区间直至剩 1 个, 剩下的这个可能就是要找的(通过 i)&lt;/li> * &lt;li>i 指针可能是查找目标&lt;/li> * &lt;li>j 指针不可能是查找目标&lt;/li> * &lt;li>因为 1. 2. 3. 当区域内还剩一个元素时, 表示为 j - i == 1&lt;/li> * &lt;li>改变 i 边界时, m 可能就是目标, 同时因为 2. 所以有 i=m&lt;/li> * &lt;li>改变 j 边界时, m 已经比较过不是目标, 同时因为 3. 所以有 j=m&lt;/li> * &lt;li>三分支改为二分支, 循环内比较次数减少&lt;/li> * &lt;/ol> * * @param a 待查找的升序数组 * @param target 待查找的目标值 * @return &lt;p>找到则返回索引&lt;/p> * &lt;p>找不到返回 -1&lt;/p> */ public static int binarySearchBalance(int[] a, int target) &#123; int i = 0, j = a.length; while (1 &lt; j - i) &#123; // 范围内待查找的元素个数 > 1 时 int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; // 目标在左边 j = m; &#125; else &#123; // 目标在 m 或右边 i = m; &#125; &#125; return (target == a[i]) ? i : -1; &#125; /** * &lt;h3>二分查找 Leftmost &lt;/h3> * * @param a 待查找的升序数组 * @param target 待查找的目标值 * @return &lt;p>找到则返回最靠左索引&lt;/p> * &lt;p>找不到返回 -1&lt;/p> */ public static int binarySearchLeftmost1(int[] a, int target) &#123; int i = 0, j = a.length - 1; int candidate = -1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m - 1; &#125; else if (a[m] &lt; target) &#123; i = m + 1; &#125; else &#123; // 记录候选位置 candidate = m; j = m - 1; &#125; &#125; return candidate; &#125; /** * &lt;h3>二分查找 Rightmost &lt;/h3> * * @param a 待查找的升序数组 * @param target 待查找的目标值 * @return &lt;p>找到则返回最靠右索引&lt;/p> * &lt;p>找不到返回 -1&lt;/p> */ public static int binarySearchRightmost1(int[] a, int target) &#123; int i = 0, j = a.length - 1; int candidate = -1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m - 1; &#125; else if (a[m] &lt; target) &#123; i = m + 1; &#125; else &#123; candidate = m; i = m + 1; &#125; &#125; return candidate; &#125; /** * &lt;h3>二分查找 Leftmost &lt;/h3> * * @param a 待查找的升序数组 * @param target 待查找的目标值 * @return &lt;p>返回 &amp;ge; target 的最靠左索引&lt;/p> */ public static int binarySearchLeftmost2(int[] a, int target) &#123; int i = 0, j = a.length - 1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt;= a[m]) &#123; j = m - 1; &#125; else &#123; i = m + 1; &#125; &#125; return i; &#125; /** * &lt;h3>二分查找 Rightmost &lt;/h3> * * @param a 待查找的升序数组 * @param target 待查找的目标值 * @return &lt;p>返回 &amp;le; target 的最靠右索引&lt;/p> */ public static int binarySearchRightmost2(int[] a, int target) &#123; int i = 0, j = a.length - 1; while (i &lt;= j) &#123; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; j = m - 1; &#125; else &#123; i = m + 1; &#125; &#125; return i - 1; &#125; 黑马程序员Java高级程序员必学的数据结构与算法","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.ehzyil.xyz/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.ehzyil.xyz/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"author":"ehzyil"},{"title":"基础数据结构","slug":"数据结构与算法/基础数据结构","date":"2023-10-11T00:00:00.000Z","updated":"2024-06-17T01:04:54.019Z","comments":true,"path":"2023/10/11/数据结构与算法/基础数据结构/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"基础数据结构 数组概述定义 在计算机科学中，数组是由一组元素（值或变量）组成的数据结构，每个元素有至少一个索引或键来标识 In computer science, an array is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key 因为数组内的元素是连续存储的，所以数组中元素的地址，可以通过其索引计算出来，例如： int[] array = &#123;1,2,3,4,5&#125; 知道了数组的数据起始地址 $BaseAddress$，就可以由公式 $BaseAddress + i * size$ 计算出索引 $i$ 元素的地址 $i$ 即索引，在 Java、C 等语言都是从 0 开始 $size$ 是每个元素占用字节，例如 $int$ 占 $4$，$double$ 占 $8$ 小测试 byte[] array = &#123;1,2,3,4,5&#125; 已知 array 的数据的起始地址是 0x7138f94c8，那么元素 3 的地址是什么？ 答： 元素3的地址可以通过数组的起始地址加上元素3的索引乘以元素的大小来计算。在这种情况下，元素3的索引是2（数组索引从0开始），元素的大小是1字节（byte类型的大小为1字节）。 0x7138f94c8 + 2 * 1 &#x3D; 0x7138f94ca 空间占用 Java 中数组结构为 8 字节 markword 4 字节 class 指针（压缩 class 指针的情况） 4 字节 数组大小（决定了数组最大容量是 $2^{32}$） 数组元素 + 对齐字节（java 中所有对象大小都是 8 字节的整数倍[^12]，不足的要用对齐字节补足） 例如 int[] array = &#123;1, 2, 3, 4, 5&#125;; 的大小为 40 个字节，组成如下 8 + 4 + 4 + 5*4 + 4(alignment) 随机访问性能 即根据索引查找元素，时间复杂度是 $O(1)$ 动态数组java 版本 public class DynamicArray implements Iterable&lt;Integer> &#123; private int size = 0; // 逻辑大小 private int capacity = 8; // 容量 private int[] array = &#123;&#125;; /** * 向最后位置 [size] 添加元素 * * @param element 待添加元素 */ public void addLast(int element) &#123; add(size, element); &#125; /** * 向 [0 .. size] 位置添加元素 * * @param index 索引位置 * @param element 待添加元素 */ public void add(int index, int element) &#123; checkAndGrow(); // 添加逻辑 if (index >= 0 &amp;&amp; index &lt; size) &#123; // 向后挪动, 空出待插入位置 System.arraycopy(array, index, array, index + 1, size - index); &#125; array[index] = element; size++; &#125; private void checkAndGrow() &#123; // 容量检查 if (size == 0) &#123; array = new int[capacity]; &#125; else if (size == capacity) &#123; // 进行扩容, 1.5 1.618 2 capacity += capacity >> 1; int[] newArray = new int[capacity]; System.arraycopy(array, 0, newArray, 0, size); array = newArray; &#125; &#125; /** * 从 [0 .. size) 范围删除元素 * * @param index 索引位置 * @return 被删除元素 */ public int remove(int index) &#123; // [0..size) int removed = array[index]; if (index &lt; size - 1) &#123; // 向前挪动 System.arraycopy(array, index + 1, array, index, size - index - 1); &#125; size--; return removed; &#125; /** * 查询元素 * * @param index 索引位置, 在 [0..size) 区间内 * @return 该索引位置的元素 */ public int get(int index) &#123; return array[index]; &#125; /** * 遍历方法1 * * @param consumer 遍历要执行的操作, 入参: 每个元素 */ public void foreach(Consumer&lt;Integer> consumer) &#123; for (int i = 0; i &lt; size; i++) &#123; // 提供 array[i] // 返回 void consumer.accept(array[i]); &#125; &#125; /** * 遍历方法2 - 迭代器遍历 */ @Override public Iterator&lt;Integer> iterator() &#123; return new Iterator&lt;Integer>() &#123; int i = 0; @Override public boolean hasNext() &#123; // 有没有下一个元素 return i &lt; size; &#125; @Override public Integer next() &#123; // 返回当前元素,并移动到下一个元素 return array[i++]; &#125; &#125;; &#125; /** * 遍历方法3 - stream 遍历 * * @return stream 流 */ public IntStream stream() &#123; return IntStream.of(Arrays.copyOfRange(array, 0, size)); &#125; &#125; 这些方法实现，都简化了 index 的有效性判断，假设输入的 index 都是合法的 插入或删除性能 头部位置，时间复杂度是 $O(n)$ 中间位置，时间复杂度是 $O(n)$ 尾部位置，时间复杂度是 $O(1)$（均摊来说） 二维数组int[][] array = &#123; &#123;11, 12, 13, 14, 15&#125;, &#123;21, 22, 23, 24, 25&#125;, &#123;31, 32, 33, 34, 35&#125;, &#125;; 内存图如下 二维数组占 32 个字节，其中 array[0]，array[1]，array[2] 三个元素分别保存了指向三个一维数组的引用 三个一维数组各占 40 个字节 它们在内层布局上是连续的 更一般的，对一个二维数组 $Array[m][n]$ $m$ 是外层数组的长度，可以看作 row 行 $n$ 是内层数组的长度，可以看作 column 列 当访问 $Array[i][j]$，$0\\leq i \\lt m, 0\\leq j \\lt n$时，就相当于 先找到第 $i$ 个内层数组（行） 再找到此内层数组中第 $j$ 个元素（列） 小测试 Java 环境下（不考虑类指针和引用压缩，此为默认情况），有下面的二维数组 byte[][] array = &#123; &#123;11, 12, 13, 14, 15&#125;, &#123;21, 22, 23, 24, 25&#125;, &#123;31, 32, 33, 34, 35&#125;, &#125;; 已知 array 对象起始地址是 0x1000，那么 23 这个元素的地址是什么？（不太理解对齐字节！！！） 答： 起始地址 0x1000 外层数组大小：16字节对象头 + 3元素 * 每个引用4字节 + 4 对齐字节 &#x3D; 32 &#x3D; 0x20 第一个内层数组大小：16字节对象头 + 5元素 * 每个byte1字节 + 3 对齐字节 &#x3D; 24 &#x3D; 0x18 第二个内层数组，16字节对象头 &#x3D; 0x10，待查找元素索引为 2 最后结果 &#x3D; 0x1000 + 0x20 + 0x18 + 0x10 + 2*1 &#x3D; 0x104a 局部性原理这里只讨论空间局部性 cpu 读取内存（速度慢）数据后，会将其放入高速缓存（速度快）当中，如果后来的计算再用到此数据，在缓存中能读到的话，就不必读内存了 缓存的最小存储单位是缓存行（cache line），一般是 64 bytes，一次读的数据少了不划算啊，因此最少读 64 bytes 填满一个缓存行，因此读入某个数据时也会读取其临近的数据，这就是所谓空间局部性 对效率的影响 比较下面 ij 和 ji 两个方法的执行效率 int rows = 1000000; int columns = 14; int[][] a = new int[rows][columns]; StopWatch sw = new StopWatch(); sw.start(\"ij\"); ij(a, rows, columns); sw.stop(); sw.start(\"ji\"); ji(a, rows, columns); sw.stop(); System.out.println(sw.prettyPrint()); ij 方法 public static void ij(int[][] a, int rows, int columns) &#123; long sum = 0L; for (int i = 0; i &lt; rows; i++) &#123; for (int j = 0; j &lt; columns; j++) &#123; sum += a[i][j]; &#125; &#125; System.out.println(sum); &#125; ji 方法 public static void ji(int[][] a, int rows, int columns) &#123; long sum = 0L; for (int j = 0; j &lt; columns; j++) &#123; for (int i = 0; i &lt; rows; i++) &#123; sum += a[i][j]; &#125; &#125; System.out.println(sum); &#125; 执行结果 0 0 StopWatch &#39;&#39;: running time &#x3D; 96283300 ns --------------------------------------------- ns % Task name --------------------------------------------- 016196200 017% ij 080087100 083% ji 可以看到 ij 的效率比 ji 快很多，为什么呢？ 缓存是有限的，当新数据来了后，一些旧的缓存行数据就会被覆盖 如果不能充分利用缓存的数据，就会造成效率低下 以 ji 执行为例，第一次内循环要读入 $[0,0]$ 这条数据，由于局部性原理，读入 $[0,0]$ 的同时也读入了 $[0,1] … [0,13]$，如图所示 但很遗憾，第二次内循环要的是 $[1,0]$ 这条数据，缓存中没有，于是再读入了下图的数据 这显然是一种浪费，因为 $[0,1] … [0,13]$ 包括 $[1,1] … [1,13]$ 这些数据虽然读入了缓存，却没有及时用上，而缓存的大小是有限的，等执行到第九次内循环时 缓存的第一行数据已经被新的数据 $[8,0] … [8,13]$ 覆盖掉了，以后如果再想读，比如 $[0,1]$，又得到内存去读了 同理可以分析 ij 函数则能充分利用局部性原理加载到的缓存数据 举一反三 I&#x2F;O 读写时同样可以体现局部性原理 数组可以充分利用局部性原理，那么链表呢？ 答：链表不行，因为链表的元素并非相邻存储 链表概述定义 在计算机科学中，链表是数据元素的线性集合，其每个元素都指向下一个元素，元素存储上并不连续 In computer science, a linked list is a linear collection of data elements whose order is not given by their physical placement in memory. Instead, each element points to the next. 可以分类为[^5] 单向链表，每个元素只知道其下一个元素是谁 双向链表，每个元素知道其上一个元素和下一个元素 循环链表，通常的链表尾节点 tail 指向的都是 null，而循环链表的 tail 指向的是头节点 head 链表内还有一种特殊的节点称为哨兵（Sentinel）节点，也叫做哑元（ Dummy）节点，它不存储数据，通常用作头尾，用来简化边界判断，如下图所示 随机访问性能 根据 index 查找，时间复杂度 $O(n)$ 插入或删除性能 起始位置：$O(1)$ 结束位置：如果已知 tail 尾节点是 $O(1)$，不知道 tail 尾节点是 $O(n)$ 中间位置：根据 index 查找时间 + $O(1)$ 单向链表根据单向链表的定义，首先定义一个存储 value 和 next 指针的类 Node，和一个描述头部节点的引用 public class SinglyLinkedList &#123; private Node head; // 头部节点 private static class Node &#123; // 节点类 int value; Node next; public Node(int value, Node next) &#123; this.value = value; this.next = next; &#125; &#125; &#125; Node 定义为内部类，是为了对外隐藏实现细节，没必要让类的使用者关心 Node 结构 定义为 static 内部类，是因为 Node 不需要与 SinglyLinkedList 实例相关，多个 SinglyLinkedList实例能共用 Node 类定义 头部添加 public class SinglyLinkedList &#123; // ... public void addFirst(int value) &#123; this.head = new Node(value, this.head); &#125; &#125; 如果 this.head &#x3D;&#x3D; null，新增节点指向 null，并作为新的 this.head 如果 this.head !&#x3D; null，新增节点指向原来的 this.head，并作为新的 this.head 注意赋值操作执行顺序是从右到左 while 遍历 public class SinglyLinkedList &#123; // ... public void loop() &#123; Node curr = this.head; while (curr != null) &#123; // 做一些事 System.out.println(curr.value); curr = curr.next; &#125; &#125; &#125; for 遍历 public class SinglyLinkedList &#123; // ... public void loop() &#123; for (Node curr = this.head; curr != null; curr = curr.next) &#123; // 做一些事 System.out.println(curr.value); &#125; &#125; &#125; 以上两种遍历都可以把要做的事以 Consumer 函数的方式传递进来 Consumer 的规则是一个参数，无返回值，因此像 System.out::println 方法等都是 Consumer 调用 Consumer 时，将当前节点 curr.value 作为参数传递给它 xxx.traverse(System.out::println); public void traverse(Consumer&lt;Integer> consumer) &#123; Node p = head; while (p != null) &#123; consumer.accept(p.value); p = p.next; &#125; &#125; 迭代器遍历 public class SinglyLinkedList implements Iterable&lt;Integer> &#123; // ... private class NodeIterator implements Iterator&lt;Integer> &#123; Node curr = head; public boolean hasNext() &#123; return curr != null; &#125; public Integer next() &#123; int value = curr.value; curr = curr.next; return value; &#125; &#125; public Iterator&lt;Integer> iterator() &#123; return new NodeIterator(); &#125; &#125; hasNext 用来判断是否还有必要调用 next next 做两件事 返回当前节点的 value 指向下一个节点 NodeIterator 要定义为非 static 内部类，是因为它与 SinglyLinkedList 实例相关，是对某个 SinglyLinkedList 实例的迭代 递归遍历 public class SinglyLinkedList implements Iterable&lt;Integer> &#123; // ... public void loop() &#123; recursion(this.head); &#125; private void recursion(Node curr) &#123; if (curr == null) &#123; return; &#125; // 前面做些事 recursion(curr.next); // 后面做些事 &#125; &#125; 尾部添加 public class SinglyLinkedList &#123; // ... private Node findLast() &#123; if (this.head == null) &#123; return null; &#125; Node curr; for (curr = this.head; curr.next != null; ) &#123; curr = curr.next; &#125; return curr; &#125; public void addLast(int value) &#123; Node last = findLast(); if (last == null) &#123; addFirst(value); return; &#125; last.next = new Node(value, null); &#125; &#125; 注意，找最后一个节点，终止条件是 curr.next &#x3D;&#x3D; null 分成两个方法是为了代码清晰，而且 findLast() 之后还能复用 尾部添加多个 public class SinglyLinkedList &#123; // ... public void addLast(int first, int... rest) &#123; Node sublist = new Node(first, null); Node curr = sublist; for (int value : rest) &#123; //仅是遍历 curr.next = new Node(value, null); curr = curr.next; &#125; Node last = findLast(); if (last == null) &#123; this.head = sublist; return; &#125; last.next = sublist; &#125; &#125; 先串成一串 sublist 再作为一个整体添加 根据索引获取 public class SinglyLinkedList &#123; // ... private Node findNode(int index) &#123; int i = 0; for (Node curr = this.head; curr != null; curr = curr.next, i++) &#123; if (index == i) &#123; return curr; &#125; &#125; return null; &#125; private IllegalArgumentException illegalIndex(int index) &#123; return new IllegalArgumentException(String.format(\"index [%d] 不合法%n\", index)); &#125; public int get(int index) &#123; Node node = findNode(index); if (node != null) &#123; return node.value; &#125; throw illegalIndex(index); &#125; &#125; 同样，分方法可以实现复用 插入 public class SinglyLinkedList &#123; // ... public void insert(int index, int value) &#123; if (index == 0) &#123; addFirst(value); return; &#125; Node prev = findNode(index - 1); // 找到上一个节点 if (prev == null) &#123; // 找不到 throw illegalIndex(index); &#125; prev.next = new Node(value, prev.next); &#125; &#125; 插入包括下面的删除，都必须找到上一个节点 删除 public class SinglyLinkedList &#123; // ... public void remove(int index) &#123; if (index == 0) &#123; if (this.head != null) &#123; this.head = this.head.next; return; &#125; else &#123; throw illegalIndex(index); &#125; &#125; Node prev = findNode(index - 1); Node curr; if (prev != null &amp;&amp; (curr = prev.next) != null) &#123; prev.next = curr.next; &#125; else &#123; throw illegalIndex(index); &#125; &#125; &#125; 第一个 if 块对应着 removeFirst 情况 最后一个 if 块对应着至少得两个节点的情况 不仅仅判断上一个节点非空，还要保证当前节点非空 完整代码如下： 查看代码 public class SinglyLinkedList implements Iterable&lt;Integer&gt; &#123; private Node head; &#x2F;** * 根据索引移除节点 * * @param index *&#x2F; public void remove(int index) &#123; if (index &#x3D;&#x3D; 0) &#123; if (head &#x3D;&#x3D; null) &#123; this.head.next &#x3D; head.next; &#125; else &#123; throw illegalIndex(index); &#125; &#125; Node prev &#x3D; findNode(index - 1); Node curr; if (prev !&#x3D; null &amp;&amp; (curr &#x3D; prev.next) !&#x3D; null) &#123; prev.next &#x3D; curr.next; &#125; else &#123; throw illegalIndex(index); &#125; &#125; &#x2F;** * 根据索引插入节点 * * @param index * @param value *&#x2F; public void insert(int index, int value) &#123; &#x2F;&#x2F;为空 if (head &#x3D;&#x3D; null) &#123; addLast(value); return; &#125; Node prev &#x3D; findNode(index - 1); &#x2F;&#x2F; 找到上一个节点 if (prev &#x3D;&#x3D; null) &#123; &#x2F;&#x2F; 找不到 throw illegalIndex(index); &#125; prev.next &#x3D; new Node(value, prev.next); &#125; &#x2F;** * 根据索引查询节点 * * @param index * @return 查找到的节点 *&#x2F; public Node findNode(int index) &#123; int count &#x3D; 0; for (Node curr &#x3D; head; curr !&#x3D; null; curr &#x3D; curr.next, count++) &#123; if (index &#x3D;&#x3D; count) &#123; return curr; &#125; &#125; return null; &#125; &#x2F;** * 根据索引获取节点的值 * * @param index * @return *&#x2F; public int get(int index) &#123; Node node &#x3D; findNode(index); if (node !&#x3D; null) &#123; return node.value; &#125; throw illegalIndex(index); &#125; private IllegalArgumentException illegalIndex(int index) &#123; return new IllegalArgumentException(String.format(&quot;index [%d] 不合法%n&quot;, index)); &#125; &#x2F;** * 找到最后一个节点 * * @return *&#x2F; private Node findLast() &#123; if (this.head &#x3D;&#x3D; null) return null; Node curr; for (curr &#x3D; this.head; curr.next !&#x3D; null; curr &#x3D; curr.next) ; return curr; &#125; &#x2F;** * 尾部添加多个 *&#x2F; public void addLast(int first, int... rest) &#123; Node sublist &#x3D; new Node(first, null); Node curr &#x3D; sublist; for (int i : rest) &#123; curr.next &#x3D; new Node(i, null); curr &#x3D; curr.next; &#125; Node last &#x3D; findLast(); if (last &#x3D;&#x3D; null) &#123; this.head &#x3D; sublist; return; &#125; last.next &#x3D; sublist; &#125; &#x2F;** * 在链表末尾添加元素 * * @param value *&#x2F; public void addLast(int value) &#123; Node last &#x3D; findLast(); &#x2F;&#x2F;链表为空 在头插入 if (last &#x3D;&#x3D; null) &#123; addFirst(value); return; &#125; last.next &#x3D; new Node(value, null); &#125; &#x2F;** * 头部添加 * * @param value *&#x2F; public void addFirst(int value) &#123; this.head &#x3D; new Node(value, head); &#125; &#x2F;** * while 循环遍历 *&#x2F; public void loopWithWhile() &#123; Node curr &#x3D; this.head; while (curr !&#x3D; null) &#123; System.out.println(curr.value); curr &#x3D; curr.next; &#125; &#125; &#x2F;** * for 循环遍历 *&#x2F; public void loopWithFor() &#123; for (Node curr &#x3D; this.head; curr !&#x3D; null; curr &#x3D; curr.next) System.out.println(curr.value); &#125; &#x2F;** * Consumer&lt;T&gt; 循环遍历 *&#x2F; public void traverse(Consumer&lt;Integer&gt; consumer) &#123; Node p &#x3D; head; while (p !&#x3D; null) &#123; consumer.accept(p.value); p &#x3D; p.next; &#125; &#125; @Override public Iterator&lt;Integer&gt; iterator() &#123; return new NodeIterator(); &#125; &#x2F;** * 递归遍历 *&#x2F; public void loop() &#123; recursion(this.head); &#125; private void recursion(Node curr) &#123; if (curr &#x3D;&#x3D; null) return; &#x2F;&#x2F;正序 &#x2F;&#x2F; System.out.println(curr.value); recursion(curr.next); &#x2F;&#x2F;逆序 System.out.println(curr.value); &#125; static class Node &#123; int value; Node next; public Node(int value, Node next) &#123; this.value &#x3D; value; this.next &#x3D; next; &#125; &#125; private class NodeIterator implements Iterator&lt;Integer&gt; &#123; Node p &#x3D; head; @Override public boolean hasNext() &#123; return p !&#x3D; null; &#125; @Override public Integer next() &#123; int value &#x3D; p.value; p &#x3D; p.next; return value; &#125; &#125; &#125; 单向链表（带哨兵）观察之前单向链表的实现，发现每个方法内几乎都有判断是不是 head 这样的代码，能不能简化呢？ 用一个不参与数据存储的特殊 Node 作为哨兵，它一般被称为哨兵或哑元，拥有哨兵节点的链表称为带头链表 public class SinglyLinkedListSentinel &#123; // ... private Node head = new Node(Integer.MIN_VALUE, null); &#125; 具体存什么值无所谓，因为不会用到它的值 加入哨兵节点后，代码会变得比较简单，先看几个工具方法 public class SinglyLinkedListSentinel &#123; // ... // 根据索引获取节点 private Node findNode(int index) &#123; int i = -1; for (Node curr = this.head; curr != null; curr = curr.next, i++) &#123; if (i == index) &#123; return curr; &#125; &#125; return null; &#125; // 获取最后一个节点 private Node findLast() &#123; Node curr; for (curr = this.head; curr.next != null; ) &#123; curr = curr.next; &#125; return curr; &#125; &#125; findNode 与之前类似，只是 i 初始值设置为 -1 对应哨兵，实际传入的 index 也是 $[-1, \\infty)$ findLast 绝不会返回 null 了，就算没有其它节点，也会返回哨兵作为最后一个节点 这样，代码简化为 public class SinglyLinkedListSentinel &#123; // ... public void addLast(int value) &#123; Node last = findLast(); /* 改动前 if (last == null) &#123; this.head = new Node(value, null); return; &#125; */ last.next = new Node(value, null); &#125; public void insert(int index, int value) &#123; /* 改动前 if (index == 0) &#123; this.head = new Node(value, this.head); return; &#125; */ // index 传入 0 时，返回的是哨兵 Node prev = findNode(index - 1); if (prev != null) &#123; prev.next = new Node(value, prev.next); &#125; else &#123; throw illegalIndex(index); &#125; &#125; public void remove(int index) &#123; /* 改动前 if (index == 0) &#123; if (this.head != null) &#123; this.head = this.head.next; return; &#125; else &#123; throw illegalIndex(index); &#125; &#125; */ // index 传入 0 时，返回的是哨兵 Node prev = findNode(index - 1); Node curr; if (prev != null &amp;&amp; (curr = prev.next) != null) &#123; prev.next = curr.next; &#125; else &#123; throw illegalIndex(index); &#125; &#125; public void addFirst(int value) &#123; /* 改动前 this.head = new Node(value, this.head); */ this.head.next = new Node(value, this.head.next); // 也可以视为 insert 的特例, 即 insert(0, value); &#125; &#125; 对于删除，前面说了【最后一个 if 块对应着至少得两个节点的情况】，现在有了哨兵，就凑足了两个节点 完整代码如下： 查看代码 public class SinglyLinkedListSentinel &#123; static class Node &#123; int value; Node next; public Node(int value, Node next) &#123; this.value &#x3D; value; this.next &#x3D; next; &#125; &#125; &#x2F;&#x2F;哨兵节点 private Node head &#x3D; new Node(Integer.MIN_VALUE, null); &#x2F;** * 根据索引获取节点 * count当不带哨兵因为有头结点所以从0开始，带了哨兵，初始值设置为 -1 对应哨兵 * @param index * @return 查找到的节点 *&#x2F; public Node findNode(int index) &#123; int count &#x3D; -1; for (Node curr &#x3D; head; curr !&#x3D; null; curr &#x3D; curr.next, count++) &#123; if (index &#x3D;&#x3D; count) &#123; return curr; &#125; &#125; return null; &#125; &#x2F;** * 在链表末尾添加元素 * * @param value *&#x2F; public void addLast(int value) &#123; &#x2F;&#x2F;找到最后一个节点 Node last &#x3D; findLast(); &#x2F;&#x2F;添加节点 last.next &#x3D; new Node(value, null); &#125; &#x2F;&#x2F; 获取最后一个节点 不用再判断头结点为空的情况 private Node findLast() &#123; Node curr; for (curr &#x3D; this.head; curr.next !&#x3D; null; ) &#123; curr &#x3D; curr.next; &#125; return curr; &#125; &#x2F;** * 尾部添加多个 *&#x2F; public void addLast(int first, int... rest) &#123; Node sublist &#x3D; new Node(first, null); Node curr &#x3D; sublist; for (int i : rest) &#123; curr.next &#x3D; new Node(i, null); curr &#x3D; curr.next; &#125; Node last &#x3D; findLast(); last.next &#x3D; sublist; &#125; &#x2F;** * 头部添加 * * @param value *&#x2F; public void addFirst(int value) &#123; &#x2F;* 改动前 this.head &#x3D; new Node(value, this.head); *&#x2F; this.head.next &#x3D; new Node(value, this.head.next); &#x2F;&#x2F; 也可以视为 insert 的特例, 即 insert(0, value); &#125; &#x2F;** * 索引越界异常 * @param index * @return *&#x2F; private IllegalArgumentException illegalIndex(int index) &#123; return new IllegalArgumentException(String.format(&quot;index [%d] 不合法%n&quot;, index)); &#125; &#x2F;** * 根据索引获取节点的值 * * @param index * @return *&#x2F; public int get(int index) &#123; Node node &#x3D; findNode(index); if (node !&#x3D; null) &#123; return node.value; &#125; throw illegalIndex(index); &#125; &#x2F;** * 根据索引插入节点 * * @param index * @param value *&#x2F; public void insert(int index, int value) &#123; &#x2F;&#x2F; index 传入 0 时，返回的是哨兵 Node prev &#x3D; findNode(index - 1); &#x2F;&#x2F; 找到上一个节点 if (prev !&#x3D;null) &#123; prev.next &#x3D; new Node(value, prev.next); &#125;else &#123; throw illegalIndex(index); &#125; &#125; &#x2F;** * 根据索引移除 * @param index *&#x2F; public void remove(int index) &#123; Node prev &#x3D; findNode(index - 1); Node curr; if (prev !&#x3D; null &amp;&amp; (curr &#x3D; prev.next) !&#x3D; null) &#123; prev.next &#x3D; curr.next; &#125; else &#123; throw illegalIndex(index); &#125; &#125; public void traverse(Consumer&lt;Integer&gt; consumer) &#123; Node p &#x3D; head.next; while (p !&#x3D; null) &#123; consumer.accept(p.value); p &#x3D; p.next; &#125; &#125; &#125; 双向链表（带哨兵）完整代码如下： 查看代码 public class DoublyLinkedListSentinel implements Iterable&lt;Integer> &#123; private final Node head; private final Node tail; public DoublyLinkedListSentinel() &#123; this.head = new Node(null, 666, null); this.tail = new Node(null, 888, null); head.next = tail; tail.prev = head; &#125; @Override public Iterator&lt;Integer> iterator() &#123; return new Iterator&lt;Integer>() &#123; Node p = head.next; @Override public boolean hasNext() &#123; return p != tail; &#125; @Override public Integer next() &#123; int value = p.value; p = p.next; return value; &#125; &#125;; &#125; public void addFirst(int value) &#123; insert(0, value); &#125; private Node findNode(int index) &#123; int i = -1;//从头节点开始 Node curr; for (curr = head; curr != tail; curr = curr.next, i++) &#123; if (i == index) &#123; return curr; &#125; &#125; return null; &#125; private void insert(int index, int value) &#123; //找到要插入的前一个位置 Node prev = findNode(index - 1); if (prev == null) &#123; throw illegalIndex(index); &#125; Node next = prev.next; //插入 Node added=new Node(prev,value,next); //重新指向 prev.next=added; next.prev=added; &#125; public void removeFirst() &#123; remove(0); &#125; private void remove(int index) &#123; //找到要删除的前一个位置 Node prev = findNode(index - 1); if (prev == null) &#123; throw illegalIndex(index); &#125; //需要用它指向删除后的下一个 Node removed = prev.next; if (removed == tail) &#123; throw illegalIndex(index); &#125; Node next = removed.next; prev.next=next; next.prev=prev; &#125; public void addLast(int value) &#123; Node prev = tail.prev; //单向指向 Node added = new Node(prev, value, tail); //添加的前一个和tail 指向 prev.next = added; tail.prev = added; &#125; public void removeLast() &#123; Node removed = tail.prev; Node prev = removed.prev; if (prev == null) &#123; throw illegalIndex(0); &#125; //移除 removed 重新指向 prev.next = tail; tail.prev = prev; &#125; /** * 参数异常 * * @param index * @return */ private IllegalArgumentException illegalIndex(int index) &#123; return new IllegalArgumentException( String.format(\"index [%d] 不合法%n\", index)); &#125; static class Node &#123; Node prev; int value; Node next; public Node(Node prev, int value, Node next) &#123; this.prev = prev; this.value = value; this.next = next; &#125; &#125; &#125; 环形链表（带哨兵）双向环形链表带哨兵，这时哨兵既作为头，也作为尾 参考实现： 查看代码 public class CircularLinkedList implements Iterable&lt;Integer> &#123; private final Node sentinel = new Node(null, -1, null); // 哨兵 public CircularLinkedList() &#123; //一个哨兵 哨兵指向自己 sentinel.next = sentinel; sentinel.prev = sentinel; &#125; public static void main(String[] args) &#123; CircularLinkedList list = new CircularLinkedList(); list.addFirst(1); list.addLast(2); list.addLast(3); list.addLast(4); Node nodeByValue = list.findNodeByValue(2); list.removeByValue(5); Iterator&lt;Integer> iterator = list.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; &#125; /** * 添加到第一个 * * @param value 待添加值 */ public void addFirst(int value) &#123; Node next = sentinel.next; Node added = new Node(sentinel, value, next); sentinel.next = added; next.prev = added; &#125; /** * 添加到最后一个 * * @param value 待添加值 */ public void addLast(int value) &#123; Node prev = sentinel.prev; Node added = new Node(prev, value, sentinel); prev.next = added; sentinel.prev = added; &#125; /** * 删除第一个 */ public void removeFirst() &#123; Node removed = sentinel.next; if (removed == sentinel) &#123; throw new IllegalArgumentException(\"非法\"); &#125; Node a = sentinel; Node b = removed.next; a.next = b; b.prev = a; &#125; /** * 删除最后一个 */ public void removeLast() &#123; Node removed = sentinel.prev; if (removed == sentinel) &#123; throw new IllegalArgumentException(\"非法\"); &#125; Node a = removed.prev; Node b = sentinel; a.next = b; b.prev = a; &#125; /** * 根据值删除节点 * &lt;p>假定 value 在链表中作为 key, 有唯一性&lt;/p> * * @param value 待删除值 */ public void removeByValue(int value) &#123; Node removed = findNodeByValue(value); if (removed!=null)&#123; Node prev = removed.prev; Node next = removed.next; prev.next=next; next.prev=prev; &#125; throw new IllegalArgumentException(value+\"不存在！\"); &#125; private Node findNodeByValue(int value) &#123; for (Node p = sentinel.next; p != sentinel; p = p.next) &#123; if (p.value == value) &#123; return p; &#125; &#125; return null; &#125; @Override public Iterator&lt;Integer> iterator() &#123; return new Iterator&lt;Integer>() &#123; Node p = sentinel.next; @Override public boolean hasNext() &#123; return p != sentinel; &#125; @Override public Integer next() &#123; int value = p.value; p = p.next; return value; &#125; &#125;; &#125; static class Node &#123; Node prev; // 上一个节点指针 int value; // 值 Node next; // 下一个节点指针 public Node(Node prev, int value, Node next) &#123; this.prev = prev; this.value = value; this.next = next; &#125; &#125; &#125; 2.3 递归概述定义 计算机科学中，递归是一种解决计算问题的方法，其中解决方案取决于同一类问题的更小子集 In computer science, recursion is a method of solving a computational problem where the solution depends on solutions to smaller instances of the same problem. 比如单链表递归遍历的例子： void f(Node node) &#123; if(node == null) &#123; return; &#125; println(\"before:\" + node.value) f(node.next); println(\"after:\" + node.value) &#125; 说明： 自己调用自己，如果说每个函数对应着一种解决方案，自己调用自己意味着解决方案是一样的（有规律的） 每次调用，函数处理的数据会较上次缩减（子集），而且最后会缩减至无需继续递归 内层函数调用（子集处理）完成，外层函数才能算调用完成 原理 假设链表中有 3 个节点，value 分别为 1，2，3，以上代码的执行流程就类似于下面的伪码 // 1 -> 2 -> 3 -> null f(1) void f(Node node = 1) &#123; println(\"before:\" + node.value) // 1 void f(Node node = 2) &#123; println(\"before:\" + node.value) // 2 void f(Node node = 3) &#123; println(\"before:\" + node.value) // 3 void f(Node node = null) &#123; if(node == null) &#123; return; &#125; &#125; println(\"after:\" + node.value) // 3 &#125; println(\"after:\" + node.value) // 2 &#125; println(\"after:\" + node.value) // 1 &#125; 思路 确定能否使用递归求解 推导出递推关系，即父问题与子问题的关系，以及递归的结束条件 例如之前遍历链表的递推关系为$$f(n) &#x3D;\\begin{cases}停止&amp; n &#x3D; null \\f(n.next) &amp; n \\neq null\\end{cases}$$ 深入到最里层叫做递 从最里层出来叫做归 在递的过程中，外层函数内的局部变量（以及方法参数）并未消失，归的时候还可以用到 单路递归 Single RecursionE01. 阶乘 用递归方法求阶乘 阶乘的定义 $n!&#x3D; 1⋅2⋅3⋯(n-2)⋅(n-1)⋅n$，其中 $n$ 为自然数，当然 $0! &#x3D; 1$ 递推关系 $$f(n) &#x3D;\\begin{cases}1 &amp; n &#x3D; 1\\n * f(n-1) &amp; n &gt; 1\\end{cases}$$ 代码 private static int f(int n) &#123; if (n == 1) &#123; return 1; &#125; return n * f(n - 1); &#125; 拆解伪码如下，假设 n 初始值为 3 f(int n = 3) &#123; // 解决不了,递 return 3 * f(int n = 2) &#123; // 解决不了,继续递 return 2 * f(int n = 1) &#123; if (n == 1) &#123; // 可以解决, 开始归 return 1; &#125; &#125; &#125; &#125; E02. 反向打印字符串 用递归反向打印字符串，n 为字符在整个字符串 str 中的索引位置 递：n 从 0 开始，每次 n + 1，一直递到 n &#x3D;&#x3D; str.length() - 1 归：从 n &#x3D;&#x3D; str.length() 开始归，从归打印，自然是逆序的 递推关系$$f(n) &#x3D;\\begin{cases}停止 &amp; n &#x3D; str.length() \\f(n+1) &amp; 0 \\leq n \\leq str.length() - 1\\end{cases}$$代码为 public static void reversePrint(String str, int index) &#123; if (index == str.length()) &#123; return; &#125; reversePrint(str, index + 1); System.out.println(str.charAt(index)); &#125; 拆解伪码如下，假设字符串为 “abc” void reversePrint(String str, int index = 0) &#123; void reversePrint(String str, int index = 1) &#123; void reversePrint(String str, int index = 2) &#123; void reversePrint(String str, int index = 3) &#123; if (index == str.length()) &#123; return; // 开始归 &#125; &#125; System.out.println(str.charAt(index)); // 打印 c &#125; System.out.println(str.charAt(index)); // 打印 b &#125; System.out.println(str.charAt(index)); // 打印 a &#125; 多路递归 Multi RecursionE01. 斐波那契数列 之前的例子是每个递归函数只包含一个自身的调用，这称之为 single recursion 如果每个递归函数例包含多个自身调用，称之为 multi recursion 递推关系$$f(n) &#x3D;\\begin{cases}0 &amp; n&#x3D;0 \\1 &amp; n&#x3D;1 \\f(n-1) + f(n-2) &amp; n&gt;1\\end{cases}$$ 下面的表格列出了数列的前几项 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 0 1 1 2 3 5 8 13 21 34 55 89 144 233 实现 public static int f(int n) &#123; if (n == 0) &#123; return 0; &#125; if (n == 1) &#123; return 1; &#125; return f(n - 1) + f(n - 2); &#125; 执行流程 绿色代表正在执行（对应递），灰色代表执行结束（对应归） 递不到头，不能归，对应着深度优先搜索 时间复杂度 递归的次数也符合斐波那契规律，$2 * f(n+1)-1$ 时间复杂度推导过程 斐波那契通项公式 $f(n) &#x3D; \\frac{1}{\\sqrt{5}}*({\\frac{1+\\sqrt{5}}{2}}^n - {\\frac{1-\\sqrt{5}}{2}}^n)$ 简化为：$f(n) &#x3D; \\frac{1}{2.236}*({1.618}^n - {(-0.618)}^n)$ 带入递归次数公式 $2\\frac{1}{2.236}({1.618}^{n+1} - {(-0.618)}^{n+1})-1$ 时间复杂度为 $\\Theta(1.618^n)$ 更多 Fibonacci 参考[^8][^9][^10] 以上时间复杂度分析，未考虑大数相加的因素 变体1 - 兔子问题[^8] 第一个月，有一对未成熟的兔子（黑色，注意图中个头较小） 第二个月，它们成熟 第三个月，它们能产下一对新的小兔子（蓝色） 所有兔子遵循相同规律，求第 $n$ 个月的兔子数 分析 兔子问题如何与斐波那契联系起来呢？设第 n 个月兔子数为 $f(n)$ $f(n)$ &#x3D; 上个月兔子数 + 新生的小兔子数 而【新生的小兔子数】实际就是【上个月成熟的兔子数】 因为需要一个月兔子就成熟，所以【上个月成熟的兔子数】也就是【上上个月的兔子数】 上个月兔子数，即 $f(n-1)$ 上上个月的兔子数，即 $f(n-2)$ 因此本质还是斐波那契数列，只是从其第一项开始 变体2 - 青蛙爬楼梯 楼梯有 $n$ 阶 青蛙要爬到楼顶，可以一次跳一阶，也可以一次跳两阶 只能向上跳，问有多少种跳法 分析 n 跳法 规律 1 (1) 暂时看不出 2 (1,1) (2) 暂时看不出 3 (1,1,1) (1,2) (2,1) 暂时看不出 4 (1,1,1,1) (1,2,1) (2,1,1)(1,1,2) (2,2) 最后一跳，跳一个台阶的，基于f(3)最后一跳，跳两个台阶的，基于f(2) 5 … … 因此本质上还是斐波那契数列，只是从其第二项开始 对应 leetcode 题目 70. 爬楼梯 - 力扣（LeetCode） 实现(Leetcode 运行会超时 待优化) class Solution &#123; public int climbStairs(int n) &#123; if(n==1) return 1; if(n==2) return 2; if(n==3) return 3; return climbStairs(n-1)+climbStairs(n-2); &#125; &#125; 递归优化-记忆法上述代码存在很多重复的计算，例如求 $f(5)$ 递归分解过程 可以看到（颜色相同的是重复的）： $f(3)$ 重复了 2 次 $f(2)$ 重复了 3 次 $f(1)$ 重复了 5 次 $f(0)$ 重复了 3 次 随着 $n$ 的增大，重复次数非常可观，如何优化呢？ Memoization 记忆法（也称备忘录）是一种优化技术，通过存储函数调用结果（通常比较昂贵），当再次出现相同的输入（子问题）时，就能实现加速效果，改进后的代码 public static void main(String[] args) &#123; int n = 13; int[] cache = new int[n + 1]; Arrays.fill(cache, -1); cache[0] = 0; cache[1] = 1; System.out.println(f(cache, n)); &#125; public static int f(int[] cache, int n) &#123; if (cache[n] != -1) &#123; return cache[n]; &#125; cache[n] = f(cache, n - 1) + f(cache, n - 2); return cache[n]; &#125; 优化后的图示，只要结果被缓存，就不会执行其子问题 改进后的时间复杂度为 $O(n)$ 请自行验证改进后的效果 请自行分析改进后的空间复杂度 注意 记忆法是动态规划的一种情况，强调的是自顶向下的解决 记忆法的本质是空间换时间 递归优化-尾递归爆栈 用递归做 $n + (n-1) + (n-2) … + 1$ public static long sum(long n) &#123; if (n == 1) &#123; return 1; &#125; return n + sum(n - 1); &#125; 在我的机器上 $n &#x3D; 12000$ 时，爆栈了 Exception in thread &quot;main&quot; java.lang.StackOverflowError at Test.sum(Test.java:10) at Test.sum(Test.java:10) at Test.sum(Test.java:10) at Test.sum(Test.java:10) at Test.sum(Test.java:10) ... 为什么呢？ 每次方法调用是需要消耗一定的栈内存的，这些内存用来存储方法参数、方法内局部变量、返回地址等等 方法调用占用的内存需要等到方法结束时才会释放 而递归调用我们之前讲过，不到最深不会回头，最内层方法没完成之前，外层方法都结束不了 例如，$sum(3)$ 这个方法内有个需要执行 $3 + sum(2)$，$sum(2)$ 没返回前，加号前面的 $3$ 不能释放 看下面伪码 long sum(long n = 3) &#123; return 3 + long sum(long n = 2) &#123; return 2 + long sum(long n = 1) &#123; return 1; &#125; &#125; &#125; 尾调用 如果函数的最后一步是调用一个函数，那么称为尾调用，例如 function a() &#123; return b() &#125; 下面三段代码不能叫做尾调用 function a() &#123; const c = b() return c &#125; 因为最后一步并非调用函数 function a() &#123; return b() + 1 &#125; 最后一步执行的是加法 function a(x) &#123; return b() + x &#125; 最后一步执行的是加法 一些语言[^11]的编译器能够对尾调用做优化，例如 function a() &#123; // 做前面的事 return b() &#125; function b() &#123; // 做前面的事 return c() &#125; function c() &#123; return 1000 &#125; a() 没优化之前的伪码 function a() &#123; return function b() &#123; return function c() &#123; return 1000 &#125; &#125; &#125; 优化后伪码如下 a() b() c() 为何尾递归才能优化？ 调用 a 时 a 返回时发现：没什么可留给 b 的，将来返回的结果 b 提供就可以了，用不着我 a 了，我的内存就可以释放 调用 b 时 b 返回时发现：没什么可留给 c 的，将来返回的结果 c 提供就可以了，用不着我 b 了，我的内存就可以释放 如果调用 a 时 不是尾调用，例如 return b() + 1，那么 a 就不能提前结束，因为它还得利用 b 的结果做加法 尾递归 尾递归是尾调用的一种特例，也就是最后一步执行的是同一个函数 尾递归避免爆栈 安装 Scala Scala 入门 object Main &#123; def main(args: Array[String]): Unit = &#123; println(\"Hello Scala\") &#125; &#125; Scala 是 java 的近亲，java 中的类都可以拿来重用 类型是放在变量后面的 Unit 表示无返回值，类似于 void 不需要以分号作为结尾，当然加上也对 还是先写一个会爆栈的函数 def sum(n: Long): Long = &#123; if (n == 1) &#123; return 1 &#125; return n + sum(n - 1) &#125; Scala 最后一行代码若作为返回值，可以省略 return 不出所料，在 $n &#x3D; 11000$ 时，还是出了异常 println(sum(11000)) Exception in thread \"main\" java.lang.StackOverflowError at Main$.sum(Main.scala:25) at Main$.sum(Main.scala:25) at Main$.sum(Main.scala:25) at Main$.sum(Main.scala:25) ... 这是因为以上代码，还不是尾调用，要想成为尾调用，那么： 最后一行代码，必须是一次函数调用 内层函数必须摆脱与外层函数的关系，内层函数执行后不依赖于外层的变量或常量 def sum(n: Long): Long = &#123; if (n == 1) &#123; return 1 &#125; return n + sum(n - 1) // 依赖于外层函数的 n 变量 &#125; 如何让它执行后就摆脱对 n 的依赖呢？ 不能等递归回来再做加法，那样就必须保留外层的 n 把 n 当做内层函数的一个参数传进去，这时 n 就属于内层函数了 传参时就完成累加, 不必等回来时累加 sum(n - 1, n + 累加器) 改写后代码如下 @tailrec def sum(n: Long, accumulator: Long): Long = &#123; if (n == 1) &#123; return 1 + accumulator &#125; return sum(n - 1, n + accumulator) &#125; accumulator 作为累加器 @tailrec 注解是 scala 提供的，用来检查方法是否符合尾递归 这回 sum(10000000, 0) 也没有问题，打印 50000005000000 执行流程如下，以伪码表示 $sum(4, 0)$ // 首次调用 def sum(n = 4, accumulator = 0): Long = &#123; return sum(4 - 1, 4 + accumulator) &#125; // 接下来调用内层 sum, 传参时就完成了累加, 不必等回来时累加，当内层 sum 调用后，外层 sum 空间没必要保留 def sum(n = 3, accumulator = 4): Long = &#123; return sum(3 - 1, 3 + accumulator) &#125; // 继续调用内层 sum def sum(n = 2, accumulator = 7): Long = &#123; return sum(2 - 1, 2 + accumulator) &#125; // 继续调用内层 sum, 这是最后的 sum 调用完就返回最后结果 10, 前面所有其它 sum 的空间早已释放 def sum(n = 1, accumulator = 9): Long = &#123; if (1 == 1) &#123; return 1 + accumulator &#125; &#125; 本质上，尾递归优化是将函数的递归调用，变成了函数的循环调用 改循环避免爆栈 public static void main(String[] args) &#123; long n = 100000000; long sum = 0; for (long i = n; i >= 1; i--) &#123; sum += i; &#125; System.out.println(sum); &#125; 递归时间复杂度-Master theorem[^14]若有递归式$$T(n) &#x3D; aT(\\frac{n}{b}) + f(n)$$其中 $T(n)$ 是问题的运行时间，$n$ 是数据规模 $a$ 是子问题个数 $T(\\frac{n}{b})$ 是子问题运行时间，每个子问题被拆成原问题数据规模的 $\\frac{n}{b}$ $f(n)$ 是除递归外执行的计算 令 $x &#x3D; \\log_{b}{a}$，即 $x &#x3D; \\log_{子问题缩小倍数}{子问题个数}$ 那么$$T(n) &#x3D;\\begin{cases}\\Theta(n^x) &amp; f(n) &#x3D; O(n^c) 并且 c \\lt x\\\\Theta(n^x\\log{n}) &amp; f(n) &#x3D; \\Theta(n^x)\\\\Theta(n^c) &amp; f(n) &#x3D; \\Omega(n^c) 并且 c \\gt x\\end{cases}$$ 例1 $T(n) &#x3D; 2T(\\frac{n}{2}) + n^4$ 此时 $x &#x3D; 1 &lt; 4$，由后者决定整个时间复杂度 $\\Theta(n^4)$ 如果觉得对数不好算，可以换为求【$b$ 的几次方能等于 $a$】 例2 $T(n) &#x3D; T(\\frac{7n}{10}) + n$ $a&#x3D;1, b&#x3D;\\frac{10}{7}, x&#x3D;0, c&#x3D;1$ 此时 $x &#x3D; 0 &lt; 1$，由后者决定整个时间复杂度 $\\Theta(n)$ 例3 $T(n) &#x3D; 16T(\\frac{n}{4}) + n^2$ $a&#x3D;16, b&#x3D;4, x&#x3D;2, c&#x3D;2$ 此时 $x&#x3D;2 &#x3D; c$，时间复杂度 $\\Theta(n^2 \\log{n})$ 例4 $T(n)&#x3D;7T(\\frac{n}{3}) + n^2$ $a&#x3D;7, b&#x3D;3, x&#x3D;1.?, c&#x3D;2$ 此时 $x &#x3D; \\log_{3}{7} &lt; 2$，由后者决定整个时间复杂度 $\\Theta(n^2)$ 例5 $T(n) &#x3D; 7T(\\frac{n}{2}) + n^2$ $a&#x3D;7, b&#x3D;2, x&#x3D;2.?, c&#x3D;2$ 此时 $x &#x3D; log_2{7} &gt; 2$，由前者决定整个时间复杂度 $\\Theta(n^{\\log_2{7}})$ 例6 $T(n) &#x3D; 2T(\\frac{n}{4}) + \\sqrt{n}$ $a&#x3D;2, b&#x3D;4, x &#x3D; 0.5, c&#x3D;0.5$ 此时 $x &#x3D; 0.5 &#x3D; c$，时间复杂度 $\\Theta(\\sqrt{n}\\ \\log{n})$ 例7. 二分查找递归 int f(int[] a, int target, int i, int j) &#123; if (i > j) &#123; return -1; &#125; int m = (i + j) >>> 1; if (target &lt; a[m]) &#123; return f(a, target, i, m - 1); &#125; else if (a[m] &lt; target) &#123; return f(a, target, m + 1, j); &#125; else &#123; return m; &#125; &#125; 子问题个数 $a &#x3D; 1$ 子问题数据规模缩小倍数 $b &#x3D; 2$ 除递归外执行的计算是常数级 $c&#x3D;0$ $T(n) &#x3D; T(\\frac{n}{2}) + n^0$ 此时 $x&#x3D;0 &#x3D; c$，时间复杂度 $\\Theta(\\log{n})$ 例8. 归并排序递归 void split(B[], i, j, A[]) &#123; if (j - i &lt;= 1) return; m = (i + j) / 2; // 递归 split(A, i, m, B); split(A, m, j, B); // 合并 merge(B, i, m, j, A); &#125; 子问题个数 $a&#x3D;2$ 子问题数据规模缩小倍数 $b&#x3D;2$ 除递归外，主要时间花在合并上，它可以用 $f(n) &#x3D; n$ 表示 $T(n) &#x3D; 2T(\\frac{n}{2}) + n$ 此时 $x&#x3D;1&#x3D;c$，时间复杂度 $\\Theta(n\\log{n})$ 例9. 快速排序递归 algorithm quicksort(A, lo, hi) is if lo >= hi || lo &lt; 0 then return // 分区 p := partition(A, lo, hi) // 递归 quicksort(A, lo, p - 1) quicksort(A, p + 1, hi) 子问题个数 $a&#x3D;2$ 子问题数据规模缩小倍数 如果分区分的好，$b&#x3D;2$ 如果分区没分好，例如分区1 的数据是 0，分区 2 的数据是 $n-1$ 除递归外，主要时间花在分区上，它可以用 $f(n) &#x3D; n$ 表示 情况1 - 分区分的好 $T(n) &#x3D; 2T(\\frac{n}{2}) + n$ 此时 $x&#x3D;1&#x3D;c$，时间复杂度 $\\Theta(n\\log{n})$ 情况2 - 分区没分好 $T(n) &#x3D; T(n-1) + T(1) + n$ 此时不能用主定理求解 递归时间复杂度-展开求解像下面的递归式，都不能用主定理求解 例1 - 递归求和 long sum(long n) &#123; if (n == 1) &#123; return 1; &#125; return n + sum(n - 1); &#125; $T(n) &#x3D; T(n-1) + c$，$T(1) &#x3D; c$ 下面为展开过程 $T(n) &#x3D; T(n-2) + c + c$ $T(n) &#x3D; T(n-3) + c + c + c$ … $T(n) &#x3D; T(n-(n-1)) + (n-1)c$ 其中 $T(n-(n-1))$ 即 $T(1)$ 带入求得 $T(n) &#x3D; c + (n-1)c &#x3D; nc$ 时间复杂度为 $O(n)$ 例2 - 递归冒泡排序 void bubble(int[] a, int high) &#123; if(0 == high) &#123; return; &#125; for (int i = 0; i &lt; high; i++) &#123; if (a[i] > a[i + 1]) &#123; swap(a, i, i + 1); &#125; &#125; bubble(a, high - 1); &#125; $T(n) &#x3D; T(n-1) + n$，$T(1) &#x3D; c$ 下面为展开过程 $T(n) &#x3D; T(n-2) + (n-1) + n$ $T(n) &#x3D; T(n-3) + (n-2) + (n-1) + n$ … $T(n) &#x3D; T(1) + 2 + … + n &#x3D; T(1) + (n-1)\\frac{2+n}{2} &#x3D; c + \\frac{n^2}{2} + \\frac{n}{2} -1$ 时间复杂度 $O(n^2)$ 注： 等差数列求和为 $个数*\\frac{\\vert首项-末项\\vert}{2}$ 例3 - 递归快排 快速排序分区没分好的极端情况 $T(n) &#x3D; T(n-1) + T(1) + n$，$T(1) &#x3D; c$ $T(n) &#x3D; T(n-1) + c + n$ 下面为展开过程 $T(n) &#x3D; T(n-2) + c + (n-1) + c + n$ $T(n) &#x3D; T(n-3) + c + (n-2) + c + (n-1) + c + n$ … $T(n) &#x3D; T(n-(n-1)) + (n-1)c + 2+…+n &#x3D; \\frac{n^2}{2} + \\frac{2cn+n}{2} -1$ 时间复杂度 $O(n^2)$ 不会推导的同学可以进入 https://www.wolframalpha.com/ 例1 输入 f(n) &#x3D; f(n - 1) + c, f(1) &#x3D; c 例2 输入 f(n) &#x3D; f(n - 1) + n, f(1) &#x3D; c 例3 输入 f(n) &#x3D; f(n - 1) + n + c, f(1) &#x3D; c 附录参考文章[^1]: “Definition of ALGORITHM”. Merriam-Webster Online Dictionary. Archived from the original on February 14, 2020. Retrieved November 14, 2019.[^2]: Introduction to Algorithm 中文译作《算法导论》[^3]: 主要参考文档 https://en.wikipedia.org/wiki/Binary_search_algorithm[^4]: 图片及概念均摘自 Introduction to Algorithm 4th，3.1节，3.2 节[^5]: 图片引用自 wikipedia linkedlist 条目，https://en.wikipedia.org/wiki/Linked_list [^6]: 也称为 Pascal’s triangle https://en.wikipedia.org/wiki/Pascal%27s_triangle [^7]: 递归求解斐波那契数列的时间复杂度——几种简洁证明 - 知乎 (zhihu.com)[^8]: Fibonacci 介绍：https://en.wikipedia.org/wiki/Fibonacci_number[^9]: 几种计算Fibonacci数列算法的时间复杂度比较 - 知乎 (zhihu.com)[^10]: 几种斐波那契数列算法比较 Fast Fibonacci algorithms (nayuki.io) [^11]: 我知道的有 C++，Scala[^12]: jdk 版本有关，64 位 jdk，按 8 字节对齐[^13]: 汉诺塔图片资料均来自 https://en.wikipedia.org/wiki/Tower_of_Hanoi[^14]: 与主定理类似的还有 Akra–Bazzi method，https://en.wikipedia.org/wiki/Akra%E2%80%93Bazzi_method [^15]: 龟兔赛跑动画来自于 Floyd’s Hare and Tortoise Algorithm Demo - One Step! Code (onestepcode.com) [^16]: Josephus problem 主要参考 https://en.wikipedia.org/wiki/Josephus_problem","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.ehzyil.xyz/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.ehzyil.xyz/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"author":"ehzyil"},{"title":"Java 中常用的日期类","slug":"2023/Java 中常用的日期类","date":"2023-10-09T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/10/09/2023/Java 中常用的日期类/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/09/2023/Java%20%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%97%A5%E6%9C%9F%E7%B1%BB/","excerpt":"","text":"Java 中常用的日期类一、 java.util.Date**System.currentTimeMillis()**：返回当前时间与格林威治时间 1970-01-01 00:00:00 （北京时间：1970-01-01 08:00:00）之间以毫秒为单位的时间差。此方法适用于计算时间差 // 1687936118101 时间戳 Long time = System.currentTimeMillis(); System.currentTimeMillis() 方法的返回类型是 Long 类型。一般用于获取某个方法或其它的执行时间差。即：在开始前获取一次，在结束时获取一次，结束时间减去开始时间，得到执行时间。如： Long startTime = System.currentTimeMillis(); for (int i = 0; i &lt; 10000; i++) &#123; System.out.print(\"*\"); &#125; Long endTime = System.currentTimeMillis(); Long result = endTime - startTime; System.out.println(\"用时：\"+result); 时间戳与 java.util.Date 类互相转换： // 时间戳转换为 Date 类型 Date date = new Date(time); System.out.println(date); // Date 类型转换为时间戳 Long time = new Date().getTime(); System.out.println(time); 一般 java.util.Date 类与 java.text.SimpleDateFormat 类一起用，用于格式化日期。 如：将日期与字符串互转： // 格式化时间 SimpleDateFormat sdf=new SimpleDateFormat(\"yyyy-MM-dd HH时mm分ss秒\"); System.out.println(sdf.format(new Date())); // 将字符串转换为日期： String pattern= \"yyyy-MM-dd HH:mm:ss\"; String time = \"2022-07-18 22:53:22\"; SimpleDateFormat simpleDateFormat=new SimpleDateFormat(pattern); try &#123; Date date=simpleDateFormat.parse(time); System.out.println(date); &#125; catch (Exception e) &#123; throw new RuntimeException(\"日期格式化错误！！，日期为：\" + time); &#125; 注意：将字符串转换为日期时 pattern要对应。 二. java.sql.Datejava.sql.Date 是 java.util.Date 类的子类；是针对 SQL 语句使用的，它只包含日期而没有时间部分，一般在读写数据库的时候用，PreparedStament#setDate() 方法的参数和 ResultSet#getDate() 方法的都是 java.sql.Date。 Date date=new Date(0L); System.out.println(date); //1970-01-01 java.util.Date 与 java.sql.Date 互相转换： java.util.Date utilDate=new java.util.Date(); System.out.println(utilDate); //Wed Jun 28 15:45:45 CST 2023 java.sql.Date sqldate=new Date(utilDate.getTime()); System.out.println(sqldate); //2023-06-28 三. java.util.Calendarjava.util.Calendar：日历类，是个抽象类 Calendar 实例化的方法： 创建其子类 GregorianCalendar 的对象 调用其静态方法 getInstance() 从下列方法可以得出两种实例化方法一样 Calendar calendar&#x3D;Calendar.getInstance(); System.out.println(calendar.getClass()); &#x2F;&#x2F;class java.util.GregorianCalendar Calendar GregorianCalendar&#x3D;new GregorianCalendar(); System.out.println(calendar.getClass()&#x3D;&#x3D;GregorianCalendar.getClass()); &#x2F;&#x2F;true 常用的方法： get(int field)：获取某个时间set()：设置某个时间add()：在某个时间上，加上某个时间getTime()：将 Calendar 类转换为 Date 类setTime()：将 Date 类转换为 Calendar 类field 取值有： DAY_OF_MONTH：这个月的第几天DAY_OF_YEAR：这年的第几天…注意： 获取月份时：一月是 0，二月是1，…获取星期时：周日是1，周一是2，… getTime() 方法 源码 public final Date getTime() &#123; return new Date(getTimeInMillis()); &#125; Calendar calendar = Calendar.getInstance(); System.out.println(calendar.getTime()); //Wed Jun 28 16:04:30 CST 2023 public int get(int field)方法 Calendar calendar=Calendar.getInstance(); System.out.println(calendar.getTime()); //Wed Jun 28 15:59:38 CST 2023 calendar.set(Calendar.DAY_OF_MONTH, 5); //将当前时间设置成某个时间 System.out.println(calendar.get(Calendar.DAY_OF_MONTH)); //1 System.out.println(calendar.getTime()); //Thu Jun 01 15:59:38 CST 2023 set(int field, int value)方法 Calendar calendar=Calendar.getInstance(); System.out.println(calendar.getTime()); calendar.set(Calendar.DAY_OF_MONTH, 1); //将当前时间设置成某个时间 System.out.println(calendar.get(Calendar.DAY_OF_MONTH)); System.out.println(calendar.getTime()); add(int field, int amount)方法 Calendar calendar = Calendar.getInstance(); System.out.println(calendar.getTime()); //Wed Jun 28 16:04:30 CST 2023 // 给当前时间加上 5 天 calendar.add(Calendar.DAY_OF_MONTH, +5); System.out.println(calendar.getTime()); //Mon Jul 03 16:04:30 CST 2023 // 给当前时间加上 2 月 calendar.add(Calendar.MONTH, 2); System.out.println(calendar.getTime()); //Sun Sep 03 16:04:30 CST 2023 // 给当前时间减去 3 天 calendar.add(Calendar.DAY_OF_MONTH, -3); System.out.println(calendar.getTime()); //Thu Aug 31 16:04:30 CST 2023 setTime(Date date) 方法 源码 public final void setTime(Date date) &#123; setTimeInMillis(date.getTime()); &#125; 四、java.time APIjava.time API 背景java.util.Date、java.util.Calendar 面临的问题： 1.可变性：像日期、时间这样的类应该是不可变的 2.偏移性：Date 中的年份是从 1900 年开始的；月份是从 0 开始的 3.格式化：格式化只对 Date 有用，而对 Calendar 则不行 4.线程安全：它们都不是线程安全 java.time 中包含了： LocalDate：本地日期。yyyy-MM-dd 格式的日期。可以存储生日、纪念日等 LocalTime：本地时间。代表的是时间，不是日期 LocalDateTime：本地日期时间 ZonedDateTime：时区 Duration：持续时间 大大地简化了对日期、时间的操作 java.time API 中常用的 API now()：获取当前时间、日期 of()：获取指定的日期、时间 getXxx()：获取值 withXxx()：设置值 plusXxx()：加 minusXxx()：减 now() 获取当前日期 LocalDate localDate = LocalDate.now(); LocalTime localTime = LocalTime.now(); LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDate);//2023-06-28 System.out.println(localTime);//16:17:21.720 System.out.println(localDateTime);//2023-06-28T16:17:21.720 localDate：表示当前的日期localTime：表示当前的时间localDateTime：表示当前的日期时间 of() 指定一个特定的日期 //给出时间参数，表示出日期和时间 再生成localDateTime对象返回 public static LocalDateTime of(int year, Month month, int dayOfMonth, int hour, int minute) &#123; LocalDate date = LocalDate.of(year, month, dayOfMonth); LocalTime time = LocalTime.of(hour, minute); return new LocalDateTime(date, time); &#125; LocalDateTime localDateTime = LocalDateTime.of(2035,10,1,10,00); System.out.println(localDateTime); //2035-10-01T10:00 getXxx() withXxx() 每次修改，返回的是一个新对象，部分源码如下。 private LocalDateTime with(LocalDate newDate, LocalTime newTime) &#123; ... return new LocalDateTime(newDate, newTime); &#125; LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDateTime); //2023-06-28T16:35:38.764 LocalDateTime dayOfMonth = localDateTime.withDayOfMonth(25); System.out.println(dayOfMonth); //2023-06-25T16:35:38.764 plusXxx() 该方法每次调用返回时调用withXxx()方法，因此每次修改，返回的也是一个新对象。 LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDateTime); //2023-06-28T16:38:55.066 LocalDateTime localDateTime1 = localDateTime.plusDays(2); System.out.println(localDateTime1); //2023-06-30T16:38:55.066 LocalDateTime localDateTime2 = localDateTime.plusMonths(-1); System.out.println(localDateTime2); //2023-05-28T16:38:55.066 minusXxx() 调用plusXxx()把传入的值取反。 public LocalDateTime minusDays(long days) &#123; return (days == Long.MIN_VALUE ? plusDays(Long.MAX_VALUE).plusDays(1) : plusDays(-days)); &#125; 五、java.time.format.DateTimeFormatterjava.time.format.DateTimeFormatter：格式化、解析日期或时间（类似于：SimpleDateFormat） 此类提供了三种格式化方法： 预定义的标准格式：ISO_LOCAL_DATE_TIME… 本地化相关的格式： 自定义格式：ofPattern(“yyyy-MM-dd HH:mm:ss”) LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDateTime); //2023-06-28T16:50:06.649 DateTimeFormatter dtf=DateTimeFormatter.ofPattern(\"yyyy/MM/dd HH:mm:ss\"); System.out.println(dtf.format(localDateTime)); //2023/06/28 16:50:06 六、java.time.InstantInstant：时间线上的一个瞬时点。 在UNIX中，这个数从1970年开始，以秒为的单位；同样的，在Java中，也是从1970年开始，但以毫秒为单位。java.time包通过值类型Instant提供机器视图，不提供处理人类意义上的时间单位。 Instant表示时间线上的一点，而不需要任何上下文信息，例如，时区。概念上讲， 它只是简单的表示自1970年1月1日0时0分0秒（ UTC）开始的秒数。 因为java.time包是基于纳秒计算的，所以Instant的精度可以达到纳秒级。 方法 描述 now() 静态方法， 返回默认UTC时区的Instant类的对象 ofEpochMilli(long epochMilli) 静态方法，返回在 1970-01-01 00:00:00基础上加上指定毫秒数之后的Instant 类的对象 atOffset(ZoneOffset offset) 结合即时的偏移来创建一个 OffsetDateTime toEpochMilli() 返回1970-01-01 00:00:00到当前时间的毫秒数， 即为时间戳 //now():获取本初子午线对应的标准时间，所以和我们当前的时间约有8个小时的时差。 Instant now = Instant.now(); System.out.println(now);//2023-06-28T09:52:51.124Z //添加时间的偏移量 OffsetDateTime offsetDateTime = now.atOffset(ZoneOffset.ofHours(8)); System.out.println(offsetDateTime);//2023-06-28T17:52:51.124+08:00 //toEpochMilli():获取自1970年1月1日0时0分0秒（UTC）开始的毫秒数 ---> Date类的getTime() long milli = now.toEpochMilli(); System.out.println(milli);//1687945971124 //ofEpochMilli():通过给定的毫秒数，获取Instant实例 -->Date(long millis) Instant instant = Instant.ofEpochMilli(System.currentTimeMillis()); System.out.println(instant);//2023-06-28T09:52:51.131Z 七、java.time.temporal.TemporalJava的Temporal类是Java 8引入的日期和时间API的一部分，它是一个抽象类，用于处理各种日期和时间对象。下面是Temporal类的一些常用方法以及对应的例子： of(TemporalField field, long value)：使用指定的字段和值创建一个Temporal对象。 LocalDate date = LocalDate.of(2023, 6, 28); get(TemporalField field)：获取指定字段的值。 int year = LocalDate.now().get(ChronoField.YEAR); with(TemporalField field, long newValue)：将指定字段的值设置为新值，返回一个新的Temporal对象。 LocalDate newDate = LocalDate.now().with(ChronoField.MONTH_OF_YEAR, 7); plus(TemporalAmount amountToAdd)：在当前Temporal对象上添加指定的时间量。 LocalDateTime dateTime = LocalDateTime.now().plus(Duration.ofHours(2)); minus(TemporalAmount amountToSubtract)：在当前Temporal对象上减去指定的时间量。 LocalTime newTime = LocalTime.now().minus(Duration.ofMinutes(30)); isSupported(TemporalUnit unit)：检查指定的时间单位是否支持。 boolean isSupported = LocalDateTime.now().isSupported(ChronoUnit.DAYS); until(Temporal endExclusive, TemporalUnit unit)：计算当前Temporal对象与指定Temporal对象之间的时间间隔。 long daysBetween = LocalDate.of(2023, 12, 31).until(LocalDate.now(), ChronoUnit.DAYS); format(DateTimeFormatter formatter)：使用指定的日期时间格式化器将Temporal对象格式化为字符串。 String formattedDate = LocalDate.now().format(DateTimeFormatter.ofPattern(\"dd-MM-yyyy\")); 这些只是Temporal类的一些常用方法和示例。Temporal类还有其他方法，可以根据具体的需求进行查阅和使用。 案例在 Java 8 中获取年、月、日信息//在 Java 8 中获取年、月、日信息 LocalDateTime localDateTime = LocalDateTime.now(); System.out.println(localDateTime); //2023-06-28T16:56:25.874 int year = localDateTime.getYear(); System.out.println(year); //2023 Month month = localDateTime.getMonth(); System.out.println(month); //JUNE int day = localDateTime.getDayOfMonth(); System.out.println(day); //28 //某日期 一年中的第几天 int dayOfYear = localDateTime.getDayOfYear(); System.out.println(dayOfYear); //179 在 Java 8 中处理特定日期LocalDateTime dateTime = LocalDateTime.of(2023, 6, 28, 16, 0, 0); // 2023-06-28T16:00 System.out.println(dateTime); 在 Java 8 中判断两个日期是否相等LocalDate localDate=LocalDate.now(); LocalDate nowDate=LocalDate.of(2023,6,28); System.out.println(localDate.equals(nowDate)); //true 在 Java 8 中检查像生日这种周期性事件LocalDate now = LocalDate.now(); LocalDate dateOfBirth = LocalDate.of(2001, 2, 17); MonthDay birthday = MonthDay.of(dateOfBirth.getMonth(), dateOfBirth.getDayOfMonth()); MonthDay currentMonthDay = MonthDay.from(now); if (currentMonthDay.equals(birthday)) &#123; System.out.println(\"Happy Birthday\"); &#125; else &#123; System.out.println(\"Sorry, today is not your birthday\"); &#125; //Sorry, today is not your birthday 如何计算一周后的日期LocalDate 日期不包含时间信息，它的 plus() 方法用来增加天、周、月，ChronoUnit 类声明了这些时间单位。由于 LocalDate 也是不变类型，返回后一定要用变量赋值。minus() 方法用来减去天、周、月等 //1.8之前的方法 //一周后的日期 SimpleDateFormat formatDate = new SimpleDateFormat(\"yyyy-MM-dd\"); Calendar ca = Calendar.getInstance(); ca.add(Calendar.DATE, 7); Date d = ca.getTime(); String after0 = formatDate.format(d); System.out.println(\"一周后日期：\" + after0); //1.8之后的方法 //一周后的日期 LocalDate localDate = LocalDate.now(); //方法1 LocalDate after = localDate.plus(1, ChronoUnit.WEEKS); //方法2 LocalDate after2 = localDate.plusWeeks(1); System.out.println(\"一周后日期：\" + after); /** * 一周后日期：2023-07-07 * 一周后日期：2023-07-07 */ 如何用 Java 判断日期或时间是早于还是晚于另一个日期或时间//日期的比较 LocalDate localDate=LocalDate.now(); System.out.println(localDate);//2023-06-28 LocalDate dateTime = LocalDate.of(2023, 6, 29); boolean before = dateTime.isBefore(localDate); System.out.println(before); boolean after = dateTime.isAfter(localDate); System.out.println(after); //时间的比较 LocalTime localTime=LocalTime.now(); System.out.println(localTime);//17:13:12.341 LocalTime time = LocalTime.of(19, 00, 00); boolean timeAfter = localTime.isAfter(time); System.out.println(timeAfter); boolean timeBefore = localTime.isBefore(time); System.out.println(timeBefore); 如何表示信用卡到期这类固定日期YearMonth currentYearMonth = YearMonth.now(); System.out.printf(\"Days in month year %s has %d days %n\", currentYearMonth, currentYearMonth.lengthOfMonth()); //Days in month year 2023-06 has 30 days //返回到期日期 YearMonth creditCardExpiry = YearMonth.of(2023, Month.OCTOBER); System.out.printf(\"Your credit card expires on %s %n\", creditCardExpiry); //Your credit card expires on 2023-10 //返回月底的本地日期 System.out.println(currentYearMonth.atEndOfMonth()); //2023-06-30 //获取有多少月份 System.out.println(currentYearMonth.lengthOfMonth()); //30 计算两个日期之间的天数和月数1.8之前 //算两个日期间隔多少天，计算间隔多少年，多少月方法类似 String dates1 = \"2023-12-23\"; String dates2 = \"2023-02-26\"; SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\"); Date date1 = format.parse(dates1); Date date2 = format.parse(dates2); int day = (int) ((date1.getTime() - date2.getTime()) / (1000 * 3600 * 24)); System.out.println(dates1 + \"和\" + dates2 + \"相差\" + day + \"天\"); //2023-12-23和2023-02-26相差300天 1.8之后 用 java.time.Period 类来做计算 LocalDate now = LocalDate.now(); LocalDate date = LocalDate.of(2025, Month.JUNE, 5); //计算两日期间相差多少天 long day = Math.abs(now.toEpochDay() - date.toEpochDay()); System.out.println(now + \"和\" + date + \"相差\" + day + \"天\"); System.out.println(\"====================\"); Period period = Period.between(now, date); //若第一个日期晚于第二个日期 则返回早日期距较晚日期的天数 //这里period.getDays()得到的天是抛去年月以外的天数，并不是总天数 //月是抛去年以外的月数 System.out.printf(\"离下个时间还有 %s年,%s月,%s天\", period.getYears() ,period.getMonths(),period.getDays()); 计算两日期间相差的月份 ChronoUnit.MONTHS.between(temporal1, temporal2)本质上是调用Temporal的util方法。 Temporal temporal1 = LocalDate.parse(\"2023-06-10\", DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")); Temporal temporal2 = LocalDate.now(); //方法返回为相差月份 long monthsDifference = ChronoUnit.MONTHS.between(temporal1, temporal2); //月份差转为正数 monthsDifference = Math.abs(monthsDifference); if (monthsDifference == 0) &#123; System.out.println(\"当月为宽带最后一个月\"); &#125; else if (monthsDifference == 1) &#123; System.out.println(\"下月为宽带最后一个月\"); &#125; else &#123; System.out.println(\"宽带还有更多时间\"); &#125; 可以计算两日期间相差的天数，但要注意两日期 Temporal temporal1 = LocalDate.parse(\"2023-06-10\", DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")); Temporal temporal2 = LocalDate.now(); long daysDifference = ChronoUnit.DAYS.between(temporal1, temporal2); daysDifference = Math.abs(daysDifference); System.out.println(daysDifference); //18 获取指定日期Java 8 之前: public void getDay() &#123; SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\"); //获取当前月第一天： Calendar c = Calendar.getInstance(); c.set(Calendar.DAY_OF_MONTH, 1); String first = format.format(c.getTime()); System.out.println(\"first day:\" + first); //获取当前月最后一天 Calendar ca = Calendar.getInstance(); ca.set(Calendar.DAY_OF_MONTH, ca.getActualMaximum(Calendar.DAY_OF_MONTH)); String last = format.format(ca.getTime()); System.out.println(\"last day:\" + last); //当年最后一天 Calendar currCal = Calendar.getInstance(); Calendar calendar = Calendar.getInstance(); calendar.clear(); calendar.set(Calendar.YEAR, currCal.get(Calendar.YEAR)); calendar.roll(Calendar.DAY_OF_YEAR, -1); Date time = calendar.getTime(); System.out.println(\"last day:\" + format.format(time)); &#125; Java 8 之后: LocalDate today = LocalDate.now(); //获取当前月第一天： LocalDate firstDayOfThisMonth = today.with(TemporalAdjusters.firstDayOfMonth()); // 取本月最后一天 LocalDate lastDayOfThisMonth = today.with(TemporalAdjusters.lastDayOfMonth()); //下一个月的第一天 LocalDate firstDayOfNextMonth = today.with(TemporalAdjusters.firstDayOfNextMonth()); //取下一天： LocalDate nextDay = lastDayOfThisMonth.plusDays(1); LocalDate nextDay1 = lastDayOfThisMonth.plus(1,ChronoUnit.DAYS); //当年的第一天 LocalDate firstDayOfYear = today.with(TemporalAdjusters.firstDayOfYear()); //当年最后一天 LocalDate lastday = today.with(TemporalAdjusters.lastDayOfYear()); //下一年的第一天 LocalDate firstDayOfNextYear = today.with(TemporalAdjusters.firstDayOfNextYear()); //2021年最后一个周日，如果用Calendar是不得烦死。 LocalDate lastMondayOf2021 = LocalDate.parse(\"2024-6-30\").with(TemporalAdjusters.lastInMonth(DayOfWeek.SUNDAY)); 获取指定日期的0点以及24点// 返回时间格式如：2020-02-17 00:00:00 public static String getStartOfDay(Date time) &#123; Calendar calendar = Calendar.getInstance(); calendar.setTime(time); calendar.set(Calendar.HOUR_OF_DAY, 0); calendar.set(Calendar.MINUTE, 0); calendar.set(Calendar.SECOND, 0); calendar.set(Calendar.MILLISECOND, 0); return new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(calendar.getTime()); &#125; // 返回时间格式如：2020-02-19 23:59:59 public static String getEndOfDay(Date time) &#123; Calendar calendar = Calendar.getInstance(); calendar.setTime(time); calendar.set(Calendar.HOUR_OF_DAY, 23); calendar.set(Calendar.MINUTE, 59); calendar.set(Calendar.SECOND, 59); calendar.set(Calendar.MILLISECOND, 999); return new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(calendar.getTime()); &#125; // 获取30天以前的时间，同样是比较常用的 public static String getThirtyDaysAgo(Date time) &#123; Calendar calendar = Calendar.getInstance(); calendar.setTime(time); calendar.add(calendar.DATE, -30); calendar.set(Calendar.HOUR_OF_DAY, 0); calendar.set(Calendar.MINUTE, 0); calendar.set(Calendar.SECOND, 0); calendar.set(Calendar.MILLISECOND, 0); return new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(calendar.getTime()); &#125; // 获得某天最大时间 2020-02-19 23:59:59 public static Date getEndOfDay(Date date) &#123; LocalDateTime localDateTime = LocalDateTime.ofInstant(Instant.ofEpochMilli(date.getTime()), ZoneId.systemDefault());; LocalDateTime endOfDay = localDateTime.with(LocalTime.MAX); return Date.from(endOfDay.atZone(ZoneId.systemDefault()).toInstant()); &#125; // 获得某天最小时间 2020-02-17 00:00:00 public static Date getStartOfDay(Date date) &#123; LocalDateTime localDateTime = LocalDateTime.ofInstant(Instant.ofEpochMilli(date.getTime()), ZoneId.systemDefault()); LocalDateTime startOfDay = localDateTime.with(LocalTime.MIN); return Date.from(startOfDay.atZone(ZoneId.systemDefault()).toInstant()); &#125; Date now &#x3D; new Date(); &#x2F;&#x2F;获取当前时间 SimpleDateFormat sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); String nowStr &#x3D; sdf.format(now)+&quot; 00:00:00&quot;; &#x2F;&#x2F;得到今天凌晨时间 Calendar calendar &#x3D; Calendar.getInstance(); calendar.setTime(now); calendar.add(Calendar.DAY_OF_MONTH, +1);&#x2F;&#x2F;+1今天的时间加一天 String tomorrow &#x3D; sdf.format(calendar.getTime())+&quot; 00:00:00&quot;; &#x2F;&#x2F;得到明天凌晨的时间","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"}],"author":"ehzyil"},{"title":"Java8特性","slug":"2023/Java8特性","date":"2023-10-09T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/10/09/2023/Java8特性/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/09/2023/Java8%E7%89%B9%E6%80%A7/","excerpt":"","text":"functional interface 函数式接口函数式接口介绍定义：也称 SAM 接口，即 Single Abstract Method interfaces，有且只有一个抽象方法，但可以有多个非抽象方法的接口。 在 java 8 中专门有一个包放函数式接口java.util.function，该包下的所有接口都有 @FunctionalInterface 注解，提供函数式编程。 在其他包中也有函数式接口，其中一些没有@FunctionalInterface 注解，但是只要符合函数式接口的定义就是函数式接口，与是否有@FunctionalInterface注解无关，注解只是在编译时起到强制规范定义的作用。其在 Lambda 表达式中有广泛的应用。 Java内置的函数式接口介绍及使用举例 函数式接口 参数类型 返回类型 用途 Consumer 消费型接口 T void 对类型为T的对象应用操作，包含方法：void accept(T t) Supplier 供给型接口 无 T 返回类型为T的对象，包含方法：T get() Function函数型接口 T R 对类型为T的对象应用操作，并返回结果。结果是R类型的对象。包含方法：R apply(T t) Predicate断定型接口 T boolean 确定类型为T的对象是否满足某约束，并返回boolean 值。包含方法：boolean test(T t) BiFunction T, U R 对类型为T,U参数应用操作，返回R类型的结果。包含方法为：Rapply(T t,U u); UnaryOperator(Function子接口) T T 对类型为T的对象进行一元运算，并返回T类型的结果。包含方法为：Tapply(T t); BinaryOperator(BiFunction子接口) T,T T 对类型为T的对象进行二元运算，并返回T类型的结果。包含方法为：Tapply(T t1,T t2); BiConsumer T,U void 对类型为T,U参数应用操作。包含方法为：voidaccept(Tt,Uu) BiPredicate T,U boolean 包含方法为：booleantest(Tt,Uu) ToIntFunction T int 计算int值的函数 ToLongFunction T long 计算long值的函数 ToDoubleFunction T double 计算double值的函数 IntFunction int R 参数为int类型的函数 LongFunction long R 参数为long类型的函数 DoubleFunction double R 参数为double类型的函数 消费型接口使用举例 //消费型接口使用举例 public void happyTime(double money, Consumer&lt;Double> consumer) &#123; consumer.accept(money); &#125; @org.junit.jupiter.api.Test public void test() &#123; //1. 以前的写法 happyTime(1234, new Consumer&lt;Double>() &#123; @Override public void accept(Double money) &#123; System.out.println(\"突然想回一趟成都了，机票花费\" + money); &#125; &#125;); System.out.println(\"------------------------\"); //2. Lambda表达式，将之前的6行代码压缩到了1行 happyTime(648, money -> System.out.println(\"学习太累了，奖励自己一发十连，花费\" + money)); &#125; /** * 突然想回一趟成都了，机票花费1234.0 * ------------------------ * 学习太累了，奖励自己一发十连，花费648.0 */ 根据给定的规则，过滤集合中的字符串。此规则由Predicate的方法决定 //根据给定的规则，过滤集合中的字符串。此规则由Predicate的方法决定 public List&lt;String> filterString(List&lt;String> strings, Predicate&lt;String> predicate) &#123; ArrayList&lt;String> res = new ArrayList&lt;>(); for (String string : strings) &#123; if (predicate.test(string)) res.add(string); &#125; return res; &#125; @org.junit.jupiter.api.Test public void test2() &#123; List&lt;String> strings = Arrays.asList(\"东京\", \"西京\", \"南京\", \"北京\", \"天津\", \"中京\"); //1. 以前的写法 List&lt;String> list = filterString(strings, new Predicate&lt;String>() &#123; @Override public boolean test(String s) &#123; return s.contains(\"京\"); &#125; &#125;); System.out.println(list); System.out.println(\"------------------------\"); //2. 现在的写法，函数式接口 List&lt;String> list1 = filterString(strings, s -> s.contains(\"京\")); System.out.println(list1); &#125; Lambda表达式 Lambda是一个匿名函数，我们可以把Lambda表达式理解为是一段可以传递的代码（将代码像数据一样进行传递）。 使用它可以写出更简洁、更灵活的代码。作为一种更紧凑的代码风格，是Java的语言表达能力得到了提升 Lambda表达式使用举例 举例一 @Test public void test01()&#123; &#x2F;&#x2F;1. 以前的写法 Runnable runnable01 &#x3D; new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;你 的 城 市 好 像 不 欢 迎 我&quot;); &#125; &#125;; runnable01.run(); System.out.println(&quot;------------------------&quot;); &#x2F;&#x2F;2. Lambda表达式 Runnable runnable02 &#x3D; () -&gt; System.out.println(&quot;所 以 我 只 好 转 身 离 开 了&quot;); runnable02.run(); &#125; 举例二 @Test public void test02()&#123; &#x2F;&#x2F;1. 以前的写法 Comparator&lt;Integer&gt; comparator01 &#x3D; new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return o1.compareTo(o2); &#125; &#125;; System.out.println(comparator01.compare(95, 27)); System.out.println(&quot;------------------------&quot;); &#x2F;&#x2F;2. Lambda表达式 Comparator&lt;Integer&gt; comparator02 &#x3D; (o1,o2) -&gt; o1.compareTo(o2); System.out.println(comparator02.compare(12, 21)); System.out.println(&quot;------------------------&quot;); &#x2F;&#x2F;3. 方法引用 Comparator&lt;Integer&gt; comparator03 &#x3D; Integer::compareTo; System.out.println(comparator03.compare(20, 77)); &#125; Lambda表达式语法的使用 举例： (o1,o2) -&gt; Integer.compare(o1,o2); 格式： -&gt;：lambda操作符或箭头操作符 -&gt;左边：lambda形参列表（其实就是接口中的抽象方法的形参列表） -&gt;右边：lambda体（其实就是重写的抽象方法的方法体） Lambda表达式的使用：（分为6种情况介绍） 语法格式一：无参无返回值 @Test public void test01()&#123; &#x2F;&#x2F;1. 以前的写法 Runnable runnable01 &#x3D; new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;你 的 城 市 好 像 不 欢 迎 我&quot;); &#125; &#125;; runnable01.run(); System.out.println(&quot;-------------------------&quot;); &#x2F;&#x2F;2. Lambda表达式 Runnable runnable02 &#x3D; () -&gt; System.out.println(&quot;所 以 我 只 好 转 身 离 开 了&quot;); runnable02.run(); &#125; 语法格式二：Lambda需要一个参数，但是没有返回值 @Test public void test03()&#123; &#x2F;&#x2F;1. 以前的写法 Consumer&lt;String&gt; consumer01 &#x3D; new Consumer&lt;String&gt;() &#123; @Override public void accept(String s) &#123; System.out.println(s); &#125; &#125;; consumer01.accept(&quot;其实我存过你照片 也研究过你的星座&quot;); System.out.println(&quot;-------------------------&quot;); &#x2F;&#x2F;2. Lambda表达式 Consumer&lt;String&gt; consumer02 &#x3D; (String s) -&gt; &#123;System.out.println(s);&#125;; consumer02.accept(&quot;你喜欢的歌我也会去听 你喜欢的事物我也会想去了解&quot;); &#125; 语法格式三： 数据类型可以省略，因为可由类型推断得出 @Test public void test04()&#123; &#x2F;&#x2F;1. 以前的写法 Consumer&lt;String&gt; consumer01 &#x3D; new Consumer&lt;String&gt;() &#123; @Override public void accept(String s) &#123; System.out.println(s); &#125; &#125;; consumer01.accept(&quot;我远比表面上更喜欢你&quot;); System.out.println(&quot;-------------------------&quot;); &#x2F;&#x2F;2. Lambda表达式 Consumer&lt;String&gt; consumer02 &#x3D; (s) -&gt; &#123;System.out.println(s);&#125;; consumer02.accept(&quot;但我没有说&quot;); &#125; 语法格式四： Lambda若只需要一个参数，参数的小括号可以省略 @Test public void test04()&#123; &#x2F;&#x2F;1. 以前的写法 Consumer&lt;String&gt; consumer01 &#x3D; new Consumer&lt;String&gt;() &#123; @Override public void accept(String s) &#123; System.out.println(s); &#125; &#125;; consumer01.accept(&quot;我远比表面上更喜欢你&quot;); System.out.println(&quot;-------------------------&quot;); &#x2F;&#x2F;2. Lambda表达式 Consumer&lt;String&gt; consumer02 &#x3D; s -&gt; &#123;System.out.println(s);&#125;; consumer02.accept(&quot;但我没有说&quot;); &#125; 语法格式五： Lambda需要两个或以上参数，多条执行语句，并且有返回值 @Test public void test02() &#123; &#x2F;&#x2F;1. 以前的写法 Comparator&lt;Integer&gt; comparator01 &#x3D; new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; System.out.println(o1); System.out.println(o2); return o1.compareTo(o2); &#125; &#125;; System.out.println(comparator01.compare(95, 27)); System.out.println(&quot;-------------------------&quot;); &#x2F;&#x2F;2. Lambda表达式 Comparator&lt;Integer&gt; comparator02 &#x3D; (o1, o2) -&gt; &#123; System.out.println(o1); System.out.println(o2); return o1.compareTo(o2); &#125;; System.out.println(comparator02.compare(12, 21)); &#125; 语法格式六： 当Lambda体只有一条语句时，return与{}若有，则都可以省略 public void test02() &#123; &#x2F;&#x2F;1. 以前的写法 Comparator&lt;Integer&gt; comparator01 &#x3D; new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return o1.compareTo(o2); &#125; &#125;; System.out.println(comparator01.compare(95, 27)); System.out.println(&quot;-------------------------&quot;); &#x2F;&#x2F;2. Lambda表达式 Comparator&lt;Integer&gt; comparator02 &#x3D; (o1, o2) -&gt; o1.compareTo(o2); System.out.println(comparator02.compare(12, 21)); &#125; 方法引用和构造器引用 当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用 方法引用可以看做会Lambda表达式的深层次表达，换句话说，方法引用就是Lambda表达式，也就是函数式接口的一个实例，通过方法的名字来指向一个方法，可以认为是Lambda表达式的一个语法糖 要求：实现接口的抽象方法的参数列表和返回值类型，必须与方法引用的方法的参数列表和返回值类型保持一致 方法引用的使用情况 方法引用的使用 使用情境：当要传递给Lambda体的操作，已经有实现的方法了，可以使用方法引用！ 方法引用，本质上就是Lambda表达式，而Lambda表达式作为函数式接口的实例。所以方法引用，也是函数式接口的实例。 使用格式： 类(或对象) :: 方法名 具体分为如下的三种情况： 对象::实例方法名 类::静态方法名 类::实例方法名 方法引用使用的要求：要求接口中的抽象方法的形参列表和返回值类型与方法引用的方法的形参列表和返回值类型相同！（针对于情况1和情况2） 先写一个实体类 @Data public class Student &#123; private String name; private Integer id; &#125; 情况一：对象::实例方法名，抽象方法的形参列表和返回值类型与方法引用的方法的形参列表和返回值类型相同 //情况一：对象::实例方法名，抽象方法的形参列表和返回值类型与方法引用的方法的形参列表和返回值类型相同 //Consumer中的void accept(T t) //PrintStream中的void println(T t) //形参列表均为(T t)，返回值均为void，可以使用方法引用 @org.junit.jupiter.api.Test public void test03() &#123; //1. Lambda Consumer&lt;String> consumer01 = s -> System.out.println(s); consumer01.accept(\"她的手只有我的手四分之三那麼大\"); System.out.println(\"-----------------------------\"); //2. 方法引用 PrintStream printStream = System.out; Consumer&lt;String> consumer02 = printStream::println; consumer02.accept(\"\\\"可我還是沒能抓住\\\"\"); System.out.println(\"-----------------------------\"); //3. 但貌似也可以这么写 Consumer&lt;String> consumer03 = System.out::println; consumer03.accept(\"\\\"花落下的时候没死 风捡起花 又丢下 花才死了\\\"\"); &#125; /** * 她的手只有我的手四分之三那麼大 * ----------------------------- * \"可我還是沒能抓住\" * ----------------------------- * \"花落下的时候没死 风捡起花 又丢下 花才死了\" */ 情况二：类 :: 静态方法 //情况二：类 :: 静态方法 //Comparator中的int compare(T t1,T t2) //Integer中的int compare(T t1,T t2) //形参列表均为`(T t1,T t2)`，返回值均为`int`，可以使用方法引用 @org.junit.jupiter.api.Test public void test04() &#123; //1. Lambda Comparator&lt;Integer> comparator01 = (o1, o2) -> Integer.compare(01, 02); System.out.println(comparator01.compare(20, 30)); System.out.println(\"----------------------------\"); //2. 方法引用 Comparator&lt;Integer> comparator02 = Integer::compare; System.out.println(comparator02.compare(64, 30)); &#125; /** * -1 * ---------------------------- * 1 */ //Function中的R apply(T t) //Math中的Long round(Double d) //返回值和参数列表为泛型，也可以匹配上，可以使用方法引用 @org.junit.jupiter.api.Test public void test05() &#123; //1. Lambda Function&lt;Double, Long> function01 = aDouble -> Math.round(aDouble); System.out.println(function01.apply(3.1415926)); System.out.println(\"----------------------------\"); //2. 方法引用 Function&lt;Double, Long> function02 = Math::round; System.out.println(function02.apply(0.876)); &#125; /** * 3 * ---------------------------- * 1 */ 情况三：类 :: 实例方法 //情况三：类 :: 实例方法 // Comparator中的int comapre(T t1,T t2) // String中的int t1.compareTo(t2) @org.junit.jupiter.api.Test public void test06() &#123; //1. Lambda Comparator&lt;Integer> comparator01 = (o1, o2) -> o1.compareTo(o2); System.out.println(comparator01.compare(94, 21)); System.out.println(\"---------------------------\"); //2. 方法引用 Comparator&lt;Integer> comparator02 = Integer::compareTo; System.out.println(comparator02.compare(43, 96)); &#125; //BiPredicate中的boolean test(T t1, T t2); //String中的boolean t1.equals(t2) @org.junit.jupiter.api.Test public void test10() &#123; //1. Lambda BiPredicate&lt;String, String> biPredicate01 = (o1, o2) -> o1.equals(o2); System.out.println(biPredicate01.test(\"Kyle\", \"Kyle\")); System.out.println(\"----------------------------------\"); //2. 方法引用 BiPredicate&lt;String, String> biPredicate02 = String::equals; System.out.println(biPredicate02.test(\"Viole\", \"Violet\")); &#125; /** * true * ---------------------------------- * false */ // Function中的R apply(T t) // Employee中的String toString(); @org.junit.jupiter.api.Test public void test7() &#123; Student student = new Student(\"Kyle\", 9527); //1. Lambda Function&lt;Student, String> function01 = stu -> stu.toString(); System.out.println(function01.apply(student)); System.out.println(\"------------------------------\"); //2. 方法引用 Function&lt;Student, String> function02 = Student::toString; System.out.println(function02.apply(student)); &#125; /** * Student(name=Kyle, id=9527) * ------------------------------ * Student(name=Kyle, id=9527) */ 构造器引用和数组引用的使用 与函数式接口相结合，自动与函数式接口中方法兼容。 可以把构造器引用赋值给定义的方法，要求构造器参数列表要与接口中抽象方法的参数列表一致！且方法的返回值即为构造器对应类的对象。 1.构造器引用 和方法引用类似，函数式接口的抽象方法的形参列表和构造器的形参列表一致。 抽象方法的返回值类型即为构造器所属的类的类型 @org.junit.jupiter.api.Test public void test8() &#123; &#x2F;&#x2F;1. Lambda BiFunction&lt;String, Integer, Student&gt; biFunction01 &#x3D; (string, integer) -&gt; new Student(string, integer); System.out.println(biFunction01.apply(&quot;Kyle&quot;, 9527)); System.out.println(&quot;------------------------------&quot;); &#x2F;&#x2F;2. 方法引用 BiFunction&lt;String, Integer, Student&gt; biFunction02 &#x3D; Student::new; System.out.println(biFunction02.apply(&quot;Lucy&quot;, 9421)); &#125; &#x2F;** * Student(name&#x3D;Kyle, id&#x3D;9527) * ------------------------------ * Student(name&#x3D;Lucy, id&#x3D;9421) *&#x2F; 2.数组引用 可以把数组看做是一个特殊的类，则写法与构造器引用一致 @org.junit.jupiter.api.Test public void test9() &#123; &#x2F;&#x2F;1. Lambda 创建一个指定长度的string数组 Function&lt;Integer, String[]&gt; function01 &#x3D; integer -&gt; new String[integer]; System.out.println(Arrays.toString(function01.apply(5))); System.out.println(&quot;-----------------------------&quot;); &#x2F;&#x2F;2. 数组引用 Function&lt;Integer, String[]&gt; function02 &#x3D; String[]::new; System.out.println(Arrays.toString(function02.apply(7))); &#125; &#x2F;** * [null, null, null, null, null] * ----------------------------- * [null, null, null, null, null, null, null] *&#x2F; Stream APIStream API概述 Java8中有两个最为重要的改变，第一个就是Lambda表达式，另外一个则是Stream API Stream API(java.util.stream)把真正的函数式编程风格引入到Java中，这是目前为止对Java类库最好的补充，因为Stream API可以极大地提高程序员生产力，让程序员写出高效、简洁的代码 Stream是Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。 使用Stream API 对集合数据进行操作，就类似于使用SQL 执行的数据库查询，也可以使用Stream API 来并行执行操作。简言之，Stream API 提供了一种高效且易于使用的处理数据的方式 为什么要使用Stream API 实际开发中，项目中多数数据源都是来自MySQL、Oracle 等。但现在数据源可以更多了，有MongDB、Redis等，而这些NoSQL的数据就需要Java层面去处理。我就是学完Redis再来补票的.. Stream 和Collection 集合的区别：Collection 是一种静态的内存数据结构，而Stream 是有关计算的。前者是主要面向内存，存储在内存中，后者主要是面向CPU，通过CPU 实现计算（这也就是为什么一旦执行终止操作之后，Stream 就不能被再次使用，得重新创建一个新的流才行） 小结 Stream 关注的是对数据的运算，与CPU 打交道；集合关注的是数据的存储，与内存打交道 Stream 自己不会存储数据；Stream 不会改变源对象，相反，他们会返回一个持有结果的新StreamStream 操作是延迟执行的，这意味着他们会等到需要结果的时候才执行 Stream 执行流程 Stream实例化 一系列中间操作（过滤、映射、..） 终止操作 说明 一系列中间操作链，对数据源的数据进行处理 一旦执行终止操作，就执行中间操作链，并产生结果，之后，不会再被使用 Stream的实例化先新建两个类 public class Employee &#123; private int id; private String name; private int age; private double salary; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public double getSalary() &#123; return salary; &#125; public void setSalary(double salary) &#123; this.salary = salary; &#125; public Employee() &#123; System.out.println(\"Employee().....\"); &#125; public Employee(int id) &#123; this.id = id; System.out.println(\"Employee(int id).....\"); &#125; public Employee(int id, String name) &#123; this.id = id; this.name = name; &#125; public Employee(int id, String name, int age, double salary) &#123; this.id = id; this.name = name; this.age = age; this.salary = salary; &#125; @Override public String toString() &#123; return \"Employee&#123;\" + \"id=\" + id + \", name='\" + name + '\\'' + \", age=\" + age + \", salary=\" + salary + '&#125;'; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Employee employee = (Employee) o; if (id != employee.id) return false; if (age != employee.age) return false; if (Double.compare(employee.salary, salary) != 0) return false; return name != null ? name.equals(employee.name) : employee.name == null; &#125; @Override public int hashCode() &#123; int result; long temp; result = id; result = 31 * result + (name != null ? name.hashCode() : 0); result = 31 * result + age; temp = Double.doubleToLongBits(salary); result = 31 * result + (int) (temp ^ (temp >>> 32)); return result; &#125; &#125; /** * 提供用于测试的数据 */ public class EmployeeData &#123; public static List&lt;Employee> getEmployees() &#123; List&lt;Employee> list = new ArrayList&lt;>(); list.add(new Employee(1001, \"马化腾\", 34, 6000.38)); list.add(new Employee(1002, \"马云\", 12, 9876.12)); list.add(new Employee(1003, \"刘强东\", 33, 3000.82)); list.add(new Employee(1004, \"雷军\", 26, 7657.37)); list.add(new Employee(1005, \"李彦宏\", 65, 5555.32)); list.add(new Employee(1006, \"比尔盖茨\", 42, 9500.43)); list.add(new Employee(1007, \"任正非\", 26, 4333.32)); list.add(new Employee(1008, \"扎克伯格\", 35, 2500.32)); return list; &#125; &#125; 创建Stream方式一：通过集合 @org.junit.jupiter.api.Test public void test() &#123; List&lt;Employee> employees= EmployeeData.getEmployees(); //default Stream&lt;E> stream() 返回一个顺序流 Stream&lt;Employee> stream = employees.stream(); //default Stream&lt;E> parallelStream 返回一个并行流 Stream&lt;Employee> employeeStream=employees.parallelStream(); &#125; 创建Stream方式二：通过数组 @org.junit.jupiter.api.Test public void test2() &#123; int[] arr = new int[]&#123;1, 3, 5, 7, 9, 2, 4, 6, 8&#125;; //调用Arrays的static &lt;T> Stream&lt;T> stream(T[] array) 返回一个流 IntStream stream=Arrays.stream(arr); Employee kyle=new Employee(9527,\"Kyle\"); Employee lucy=new Employee(9527,\"Lucy\"); Employee[] employees=&#123;kyle,lucy&#125;; Stream&lt;Employee> stream1=Arrays.stream(employees); &#125; - 创建Stream方式三：通过Stream的of() @org.junit.jupiter.api.Test public void test03() &#123; Stream&lt;Integer> stream=Stream.of(9,4,2,8,2,5,2,7); &#125; 创建Stream方式四：创建无限流 如果不用limit限制输出，则会一直输出下去，forEach就相当于是终止操作 @org.junit.jupiter.api.Test public void test04() &#123; // 迭代 // 遍历前10个数 // public static&lt;T> Stream&lt;T> iterate(final T seed, final UnaryOperator&lt;T> f) Stream.iterate(0,t->t+1).limit(10).forEach(System.out::println); // 生成 // 10个随机数 // public static&lt;T> Stream&lt;T> generate(Supplier&lt;T> s) Stream.generate(Math::random).limit(10).forEach(System.out::println); &#125; Stream的中间操作：筛选与切片 多个中间操作可以连接起来形成一个流水线，除非流水线上触发终止操作，否则中间操作不会执行任何的处理，而在终止操作时一次性全部处理，称为惰性求值 方法 描述 filter(Predicate p) 接收Lambda ，从流中排除某些元素 distinct() 筛选，通过流所生成元素的hashCode() 和equals() 去除重复元素 limit(long maxSize) 截断流，使其元素不超过给定数量 skip(long n) 跳过元素，返回一个扔掉了前n 个元素的流。若流中元素不足n 个，则返回一个空流。与limit(n)互补 @org.junit.jupiter.api.Test public void test05() &#123; List&lt;Employee> employees = EmployeeData.getEmployees(); //1. filter 查询工资大于7000的员工信息 employees.stream().filter(employee -> employee.getSalary()>7000).forEach(System.out::println); System.out.println(\"----------------------------\"); //2. limit(n)——截断流，使其元素不超过给定数量。只输出3条员工信息 employees.stream().limit(3).forEach(System.out::println); System.out.println(\"----------------------------\"); //3. skip(n) —— 跳过元素，返回一个扔掉了前 n 个元素的流。若流中元素不足 n 个，则返回一个空流。与 limit(n) 互补 employees.stream().skip(3).forEach(System.out::println); System.out.println(\"----------------------------\"); //4. distinct()——筛选，通过流所生成元素的 hashCode() 和 equals() 去除重复元素 employees.add(new Employee(9527, \"Kyle\", 20, 9999)); employees.add(new Employee(9527, \"Kyle\", 20, 9999)); employees.add(new Employee(9527, \"Kyle\", 20, 9999)); employees.add(new Employee(9527, \"Kyle\", 20, 9999)); employees.add(new Employee(9527, \"Kyle\", 20, 9999)); employees.stream().distinct().forEach(System.out::println); &#125; /** * Employee&#123;id=1002, name='马云', age=12, salary=9876.12&#125; * Employee&#123;id=1004, name='雷军', age=26, salary=7657.37&#125; * Employee&#123;id=1006, name='比尔盖茨', age=42, salary=9500.43&#125; * ---------------------------- * Employee&#123;id=1001, name='马化腾', age=34, salary=6000.38&#125; * Employee&#123;id=1002, name='马云', age=12, salary=9876.12&#125; * Employee&#123;id=1003, name='刘强东', age=33, salary=3000.82&#125; * ---------------------------- * Employee&#123;id=1004, name='雷军', age=26, salary=7657.37&#125; * Employee&#123;id=1005, name='李彦宏', age=65, salary=5555.32&#125; * Employee&#123;id=1006, name='比尔盖茨', age=42, salary=9500.43&#125; * Employee&#123;id=1007, name='任正非', age=26, salary=4333.32&#125; * Employee&#123;id=1008, name='扎克伯格', age=35, salary=2500.32&#125; * ---------------------------- * Employee&#123;id=1001, name='马化腾', age=34, salary=6000.38&#125; * Employee&#123;id=1002, name='马云', age=12, salary=9876.12&#125; * Employee&#123;id=1003, name='刘强东', age=33, salary=3000.82&#125; * Employee&#123;id=1004, name='雷军', age=26, salary=7657.37&#125; * Employee&#123;id=1005, name='李彦宏', age=65, salary=5555.32&#125; * Employee&#123;id=1006, name='比尔盖茨', age=42, salary=9500.43&#125; * Employee&#123;id=1007, name='任正非', age=26, salary=4333.32&#125; * Employee&#123;id=1008, name='扎克伯格', age=35, salary=2500.32&#125; * Employee&#123;id=9527, name='Kyle', age=20, salary=9999.0&#125; * */ Stream的中间操作:映射 方法 描述 map(Function f) 接收一个函数作为参数，该函数会被应用到每个元素上，并将其映射成一个新的元素。 mapToDouble(ToDoubleFunction f) 接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的DoubleStream。 mapToInt(ToIntFunction f) 接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的IntStream。 mapToLong(ToLongFunction f) 接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的LongStream。 flatMap(Function f) 接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流 @org.junit.jupiter.api.Test public void test06() &#123; List&lt;String> strings = Arrays.asList(\"aa\", \"bb\", \"cc\", \"dd\"); List&lt;Employee> employees = EmployeeData.getEmployees(); //map(Function f)——接收一个函数作为参数，将元素转换成其他形式或提取信息，该函数会被应用到每个元素上，并将其映射成一个新的元素。 // 练习：将字符串转为大写并输出 strings.stream().map(String::toUpperCase).forEach(System.out::println);//方法引用 System.out.println(\"--------------------------\"); strings.stream().map(s -> s.toUpperCase()).forEach(System.out::println);//lambda System.out.println(\"--------------------------\"); // 练习：获取员工姓名长度大于3的员工的姓名。 employees.stream().map(Employee::getName).filter(name->name.length()>3).forEach(System.out::println); System.out.println(\"--------------------------\"); // 练习：将字符串中的多个字符构成的集合转换为对应的Stream实例 strings.stream().map(Test::formStringToStream).forEach(characterStream -> characterStream.forEach(System.out::println)); // 使用flatMap(Function f)达到同样的效果 strings.stream().flatMap(Test::formStringToStream).forEach(System.out::println); &#125; /** *AA * BB * CC * DD * -------------------------- * AA * BB * CC * DD * -------------------------- * 比尔盖茨 * 扎克伯格 * -------------------------- * a * a * b * b * c * c * d * d * a * a * b * b * c * c * d * d * */ public static Stream&lt;Character> formStringToStream(String str) &#123; ArrayList&lt;Character> list = new ArrayList&lt;>(); for (char c : str.toCharArray()) &#123; list.add(c); &#125; return list.stream(); &#125; Stream的中间操作:排序 方法 描述 sorted() 产生一个新流，其中按自然顺序排序 sorted(Comparator com) 产生一个新流，其中按比较器顺序排序 @org.junit.jupiter.api.Test public void test10() &#123; List&lt;Integer> nums = Arrays.asList(13, 54, 97, 52, 43, 64, 27); List&lt;Employee> employees = EmployeeData.getEmployees(); //自然排序 nums.stream().sorted().forEach(System.out::println); //定制排序，先按照年龄升序排，再按照工资降序排 employees.stream().sorted((o1, o2) -> &#123; int result = Integer.compare(o1.getAge(), o2.getAge()); if (result != 0) &#123; return result; &#125; else &#123; return -Double.compare(o1.getSalary(), o2.getSalary()); &#125; &#125;).forEach(System.out::println); &#125; /** * 13 * 27 * 43 * 52 * 54 * 64 * 97 * Employee&#123;id=1002, name='马云', age=12, salary=9876.12&#125; * Employee&#123;id=1004, name='雷军', age=26, salary=7657.37&#125; * Employee&#123;id=1007, name='任正非', age=26, salary=4333.32&#125; * Employee&#123;id=1003, name='刘强东', age=33, salary=3000.82&#125; * Employee&#123;id=1001, name='马化腾', age=34, salary=6000.38&#125; * Employee&#123;id=1008, name='扎克伯格', age=35, salary=2500.32&#125; * Employee&#123;id=1006, name='比尔盖茨', age=42, salary=9500.43&#125; * Employee&#123;id=1005, name='李彦宏', age=65, salary=5555.32&#125; * */ Stream的中间操作:匹配与查找 终端操作会从流的流水线生成结果。其结果可以是任何不是流的值，例如：List、Integer，甚至是void 。 流进行了终止操作后，不能再次使用。 方法 描述 allMatch(Predicate p) 检查是否匹配所有元素 anyMatch(Predicate p) 检查是否至少匹配一个元素 noneMatch(Predicate p) 检查是否没有匹配所有元素 findFirst() 返回第一个元素 findAny() 返回当前流中的任意元素 count() 返回流中元素总数 max(Comparator c) 返回流中最大值 min(Comparator c) 返回流中最小值 forEach(Consumer c) 内部迭代(使用Collection 接口需要用户去做迭代，称为外部迭代。相反，Stream API 使用内部迭代——它帮你把迭代做了) @org.junit.jupiter.api.Test public void test7() &#123; List&lt;Employee> employees = EmployeeData.getEmployees(); // allMatch(Predicate p)——检查是否匹配所有元素。 // 练习：是否所有的员工的工资是否都大于5000 System.out.println(\"是否所有的员工的工资是否都大于5000：\" + employees.stream().allMatch(employee -> employee.getSalary() > 5000)); // anyMatch(Predicate p)——检查是否至少匹配一个元素。 // 练习：是否存在员工年龄小于15 System.out.println(\"是否存在员工年龄小于15：\" + employees.stream().allMatch(employee -> employee.getAge() &lt; 15)); // noneMatch(Predicate p)——检查是否没有匹配的元素。 // 练习：是否不存在员工姓“马” System.out.println(\"是否不存在员工姓马：\" + employees.stream().noneMatch(employee -> employee.getName().startsWith(\"马\"))); //findFirst——返回第一个元素 System.out.println(\"返回第一个元素：\" + employees.stream().findFirst()); //findAny——返回当前流中的任意元素 System.out.println(\"返回当前流中的任意元素\" + employees.stream().findAny()); //count——返回流中元素的总个数 System.out.println(\"返回元素总数：\" + employees.stream().count()); //max(Comparator c)——返回流中最大值 System.out.println(\"返回最高工资：\" + employees.stream().map(Employee::getSalary).max(Double::compare)); //min(Comparator c)——返回流中最小值 System.out.println(\"返回最小年龄：\" + employees.stream().map(Employee::getAge).min(Integer::compare)); //forEach(Consumer c)——内部迭代 employees.stream().forEach(System.out::println); System.out.println(\"-------------\"); ////使用集合的遍历操作 employees.forEach(System.out::println); &#125; /** * 是否所有的员工的工资是否都大于5000：false * 是否存在员工年龄小于15：false * 是否不存在员工姓马：false * 返回第一个元素：Optional[Employee&#123;id=1001, name='马化腾', age=34, salary=6000.38&#125;] * 返回当前流中的任意元素Optional[Employee&#123;id=1001, name='马化腾', age=34, salary=6000.38&#125;] * 返回元素总数：8 * 返回最高工资：Optional[9876.12] * 返回最小年龄：Optional[12] * Employee&#123;id=1001, name='马化腾', age=34, salary=6000.38&#125; * Employee&#123;id=1002, name='马云', age=12, salary=9876.12&#125; * Employee&#123;id=1003, name='刘强东', age=33, salary=3000.82&#125; * Employee&#123;id=1004, name='雷军', age=26, salary=7657.37&#125; * Employee&#123;id=1005, name='李彦宏', age=65, salary=5555.32&#125; * Employee&#123;id=1006, name='比尔盖茨', age=42, salary=9500.43&#125; * Employee&#123;id=1007, name='任正非', age=26, salary=4333.32&#125; * Employee&#123;id=1008, name='扎克伯格', age=35, salary=2500.32&#125; * ------------- * Employee&#123;id=1001, name='马化腾', age=34, salary=6000.38&#125; * Employee&#123;id=1002, name='马云', age=12, salary=9876.12&#125; * Employee&#123;id=1003, name='刘强东', age=33, salary=3000.82&#125; * Employee&#123;id=1004, name='雷军', age=26, salary=7657.37&#125; * Employee&#123;id=1005, name='李彦宏', age=65, salary=5555.32&#125; * Employee&#123;id=1006, name='比尔盖茨', age=42, salary=9500.43&#125; * Employee&#123;id=1007, name='任正非', age=26, salary=4333.32&#125; * Employee&#123;id=1008, name='扎克伯格', age=35, salary=2500.32&#125; * * Process finished with exit code 0 */ Stream的终止操作:归约 方法 描述 reduce(T iden, BinaryOperator b) 可以将流中元素反复结合起来，得到一个值。返回T reduce(BinaryOperator b) 可以将流中元素反复结合起来，得到一个值。返回Optional @org.junit.jupiter.api.Test public void test8() &#123; List&lt;Integer> nums = Arrays.asList(13, 32, 23, 31, 94, 20, 77, 21, 17); List&lt;Employee> employees = EmployeeData.getEmployees(); // reduce(T identity, BinaryOperator)——可以将流中元素反复结合起来，得到一个值。返回 T // 练习1：计算1-10的自然数的和 System.out.println(nums.stream().reduce(0,Integer::sum)); //reduce(BinaryOperator) ——可以将流中元素反复结合起来，得到一个值。返回 Optional&lt;T> // 练习2：计算公司所有员工工资总和 System.out.println(employees.stream().map(Employee::getSalary).reduce((o1,o2)->o1+o2)); // 别的写法，计算年龄总和 System.out.println(employees.stream().map(Employee::getAge).reduce(0,Integer::sum)); &#125; /** * 328 * Optional[48424.08] * 273 */ Stream的终止操作:收集 方法 描述 collect(Collector c) 将流转换为其他形式。接收一个Collector接口的实现，用于给Stream中元素做汇总的方法 @org.junit.jupiter.api.Test public void test9() &#123; // collect(Collector c)——将流转换为其他形式。接收一个 Collector接口的实现，用于给Stream中元素做汇总的方法 // 练习1：查找工资大于6000的员工，结果返回为一个List List&lt;Employee> employees = EmployeeData.getEmployees(); List&lt;Employee> collect = employees.stream().filter(employee -> employee.getSalary() > 6000).collect(Collectors.toList()); collect.forEach(System.out::println); System.out.println(\"--------------------\"); // 练习2：查找年龄大于20的员工，结果返回为一个List List&lt;Employee> collect1 = employees.stream().filter(employee -> employee.getAge() > 20).collect(Collectors.toList()); collect1.forEach(System.out::println); &#125; /** * Employee&#123;id=1001, name='马化腾', age=34, salary=6000.38&#125; * Employee&#123;id=1002, name='马云', age=12, salary=9876.12&#125; * Employee&#123;id=1004, name='雷军', age=26, salary=7657.37&#125; * Employee&#123;id=1006, name='比尔盖茨', age=42, salary=9500.43&#125; * -------------------- * Employee&#123;id=1001, name='马化腾', age=34, salary=6000.38&#125; * Employee&#123;id=1003, name='刘强东', age=33, salary=3000.82&#125; * Employee&#123;id=1004, name='雷军', age=26, salary=7657.37&#125; * Employee&#123;id=1005, name='李彦宏', age=65, salary=5555.32&#125; * Employee&#123;id=1006, name='比尔盖茨', age=42, salary=9500.43&#125; * Employee&#123;id=1007, name='任正非', age=26, salary=4333.32&#125; * Employee&#123;id=1008, name='扎克伯格', age=35, salary=2500.32&#125; * * Process finished with exit code 0 */ Optional类简介 到目前为止，臭名昭著的空指针异常是导致Java应用程序失败的最常见原因。以前，为了解决空指针异常，Google公司著名的Guava项目引入了Optional类，Guava通过使用检查空值的方式来防止代码污染，它鼓励程序员写更干净的代码。受到Google Guava的启发，Optional类已经成为Java 8类库的一部分。 Optional 类(java.util.Optional) 是一个容器类，它可以保存类型T的值，代表这个值存在。或者仅仅保存null，表示这个值不存在。原来用null 表示一个值不存在，现在Optional 可以更好的表达这个概念。并且可以避免空指针异常。 Optional类的Javadoc描述如下：这是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。 Optional提供很多有用的方法，这样我们就不用显式进行空值检测。 创建Optional类对象的方法： Optional.of(T t): 创建一个Optional 实例，t必须非空； Optional.empty() : 创建一个空的Optional 实例 Optional.ofNullable(T t)：t可以为null 判断Optional容器中是否包含对象： boolean isPresent() : 判断是否包含对象 void ifPresent(Consumer&lt;? super T&gt; consumer) ：如果有值，就执行Consumer接口的实现代码，并且该值会作为参数传给它。 获取Optional容器的对象： T get(): 如果调用对象包含值，返回该值，否则抛异常 T orElse(T other) ：如果有值则将其返回，否则返回指定的other对象。 T orElseGet(Supplier&lt;? extends T&gt; other) ：如果有值则将其返回，否则返回由Supplier接口实现提供的对象。 T orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier) ：如果有值则将其返回，否则抛出由Supplier接口实现提供的异常。 使用举例Boy类 public class Boy &#123; private Girl girl; public Boy() &#123; &#125; public Boy(Girl girl) &#123; this.girl = girl; &#125; public Girl getGirl() &#123; return girl; &#125; public void setGirl(Girl girl) &#123; this.girl = girl; &#125; @Override public String toString() &#123; return \"Boy&#123;\" + \"girl=\" + girl + '&#125;'; &#125; &#125; Gril类 public class Girl &#123; private String name; public Girl() &#123; &#125; public Girl(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return \"Girl&#123;\" + \"name='\" + name + '\\'' + '&#125;'; &#125; &#125; 测试类 @Test public void test()&#123; Girl girl = new Girl(); // girl = null; // of(T t):保证t是非空的 Optional&lt;Girl> optionalGirl = Optional.of(girl); &#125; @Test public void test25()&#123; Girl girl = new Girl(); // girl = null; // ofNullable(T t)：t可以为null Optional&lt;Girl> optionalGirl = Optional.ofNullable(girl); System.out.println(optionalGirl); // orElse(T t1):如果单前的Optional内部封装的t是非空的，则返回内部的t. // 如果内部的t是空的，则返回orElse()方法中的参数t1. Girl girl1 = optionalGirl.orElse(new Girl(\"\")); System.out.println(girl1); &#125;","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"}],"author":"ehzyil"},{"title":"Linux 安装 docker遇到版本问题","slug":"2023/Linux 安装 docker遇到版本问题","date":"2023-10-09T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/10/09/2023/Linux 安装 docker遇到版本问题/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/09/2023/Linux%20%E5%AE%89%E8%A3%85%20docker%E9%81%87%E5%88%B0%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98/","excerpt":"","text":"linux 安装 docker遇到版本问题小记 实验环境 Centos 7 linux 小版本更新docker 对 linux 版本有要求(次要版本 &gt;300 左右). 版本过低会导致一系列: 比如端口映射了,但范围不通的情况 找出系统上正在运行的Linux内核版本$ uname -srm Linux 3.10.0-327.el7.x86_64 x86_64 Linux 3.10.0-327.el7.x86_64 x86_64 3 - 内核版本. 10 - 主修订版本. 0-957 - 次要修订版本. 查询可升级最新版本[root@localhost ~]# yum list kernel 已加载插件：fastestmirror, langpacks Loading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.com 已安装的软件包 kernel.x86_64 3.10.0-327.el7 @anaconda 可安装的软件包 kernel.x86_64 3.10.0-1160.92.1.el7 updates 上面我们可以看到可以升级到 3.10.0-1160.92.1.el7, 于是执行命令 yum update -y kernel 进行小版本升级 重启系统命令 sudo init 6 重启系统后, 在查看 linux 内核版本 [root@localhost uname -srm Linux 3.10.0-1160.92.1.el7.x86_64 x86_64 卸载并重新安装Docker，问题解决。","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.ehzyil.xyz/tags/Docker/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.ehzyil.xyz/tags/Linux/"}],"author":"ehzyil"},{"title":"Yauaa：另一个 UserAgent 分析器","slug":"2023/Yauaa：另一个 UserAgent 分析器的使用","date":"2023-10-09T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/10/09/2023/Yauaa：另一个 UserAgent 分析器的使用/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/09/2023/Yauaa%EF%BC%9A%E5%8F%A6%E4%B8%80%E4%B8%AA%20UserAgent%20%E5%88%86%E6%9E%90%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Yauaa自述Yauaa: Yet Another UserAgent Analyzer 这是一个 java 库，尝试解析和分析用户代理字符串（以及用户代理客户端提示可用时）并提取尽可能多的相关属性。可与 Java、Scala、Kotlin 配合使用，并为多种处理系统提供现成的 UDF。完整的文档可以在这里找到https://yauaa.basjes.nl 使用Yauaa第一步 添加Yauaa依赖 如果使用基于 Maven 的项目，只需将此依赖项添加到Maven 的项目中即可。 &lt;dependency&gt; &lt;groupId&gt;nl.basjes.parse.useragent&lt;&#x2F;groupId&gt; &lt;artifactId&gt;yauaa&lt;&#x2F;artifactId&gt; &lt;version&gt;7.22.0&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; 第二步 创建创建一个工具类 UserAgentAnalyzer. public class UserAgentUtils &#123; private static final UserAgentAnalyzer USER_AGENT_ANALYZER; static &#123; USER_AGENT_ANALYZER = UserAgentAnalyzer .newBuilder() .hideMatcherLoadStats() .withField(UserAgent.OPERATING_SYSTEM_NAME_VERSION_MAJOR) .withField(UserAgent.AGENT_NAME_VERSION) .build(); &#125; /** * 从User-Agent解析客户端操作系统和浏览器版本 */ public static Map&lt;String, String> parseOsAndBrowser(String userAgent) &#123; UserAgent agent = USER_AGENT_ANALYZER.parse(userAgent); String os = agent.getValue(UserAgent.OPERATING_SYSTEM_NAME_VERSION_MAJOR); String browser = agent.getValue(UserAgent.AGENT_NAME_VERSION); Map&lt;String, String> map = new HashMap&lt;>(2); map.put(\"os\", os); map.put(\"browser\", browser); return map; &#125; &#125; 第三步使用 对于每个UserAgent（或一组请求标头），您可以调用该parse方法来获得所需的结果： UserAgent agent = uaa.parse(\"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11\"); for (String fieldName: agent.getAvailableFieldNamesSorted()) &#123; System.out.println(fieldName + \" = \" + agent.getValue(fieldName)); &#125; 请注意，并非所有字段在每次解析后都可用。因此，请准备好接收null或Unknown其他字段特定的默认值 String userAgent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"; Map&lt;String, String> userAgentMap = userAgentUtils.parseOsAndBrowser(userAgent); System.err.println(userAgentMap); String os = userAgentMap.get(\"os\"); String browser = userAgentMap.get(\"browser\"); System.err.println(os); System.err.println(browser); 在工具类中 只获取了操作系统和浏览器的名称版本(如有其他需要可自行添加，详见package nl.basjes.parse.useragent;)","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ehzyil.xyz/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Yauaa","slug":"Yauaa","permalink":"https://blog.ehzyil.xyz/tags/Yauaa/"}],"author":"ehzyil"},{"title":"node的卸载与安装","slug":"2023/node的卸载与安装","date":"2023-10-09T00:00:00.000Z","updated":"2024-06-17T01:04:53.991Z","comments":true,"path":"2023/10/09/2023/node的卸载与安装/","link":"","permalink":"https://blog.ehzyil.xyz/2023/10/09/2023/node%E7%9A%84%E5%8D%B8%E8%BD%BD%E4%B8%8E%E5%AE%89%E8%A3%85/","excerpt":"","text":"一、node.js卸载1、打开cmd 输入下列命令 查看npm包路径 #查看全局安装位置 npm root -g 得到npm的文件路径 下面的命令，可以用于查看本机的npm缓存的位置： npm config get cache 2、删除C:\\Users\\用户名\\AppData\\Roaming目录下的npm和npm-cache； 3、在控制面板中找到Node.js的卸载程序，运行卸载程序。 二、安装1、进入node.js各版本下载链接 找到要安装的版本，下载进行安装 2、查看node.js安装版本和npm版本 ehZyiL@DESKTOP-1H567GC MINGW64 ~&#x2F;Desktop $ node -v v18.16.0 ehZyiL@DESKTOP-1H567GC MINGW64 ~&#x2F;Desktop $ npm -v 9.5.1 cmd进入命令行界面，输入node -v 显示node版本，输入npm -v显示npm版本，如果都能显示则安装成功。","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"软件","slug":"软件","permalink":"https://blog.ehzyil.xyz/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"node.js","slug":"node-js","permalink":"https://blog.ehzyil.xyz/tags/node-js/"}],"author":"ehzyil"},{"title":"Spring Security","slug":"2023/Spring Security","date":"2023-09-16T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/09/16/2023/Spring Security/","link":"","permalink":"https://blog.ehzyil.xyz/2023/09/16/2023/Spring%20Security/","excerpt":"","text":"Spring Security1.认证认证原理SpringSecurity的原理其实就是一个过滤器链，内部包含了提供各种功能的过滤器。例如快速入门案例里面使用到的三种过滤器，如下图 监听器 -&gt; 过滤器链 -&gt; dispatcherservlet(前置拦截器 -&gt; mapperHandle -&gt; 后置拦截器 -&gt; 最终拦截器) 下面介绍过滤器链中主要的几个过滤器及其作用： SecurityContextPersistenceFilter 这个Filter是整个拦截过程的入口和出口（也就是第一个和最后一个拦截器），会在请求开始时从配置好的 SecurityContextRepository 中获取 SecurityContext，然后把它设置给 SecurityContextHolder。在请求完成后将 SecurityContextHolder 持有的 SecurityContext 再保存到配置好的 SecurityContextRepository，同时清除 securityContextHolder 所持有的 SecurityContext； UsernamePasswordAuthenticationFilter 用于处理来自表单提交的认证。该表单必须提供对应的用户名和密码，其内部还有登录成功或失败后进行处理的 AuthenticationSuccessHandler 和 AuthenticationFailureHandler，这些都可以根据需求做相关改变； ExceptionTranslationFilter 能够捕获来自 FilterChain 所有的异常，并进行处理。但是它只会处理两类异常：AuthenticationException 和 AccessDeniedException，其它的异常它会继续抛出。 FilterSecurityInterceptor 是用于保护web资源的，使用AccessDecisionManager对当前用户进行授权访问，前面已经详细介绍过了； 认证流程Spring Security的执行流程如下： 用户提交用户名、密码被SecurityFilterChain中的UsernamePasswordAuthenticationFilter过滤器获取到，封装为请求Authentication，通常情况下是UsernamePasswordAuthenticationToken这个实现类。 然后过滤器将Authentication提交至认证管理器（AuthenticationManager）进行认证 认证成功后，AuthenticationManager身份管理器返回一个被填充满了信息的（包括上面提到的权限信息，身份信息，细节信息，但密码通常会被移除）Authentication实例。 SecurityContextHolderAuthentication，安全上下文容器将第3步填充了信息的通 过SecurityContextHolder.getContext().setAuthentication(…)方法，设置到其中。 可以看出AuthenticationManager接口（认证管理器）是认证相关的核心接口，也是发起认证的出发点，它的实现类为ProviderManager。而Spring Security支持多种认证方式，因此ProviderManager维护着一个List列表，存放多种认证方式，最终实际的认证工作是由AuthenticationProvider完成的。咱们知道web表单的对应的AuthenticationProvider实现类为DaoAuthenticationProvider，它的内部又维护着一个UserDetailsService负责UserDetails的获取。最终AuthenticationProvider将UserDetails填充至Authentication。 实践自定义security的思路【登录】 ①、自定义登录接口。用于调用ProviderManager的方法进行认证 如果认证通过生成jwt，然后把用户信息存入redis中 ②、自定义UserDetailsService接口的实现类。在这个实现类中去查询数据库 【校验】 ①、定义Jwt认证过滤器。用于获取token，然后解析token获取其中的userid，还需要从redis中获取用户信息，然后存入SecurityContextHolder 自定义security的搭建第一步:新建Maven项目引入以下依赖 &lt;parent> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-parent&lt;/artifactId> &lt;version>2.5.0&lt;/version> &lt;/parent> &lt;dependencies> &lt;!--还要引入这个，不然后面javax.servlet依赖找不到--> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> &lt;!--springboot整合springsecurity--> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-security&lt;/artifactId> &lt;/dependency> &lt;!--redis依赖--> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-data-redis&lt;/artifactId> &lt;/dependency> &lt;!--fastjson依赖--> &lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>fastjson&lt;/artifactId> &lt;version>1.2.33&lt;/version> &lt;/dependency> &lt;!--jwt依赖--> &lt;dependency> &lt;groupId>io.jsonwebtoken&lt;/groupId> &lt;artifactId>jjwt&lt;/artifactId> &lt;version>0.9.0&lt;/version> &lt;/dependency> &lt;!--引入Lombok依赖，方便实体类开发--> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;/dependency> &lt;/dependencies> 第一步：创建以下类 启动类 @SpringBootApplication @MapperScan(\"com.ehzyil.mapper\") public class SecurityApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(SecurityApplication.class); System.out.println(run); &#125; &#125; 测试接口 @RestController public class HelloController &#123; @RequestMapping(\"/hello\") public String hello()&#123; return \"欢迎，开始你新的学习旅程吧\"; &#125; &#125; utils.FastJsonRedisSerializer package com.ehzyil.utils; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.parser.ParserConfig; import com.alibaba.fastjson.serializer.SerializerFeature; import com.fasterxml.jackson.databind.JavaType; import com.fasterxml.jackson.databind.type.TypeFactory; import org.springframework.data.redis.serializer.RedisSerializer; import org.springframework.data.redis.serializer.SerializationException; import java.nio.charset.Charset; /** * Redis使用FastJson序列化 * * @param &lt;T> */ public class FastJsonRedisSerializer&lt;T> implements RedisSerializer&lt;T> &#123; public static final Charset DEFAULT_CHARSET = Charset.forName(\"UTF-8\"); static &#123; ParserConfig.getGlobalInstance().setAutoTypeSupport(true); &#125; private Class&lt;T> clazz; public FastJsonRedisSerializer(Class&lt;T> clazz) &#123; super(); this.clazz = clazz; &#125; @Override public byte[] serialize(T t) throws SerializationException &#123; if (t == null) &#123; return new byte[0]; &#125; return JSON.toJSONString(t, SerializerFeature.WriteClassName).getBytes(DEFAULT_CHARSET); &#125; @Override public T deserialize(byte[] bytes) throws SerializationException &#123; if (bytes == null || bytes.length &lt;= 0) &#123; return null; &#125; String str = new String(bytes, DEFAULT_CHARSET); return JSON.parseObject(str, clazz); &#125; protected JavaType getJavaType(Class&lt;?> clazz) &#123; return TypeFactory.defaultInstance().constructType(clazz); &#125; &#125; JwtUtil package com.ehzyil.utils; import io.jsonwebtoken.Claims; import io.jsonwebtoken.JwtBuilder; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; import javax.crypto.SecretKey; import javax.crypto.spec.SecretKeySpec; import java.util.Base64; import java.util.Date; import java.util.UUID; &#x2F;&#x2F;JWT工具类 public class JwtUtil &#123; &#x2F;&#x2F;有效期为 public static final Long JWT_TTL &#x3D; 60 * 60 * 1000L;&#x2F;&#x2F; 60 * 60 *1000 一个小时 &#x2F;&#x2F;设置秘钥明文, 注意长度必须大于等于6位 public static final String JWT_KEY &#x3D; &quot;huanfqc&quot;; public static String getUUID() &#123; String token &#x3D; UUID.randomUUID().toString().replaceAll(&quot;-&quot;, &quot;&quot;); return token; &#125; &#x2F;** * 生成jtw * * @param subject token中要存放的数据（json格式） * @return *&#x2F; public static String createJWT(String subject) &#123; JwtBuilder builder &#x3D; getJwtBuilder(subject, null, getUUID());&#x2F;&#x2F; 设置过期时间 return builder.compact(); &#125; &#x2F;** * 生成jtw * * @param subject token中要存放的数据（json格式） * @param ttlMillis token超时时间 * @return *&#x2F; public static String createJWT(String subject, Long ttlMillis) &#123; JwtBuilder builder &#x3D; getJwtBuilder(subject, ttlMillis, getUUID());&#x2F;&#x2F; 设置过期时间 return builder.compact(); &#125; private static JwtBuilder getJwtBuilder(String subject, Long ttlMillis, String uuid) &#123; SignatureAlgorithm signatureAlgorithm &#x3D; SignatureAlgorithm.HS256; SecretKey secretKey &#x3D; generalKey(); long nowMillis &#x3D; System.currentTimeMillis(); Date now &#x3D; new Date(nowMillis); if (ttlMillis &#x3D;&#x3D; null) &#123; ttlMillis &#x3D; JwtUtil.JWT_TTL; &#125; long expMillis &#x3D; nowMillis + ttlMillis; Date expDate &#x3D; new Date(expMillis); return Jwts.builder() .setId(uuid) &#x2F;&#x2F;唯一的ID .setSubject(subject) &#x2F;&#x2F; 主题 可以是JSON数据 .setIssuer(&quot;huanf&quot;) &#x2F;&#x2F; 签发者 .setIssuedAt(now) &#x2F;&#x2F; 签发时间 .signWith(signatureAlgorithm, secretKey) &#x2F;&#x2F;使用HS256对称加密算法签名, 第二个参数为秘钥 .setExpiration(expDate); &#125; &#x2F;** * 创建token * * @param id * @param subject * @param ttlMillis * @return *&#x2F; public static String createJWT(String id, String subject, Long ttlMillis) &#123; JwtBuilder builder &#x3D; getJwtBuilder(subject, ttlMillis, id);&#x2F;&#x2F; 设置过期时间 return builder.compact(); &#125; public static void main(String[] args) throws Exception &#123; &#x2F;&#x2F;加密指定字符串，jwt是123456加密后的密文 String jwt &#x3D; createJWT(&quot;123456&quot;); System.out.println(jwt); String token &#x3D; &quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI0Mzg2MTUxZTFkNTE0ZmUwYjRlOWUzZDZiYjFlODA4OSIsInN1YiI6IjEyMzQ1NiIsImlzcyI6Imh1YW5mIiwiaWF0IjoxNjk0NzgzODI3LCJleHAiOjE2OTQ3ODc0Mjd9.z4QnAiSNBQcCAhZPwC5Xfzb0Py4np7qnyrUg0Ih8Qr4&quot;; Claims claims &#x3D; parseJWT(token); System.out.println(claims); &#125; &#x2F;** * 生成加密后的秘钥 secretKey * * @return *&#x2F; public static SecretKey generalKey() &#123; byte[] encodedKey &#x3D; Base64.getDecoder().decode(JwtUtil.JWT_KEY); SecretKey key &#x3D; new SecretKeySpec(encodedKey, 0, encodedKey.length, &quot;AES&quot;); return key; &#125; &#x2F;** * 解析 * * @param jwt * @return * @throws Exception *&#x2F; public static Claims parseJWT(String jwt) throws Exception &#123; SecretKey secretKey &#x3D; generalKey(); return Jwts.parser() .setSigningKey(secretKey) .parseClaimsJws(jwt) .getBody(); &#125; &#125; redis工具类 RedisCache package com.ehzyil.utils; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.BoundSetOperations; import org.springframework.data.redis.core.HashOperations; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.core.ValueOperations; import org.springframework.stereotype.Component; import java.util.*; import java.util.concurrent.TimeUnit; @SuppressWarnings(value = &#123;\"unchecked\", \"rawtypes\"&#125;) @Component //redis工具类 public class RedisCache &#123; @Autowired public RedisTemplate redisTemplate; /** * 缓存基本的对象，Integer、String、实体类等 * * @param key 缓存的键值 * @param value 缓存的值 */ public &lt;T> void setCacheObject(final String key, final T value) &#123; redisTemplate.opsForValue().set(key, value); &#125; /** * 缓存基本的对象，Integer、String、实体类等 * * @param key 缓存的键值 * @param value 缓存的值 * @param timeout 时间 * @param timeUnit 时间颗粒度 */ public &lt;T> void setCacheObject(final String key, final T value, final Integer timeout, final TimeUnit timeUnit) &#123; redisTemplate.opsForValue().set(key, value, timeout, timeUnit); &#125; /** * 设置有效时间 * * @param key Redis键 * @param timeout 超时时间 * @return true=设置成功；false=设置失败 */ public boolean expire(final String key, final long timeout) &#123; return expire(key, timeout, TimeUnit.SECONDS); &#125; /** * 设置有效时间 * * @param key Redis键 * @param timeout 超时时间 * @param unit 时间单位 * @return true=设置成功；false=设置失败 */ public boolean expire(final String key, final long timeout, final TimeUnit unit) &#123; return redisTemplate.expire(key, timeout, unit); &#125; /** * 获得缓存的基本对象。 * * @param key 缓存键值 * @return 缓存键值对应的数据 */ public &lt;T> T getCacheObject(final String key) &#123; ValueOperations&lt;String, T> operation = redisTemplate.opsForValue(); return operation.get(key); &#125; /** * 删除单个对象 * * @param key */ public boolean deleteObject(final String key) &#123; return redisTemplate.delete(key); &#125; /** * 删除集合对象 * * @param collection 多个对象 * @return */ public long deleteObject(final Collection collection) &#123; return redisTemplate.delete(collection); &#125; /** * 缓存List数据 * * @param key 缓存的键值 * @param dataList 待缓存的List数据 * @return 缓存的对象 */ public &lt;T> long setCacheList(final String key, final List&lt;T> dataList) &#123; Long count = redisTemplate.opsForList().rightPushAll(key, dataList); return count == null ? 0 : count; &#125; /** * 获得缓存的list对象 * * @param key 缓存的键值 * @return 缓存键值对应的数据 */ public &lt;T> List&lt;T> getCacheList(final String key) &#123; return redisTemplate.opsForList().range(key, 0, -1); &#125; /** * 缓存Set * * @param key 缓存键值 * @param dataSet 缓存的数据 * @return 缓存数据的对象 */ public &lt;T> BoundSetOperations&lt;String, T> setCacheSet(final String key, final Set&lt;T> dataSet) &#123; BoundSetOperations&lt;String, T> setOperation = redisTemplate.boundSetOps(key); Iterator&lt;T> it = dataSet.iterator(); while (it.hasNext()) &#123; setOperation.add(it.next()); &#125; return setOperation; &#125; /** * 获得缓存的set * * @param key * @return */ public &lt;T> Set&lt;T> getCacheSet(final String key) &#123; return redisTemplate.opsForSet().members(key); &#125; /** * 缓存Map * * @param key * @param dataMap */ public &lt;T> void setCacheMap(final String key, final Map&lt;String, T> dataMap) &#123; if (dataMap != null) &#123; redisTemplate.opsForHash().putAll(key, dataMap); &#125; &#125; /** * 获得缓存的Map * * @param key * @return */ public &lt;T> Map&lt;String, T> getCacheMap(final String key) &#123; return redisTemplate.opsForHash().entries(key); &#125; /** * 往Hash中存入数据 * * @param key Redis键 * @param hKey Hash键 * @param value 值 */ public &lt;T> void setCacheMapValue(final String key, final String hKey, final T value) &#123; redisTemplate.opsForHash().put(key, hKey, value); &#125; /** * 获取Hash中的数据 * * @param key Redis键 * @param hKey Hash键 * @return Hash中的对象 */ public &lt;T> T getCacheMapValue(final String key, final String hKey) &#123; HashOperations&lt;String, String, T> opsForHash = redisTemplate.opsForHash(); return opsForHash.get(key, hKey); &#125; /** * 删除Hash中的数据 * * @param key * @param hkey */ public void delCacheMapValue(final String key, final String hkey) &#123; HashOperations hashOperations = redisTemplate.opsForHash(); hashOperations.delete(key, hkey); &#125; /** * 获取多个Hash中的数据 * * @param key Redis键 * @param hKeys Hash键集合 * @return Hash对象集合 */ public &lt;T> List&lt;T> getMultiCacheMapValue(final String key, final Collection&lt;Object> hKeys) &#123; return redisTemplate.opsForHash().multiGet(key, hKeys); &#125; /** * 获得缓存的基本对象列表 * * @param pattern 字符串前缀 * @return 对象列表 */ public Collection&lt;String> keys(final String pattern) &#123; return redisTemplate.keys(pattern); &#125; &#125; config.RedisConfig 类 package com.ehzyil.config; import com.ehzyil.utils.FastJsonRedisSerializer; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.StringRedisSerializer; /** * @author 35238 * @date 2023/7/11 0011 15:40 */ @Configuration public class RedisConfig &#123; @Bean @SuppressWarnings(value = &#123;\"unchecked\", \"rawtypes\"&#125;) public RedisTemplate&lt;Object, Object> redisTemplate(RedisConnectionFactory connectionFactory) &#123; RedisTemplate&lt;Object, Object> template = new RedisTemplate&lt;>(); template.setConnectionFactory(connectionFactory); FastJsonRedisSerializer serializer = new FastJsonRedisSerializer(Object.class); // 使用StringRedisSerializer来序列化和反序列化redis的key值 template.setKeySerializer(new StringRedisSerializer()); template.setValueSerializer(serializer); // Hash的key也采用StringRedisSerializer的序列化方式 template.setHashKeySerializer(new StringRedisSerializer()); template.setHashValueSerializer(serializer); template.afterPropertiesSet(); return template; &#125; &#125; domain.ResponseResult package com.ehzyil.domain; import com.fasterxml.jackson.annotation.JsonInclude; //响应类 @JsonInclude(JsonInclude.Include.NON_NULL) public class ResponseResult&lt;T> &#123; /** * 状态码 */ private Integer code; /** * 提示信息，如果有错误时，前端可以获取该字段进行提示 */ private String msg; /** * 查询到的结果数据， */ private T data; public ResponseResult(Integer code, String msg) &#123; this.code = code; this.msg = msg; &#125; public ResponseResult(Integer code, T data) &#123; this.code = code; this.data = data; &#125; public Integer getCode() &#123; return code; &#125; public void setCode(Integer code) &#123; this.code = code; &#125; public String getMsg() &#123; return msg; &#125; public void setMsg(String msg) &#123; this.msg = msg; &#125; public T getData() &#123; return data; &#125; public void setData(T data) &#123; this.data = data; &#125; public ResponseResult(Integer code, String msg, T data) &#123; this.code = code; this.msg = msg; this.data = data; &#125; &#125; 在 utile 目录新建 WebUtils 类 package com.ehzyil.utils; import javax.servlet.http.HttpServletResponse; import java.io.IOException; public class WebUtils &#123; /** * 将字符串渲染到客户端 * * @param response 渲染对象 * @param string 待渲染的字符串 * @return null */ public static String renderString(HttpServletResponse response, String string) &#123; try &#123; response.setStatus(200); response.setContentType(\"application/json\"); response.setCharacterEncoding(\"utf-8\"); response.getWriter().print(string); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; &#125; 在 domain目录新建 User类，写入如下 package com.ehzyil.domain; import com.baomidou.mybatisplus.annotation.TableId; import com.baomidou.mybatisplus.annotation.TableName; import lombok.AllArgsConstructor; import lombok.Data; import lombok.NoArgsConstructor; import java.io.Serializable; import java.util.Date; //用户表(User)实体类 @Data @AllArgsConstructor @NoArgsConstructor @TableName(\"sys_user\") public class User implements Serializable &#123; private static final long serialVersionUID = -40356785423868312L; /** * 主键 */ @TableId private Long id; /** * 用户名 */ private String userName; /** * 昵称 */ private String nickName; /** * 密码 */ private String password; /** * 账号状态（0正常 1停用） */ private String status; /** * 邮箱 */ private String email; /** * 手机号 */ private String phonenumber; /** * 用户性别（0男，1女，2未知） */ private String sex; /** * 头像 */ private String avatar; /** * 用户类型（0管理员，1普通用户） */ private String userType; /** * 创建人的用户id */ private Long createBy; /** * 创建时间 */ private Date createTime; /** * 更新人 */ private Long updateBy; /** * 更新时间 */ private Date updateTime; /** * 删除标志（0代表未删除，1代表已删除） */ private Integer delFlag; &#125; 自定义security的数据库第一步: 数据库校验用户。从之前的分析我们可以知道，我们自定义了一个UserDetailsService，让SpringSecurity使用我们的UserDetailsService。我们自己的UserDetailsService可以从数据库中查询用户名和密码。我们先创建一个用户表， 建表语句如下： 注意: 要想让用户的密码是明文存储，需要在密码前加{noop}，作用是例如等下在浏览器登陆的时候就可以用huanf作为用户名，112233作为密码来登陆了 create database if not exists huanf_security; use huanf_security; CREATE TABLE `sys_user` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT COMMENT '主键', `user_name` VARCHAR(64) NOT NULL DEFAULT 'NULL' COMMENT '用户名', `nick_name` VARCHAR(64) NOT NULL DEFAULT 'NULL' COMMENT '昵称', `password` VARCHAR(64) NOT NULL DEFAULT 'NULL' COMMENT '密码', `status` CHAR(1) DEFAULT '0' COMMENT '账号状态（0正常 1停用）', `email` VARCHAR(64) DEFAULT NULL COMMENT '邮箱', `phonenumber` VARCHAR(32) DEFAULT NULL COMMENT '手机号', `sex` CHAR(1) DEFAULT NULL COMMENT '用户性别（0男，1女，2未知）', `avatar` VARCHAR(128) DEFAULT NULL COMMENT '头像', `user_type` CHAR(1) NOT NULL DEFAULT '1' COMMENT '用户类型（0管理员，1普通用户）', `create_by` BIGINT(20) DEFAULT NULL COMMENT '创建人的用户id', `create_time` DATETIME DEFAULT NULL COMMENT '创建时间', `update_by` BIGINT(20) DEFAULT NULL COMMENT '更新人', `update_time` DATETIME DEFAULT NULL COMMENT '更新时间', `del_flag` INT(11) DEFAULT '0' COMMENT '删除标志（0代表未删除，1代表已删除）', PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COMMENT='用户表'; insert into sys_user values (1,'admin','管理员','&#123;noop&#125;123456','0',DEFAULT,DEFAULT,DEFAULT,DEFAULT,'0',DEFAULT,DEFAULT,DEFAULT,DEFAULT,DEFAULT); insert into sys_user values (2,'huanf','涣沷a靑惷','&#123;noop&#125;112233','0',DEFAULT,DEFAULT,DEFAULT,DEFAULT,'1',DEFAULT,DEFAULT,DEFAULT,DEFAULT,DEFAULT); 第二步: 在pom.xml添加如下 &lt;!--引入MybatisPuls依赖--> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId> &lt;version>3.4.3&lt;/version> &lt;/dependency> &lt;!--引入mysql驱动的依赖--> &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;/dependency> 第三步: 在 src&#x2F;main&#x2F;resources 目录新建File，文件名为application.yml，写入如下 server: port: 8089 spring: # 数据库连接信息 datasource: url: jdbc:mysql://localhost:3306/huanf_security?characterEncoding=utf-8&amp;serverTimezone=Asia/Shanghai username: root password: 666666 driver-class-name: com.mysql.cj.jdbc.Driver 第四步: 在 src&#x2F;main&#x2F;java&#x2F;com.ehzyil 目录新建 mapper.UserMapper 接口，写入如下 import com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.ehzyil.domain.User; @Service public interface UserMapper extends BaseMapper&lt;User> &#123; &#125; 第五步: 在引导类添加如下. @MapperScan(\"com.ehzyil.mapper\") 第六步: 在pom.xml添加如下 &lt;!--引入Junit，用于测试--> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-test&lt;/artifactId> &lt;/dependency> 第七步: 在 src&#x2F;test&#x2F;java 目录新建 com.ehzyil.MapperTest类，写入如下。作用是测试mybatis-plus是否正常 package com.ehzyil; import com.ehzyil.domain.User; import com.ehzyil.mapper.UserMapper; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import java.util.List; @SpringBootTest public class MapperTest &#123; @Autowired private UserMapper userMapper; @Test public void testUserMapper()&#123; &#x2F;&#x2F;查询所有用户 List&lt;User&gt; users &#x3D; userMapper.selectList(null); System.out.println(users); &#125; &#125; 第八步: 运行MapperTest类的testUserMapper方法，看是否能查到数据库的所有用户。到此，可以确定数据库是没问题的，环境到此就准备好了 自定义security的认证实现上面我们已经把准备工作做好了，包括搭建、代码、数据库。接下来我们会实现让security在认证的时候，根据我们数据库的用户和密码进行认证，也就是被security拦截业务接口，出现登录页面之后，我们需要通过输入数据库里的用户和密码来登录，而不是使用security默认的用户和密码进行登录 用户提交账号和密码由DaoAuthenticationProvider调用UserDetailsService的loadUserByUsername()方法获取UserDetails用户信息。 查询DaoAuthenticationProvider的源代码如下： UserDetailsService是一个接口，如下： package org.springframework.security.core.userdetails; public interface UserDetailsService &#123; UserDetails loadUserByUsername(String var1) throws UsernameNotFoundException; &#125; UserDetails是用户信息接口 public interface UserDetails extends Serializable &#123; Collection&lt;? extends GrantedAuthority> getAuthorities(); String getPassword(); String getUsername(); boolean isAccountNonExpired(); boolean isAccountNonLocked(); boolean isCredentialsNonExpired(); boolean isEnabled(); &#125; 我们只要实现UserDetailsService 接口查询数据库得到用户信息返回UserDetails 类型的用户信息即可,框架调用loadUserByUsername()方法拿到用户信息之后是如何执行的，见下图： 思路: 只需要新建一个实现类，在这个实现类里面实现Security官方的UserDetailsService接口，然后重写里面的loadUserByUsername方法 注意: 重写好loadUserByUsername方法之后，我们需要把拿到 ‘数据库与用户输入的数据’ 进行比对的结果，也就是user对象这个结果封装成能被 ‘Security官方的UserDetailsService接口’ 接收的类型，例如可以封装成我们下面写的LoginUser类型。然后才能伪装好数据，给Security官方的认证机制去对比user对象与数据库的结果是否匹配。Security官方的认证机制会拿LoginUser类的方法数据(数据库拿，不再用默认的)，跟我们封装过去的user对象进行匹配，要使匹配一致，就证明认证通过，也就是用户在浏览器页面输入的用户名和密码能被Security认证通过，就不再拦截该用户去访问我们的业务接口 第一步: 在domain目录新建LoginUser类，作为UserDetails接口(Security官方提供的接口)的实现类 package com.ehzyil.domain; import lombok.AllArgsConstructor; import lombok.Data; import lombok.NoArgsConstructor; import org.springframework.security.core.GrantedAuthority; import org.springframework.security.core.userdetails.UserDetails; import java.util.Collection; @Data &#x2F;&#x2F;get和set方法 @NoArgsConstructor &#x2F;&#x2F;无参构造 @AllArgsConstructor &#x2F;&#x2F;带参构造 &#x2F;&#x2F;实现UserDetails接口之后，要重写UserDetails接口里面的7个方法 public class LoginUser implements UserDetails &#123; private User user; @Override &#x2F;&#x2F;用于返回权限信息。现在我们正在学&#39;认证&#39;，&#39;权限&#39;后面才学。所以返回null即可 public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() &#123; return null; &#125; @Override &#x2F;&#x2F;用于获取用户密码。由于使用的实体类是User，所以获取的是数据库的用户密码 public String getPassword() &#123; return user.getPassword(); &#125; @Override &#x2F;&#x2F;用于获取用户名。由于使用的实体类是User，所以获取的是数据库的用户名 public String getUsername() &#123; return user.getUserName(); &#125; @Override &#x2F;&#x2F;判断登录状态是否过期。把这个改成true，表示永不过期 public boolean isAccountNonExpired() &#123; return true; &#125; @Override &#x2F;&#x2F;判断账号是否被锁定。把这个改成true，表示未锁定，不然登录的时候，不让你登录 public boolean isAccountNonLocked() &#123; return true; &#125; @Override &#x2F;&#x2F;判断登录凭证是否过期。把这个改成true，表示永不过期 public boolean isCredentialsNonExpired() &#123; return true; &#125; @Override &#x2F;&#x2F;判断用户是否可用。把这个改成true，表示可用状态 public boolean isEnabled() &#123; return true; &#125; &#125; 第二步: 在 src&#x2F;main&#x2F;java&#x2F;com.ehzyil 目录新建 service.impl.MyUserDetailServiceImpl 类,实现UserDetailsService接口，写入如下 package com.ehzyil.service.impl; import com.baomidou.mybatisplus.core.conditions.query.LambdaQueryWrapper; import com.ehzyil.domain.LoginUser; import com.ehzyil.domain.User; import com.ehzyil.mapper.UserMapper; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.security.core.userdetails.UserDetails; import org.springframework.security.core.userdetails.UserDetailsService; import org.springframework.security.core.userdetails.UsernameNotFoundException; import org.springframework.stereotype.Service; import java.util.Objects; @Service public class MyUserDetailServiceImpl implements UserDetailsService &#123; @Autowired private UserMapper userMapper; @Override //UserDetails是Security官方提供的接口 public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; //查询用户信息。我们写的userMapper接口里面是空的，所以调用的是mybatis-plus提供的方法 LambdaQueryWrapper&lt;User> queryWrapper = new LambdaQueryWrapper&lt;>(); queryWrapper.eq(User::getUserName,username); User user = userMapper.selectOne(queryWrapper); //如果用户传进来的用户名，但是数据库没有这个用户名，就会导致我们是查不到的情况，那么就进行下面的判断。避免程序安全问题 if(Objects.isNull(user))&#123; throw new RuntimeException(\"用户名或者密码错误\"); &#125; //把查询到的user结果，封装成UserDetails类型，然后返回。 //但是由于UserDetails是个接口，所以我们先需要在domino目录新建LoginUser类，作为UserDetails的实现类，再写下面那行 return new LoginUser(user); &#125; &#125; 第三步: 测试。运行引导类，浏览器输入如下，然后我们输入一下登录的用户名和密码，看是不是根据数据库来进行认证 http:&#x2F;&#x2F;localhost:8089&#x2F;hello 出现以下页面 输入 admin 123456登录 即可访问 http://localhost:8089/hello 密码加密校验问题上面我们实现了自定义Security的认证机制，让Security根据数据库的数据，来认证用户输入的数据是否正确。但是当时存在一个问题，就是我们在数据库存入用户表的时候，插入的huanf用户的密码是 {noop}112233，为什么用112233不行呢 原因: SpringSecurity默认使用的PasswordEncoder要求数据库中的密码格式为：{加密方式}密码。对应的就是{noop}112233，实际表示的是112233 但是我们在数据库直接暴露112233为密码，会造成安全问题，所以我们需要把加密后的1234的密文当作密码，此时用户在浏览器登录时输入1234，我们如何确保用户能够登录进去呢，答案是SpringSecurity默认的密码校验，替换为SpringSecurity为我们提供的BCryptPasswordEncoder 我们只需要使用把BCryptPasswordEncoder对象注入Spring容器中，SpringSecurity就会使用该PasswordEncoder来进行密码校验。我们可以定义一个SpringSecurity的配置类，SpringSecurity要求这个配置类要继承WebSecurityConfigurerAdapter。 【首先是 ‘加密’，如何实现，如下】 第一步: 在config目录新建 SecurityConfig 类，写入如下。作用是根据原文，生成一个密文 package com.ehzyil.config; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter; import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder; import org.springframework.security.crypto.password.PasswordEncoder; @Configuration //实现Security提供的WebSecurityConfigurerAdapter类，就可以改变密码校验的规则了 public class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Bean //把BCryptPasswordEncoder对象注入Spring容器中，SpringSecurity就会使用该PasswordEncoder来进行密码校验 //注意也可以注入PasswordEncoder，效果是一样的，因为PasswordEncoder是BCry..的父类 public PasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; &#125; 第二步: 测试。在MapperTest类，添加如下，然后运行 TestBCryptPasswordEncoder 方法 @Test public void TestBCryptPasswordEncoder()&#123; //如果不想在下面那行new的话，那么就在该类注入PasswordEncoder，例如如下 /** * @Autowired * private PasswordEncoder passwordEncoder; */ BCryptPasswordEncoder passwordEncoder = new BCryptPasswordEncoder(); //模拟用户输入的密码 String encode1 = passwordEncoder.encode(\"1234\"); //再模拟一次用户输入的密码 String encode2 = passwordEncoder.encode(\"1234\"); //虽然这两次的密码都是一样的，但是加密后是不一样的。每次运行，对同一原文都会有不同的加密结果 //原因:会添加随机的盐，加密结果=盐+原文+加密。达到每次加密后的密文都不相同的效果 System.out.println(encode1); System.out.println(encode2); &#125; 【然后是 ‘校验’，如何实现，如下】 第一步(已做可跳过): 在config目录新建 SecurityConfig 类，写入如下。作用是根据原文，生成一个密文 package com.ehzyil.config; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter; import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder; import org.springframework.security.crypto.password.PasswordEncoder; @Configuration //实现Security提供的WebSecurityConfigurerAdapter类，就可以改变密码校验的规则了 public class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Bean //把BCryptPasswordEncoder对象注入Spring容器中，SpringSecurity就会使用该PasswordEncoder来进行密码校验 //注意也可以注入PasswordEncoder，效果是一样的，因为PasswordEncoder是BCry..的父类 public PasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; &#125; 第二步: 测试。在MapperTest类，添加如下，然后运行 TestBCryptPasswordEncoder 方法 @Test public void TestBCryptPasswordEncoder()&#123; BCryptPasswordEncoder passwordEncoder = new BCryptPasswordEncoder(); //模拟用户输入了1234(第一个参数)，然后我们去跟数据库的密文进行比较(第二个参数) boolean result = passwordEncoder.matches(\"1234\", \"$2a$10$zOitKu6UNk.b/iPFTtIj2u80sH/dfJI9vFr57qhDGteuXj/Wl8uSy\"); //看一下比对结果 System.out.println(result); &#125; 重启测试后发现 账号1登不进去，密码加密过的账号2可以登录。 jwt工具类实现加密校验【加密】 第一步: 使用createJWT方法生成指定字符串的密文。 第一步: 使用parseJWT方法将密文解密(校验)为原文。 public static void main(String[] args) throws Exception &#123; System.out.println(\"***************加密****************\"); //加密指定字符串，token是123456加密后的密文 String token = createJWT(\"123456\"); System.out.println(token); System.out.println(\"***************解密****************\"); //把上面那行的密文解密(校验)为原文 Claims claims = parseJWT(token); System.out.println(claims); //输出解密后的原文 System.out.println(claims.getSubject()); &#125; 第二步: 运行JwtUtil类的main方法 ***************加密**************** eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiJiYzg0ZWY1NjY1MmQ0YjgzODg2MGUxOTM2MGNlY2FiIsInN1YiI6IjEyMzQ1NiIsImlzcyI6Imh1YW5mIiwiaWF0IjoxNjk0ODMzMjU3LCJleHAiOjE2OTQ4MzY4NTd9.bJDcdufq0LMhpdwlEbE5mBkZoGYZRxHIFoo1rb5MN1U ***************解密**************** &#123;jti&#x3D;bc84ef56652d4b838860e19360cecabc, sub&#x3D;123456, iss&#x3D;huanf, iat&#x3D;1694833257, exp&#x3D;1694836857&#125; 123456 登录接口的分析在上面的自定义security的思路当中，我们有一个功能需求是自定义登录接口，这个功能还没有实现，我们需要实现这个功能，但是，实现这个功能需要使用到jwt，我们刚刚也学习了使用jwt来实现加密校验，那么下面就正式学习如何实现这个登录接口，首先是分析，如下 ①我们需要自定义登陆接口，也就是在controller目录新建LoginController类，在controller方法里面去调用service接口，在service接口实现AuthenticationManager去进行用户的认证，注意，我们定义的controller方法要让SpringSecurity对这个接口放行(如果不放行的话，会被SpringSecurity拦截)，让用户访问这个接口的时候不用登录也能访问。 ②在service接口中我们通过AuthenticationManager的authenticate方法来进行用户认证,所以需要在SecurityConfig中配置把AuthenticationManager注入容器 ③认证成功的话要生成一个jwt，放入响应中返回。并且为了让用户下回请求时能通过jwt识别出具体的是哪个用户，我们需要把用户信息存入redis，可以把用户id作为key。 登录接口的实现第一步: 修改数据库的huanf用户的密码，把112233明文修改为对应的密文。密文可以用jwt工具类加密112233看一下，或者直接复制我给出的 UPDATE sys_user SET password &#x3D; &#39;$2a$10$YPnG.IYUk0mMechaxSibBuKmNeTzvuHdcxkqvoxizsll6WCQG9CHG&#39; WHERE id &#x3D; 2; ​ 第二步: 在 SecurityConfig 类添加如下 @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; return super.authenticationManagerBean(); &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; http //由于是前后端分离项目，所以要关闭csrf .csrf().disable() //由于是前后端分离项目，所以session是失效的，我们就不通过Session获取SecurityContext .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() //指定让spring security放行登录接口的规则 .authorizeRequests() // 对于登录接口 anonymous表示允许匿名访问 .antMatchers(\"/user/login\").anonymous() // 除上面外的所有请求全部需要鉴权认证 .anyRequest().authenticated(); &#125; 第三步: 在service目录新建 LoginService 接口，写入如下 package com.ehzyil.service; import com.ehzyil.domain.ResponseResult; import com.ehzyil.domain.User; import org.springframework.stereotype.Service; @Service public interface LoginService &#123; ResponseResult login(User user); &#125; 第四步: 在service目录新建 impl.LoginServiceImpl 类，写入如下 package com.ehzyil.service.impl; @Service //写登录的核心代码 public class LoginServiceImpl implements LoginService &#123; @Autowired //先在SecurityConfig，使用@Bean注解重写官方的authenticationManagerBean类，然后这里才能注入成功 private AuthenticationManager authenticationManager; @Autowired //RedisCache是我们在utils目录写好的类 private RedisCache redisCache; @Override //ResponseResult和user是我们在domain目录写好的类 public ResponseResult login(User user) &#123; //用户在登录页面输入的用户名和密码 UsernamePasswordAuthenticationToken authenticationToken = new UsernamePasswordAuthenticationToken(user.getUserName(),user.getPassword()); //获取AuthenticationManager的authenticate方法来进行用户认证 Authentication authenticate = authenticationManager.authenticate(authenticationToken); //判断上面那行的authenticate是否为null，如果是则认证没通过，就抛出异常 if(Objects.isNull(authenticate))&#123; throw new RuntimeException(\"登录失败\"); &#125; //如果认证通过，就使用userid生成一个jwt，然后把jwt存入ResponseResult后返回 LoginUser loginUser = (LoginUser) authenticate.getPrincipal(); String userid = loginUser.getuser().getId().toString(); String jwt = JwtUtil.createJWT(userid); //把完整的用户信息存入redis，其中userid作为key，注意存入redis的时候加了前缀 login: Map&lt;String, String> map = new HashMap&lt;>(); map.put(\"token\",jwt); redisCache.setCacheObject(\"login:\"+userid,loginUser); return new ResponseResult(200,\"登录成功\",map); &#125; &#125; 第五步: 在controller目录新建 LoginController 类，写入如下 package com.ehzyil.controller; import com.ehzyil.domain.ResponseResult; import com.ehzyil.domain.User; import com.ehzyil.service.LoginService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RestController; @RestController public class LoginController &#123; @Autowired //LoginService是我们在service目录写好的接口 private LoginService loginService; @PostMapping(\"/user/login\") //ResponseResult和user是我们在domain目录写好的类 public ResponseResult login(@RequestBody User user)&#123; //登录 return loginService.login(user); &#125; &#125; 第六步: 在application.yml添加如下，作用是添加redis的连接信息 redis: host: 127.0.0.1 port: 6379 第八步: 运行引导类 第九步: 测试。发送下面的POST请求 localhost:8089&#x2F;user&#x2F;login &#123; &quot;userName&quot;:&quot;huanf&quot;, &quot;password&quot;:&quot;112233&quot; &#125; &#123; \"code\": 200, \"msg\": \"登录成功\", \"data\": &#123; \"token\": \"eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI1YjgzNTU2YjQ5OWY0YjU0ODU5M2Q5OWIjk4OTMwOCIsInN1YiI6IjIiLCJpc3MiOiJodWFuZiIsImlhdCI6MTY5NDg0ODU1OCwiZXhwIjoxNjk0ODUyMTU4fQ.Jca_ufkZFNv0vMvmap3r-AvD0QAjctUzdb5TxYcwicg\" &#125; &#125; 注意：第九步的请求一定要使用未加密的密码！！！！！！！！！（搞了一上午才解决..） 认证过滤器在上面学习的自定义security的思路当中，我们有一个功能需求是定义Jwt认证过滤器，这个功能还没有实现，下面就正式学习如何实现这个功能。要实现Jwt认证过滤器，我们需要获取token，然后解析token获取其中的userid，还需要从redis中获取用户信息，然后存入SecurityContextHolder 为什么要有redis参与: 是为了防止过了很久之后，浏览器没有关闭，拿着token也能访问，这样不安全 认证过滤器的作用是什么: 上面我们实现登录接口的时，当某个用户登录之后，该用户就会有一个token值，我们可以通过认证过滤器，由于有token值，并且token值认证通过，也就是证明是这个用户的token值，那么该用户访问我们的业务接口时，就不会被Security拦截。简单理解作用就是登录过的用户可以访问我们的业务接口，拿到对应的资源 第一步: 定义过滤器。在 src&#x2F;main&#x2F;java&#x2F;com.ehzyil 目录新建 filter.JwtAuthenticationTokenFilter 类，写入如下 package com.ehzyil.filter; import com.ehzyil.domain.LoginUser; import com.ehzyil.utils.JwtUtil; import com.ehzyil.utils.RedisCache; import io.jsonwebtoken.Claims; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.security.authentication.UsernamePasswordAuthenticationToken; import org.springframework.security.core.context.SecurityContextHolder; import org.springframework.stereotype.Component; import org.springframework.util.StringUtils; import org.springframework.web.filter.OncePerRequestFilter; import javax.servlet.FilterChain; import javax.servlet.ServletException; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; import java.util.Objects; @Component public class JwtAuthenticationTokenFilter extends OncePerRequestFilter &#123; @Autowired private RedisCache redisCache; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; //获取token，指定你要获取的请求头叫什么 String token = request.getHeader(\"token\"); //判空，不一定所有的请求都有请求头，所以上面那行的token可能为空 //!StringUtils.hasText()方法用于检查给定的字符串是否为空或仅包含空格字符 if (!StringUtils.hasText(token)) &#123; //如果请求没有携带token，那么就不需要解析token，不需要获取用户信息，直接放行就可以 filterChain.doFilter(request, response); //return之后，就不会走下面那些代码 return; &#125; //解析token String userid; //把userid定义在外面，才能同时用于下面的46行和52行 try &#123; Claims claims = JwtUtil.parseJWT(token); userid = claims.getSubject(); &#125; catch (Exception e) &#123; e.printStackTrace(); throw new RuntimeException(\"token非法\"); &#125; //从redis中获取用户信息 String redisKey = \"login:\" + userid; //LoginUser是我们在domain目录写的实体类 LoginUser loginUser = redisCache.getCacheObject(redisKey); //判断获取到的用户信息是否为空，因为redis里面可能并不存在这个用户信息，例如缓存过期了 if(Objects.isNull(loginUser))&#123; //抛出一个异常 throw new RuntimeException(\"用户未登录\"); &#125; //把最终的LoginUser用户信息，通过setAuthentication方法，存入SecurityContextHolder //TODO 获取权限信息封装到Authentication中 UsernamePasswordAuthenticationToken authenticationToken = //第一个参数是LoginUser用户信息，第二个参数是凭证(null)，第三个参数是权限信息(null) new UsernamePasswordAuthenticationToken(loginUser,null,null); SecurityContextHolder.getContext().setAuthentication(authenticationToken); //全部做完之后，就放行 filterChain.doFilter(request, response); &#125; &#125; 第二步: 修改SecurityConfig类为如下，其实也就是在configure方法加了一点代码、并且注入了JwtAuthenticationTokenFilter类 @Configuration //实现Security提供的WebSecurityConfigurerAdapter类，就可以改变密码校验的规则了 public class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Bean //把BCryptPasswordEncoder对象注入Spring容器中，SpringSecurity就会使用该PasswordEncoder来进行密码校验 //注意也可以注入PasswordEncoder，效果是一样的，因为PasswordEncoder是BCry..的父类 public PasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; //---------------------------认证过滤器的实现---------------------------------- @Autowired //注入我们在filter目录写好的类 private JwtAuthenticationTokenFilter jwtAuthenticationTokenFilter; //---------------------------登录接口的实现---------------------------------- @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; return super.authenticationManagerBean(); &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; http //由于是前后端分离项目，所以要关闭csrf .csrf().disable() //由于是前后端分离项目，所以session是失效的，我们就不通过Session获取SecurityContext .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() //指定让spring security放行登录接口的规则 .authorizeRequests() // 对于登录接口 anonymous表示允许匿名访问 .antMatchers(\"/user/login\").anonymous() // 除上面外的所有请求全部需要鉴权认证 .anyRequest().authenticated(); //---------------------------认证过滤器的实现---------------------------------- //把token校验过滤器添加到过滤器链中 //第一个参数是上面注入的我们在filter目录写好的类，第二个参数表示你想添加到哪个过滤器之前 http.addFilterBefore(jwtAuthenticationTokenFilter, UsernamePasswordAuthenticationFilter.class); &#125; &#125; 第三步: 本地打开你的redis 第四步: 运行引导类 第五步: 测试。打开你的postman，发送如下的POST请求，作用是先登录一个用户，这样就能生成这个用户对应的token值 &#123; &quot;code&quot;: 200, &quot;msg&quot;: &quot;登录成功&quot;, &quot;data&quot;: &#123; &quot;token&quot;: &quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiIzZmU4MTk1YjY3MTc0MmM5YTgwODZkNzE2ZTM4OGNlNyIsInN1YiI6IjIiLCJpc3MiOiJodWFuZiIsImlhdCI6MTY5NDg0ODgzNCwiZXhwIjoxNjk0ODUyNDM0fQ.liyE_t-8Zzh2goiOg0crzQNTqMj1mNAyeg3pSQH0FZo&quot; &#125; &#125; 第六步: 测试。继续在你的postman，发送如下GET请求，作用是拿着刚刚的token值，去访问我们的业务接口，看会不会被Security拦截，如果不会拦截，那么就说明认证过滤器生效了，使用场景就是简单理解就是登录过的用户可以访问我们的业务接口，拿到对应的资源 注意，由于token值我们是存在redis，所以是有默认过期时间的。注意在请求头那里，key要写token，value要写你复制的token值，然后点击发送请求。这个token值实际上就是使用jwt工具类把112233密码加密后的密文，不信你翻一下前面笔记看当时112233的密文，长得是不是跟现在的token值格式一样 退出登录上面我们既测试了登录认证，又实现了基于密文的token认证，到此就完整地实现了我们在 ‘认证’ 的 ‘3. 自定义security的思路’ 里面的【登录】和【校验】的功能 那么，我们怎么退出登录呢，也就是让某个用户的登录状态消失，也就是让token失效 ? 实现起来也比较简单，只需要定义一个登陆接口，然后获取SecurityContextHolder中的认证信息，删除redis中对应的数据即可 注意: 我们的token其实就是用户密码的密文，token是存在redis里面 第一步: 把LoginService接口修改为如下，注意只是稍微增加了一点代码，我用虚线隔开了，增加的代码是在虚线的下方 package com.ehzyil.service; import com.ehzyil.domain.ResponseResult; import com.ehzyil.domain.User; import org.springframework.stereotype.Service; @Service public interface LoginService &#123; ResponseResult login(User user); &#x2F;&#x2F;-----------------------------------退出登录-------------------------------- ResponseResult logout(); &#125; 第二步: 把LoginServiceImpl实现类修改为如下，注意只是稍微增加了一点代码，我用虚线隔开了，增加的代码是在虚线的下方 @Service @Slf4j //写登录的核心代码 public class LoginServiceImpl implements LoginService &#123; @Autowired //先在SecurityConfig，使用@Bean注解重写官方的authenticationManagerBean类，然后这里才能注入成功 private AuthenticationManager authenticationManager; @Autowired //RedisCache是我们在utils目录写好的类 private RedisCache redisCache; @Override //ResponseResult和user是我们在domain目录写好的类 public ResponseResult login(User user) &#123; //用户在登录页面输入的用户名和密码 UsernamePasswordAuthenticationToken authenticationToken = new UsernamePasswordAuthenticationToken(user.getUserName(), user.getPassword()); //获取AuthenticationManager的authenticate方法来进行用户认证 Authentication authenticate = authenticationManager.authenticate(authenticationToken); //判断上面那行的authenticate是否为null，如果是则认证没通过，就抛出异常 if (Objects.isNull(authenticate)) &#123; throw new RuntimeException(\"登录失败\"); &#125; //如果认证通过，就使用userid生成一个jwt，然后把jwt存入ResponseResult后返回 LoginUser loginUser = (LoginUser) authenticate.getPrincipal(); String userid = loginUser.getUser().getId().toString(); String jwt = JwtUtil.createJWT(userid); //把完整的用户信息存入redis，其中userid作为key，注意存入redis的时候加了前缀 login: Map&lt;String, String> map = new HashMap&lt;>(); map.put(\"token\", jwt); redisCache.setCacheObject(\"login:\" + userid, loginUser); log.info(\"将token存储到redis！\"); return new ResponseResult(200, \"登录成功\", map); &#125; @Override public ResponseResult logout() &#123; //获取我们在JwtAuthenticationTokenFilter类写的SecurityContextHolder对象中的用户id UsernamePasswordAuthenticationToken authentication = (UsernamePasswordAuthenticationToken) SecurityContextHolder.getContext().getAuthentication(); //loginUser是我们在domain目录写好的实体类 LoginUser loginUser = (LoginUser) authentication.getPrincipal(); //获取用户id Long userid = loginUser.getUser().getId(); //根据用户id，删除redis中的token值，注意我们的key是被 login: 拼接过的，所以下面写完整key的时候要带上 longin: redisCache.deleteObject(\"login:\" + userid); return new ResponseResult(200, \"注销成功\"); &#125; &#125; 第三步: 把LoginController类修改为如下，注意只是稍微增加了一点代码，我用虚线隔开了，增加的代码是在虚线的下方 //-----------------------------------退出登录-------------------------------- @RequestMapping(\"/user/logout\") //ResponseResult是我们在domain目录写好的实体类 public ResponseResult logout()&#123; return loginService.logout(); &#125; 第四步: 本地打开你的redis 第五步: 运行引导类 第六步: 测试。打开你的postman，发送如下的POST请求，作用是先登录一个用户，这样就能生成这个用户对应的token值 第七步: 测试。继续在你的postman，发送如下GET请求，作用是拿着刚刚的token值，去访问我们的业务接口，看在有登录状态的情况下，能不能访问 注意还要带上你刚刚复制的token值，粘贴到消息头的Value输入框 第八步: 测试。继续在你的postman，发送如下GET请求，作用是退出登录，然后去访问我们的业务接口，看在没有登录状态的情况下，能不能访问 2.授权授权的基本流程在SpringSecurity中，会使用默认的FilterSecurityInterceptor来进行权限校验。在FilterSecurityInterceptor中会从SecurityContextHolder获取其中的Authentication，然后获取其中的权限信息。当前用户是否拥有访问当前资源所需的权限 所以我们在项目中只需要把当前登录用户的权限信息也存入Authentication，然后设置我们的资源所需要的权限即可。 自定义访问路径的权限SpringSecurity为我们提供了基于注解的权限控制方案，这也是我们项目中主要采用的方式。我们可以使用注解去指定访问对应的资源所需的权限 第一步: 在SecurityConfig配置类添加如下，作用是开启相关配置 @EnableGlobalMethodSecurity(prePostEnabled &#x3D; true) 第二步: 开启了相关配置之后，就能使用@PreAuthorize等注解了。 @PreAuthorize(&quot;hasAuthority(&#39;hello&#39;)&quot;) 在HelloController类的hello方法，添加如下注解，其中test表示自定义权限的名字 @RestController public class HelloController &#123; @GetMapping(\"/hello\") @PreAuthorize(\"hasAuthority('hello')\") public String hello() &#123; return \"hello world\"; &#125; @PreAuthorize(\"hasAuthority('test')\") @GetMapping(\"/test\") public String hello2() &#123; return \"hasAuthority('test')\"; &#125; &#125; 带权限访问的实现权限信息: 有特殊含义的字符串 我们前面在登录时，会调用到MyUserDetailServiceImpl类的loadUserByUsername方法，当时我们写loadUserByUsername方法时，只写了查询用户数据信息的代码，还差查询用户权限信息的代码。在登录完之后，因为携带了token，所以需要在JwtAuthenticationTokenFilter类添加 ‘获取权限信息封装到Authentication中’ 的代码，添加到UsernamePasswordAuthenticationToken的第三个参数里面，我们当时第三个参数传的是null。 第一步: 在上面的自定义访问路径的权限，我们给HelloController类的”&#x2F;hello”路径和”&#x2F;test”路径添加了权限限制，只有用户具有叫hello或test的权限，才能访问这个路径。 第二步: 把MyUserDetailServiceImpl类修改为如下，主要是增加了查询用户权限信息的代码，权限列表暂时写死 @Override //UserDetails是Security官方提供的接口 public UserDetails loadUserByUsername(String xxusername) throws UsernameNotFoundException &#123; //查询用户信息。我们写的userMapper接口里面是空的，所以调用的是mybatis-plus提供的方法 LambdaQueryWrapper&lt;User> queryWrapper = new LambdaQueryWrapper&lt;>(); //eq方法表示等值匹配，第一个参数是数据库的用户名，第二个参数是我们传进来的用户名，这两个参数进行比较是否相等 queryWrapper.eq(User::getUserName,xxusername); User user = userMapper.selectOne(queryWrapper); //如果用户传进来的用户名，但是数据库没有这个用户名，就会导致我们是查不到的情况，那么就进行下面的判断。避免程序安全问题 if(Objects.isNull(user))&#123;//判断user对象是否为空。当在数据库没有查到数据时，user就会为空，也就会进入这个判断 throw new RuntimeException(\"用户名或者密码错误\"); &#125; //--------------------------------查询用户权限信息--------------------------------- //由于我们自定义了3个权限，所以用List集合存储。注意权限实际就是'有特殊含义的字符串'，所以下面的三个字符串就是自定义的 //下面那行就是我们的权限集合，等下还要在LoginUser类做权限集合的转换 List&lt;String> list = new ArrayList&lt;>(Arrays.asList(\"test\",\"adminAuth\",\"huanfAuth\")); //------------------------------------------------------------------------------ //把查询到的user结果，封装成UserDetails类型，然后返回。 //但是由于UserDetails是个接口，所以我们先需要在domino目录新建LoginUser类，作为UserDetails的实现类，再写下面那行 return new LoginUser(user,list); //这里传了第二个参数，表示的是权限信息 &#125; 第三步: 封装权限信息。把LoginUser类修改为如下，主要是增加了把用户权限字符串的集合，转换封装在authorities变量里面 @Data //get和set方法 @NoArgsConstructor //无参构造 //实现UserDetails接口之后，要重写UserDetails接口里面的7个方法 public class LoginUser implements UserDetails &#123; private User user; //查询用户权限信息 private List&lt;String> permissions; public LoginUser(User user, List&lt;String> permissions) &#123; this.user = user; this.permissions = permissions; &#125; //我们把这个List写到外面这里了，注意成员变量名一定要是authorities，不然会出现奇奇怪怪的问题 @JSONField(serialize = false) //这个注解的作用是不让下面那行的成员变量序列化存入redis，避免redis不支持而报异常 private List&lt;SimpleGrantedAuthority> authorities; @Override //用于返回权限信息。现在我们正在学'认证'，'权限'后面才学。所以返回null即可 //当要查询用户信息的时候，我们不能单纯返回null，要重写这个方法，作用是封装权限信息 public Collection&lt;? extends GrantedAuthority> getAuthorities() &#123; //重写GrantedAuthority接口的getAuthorities方法 /* 第一种权限集合的转换写法如下，传统的方式 //把xxpermissions中的String类型的权限信息(也就是\"test\",\"adminAuth\",\"huanfAuth\")封装成SimpleGrantedAuthority对象 //List&lt;GrantedAuthority> authorities = new ArrayList&lt;>(); //简化这行如下一行，我们把authorities成员变量写到外面了 authorities = new ArrayList&lt;>(); for (String yypermission : xxpermissions) &#123; SimpleGrantedAuthority yyauthority = new SimpleGrantedAuthority(yypermission); authorities.add(yyauthority); &#125; */ /* 第二种权限集合的转换写法如下，函数式编程 + stream流 的方式，双引号表示方法引用 */ //当authorities集合为空，就说明是第一次，就需要转换，当不为空就说明不是第一次，就不需要转换直接返回 if(authorities != null)&#123; //严谨来说这个if判断是避免整个调用链中security本地线程变量在获取用户时的重复解析，和redis存取无关 return authorities; &#125; //为空的话就会执行下面的转换代码 //List&lt;SimpleGrantedAuthority> authorities = xxpermissions //简化这行如下一行，我们把authorities成员变量写到外面了 authorities = permissions .stream() .map(SimpleGrantedAuthority::new) .collect(Collectors.toList()); //最终返回转换结果 return authorities; &#125; @Override //用于获取用户密码。由于使用的实体类是User，所以获取的是数据库的用户密码 public String getPassword() &#123; return user.getPassword(); &#125; @Override //用于获取用户名。由于使用的实体类是User，所以获取的是数据库的用户名 public String getUsername() &#123; return user.getUserName(); &#125; @Override //判断登录状态是否过期。把这个改成true，表示永不过期 public boolean isAccountNonExpired() &#123; return true; &#125; @Override //判断账号是否被锁定。把这个改成true，表示未锁定，不然登录的时候，不让你登录 public boolean isAccountNonLocked() &#123; return true; &#125; @Override //判断登录凭证是否过期。把这个改成true，表示永不过期 public boolean isCredentialsNonExpired() &#123; return true; &#125; @Override //判断用户是否可用。把这个改成true，表示可用状态 public boolean isEnabled() &#123; return true; &#125; &#125; 第四步: 把JwtAuthenticationTokenFilter类修改为如下，主要是补充了前面没写的第三个参数，写成第三步封装好的权限信息 //把最终的LoginUser用户信息，通过setAuthentication方法，存入SecurityContextHolder UsernamePasswordAuthenticationToken authenticationToken = //第一个参数是LoginUser用户信息，第二个参数是凭证(null)，第三个参数是权限信息(null) //在学习封装权限信息时，就要把下面的第三个参数补充完整，getAuthorities是我们在loginUser写的方法 new UsernamePasswordAuthenticationToken(loginUser,null,loginUser.getAuthorities()); SecurityContextHolder.getContext().setAuthentication(authenticationToken); 第五步: 测试。打开postman，发送如下的POST请求，作用是先登录一个用户，这样就能生成这个用户对应的token值 继续在postman，发送如下GET请求，作用是拿着刚刚的token值，去访问我们的业务接口，看在有登录状态的情况下，能不能访问 继续访问hello路径因为没有赋予用户hello权限，因此禁止访问。 3.授权-RBAC权限模型刚刚我们实现了只有当用户具备某种权限，才能访问我们的某个业务接口。但是存在一个问题，我们在给用户设置权限的时候，是写死的，在真正的开发中，我们是需要从数据库查询权限信息，下面就来学习如何从数据库查询权限信息，然后封装给用户。这个功能需要先准备好数据库和java代码，所以，下面的 ‘授权-RBAC权限模型’ 都是在围绕这个功能进行学习，直到实现这个功能 介绍RBAC权限模型 (Role-Based Access Control) ，是权限系统用到的经典模型，基于角色的权限控制。该模型由以下五个主要组成部分构成: 一、用户: 在系统中代表具体个体的实体，可以是人员、程序或其他实体。用户需要访问系统资源 二、角色: 角色是权限的集合，用于定义一组相似权限的集合。角色可以被赋予给用户，从而授予用户相应的权限 三、权限: 权限表示系统中具体的操作或功能，例如读取、写入、执行等。每个权限定义了对系统资源的访问规则 四、用户-角色映射: 用户-角色映射用于表示用户与角色之间的关系。通过为用户分配适当的角色，用户可以获得与角色相关联的权限 五、角色-权限映射: 角色-权限映射表示角色与权限之间的关系。每个角色都被分配了一组权限，这些权限决定了角色可执行的操作 截止目前，我们数据库只有 sys_user 用户表，下面我们会新增4张表，分别是权限表(每条数据是单个’粒度细的权限’)、角色表(每条数据是多个’粒度细的权限’)、角色表与权限表的中间表、用户表与角色表的中间表。总共5张表，组成了RBAC模型，中间表的作用是维护两张表的多对多关系. 数据库表的创建第一步: 在你数据库的huanf_security 库，新建 sys_menu权限表、sys_role角色表、sys_role_menu中间表、sys_user_role中间表，并插入数据 create database if not exists huanf_security; use huanf_security; CREATE TABLE `sys_menu` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `menu_name` varchar(64) NOT NULL DEFAULT 'NULL' COMMENT '菜单名', `path` varchar(200) DEFAULT NULL COMMENT '路由地址', `component` varchar(255) DEFAULT NULL COMMENT '组件路径', `visible` char(1) DEFAULT '0' COMMENT '菜单状态（0显示 1隐藏）', `status` char(1) DEFAULT '0' COMMENT '菜单状态（0正常 1停用）', `perms` varchar(100) DEFAULT NULL COMMENT '权限标识', `icon` varchar(100) DEFAULT '#' COMMENT '菜单图标', `create_by` bigint(20) DEFAULT NULL, `create_time` datetime DEFAULT NULL, `update_by` bigint(20) DEFAULT NULL, `update_time` datetime DEFAULT NULL, `del_flag` int(11) DEFAULT '0' COMMENT '是否删除（0未删除 1已删除）', `remark` varchar(500) DEFAULT NULL COMMENT '备注', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COMMENT='权限表'; CREATE TABLE `sys_role` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(128) DEFAULT NULL, `role_key` varchar(100) DEFAULT NULL COMMENT '角色权限字符串', `status` char(1) DEFAULT '0' COMMENT '角色状态（0正常 1停用）', `del_flag` int(1) DEFAULT '0' COMMENT 'del_flag', `create_by` bigint(200) DEFAULT NULL, `create_time` datetime DEFAULT NULL, `update_by` bigint(200) DEFAULT NULL, `update_time` datetime DEFAULT NULL, `remark` varchar(500) DEFAULT NULL COMMENT '备注', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 COMMENT='角色表'; CREATE TABLE `sys_role_menu` ( `role_id` bigint(200) NOT NULL AUTO_INCREMENT COMMENT '角色ID', `menu_id` bigint(200) NOT NULL DEFAULT '0' COMMENT '菜单id', PRIMARY KEY (`role_id`,`menu_id`) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4; CREATE TABLE `sys_user_role` ( `user_id` bigint(200) NOT NULL AUTO_INCREMENT COMMENT '用户id', `role_id` bigint(200) NOT NULL DEFAULT '0' COMMENT '角色id', PRIMARY KEY (`user_id`,`role_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; insert into sys_user_role values (2,1); insert into sys_role values (1,'经理','ceo',0,0,default,default,default,default,default), (2,'程序员','coder',0,0,default,default,default,default,default); insert into sys_role_menu values (1,1),(1,2); insert into sys_menu values (1,'部门管理','dept','system/dept/index',0,0,'system:dept:list','#',default,default,default,default,default,default), (2,'测试','test','system/test/index',0,0,'system:test:list','#',default,default,default,default,default,default) 第二步: 测试SQL语句，也就是确认一下你的建表、插入数据是否达到要求 # 通过用户id去查询这个用户具有的权限列表。也就是根据userid查询perms，并且限制条件为role和menu都必须正常状态么也就是等于0 SELECT DISTINCT m.`perms` FROM sys_user_role ur LEFT JOIN `sys_role` r ON ur.`role_id` = r.`id` LEFT JOIN `sys_role_menu` rm ON ur.`role_id` = rm.`role_id` LEFT JOIN `sys_menu` m ON m.`id` = rm.`menu_id` WHERE user_id = 2 AND r.`status` = 0 AND m.`status` = 0 查询数据库的权限信息第一步: 在 domain 目录新建 Menu 实体类，写入如下 //权限表(也叫菜单表)的实体类 @TableName(value=\"sys_menu\") //指定表名，避免等下mybatisplus的影响 @Data @AllArgsConstructor @NoArgsConstructor @JsonInclude(JsonInclude.Include.NON_NULL) //Serializable是官方提供的，作用是将对象转化为字节序列 public class Menu implements Serializable &#123; private static final long serialVersionUID = -54979041104113736L; @TableId private Long id; /** * 菜单名 */ private String menuName; /** * 路由地址 */ private String path; /** * 组件路径 */ private String component; /** * 菜单状态（0显示 1隐藏） */ private String visible; /** * 菜单状态（0正常 1停用） */ private String status; /** * 权限标识 */ private String perms; /** * 菜单图标 */ private String icon; private Long createBy; private Date createTime; private Long updateBy; private Date updateTime; /** * 是否删除（0未删除 1已删除） */ private Integer delFlag; /** * 备注 */ private String remark; &#125; 第二步: 在 mapper 目录新建 MenuMapper 接口。作用是定义mapper，其中提供一个方法可以根据userid查询权限信息 @Mapper &#x2F;&#x2F;BaseMapper是mybatisplus官方提供的接口，里面提供了很多单表查询的方法 public interface MenuMapper extends BaseMapper&lt;Menu&gt; &#123; &#x2F;&#x2F;&#x2F;&#x2F;由于是多表联查，mybatisplus的BaseMapper接口没有提供，我们需要自定义方法，所以需要创建对应的mapper文件，定义对应的sql语句 List&lt;String&gt; selectPermsByUserId(Long id); &#125; 第三步: 在 resources 目录新建 mapper目录，接着在这个mapper目录新建File，名字叫 MenuMapper.xml，写入如下 &lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?> &lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" > &lt;mapper namespace=\"com.ehzyil.mapper.MenuMapper\"> &lt;select id=\"selectPermsByUserId\" resultType=\"java.lang.String\"> SELECT DISTINCT m.`perms` FROM sys_user_role ur LEFT JOIN `sys_role` r ON ur.`role_id` = r.`id` LEFT JOIN `sys_role_menu` rm ON ur.`role_id` = rm.`role_id` LEFT JOIN `sys_menu` m ON m.`id` = rm.`menu_id` WHERE user_id = #&#123;userid&#125; AND r.`status` = 0 AND m.`status` = 0 &lt;/select> &lt;/mapper> 第四步: 把application.yml修改为如下，作用是告诉MP，刚刚写的MenuMapper.xml文件是在哪个地方 mybatis-plus: # 配置MenuMapper.xml文件的路径 # 也可以不写，因为默认就是在类加载路径(resouces)下的mapper目录的任意层级的后缀为xml的文件，都会被扫描到 mapper-locations: classpath*:/mapper/**/*.xml 第五步: 测试。这里只是检查mybatismlus能不能拿到数据库的权限字符串。在MapperTest类添加如下 @Autowired private MenuMapper menuMapper; @Test public void testSelectPermsByUserId()&#123; &#x2F;&#x2F;L表示Long类型 List&lt;String&gt; list &#x3D; menuMapper.selectPermsByUserId(2L); System.out.println(list); &#125; RBAC权限模型的实现不要把RBAC模型想得很难，其实难的话只是数据库表的设计和SQL语句的编写，需要5张表。数据库设计好之后就很简单了，使用mybatis-plus去查询数据库表的权限字符串(例如我们的权限字符串是放在sys_menu表)，然后把你查到的数据去替换死数据就好了。我们只剩最后一步，就是替换死数据，如下 第一步: 把MyUserDetailServiceImpl类修改为如下，我们只是增加了查询来自数据库的权限信息的代码 @Autowired private UserMapper userMapper; @Autowired &#x2F;&#x2F;MenuMapper是我们在mapper目录写好的接口，作用是查询来自数据库的权限信息 private MenuMapper menuMapper; @Override &#x2F;&#x2F;UserDetails是Security官方提供的接口 public UserDetails loadUserByUsername(String xxusername) throws UsernameNotFoundException &#123; &#x2F;&#x2F;查询用户信息。我们写的userMapper接口里面是空的，所以调用的是mybatis-plus提供的方法 LambdaQueryWrapper&lt;User&gt; queryWrapper &#x3D; new LambdaQueryWrapper&lt;&gt;(); &#x2F;&#x2F;eq方法表示等值匹配，第一个参数是数据库的用户名，第二个参数是我们传进来的用户名，这两个参数进行比较是否相等 queryWrapper.eq(User::getUserName,xxusername); User user &#x3D; userMapper.selectOne(queryWrapper); &#x2F;&#x2F;如果用户传进来的用户名，但是数据库没有这个用户名，就会导致我们是查不到的情况，那么就进行下面的判断。避免程序安全问题 if(Objects.isNull(user))&#123;&#x2F;&#x2F;判断user对象是否为空。当在数据库没有查到数据时，user就会为空，也就会进入这个判断 throw new RuntimeException(&quot;用户名或者密码错误&quot;); &#125; &#x2F;&#x2F;--------------------------------查询用户权限信息--------------------------------- &#x2F;&#x2F;由于我们自定义了3个权限，所以用List集合存储。注意权限实际就是&#39;有特殊含义的字符串&#39;，所以下面的三个字符串就是自定义的 &#x2F;&#x2F;下面那行就是我们的权限集合，等下还要在LoginUser类做权限集合的转换 &#x2F;&#x2F; List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(Arrays.asList(&quot;test&quot;,&quot;adminAuth&quot;,&quot;huanfAuth&quot;)); &#x2F;&#x2F;-------------------------------查询来自数据库的权限信息-------------------------------- List&lt;String&gt; list &#x3D; menuMapper.selectPermsByUserId(user.getId()); &#x2F;&#x2F;------------------------------------------------------------------------------ &#x2F;&#x2F;把查询到的user结果，封装成UserDetails类型，然后返回。 &#x2F;&#x2F;但是由于UserDetails是个接口，所以我们先需要在domino目录新建LoginUser类，作为UserDetails的实现类，再写下面那行 return new LoginUser(user,list); &#x2F;&#x2F;这里传了第二个参数，表示的是权限信息 &#125; 第二步: 由于我们知道数据库传过来的权限字符串是 system:dept:list 和 system:test:list，所以我们要把HelloController类的权限字符串修改为如下 system:test:list 第三步: 测试。 自定义异常处理面的我们学习了 ‘认证’ 和 ‘授权’，实现了基本的权限管理，然后也学习了从数据库获取授权的 ‘授权-RBAC权限模型’，实现了从数据库获取用户具备的权限字符串。到此，我们完整地实现了权限管理的功能，但是，当认证或授权出现报错时，我们希望响应回来的json数据有实体类的code、msg、data这三个字段，怎么实现呢 我们需要学习Spring Security的异常处理机制，就可以在认证失败或者是授权失败的情况下也能和我们的接口一样返回相同结构的json，这样可以让前端能对响应进行统一的处理 在SpringSecurity中，如果我们在认证或者授权的过程中出现了异常会被ExceptionTranslationFilter捕获到，如上图。在ExceptionTranslationFilter中会去判断是认证失败还是授权失败出现的异常，其中有如下两种情况 一、如果是认证过程中出现的异常会被封装成AuthenticationException然后调用AuthenticationEntryPoint对象的方法去进行异常处理。 二、如果是授权过程中出现的异常会被封装成AccessDeniedException然后调用AccessDeniedHandler对象的方法去进行异常处理。 总结: 如果我们需要自定义异常处理，我们只需要创建AuthenticationEntryPoint和AccessDeniedHandler的实现类对象，然后配置给SpringSecurity即可 第一步: 在 src&#x2F;main&#x2F;java&#x2F;com.ehzyil 目录新建 handler.AuthenticationEntryPointImpl类，写入如下，作用是自定义认证的实现类 @Component //这个类只处理认证异常，不处理授权异常 public class AuthenticationEntryPointImpl implements AuthenticationEntryPoint &#123; @Override //第一个参数是请求对象，第二个参数是响应对象，第三个参数是异常对象。把异常封装成授权的对象，然后封装到handle方法 public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException, ServletException &#123; //ResponseResult是我们在domain目录写好的实体类。HttpStatus是spring提供的枚举类，UNAUTHORIZED表示401状态码 ResponseResult result = new ResponseResult(HttpStatus.UNAUTHORIZED.value(), \"用户认证失败，请重新登录\"); //把上面那行拿到的result对象转换为JSON字符串 String json = JSON.toJSONString(result); //WebUtils是我们在utils目录写好的类 WebUtils.renderString(response, json); &#125; &#125; 第二步: 在handler目录新建 AccessDeniedHandlerImpl 类，写入如下，作用是自定义授权的实现类 @Component //这个类只处理授权异常，不处理认证异常 public class AccessDeniedHandlerImpl implements AccessDeniedHandler &#123; @Override //第一个参数是请求对象，第二个参数是响应对象，第三个参数是异常对象。把异常封装成认证的对象，然后封装到handle方法 public void handle(HttpServletRequest request, HttpServletResponse response, AccessDeniedException accessDeniedException) throws IOException, ServletException &#123; //ResponseResult是我们在domain目录写好的实体类。HttpStatus是spring提供的枚举类，FORBIDDEN表示403状态码 ResponseResult result = new ResponseResult(HttpStatus.FORBIDDEN.value(), \"您没有权限进行访问\"); //把上面那行拿到的result对象转换为JSON字符串 String json = JSON.toJSONString(result); //WebUtils是我们在utils目录写好的类 WebUtils.renderString(response, json); &#125; &#125; 第三步: 把 SecurityConfig 类修改为如下，作用是把刚刚两个异常处理的实现类配置在Spring Security里面 @Configuration @EnableGlobalMethodSecurity(prePostEnabled = true) //实现Security提供的WebSecurityConfigurerAdapter类，就可以改变密码校验的规则了 public class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired //注入我们在filter目录写好的类 private JwtAuthenticationTokenFilter jwtAuthenticationTokenFilter; @Autowired //注入Security提供的认证失败的处理器，这个处理器里面的AuthenticationEntryPointImpl实现类，用的不是官方的了， //而是用的是我们在handler目录写好的AuthenticationEntryPointImpl实现类 private AuthenticationEntryPoint authenticationEntryPoint; @Autowired //注入Security提供的授权失败的处理器，这个处理器里面的AccessDeniedHandlerImpl实现类，用的不是官方的了， //而是用的是我们在handler目录写好的AccessDeniedHandlerImpl实现类 private AccessDeniedHandler accessDeniedHandler; //---------------------------认证过滤器的实现---------------------------------- @Bean //把BCryptPasswordEncoder对象注入Spring容器中，SpringSecurity就会使用该PasswordEncoder来进行密码校验 //注意也可以注入PasswordEncoder，效果是一样的，因为PasswordEncoder是BCry..的父类 public PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125; //---------------------------登录接口的实现---------------------------------- @Bean @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; return super.authenticationManagerBean(); &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; http //由于是前后端分离项目，所以要关闭csrf .csrf().disable() //由于是前后端分离项目，所以session是失效的，我们就不通过Session获取SecurityContext .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() //指定让spring security放行登录接口的规则 .authorizeRequests() // 对于登录接口 anonymous表示允许匿名访问 .antMatchers(\"/user/login\").anonymous() // 除上面外的所有请求全部需要鉴权认证 .anyRequest().authenticated(); //---------------------------认证过滤器的实现---------------------------------- //把token校验过滤器添加到过滤器链中 //第一个参数是上面注入的我们在filter目录写好的类，第二个参数表示你想添加到哪个过滤器之前 http.addFilterBefore(jwtAuthenticationTokenFilter, UsernamePasswordAuthenticationFilter.class); //---------------------------异常处理的相关配置------------------------------- http.exceptionHandling() //配置认证失败的处理器 .authenticationEntryPoint(authenticationEntryPoint) //配置授权失败的处理器 .accessDeniedHandler(accessDeniedHandler); &#125; &#125; 第六步: 测试认证异常。打开你的postman，发送如下的POST请求，作用是登录一个不存在的用户，模拟认证异常 第七步: 测试授权异常。先在HelloController类修改PreAuthorize注解的权限字符串，修改成huanf用户不存在的权限字符串，接着重新运行TokenApplication引导类，然后去正常登录一个用户并访问 &#x2F;hello 业务接口，必然会报权限异常，然后我们看一下响应回来的数据格式，是不是我们定义的json格式 跨域跨域的后端解决由于我们的SpringSecurity负责所有请求和资源的管理，当请求经过SpringSecurity时，如果SpringSecurity不允许跨域，那么也是会被拦截，所以下面我们将学习并解决跨域问题。前面我们在测试时，是在postman测试，因此没有出现跨域问题的情况，postman只是负责发请求跟浏览器没关系 浏览器出于安全的考虑，使用 XMLHttpRequest 对象发起HTTP请求时必须遵守同源策略，否则就是跨域的HTTP请求，默认情况下是被禁止的。 同源策略要求源相同才能正常进行通信，即协议、域名、端口号都完全一致。 前后端分离项目，前端项目和后端项目一般都不是同源的，所以肯定会存在跨域请求的问题 我们要实现如下两个需求 (我实际做出的效果跟教程视频不一致，第二个需求其实没必要存在，boot解决了跨域就都解决了): 1、开启SpringBoot的允许跨域访问 2、开启SpringSecurity的允许跨域访问 第一步: 开启SpringBoot的允许跨域访问。在 config 目录新建 CorsConfig 类，写入如下 @Configuration public class CorsConfig implements WebMvcConfigurer &#123; @Override //重写spring提供的WebMvcConfigurer接口的addCorsMappings方法 public void addCorsMappings(CorsRegistry registry) &#123; // 设置允许跨域的路径 registry.addMapping(\"/**\") // 设置允许跨域请求的域名 .allowedOriginPatterns(\"*\") // 是否允许cookie .allowCredentials(true) // 设置允许的请求方式 .allowedMethods(\"GET\", \"POST\", \"DELETE\", \"PUT\") // 设置允许的header属性 .allowedHeaders(\"*\") // 跨域允许时间 .maxAge(3600); &#125; &#125; 第二步: 开启SpringSecurity的允许跨域访问。在把 SecurityConfig 修改为如下。 protected void configure(HttpSecurity http) throws Exception &#123; ... //--------------------------- 设置security运行跨域访问 ------------------ http.cors(); &#125; 第三步: 由于没有前端项目，所以我们下面会跑一个前端项目，然后测试后端的跨域功能 第四步: 运行前端项目。请确保你电脑有安装node.js，然后以管理员身份打开命令行窗口，输入如下 npm install npm run serve 第五步: 访问前端的项目 http:&#x2F;&#x2F;localhost:8080&#x2F;#&#x2F;login 第六步: 由于这个前端项目的要求后端服务的端口是8888，所以我们要修改一下idea的application.yml文件，把默认的8089端口改为8888端口,为了等下直观的看出跨域的问题，我们把后端的 boot与security的跨域代码注释掉 server: # 指定后端的启动端口 port: 8888 第七步: 测试。访问前端项目，登录一个数据库的真实用户，假的也行，主要是看请求会不会出现跨域问题 第八步: 我后来呢，把security的跨域单独注释了，然后重新运行引导类，想验证是不是只要security不允许跨域，那么即使boot允许跨域，那最终也是不允许跨域的，原因是资源必然要经过security。但是实验结果却是security注释了之后，仅靠boot的跨域放行，前端竟然也能正常访问后端接口 授权-权限校验的方法上面学的是HelloController类的 @PreAuthorize注解 的三个方法 我们前面都是使用@PreAuthorize注解，然后在在其中使用的是hasAuthority方法进行校验。SpringSecurity还为我们提供了其它方法例如: hasAnyAuthority，hasRole，hasAnyRole等 1. hasAuthority方法我们最早使用@PreAuthorize注解是在前面笔记的 ‘授权’ 的自定义访问路径的权限。可以回去看看，当时并没有详细学习@PreAuthorize注解的hasAuthority方法，只是直接使用，我们下面就来学习一下hasAuthority方法的原理，然后再学习hasAnyAuthority、hasRole、hasAnyRole方法就容易理解了 hasAuthority方法: 执行到了SecurityExpressionRoot的hasAuthority，内部其实是调用authentication的getAuthorities方法获取用户的权限列表。然后判断我们存入的方法参数数据在权限列表中。hasAnyAuthority方法可以传入多个权限，只有用户有其中任意一个权限都可以访问对应资源 hasAuthority方法的执行流程如下图，图比较多，请从上往下查看 2. hasAnyAuthority方法hasAnyAuthority方法的执行流程跟上面的hasAuthority方法是一样的，只是传参不同。所以重点演示传参，执行流程是真的一模一样，把上面的hasAuthority方法的执行流程认真看一次就行了。hasAuthority方法只能传入一个参数，也就是一个权限字符串。hasAnyAuthority方法可以传入多个参数，也就是多个权限字符串，只要用户具有其中任意一个权限就能访问指定业务接口 第一步: 为方便演示hasAnyAuthority方法能够传入多个参数并看到最终效果，我们把HelloController类的@RequestMapping注解修改为如下 @PreAuthorize(&quot;hasAnyAuthority(&#39;zidingyi&#39;,&#39;huanf&#39;,&#39;system:dept:list&#39;)&quot;); &#x2F;&#x2F;传入3个自定义权限 第二步: 运行引导类，打开postman，发送登录请求，然后发送访问&#x2F;hello接口的请求 3. hasRole方法首先是执行流程，如下几张图，从上往下看，很精华没有尿点 下图是打断点后的测试图 然后是测试的操作，如下 第一步: 为方便演示hasAnyAuthority方法能够传入多个参数并看到最终效果，我们把HelloController类的@RequestMapping注解修改为如下 @PreAuthorize(\"hasRole('system:dept:list')\") //只能传一个权限字符串，多传会报红线 第二步: 运行引导类，打开postman软件，发送登录请求，然后发送访问&#x2F;hello接口的请求 4. hasAnyRole方法执行流程跟刚刚的hasRole方法是一模一样的，只是传参不同。把上面的hasRole方法的执行流程认真看一次就行了。hasRole方法只能传入一个参数，也就是一个权限字符串。hasAnyRole方法可以传入多个参数，也就是多个权限字符串，只要用户具有其中任意一个权限就能访问指定业务接口 第一步: 为方便演示hasAnyAuthority方法能够传入多个参数并看到最终效果，我们把HelloController类的@RequestMapping注解修改为如下 @PreAuthorize(\"hasAnyRole('zidingyi','huanf','system:dept:list')\") 第二步: 运行引导类，打开postman软件，发送登录请求，然后发送访问&#x2F;hello接口的请求 5.自定义权限校验的方法在上面的源码中，我们知道security校验权限的PreAuthorize注解，其实就是获取用户权限，然后跟业务接口的权限进行比较，最后返回一个布尔类型。自定义一个权限校验方法的话，就需要新建一个类，在类里面定义一个方法，按照前面学习的三种方法的定义格式，然后返回值是布尔类型。如下 第一步: 在 src&#x2F;main&#x2F;java&#x2F;com.ehzyil目录新建 expression.HuanfExpressionRoot 类，写入如下 @Component(&quot;huanfEX&quot;) &#x2F;&#x2F;自定义权限校验的方法 public class ExpressionRoot &#123; &#x2F;&#x2F;自定义权限校验的方法 public boolean huanfHasAuthority(String authority) &#123; &#x2F;&#x2F;获取用户具有的权限字符串，有可能用户具有多个权限字符串，所以获取后是一个集合 Authentication authentication &#x3D; SecurityContextHolder.getContext().getAuthentication(); &#x2F;&#x2F;LoginUser是我们在domain目录写好的实体类 LoginUser loginUser &#x3D; (LoginUser) authentication.getPrincipal(); List&lt;String&gt; permissions &#x3D; loginUser.getPermissions(); &#x2F;&#x2F;判断用户权限集合中，是否存在跟业务接口(业务接口的权限字符串会作为authority形参传进来)一样的权限 return permissions.contains(authority); &#125; &#125; 第二步: 刚刚在第一步自定义了huanfHasAuthority方法，用于权限校验，那么如何让 @PreAuthorize 注解去使用huanfHasAuthority方法，只需要在SPEL表达式来获取容器中bean的名字。修改HelloController类为如下 @RestController public class HelloController &#123; @GetMapping(\"/hello\") //官方提供的4个权限校验的方法 //@PreAuthorize(\"hasAuthority('system:dept:list')\") //@PreAuthorize(\"hasAnyAuthority('zidingyi','huanf','system:dept:list')\") //@PreAuthorize(\"hasRole('system:dept:list')\") //@PreAuthorize(\"hasAnyRole('zidingyi','huanf','system:dept:list')\") //自定义权限校验的方法，huanfHasAuthority @PreAuthorize(\"@huanfEX.huanfHasAuthority('system:dept:list')\") public String hello() &#123; return \"欢迎，开始你新的学习旅程吧\"; &#125; @PreAuthorize(\"hasAuthority('test')\") @GetMapping(\"/test\") public String hello2() &#123; return \"hasAuthority('test')\"; &#125; &#125; 使用SPEL表达式，指定@PreAuthorize注解使用我们自定义的huanfHasAuthority方法 第三步: 运行引导类，打开postman软件，发送登录请求，然后发送访问&#x2F;hello接口的请求 6.基于配置的权限控制前面学习的权限控制是基于@PreAuthorize注解来完成的，如何使用配置的方式，也就是在配置类当中，来实现权限控制，如下 第一步: 在HelloController类添加如下，作用是新增一个接口 &#x2F;&#x2F;基于配置的权限控制 @RequestMapping(&quot;&#x2F;configAuth&quot;) public ResponseResult xx()&#123; return new ResponseResult(200,&quot;访问成功&quot;); &#125; 第二步: 把SecurityConfig类修改为如下，主要就是添加权限控制相关的配置 @Override protected void configure(HttpSecurity http) throws Exception &#123; http //由于是前后端分离项目，所以要关闭csrf .csrf().disable() //由于是前后端分离项目，所以session是失效的，我们就不通过Session获取SecurityContext .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() //指定让spring security放行登录接口的规则 .authorizeRequests() // 对于登录接口 anonymous表示允许匿名访问 .antMatchers(\"/user/login\").anonymous() //基于配置的的权限控制。指定接口的地址，为HelloController类里面的/configAuth接口，指定权限为system:dept:list .antMatchers(\"/configAuth\").hasAuthority(\"system:dept:list\") //上一行的hasAuthority方法就是security官方提供的4种权限控制的方法之一 // 除上面外的所有请求全部需要鉴权认证 .anyRequest().authenticated(); //---------------------------认证过滤器的实现---------------------------------- //把token校验过滤器添加到过滤器链中 //第一个参数是上面注入的我们在filter目录写好的类，第二个参数表示你想添加到哪个过滤器之前 http.addFilterBefore(jwtAuthenticationTokenFilter, UsernamePasswordAuthenticationFilter.class); //---------------------------异常处理的相关配置------------------------------- http.exceptionHandling() //配置认证失败的处理器 .authenticationEntryPoint(authenticationEntryPoint) //配置授权失败的处理器 .accessDeniedHandler(accessDeniedHandler); //---------------------------👇 设置security运行跨域访问 👇------------------ http.cors(); &#125; 第三步: 运行引导类，打开postman软件，发送登录请求，然后发送访问&#x2F;configAuth接口的请求 防护CSRF攻击在SecurityConfig类里面的configure方法里面，有一个配置如下，我们上面都没有去学习，下面就来了解一下 http..csrf().disable(); //关闭csrf，可防护csrf攻击。如果不关闭的话 CSRF是指跨站请求伪造（Cross-site request forgery），是web常见的攻击之一，如图 详细看这篇博客: https://blog.csdn.net/freeking101/article/details/86537087 防护: SpringSecurity去防止CSRF攻击的方式就是通过csrf_token。后端会生成一个csrf_token，前端发起请求的时候需要携带这个csrf_token,后端会有过滤器进行校验，如果没有携带或者是伪造的就不允许访问 我们可以发现CSRF攻击依靠的是cookie中所携带的认证信息。但是在前后端分离的项目中我们的认证信息其实是token，而token并不是存储中cookie中，并且需要前端代码去把token设置到请求头中才可以，所以CSRF攻击也就不用担心了，前后端分离的项目，在配置类关闭csrf就能防范csrf攻击 参考：https://www.yuque.com/huanfqc/springsecurity/springsecurity","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Security","slug":"Spring-Security","permalink":"https://blog.ehzyil.xyz/tags/Spring-Security/"}],"author":"ehzyil"},{"title":"Docker的配置与软件安装","slug":"2023/Docker的配置与软件安装","date":"2023-07-05T00:00:00.000Z","updated":"2024-06-17T01:04:53.975Z","comments":true,"path":"2023/07/05/2023/Docker的配置与软件安装/","link":"","permalink":"https://blog.ehzyil.xyz/2023/07/05/2023/Docker%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/","excerpt":"","text":"1.CentOS安装DockerDocker 分为 CE 和 EE 两大版本。CE 即社区版（免费，支持周期 7 个月），EE 即企业版，强调安全，付费使用，支持周期 24 个月。 Docker CE 分为 stable test 和 nightly 三个更新频道。 官方网站上有各种环境下的 安装指南，这里主要介绍 Docker CE 在 CentOS上的安装。 Docker CE 支持 64 位版本 CentOS 7，并且要求内核版本不低于 3.10， CentOS 7 满足最低内核的要求，所以我们在CentOS 7安装Docker。 1.1.卸载（可选）如果之前安装过旧版本的Docker，可以使用下面命令卸载： yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine \\ docker-ce 1.2.安装docker首先需要大家虚拟机联网，安装yum工具 yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 --skip-broken 然后更新本地镜像源： # 设置docker镜像源 yum-config-manager \\ --add-repo \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sed -i 's/download.docker.com/mirrors.aliyun.com\\/docker-ce/g' /etc/yum.repos.d/docker-ce.repo yum makecache fast 然后输入命令： yum install -y docker-ce docker-ce为社区免费版本。稍等片刻，docker即可安装成功。 1.3.启动dockerDocker应用需要用到各种端口，逐一去修改防火墙设置。非常麻烦，因此建议大家直接关闭防火墙！ 启动docker前，一定要关闭防火墙后！！ 启动docker前，一定要关闭防火墙后！！ 启动docker前，一定要关闭防火墙后！！ # 关闭 systemctl stop firewalld # 禁止开机启动防火墙 systemctl disable firewalld 通过命令启动docker： systemctl start docker # 启动docker服务 systemctl stop docker # 停止docker服务 systemctl restart docker # 重启docker服务 然后输入命令，可以查看docker版本： docker -v 如图： 1.4.配置镜像加速docker官方镜像仓库网速较差，我们需要设置国内镜像服务： 参考阿里云的镜像加速文档：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors 针对Docker客户端版本大于 1.10.0 的用户 您可以通过修改daemon配置文件&#x2F;etc&#x2F;docker&#x2F;daemon.json来使用加速器 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF' &#123; \"registry-mirrors\": [\"https://d6kkx5k7.mirror.aliyuncs.com\"] &#125; EOF sudo systemctl daemon-reload sudo systemctl restart docker 2.CentOS7安装DockerCompose2.1.下载Linux下需要通过命令下载： # 安装 curl -L https://github.com/docker/compose/releases/download/1.23.1/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose 如果下载速度较慢，或者下载失败，可以使用课前资料提供的docker-compose文件： 上传到/usr/local/bin/目录也可以。 2.2.修改文件权限修改文件权限： # 修改权限 chmod +x /usr/local/bin/docker-compose 2.3.Base自动补全命令：# 补全命令 curl -L https://raw.githubusercontent.com/docker/compose/1.29.1/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose 如果这里出现错误，需要修改自己的hosts文件： echo \"199.232.68.133 raw.githubusercontent.com\" >> /etc/hosts 3.Docker镜像仓库搭建镜像仓库可以基于Docker官方提供的DockerRegistry来实现。 官网地址：https://hub.docker.com/_/registry 3.1.简化版镜像仓库Docker官方的Docker Registry是一个基础版本的Docker镜像仓库，具备仓库管理的完整功能，但是没有图形化界面。 搭建方式比较简单，命令如下： docker run -d \\ --restart=always \\ --name registry \\ -p 5000:5000 \\ -v registry-data:/var/lib/registry \\ registry 命令中挂载了一个数据卷registry-data到容器内的&#x2F;var&#x2F;lib&#x2F;registry 目录，这是私有镜像库存放数据的目录。 访问http://YourIp:5000/v2/_catalog 可以查看当前私有镜像服务中包含的镜像 3.2.带有图形化界面版本使用DockerCompose部署带有图象界面的DockerRegistry，命令如下： version: '3.0' services: registry: image: registry volumes: - ./registry-data:/var/lib/registry ui: image: joxit/docker-registry-ui:static ports: - 8080:80 environment: - REGISTRY_TITLE=传智教育私有仓库 - REGISTRY_URL=http://registry:5000 depends_on: - registry 3.3.配置Docker信任地址我们的私服采用的是http协议，默认不被Docker信任，所以需要做一个配置： # 打开要修改的文件 vi /etc/docker/daemon.json # 添加内容： \"insecure-registries\":[\"http://192.168.150.101:8080\"] # 重加载 systemctl daemon-reload # 重启docker systemctl restart docker 4.dibian安装更新、安装必备软件 apt-get update &amp;&amp; apt-get install -y wget vim 安装 wget -qO- get.docker.com | bash 开机自动启动 systemctl enable docker 卸载 sudo apt-get purge docker-ce docker-ce-cli containerd.io sudo rm -rf &#x2F;var&#x2F;lib&#x2F;docker sudo rm -rf &#x2F;var&#x2F;lib&#x2F;containerd 4.RabbitMQ部署指南4.1.单机部署我们在Centos7虚拟机中使用Docker来安装。 4.1.1.下载镜像方式一：在线拉取 docker pull rabbitmq:3-management 方式二：从本地加载 在课前资料已经提供了镜像包： 上传到虚拟机中后，使用命令加载镜像即可： docker load -i mq.tar 4.1.2.安装MQ执行下面的命令来运行MQ容器： docker run \\ -e RABBITMQ_DEFAULT_USER=root \\ -e RABBITMQ_DEFAULT_PASS=666666 \\ --name mq \\ --hostname mq1 \\ -p 15672:15672 \\ -p 5672:5672 \\ -d \\ rabbitmq:3-management 4.2.集群部署接下来，我们看看如何安装RabbitMQ的集群。 4.2.1.集群分类在RabbitMQ的官方文档中，讲述了两种集群的配置方式： 普通模式：普通模式集群不进行数据同步，每个MQ都有自己的队列、数据信息（其它元数据信息如交换机等会同步）。例如我们有2个MQ：mq1，和mq2，如果你的消息在mq1，而你连接到了mq2，那么mq2会去mq1拉取消息，然后返回给你。如果mq1宕机，消息就会丢失。 镜像模式：与普通模式不同，队列会在各个mq的镜像节点之间同步，因此你连接到任何一个镜像节点，均可获取到消息。而且如果一个节点宕机，并不会导致数据丢失。不过，这种方式增加了数据同步的带宽消耗。 我们先来看普通模式集群。 4.2.2.设置网络首先，我们需要让3台MQ互相知道对方的存在。 分别在3台机器中，设置 &#x2F;etc&#x2F;hosts文件，添加如下内容： 192.168.150.101 mq1 192.168.150.102 mq2 192.168.150.103 mq3 并在每台机器上测试，是否可以ping通对方： 5.Docker安装MySQL5.1.上传在将课前资料中的mysql.tar文件上传到虚拟机的&#x2F;tmp目录： 通过load命令加载为镜像： docker load -i mysql.tar 效果： 5.2.创建目录创建两个目录，作为数据库的数据卷： 创建目录&#x2F;tmp&#x2F;mysql&#x2F;data 创建目录&#x2F;tmp&#x2F;mysql&#x2F;conf # 创建目录 mkdir -p /tmp/mysql/data mkdir -p /tmp/mysql/conf 将课前资料提供的my.cnf文件上传到&#x2F;tmp&#x2F;mysql&#x2F;conf，如图： 5.3.运行docker命令运行命令： docker run \\ --name mysql \\ -d \\ -p 3306:3306 \\ --restart unless-stopped \\ -v /tmp/mysql/log:/var/log/mysql \\ -v /tmp/mysql/data:/var/lib/mysql \\ -v /tmp/mysql/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=666666 \\ -d \\ mysql:5.7.25 参考 docker run \\ --name mysql \\ -d \\ -p 3306:3306 \\ --restart unless-stopped \\ -v /mydata/mysql/log:/var/log/mysql \\ -v /tmp/mysql/conf/my.cnf:/etc/mysql/conf.d/hmy.cnf \\ -v /tmp/mysql/data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=666666 \\ mysql:5.7 [mysqld] skip-name-resolve character_set_server=utf8 datadir=/var/lib/mysql server-id=1000 bind-address=0.0.0.0 docker exec -it mysql bash mysql -uroot -p 拉取指定版本 sudo docker pull mysql:8.0.23 sudo docker run -p 3306:3306 --name mysql8 \\ --restart unless-stopped \\ -v &#x2F;tmp&#x2F;mysql&#x2F;mysql-files:&#x2F;var&#x2F;lib&#x2F;mysql-files \\ -v &#x2F;tmp&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql \\ -v &#x2F;tmp&#x2F;mysql&#x2F;logs:&#x2F;var&#x2F;log&#x2F;mysql \\ -v &#x2F;tmp&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql \\ -e MYSQL_ROOT_PASSWORD&#x3D;root \\ -d mysql:8.0.23 6.安装elasticsearch1.部署单点es1.1.创建网络因为我们还需要部署kibana容器，因此需要让es和kibana容器互联。这里先创建一个网络： docker network create es-net 1.2.加载镜像这里我们采用elasticsearch的7.12.1版本的镜像，这个镜像体积非常大，接近1G。不建议大家自己pull。 课前资料提供了镜像的tar包： 大家将其上传到虚拟机中，然后运行命令加载即可： # 导入数据 docker load -i es.tar 同理还有kibana的tar包也需要这样做。 1.3.运行运行docker命令，部署单点es： docker run -d \\ --name es \\ -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\ -e \"discovery.type=single-node\" \\ -v es-data:/usr/share/elasticsearch/data \\ -v es-plugins:/usr/share/elasticsearch/plugins \\ --privileged \\ --network es-net \\ -p 9200:9200 \\ -p 9300:9300 \\ elasticsearch:7.12.1 命令解释： -e &quot;cluster.name=es-docker-cluster&quot;：设置集群名称 -e &quot;http.host=0.0.0.0&quot;：监听的地址，可以外网访问 -e &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;：内存大小 -e &quot;discovery.type=single-node&quot;：非集群模式 -v es-data:/usr/share/elasticsearch/data：挂载逻辑卷，绑定es的数据目录 -v es-logs:/usr/share/elasticsearch/logs：挂载逻辑卷，绑定es的日志目录 -v es-plugins:/usr/share/elasticsearch/plugins：挂载逻辑卷，绑定es的插件目录 --privileged：授予逻辑卷访问权 --network es-net ：加入一个名为es-net的网络中 -p 9200:9200：端口映射配置 在浏览器中输入：http://192.168.150.101:9200 即可看到elasticsearch的响应结果： 2.部署kibanakibana可以给我们提供一个elasticsearch的可视化界面，便于我们学习。 2.1.部署运行docker命令，部署kibana docker run -d \\ --name kibana \\ -e ELASTICSEARCH_HOSTS=http://es:9200 \\ --network=es-net \\ -p 5601:5601 \\ kibana:7.12.1 --network es-net ：加入一个名为es-net的网络中，与elasticsearch在同一个网络中 -e ELASTICSEARCH_HOSTS=http://es:9200&quot;：设置elasticsearch的地址，因为kibana已经与elasticsearch在一个网络，因此可以用容器名直接访问elasticsearch -p 5601:5601：端口映射配置 kibana启动一般比较慢，需要多等待一会，可以通过命令： docker logs -f kibana 查看运行日志，当查看到下面的日志，说明成功： 此时，在浏览器输入地址访问：http://192.168.150.101:5601，即可看到结果 2.2.DevToolskibana中提供了一个DevTools界面： 这个界面中可以编写DSL来操作elasticsearch。并且对DSL语句有自动补全功能。 3.安装IK分词器3.1.在线安装ik插件（较慢）# 进入容器内部 docker exec -it elasticsearch /bin/bash # 在线下载并安装 ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.12.1/elasticsearch-analysis-ik-7.12.1.zip #退出 exit #重启容器 docker restart elasticsearch 3.2.离线安装ik插件（推荐）1）查看数据卷目录 安装插件需要知道elasticsearch的plugins目录位置，而我们用了数据卷挂载，因此需要查看elasticsearch的数据卷目录，通过下面命令查看: docker volume inspect es-plugins 显示结果： [ &#123; \"CreatedAt\": \"2022-05-06T10:06:34+08:00\", \"Driver\": \"local\", \"Labels\": null, \"Mountpoint\": \"/var/lib/docker/volumes/es-plugins/_data\", \"Name\": \"es-plugins\", \"Options\": null, \"Scope\": \"local\" &#125; ] 说明plugins目录被挂载到了：/var/lib/docker/volumes/es-plugins/_data 这个目录中。 2）解压缩分词器安装包 下面我们需要把课前资料中的ik分词器解压缩，重命名为ik 3）上传到es容器的插件数据卷中 也就是/var/lib/docker/volumes/es-plugins/_data ： 4）重启容器 # 4、重启容器 docker restart es # 查看es日志 docker logs -f es 5）测试： IK分词器包含两种模式： ik_smart：最少切分 ik_max_word：最细切分 GET /_analyze &#123; \"analyzer\": \"ik_max_word\", \"text\": \"黑马程序员学习java太棒了\" &#125; 结果： &#123; \"tokens\" : [ &#123; \"token\" : \"黑马\", \"start_offset\" : 0, \"end_offset\" : 2, \"type\" : \"CN_WORD\", \"position\" : 0 &#125;, &#123; \"token\" : \"程序员\", \"start_offset\" : 2, \"end_offset\" : 5, \"type\" : \"CN_WORD\", \"position\" : 1 &#125;, &#123; \"token\" : \"程序\", \"start_offset\" : 2, \"end_offset\" : 4, \"type\" : \"CN_WORD\", \"position\" : 2 &#125;, &#123; \"token\" : \"员\", \"start_offset\" : 4, \"end_offset\" : 5, \"type\" : \"CN_CHAR\", \"position\" : 3 &#125;, &#123; \"token\" : \"学习\", \"start_offset\" : 5, \"end_offset\" : 7, \"type\" : \"CN_WORD\", \"position\" : 4 &#125;, &#123; \"token\" : \"java\", \"start_offset\" : 7, \"end_offset\" : 11, \"type\" : \"ENGLISH\", \"position\" : 5 &#125;, &#123; \"token\" : \"太棒了\", \"start_offset\" : 11, \"end_offset\" : 14, \"type\" : \"CN_WORD\", \"position\" : 6 &#125;, &#123; \"token\" : \"太棒\", \"start_offset\" : 11, \"end_offset\" : 13, \"type\" : \"CN_WORD\", \"position\" : 7 &#125;, &#123; \"token\" : \"了\", \"start_offset\" : 13, \"end_offset\" : 14, \"type\" : \"CN_CHAR\", \"position\" : 8 &#125; ] &#125; 3.3 扩展词词典随着互联网的发展，“造词运动”也越发的频繁。出现了很多新的词语，在原有的词汇列表中并不存在。比如：“奥力给”，“传智播客” 等。 所以我们的词汇也需要不断的更新，IK分词器提供了扩展词汇的功能。 1）打开IK分词器config目录： 2）在IKAnalyzer.cfg.xml配置文件内容添加： &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"> &lt;properties> &lt;comment>IK Analyzer 扩展配置&lt;/comment> &lt;!--用户可以在这里配置自己的扩展字典 *** 添加扩展词典--> &lt;entry key=\"ext_dict\">ext.dic&lt;/entry> &lt;/properties> 3）新建一个 ext.dic，可以参考config目录下复制一个配置文件进行修改 传智播客 奥力给 4）重启elasticsearch docker restart es # 查看 日志 docker logs -f elasticsearch 日志中已经成功加载ext.dic配置文件 5）测试效果： GET /_analyze &#123; \"analyzer\": \"ik_max_word\", \"text\": \"传智播客Java就业超过90%,奥力给！\" &#125; 注意当前文件的编码必须是 UTF-8 格式，严禁使用Windows记事本编辑 3.4 停用词词典在互联网项目中，在网络间传输的速度很快，所以很多语言是不允许在网络上传递的，如：关于宗教、政治等敏感词语，那么我们在搜索时也应该忽略当前词汇。 IK分词器也提供了强大的停用词功能，让我们在索引时就直接忽略当前的停用词汇表中的内容。 1）IKAnalyzer.cfg.xml配置文件内容添加： &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"> &lt;properties> &lt;comment>IK Analyzer 扩展配置&lt;/comment> &lt;!--用户可以在这里配置自己的扩展字典--> &lt;entry key=\"ext_dict\">ext.dic&lt;/entry> &lt;!--用户可以在这里配置自己的扩展停止词字典 *** 添加停用词词典--> &lt;entry key=\"ext_stopwords\">stopword.dic&lt;/entry> &lt;/properties> 3）在 stopword.dic 添加停用词 习大大 4）重启elasticsearch # 重启服务 docker restart elasticsearch docker restart kibana # 查看 日志 docker logs -f elasticsearch 日志中已经成功加载stopword.dic配置文件 5）测试效果： GET /_analyze &#123; \"analyzer\": \"ik_max_word\", \"text\": \"传智播客Java就业率超过95%,习大大都点赞,奥力给！\" &#125; 注意当前文件的编码必须是 UTF-8 格式，严禁使用Windows记事本编辑 4.部署es集群我们会在单机上利用docker容器运行多个es实例来模拟es集群。不过生产环境推荐大家每一台服务节点仅部署一个es的实例。 部署es集群可以直接使用docker-compose来完成，但这要求你的Linux虚拟机至少有4G的内存空间 4.1.创建es集群首先编写一个docker-compose.ym文件，内容如下： version: '2.2' services: es01: image: elasticsearch:7.12.1 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" volumes: - data01:/usr/share/elasticsearch/data ports: - 9201:9201 networks: - elastic es02: image: elasticsearch:7.12.1 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" volumes: - data02:/usr/share/elasticsearch/data ports: - 9202:9202 networks: - elastic es03: image: elasticsearch:7.12.1 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" volumes: - data03:/usr/share/elasticsearch/data networks: - elastic ports: - 9203:9203 volumes: data01: driver: local data02: driver: local data03: driver: local networks: elastic: driver: bridge es运行需要修改一些linux系统权限，修改/etc/sysctl.conf文件 vi /etc/sysctl.conf 添加下面的内容： vm.max_map_count=262144 然后执行命令，让配置生效： sysctl -p 通过docker-compose启动集群： docker-compose up -d 4.2.集群状态监控kibana可以监控es集群，不过新版本需要依赖es的x-pack 功能，配置比较复杂。 这里推荐使用cerebro来监控es集群状态，官方网址：https://github.com/lmenezes/cerebro 课前资料已经提供了安装包： 解压即可使用，非常方便。 解压好的目录如下： 进入对应的bin目录： 双击其中的cerebro.bat文件即可启动服务。 访问http://localhost:9000 即可进入管理界面： 输入你的elasticsearch的任意节点的地址和端口，点击connect即可： 绿色的条，代表集群处于绿色（健康状态）。 4.3.创建索引库 1）利用kibana的DevTools创建索引库 在DevTools中输入指令： PUT /itcast &#123; \"settings\": &#123; \"number_of_shards\": 3, // 分片数量 \"number_of_replicas\": 1 // 副本数量 &#125;, \"mappings\": &#123; \"properties\": &#123; // mapping映射定义 ... &#125; &#125; &#125; 2）利用cerebro创建索引库 利用cerebro还可以创建索引库： 填写索引库信息： 点击右下角的create按钮： 4.4.查看分片效果回到首页，即可查看索引库分片效果： 7.Docker安装Minio1.Docker 搜索镜像 docker search minio 2.拉取镜像 docker pull minio&#x2F;minio 3.Docker 启动Minio镜像 docker run -d \\ -p 9000:9000 \\ -p 9001:9001 \\ --name minio \\ -v &#x2F;home&#x2F;minio&#x2F;data:&#x2F;data \\ -e &quot;MINIO_ROOT_USER&#x3D;admin&quot; \\ -e &quot;MINIO_ROOT_PASSWORD&#x3D;adminadmin&quot; \\ minio&#x2F;minio:RELEASE.2023-03-24T21-41-23Z server --address &#39;:9000&#39; &#x2F;data --console-address &quot;:9001&quot; 解释： docker run :docker 启动容器命令 -d ：后台启动 -p ：端口映射 –name 为这个容器取一个名字 -e ：设置环境变量 -v :文件挂载 minio&#x2F;minio server &#x2F;data ： minio的启动命令 （minio&#x2F;minio 是镜像名字、 &#x2F;data:数据存储位置） 密码长度要求至少8位 使用docker-compose安装 1、安装 docker-compose 并授权 2、创建编排yml文件 指定控制台路径 --console-address &quot;127.0.0.1:9000&quot;指定API路径 --address &quot;127.0.0.1:9090&quot; version: '3' services: minio: image: minio/minio hostname: \"minio\" ports: - 9000:9000 # api 端口 - 9001:9001 # 控制台端口 environment: MINIO_ACCESS_KEY: admin #管理后台用户名 MINIO_SECRET_KEY: admin123 #管理后台密码，最小8个字符 volumes: - /docker/minio/data:/data #映射当前目录下的data目录至容器内/data目录 - /docker/minio/config:/root/.minio/ #映射配置目录 command: server --console-address ':9001' /data #指定容器中的目录 /data privileged: true restart: always 3、执行流程 在yml文件所在目录执行以下命令，等待执行完毕 created...done docker-compose up -d 4.登录控制台 控制台路径：http://ip:9001/ 安装memosdocker run -d --name memos -p 5230:5230 -v &#x2F;home&#x2F;ehzyil&#x2F;data&#x2F;docker_data&#x2F;memos&#x2F;:&#x2F;var&#x2F;opt&#x2F;memos ghcr.io&#x2F;usememos&#x2F;memos:latest version: &#39;3.8&#39; services: memos: image: neosmemo&#x2F;memos:stable container_name: memos ports: - &quot;5230:5230&quot; volumes: - &quot;&#x2F;data&#x2F;docker&#x2F;memos:&#x2F;var&#x2F;opt&#x2F;memos&quot; # 使用环境变量来获取当前目录 environment: MEMOS_DRIVER: postgres MEMOS_DSN: &#39;postgresql:&#x2F;&#x2F;memos:dbformemos@172.20.0.10:5432&#x2F;memos?sslmode&#x3D;disable&#39; 安装freshrssdocker run -d --restart unless-stopped --log-opt max-size&#x3D;10m \\ -p 39954:80 \\ -e CRON_MIN&#x3D;&#39;*&#x2F;60&#39; \\ -e TZ&#x3D;Asia&#x2F;Shanghai \\ -v $(pwd)&#x2F;freshrss&#x2F;data:&#x2F;var&#x2F;www&#x2F;FreshRSS&#x2F;data \\ -v $(pwd)&#x2F;freshrss&#x2F;extensions:&#x2F;var&#x2F;www&#x2F;FreshRSS&#x2F;extensions \\ --name freshrss-app \\ freshrss&#x2F;freshrss 注意在数据库配置时 用户名、密码、数据库分别对应之前 Docker Compose 配置文件中的 POSTGRES_USER、POSTGRES_PASSWORD、POSTGRES_DB；表前缀任意填；主机名要稍微注意一下，既非 127.0.0.1&#x2F;localhost，而要用容器的 IP，用下述命令可以得到。 # 获取 Container ID docker ps # 查看指定容器信息 docker inspect &lt;container id&gt; |grep IP # 例如 root@iZj6caytd8hmoddao18kslZ:~# docker inspect 70705536578b|grep IP &quot;LinkLocalIPv6Address&quot;: &quot;&quot;, &quot;LinkLocalIPv6PrefixLen&quot;: 0, &quot;SecondaryIPAddresses&quot;: null, &quot;SecondaryIPv6Addresses&quot;: null, &quot;GlobalIPv6Address&quot;: &quot;fd00:dead:beef:c0:0:242:ac12:3&quot;, &quot;GlobalIPv6PrefixLen&quot;: 80, &quot;IPAddress&quot;: &quot;172.18.0.3&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;fd00:dead:beef:c0::1&quot;, &quot;IPAMConfig&quot;: null, &quot;IPAddress&quot;: &quot;172.18.0.3&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;fd00:dead:beef:c0::1&quot;, &quot;GlobalIPv6Address&quot;: &quot;fd00:dead:beef:c0:0:242:ac12:3&quot;, &quot;GlobalIPv6PrefixLen&quot;: 80, 解决因为docker重启造成docker内部ip变动造成的无法连接 HTTP 500: Application problem Access to database is denied for ehzyil: SQLSTATE[08006] [7] connection to server at “172.18.0.3”, port 5432 failed: Connection refused 修改/data/freshrss/data/config.php下的host为postgresql的docker内部ip &#39;db&#39; &#x3D;&gt; array ( &#39;type&#39; &#x3D;&gt; &#39;pgsql&#39;, &#39;host&#39; &#x3D;&gt; &#39;172.18.0.3&#39;, &#39;user&#39; &#x3D;&gt; &#39;ehzyil&#39;, &#39;password&#39; &#x3D;&gt; &#39;postgresql&#39;, &#39;base&#39; &#x3D;&gt; &#39;freshrss&#39;, &#39;prefix&#39; &#x3D;&gt; &#39;&#39;, &#39;connection_uri_params&#39; &#x3D;&gt; &#39;&#39;, &#39;pdo_options&#39; &#x3D;&gt; array ( ), ), 安装青龙面板docker run -dit \\ -v $PWD&#x2F;ql&#x2F;config:&#x2F;ql&#x2F;config \\ -v $PWD&#x2F;ql&#x2F;log:&#x2F;ql&#x2F;log \\ -v $PWD&#x2F;ql&#x2F;db:&#x2F;ql&#x2F;db \\ -p 5700:5700 \\ --privileged&#x3D;true \\ --name qinglong \\ --hostname qinglong \\ --restart always \\ whyour&#x2F;qinglong:latest 如果忘记账号密码，也可以在控制台执行一下命令查看 docker exec -it qinglong cat &#x2F;ql&#x2F;config&#x2F;auth.json 将容器放在一个网络下 创建网络 docker network create --subnet=172.18.0.0/16 freshrss-network 若不指定子网会报Error response from daemon: user specified IP address is supported only when connecting to networks with user configured subnets异常。 将容器连接到网络 docker network connect freshrss-network freshrss-app docker network connect freshrss-network postgres_db 现在，freshrss-app 和 postgres_db 容器将位于同一个网络中。它们可以通过其容器名称或 IP 地址相互通信。 验证 要验证容器是否已连接到网络，可以使用以下命令： docker network inspect freshrss-network 输出应显示已连接到网络的容器列表，包括 freshrss-app 和 postgres_db。 其他提示 可以使用 docker network ls 命令列出所有可用的网络。 可以使用 docker network rm 命令删除网络。 有关 Docker 网络的更多信息，请参阅 Docker 文档：https://docs.docker.com/network/ 配置反向代理 使用nginx配置反向代理的前提： 创建一个A记录，将 qingl 指向的服务器的 IP 地址 将服务放在一个网络中并分配静态 IP，若nginx也是docker部署，需要将nginx也添加到同一网络内 # ql.conf server &#123; listen 80; listen [::]:80; server_name qingl.ehzyil.xyz; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;172.19.0.3:5700; proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 0; &#125; &#125; 如何使容器重启后ip不改变 freshrss-app和postgres_db已经在一个网络了 如何让 postgres_db在容器内的ip重启后不改变 方法：创建的 Docker 网络中为容器分配静态 IP。 1.首先，从网络中删除 postgres_db 容器。注意：您可能需要先停止容器，然后再将其从网络中删除。 docker network disconnect freshrss-network postgres_db 2.接下来，您需要检查网络以查找子网和网关来定义网络使用的 IP 范围。 docker network inspect freshrss-network 3.根据子网和网关信息，选择位于同一子网内但当前未被任何其他容器使用的 IP 地址。 &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.20.0.0&#x2F;16&quot;, &quot;Gateway&quot;: &quot;172.20.0.1&quot; &#125; ] &#125;, Subnet字段的值是子网的 CIDR 表示法，&quot;Gateway&quot; 字段的值是网关的 IP 地址。 因为子网字段的值（172.20.0.0/16）是子网的 CIDR 表示法，CIDR 表示法指定了 IP 地址范围和子网掩码。在这种情况下，子网掩码是 /16，这意味着前 16 位用于网络地址，后 16 位用于主机地址。 因此，网络使用的 IP 范围是从 172.20.0.0 到 172.20.255.255。 4.将 postgres_db 容器重新连接到具有特定 IP 的网络： docker network connect --ip &lt;static_ip> freshrss-network postgres_db #例如： docker network connect --ip 172.20.0.10 freshrss-network postgres_db 操作完成后可以看到该容器的静态ip docker inspect postgres_db |grep IP \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"GlobalIPv6Address\": \"fd00:dead:beef:c0:0:242:ac12:5\", \"GlobalIPv6PrefixLen\": 80, \"IPAddress\": \"172.18.0.5\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"fd00:dead:beef:c0::1\", \"IPAMConfig\": null, \"IPAddress\": \"172.18.0.5\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"fd00:dead:beef:c0::1\", \"GlobalIPv6Address\": \"fd00:dead:beef:c0:0:242:ac12:5\", \"GlobalIPv6PrefixLen\": 80, \"IPAMConfig\": &#123; \"IPv4Address\": \"172.20.0.10\" \"IPAddress\": \"172.20.0.10\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, 设置 容器重启后自动启动1.新建容器时配置 docker run --restart&#x3D;unless-stopped [OPTIONS] IMAGE [COMMAND] [ARG...] 2.对于已经存在的容器，您可以使用 docker update 命令更新其重启策略： docker update --restart&#x3D;unless-stopped [CONTAINER ID or NAME] 移除重启策略 docker update --restart&#x3D;unless-stopped [CONTAINER ID or NAME]","categories":[{"name":"Docker","slug":"Docker","permalink":"https://blog.ehzyil.xyz/categories/Docker/"}],"tags":[{"name":"软件","slug":"软件","permalink":"https://blog.ehzyil.xyz/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"Docker","slug":"Docker","permalink":"https://blog.ehzyil.xyz/tags/Docker/"}],"author":"ehzyil"},{"title":"Nacos安装指南","slug":"2023/Nacos安装指南","date":"2023-07-05T00:00:00.000Z","updated":"2024-06-17T01:04:53.979Z","comments":true,"path":"2023/07/05/2023/Nacos安装指南/","link":"","permalink":"https://blog.ehzyil.xyz/2023/07/05/2023/Nacos%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/","excerpt":"","text":"1.Windows安装开发阶段采用单机安装即可。 1.1.下载安装包在Nacos的GitHub页面，提供有下载链接，可以下载编译好的Nacos服务端或者源代码： GitHub主页：https://github.com/alibaba/nacos GitHub的Release下载页：https://github.com/alibaba/../../../images/nacos/releases 如图： 本课程采用1.4.1.版本的Nacos，课前资料已经准备了安装包： windows版本使用nacos-server-1.4.1.zip包即可。 1.2.解压将这个包解压到任意非中文目录下，如图： 目录说明： bin：启动脚本 conf：配置文件 1.3.端口配置Nacos的默认端口是8848，如果你电脑上的其它进程占用了8848端口，请先尝试关闭该进程。 如果无法关闭占用8848端口的进程，也可以进入nacos的conf目录，修改配置文件中的端口： 修改其中的内容： 1.4.启动启动非常简单，进入bin目录，结构如下： 然后执行命令即可： windows命令： startup.cmd -m standalone 执行后的效果如图： 1.5.访问在浏览器输入地址：http://127.0.0.1:8848/nacos即可： 默认的账号和密码都是nacos，进入后： 2.Linux安装Linux或者Mac安装方式与Windows类似。 2.1.安装JDKNacos依赖于JDK运行，索引Linux上也需要安装JDK才行。 上传jdk安装包： 上传到某个目录，例如：/usr/local/ 然后解压缩： tar -xvf jdk-8u144-linux-x64.tar.gz 然后重命名为java 配置环境变量： export JAVA_HOME=/usr/local/java export PATH=$PATH:$JAVA_HOME/bin 设置环境变量： source /etc/profile 2.2.上传安装包如图： 也可以直接使用课前资料中的tar.gz： 上传到Linux服务器的某个目录，例如/usr/local/src目录下： 2.3.解压命令解压缩安装包： tar -xvf nacos-server-1.4.1.tar.gz 然后删除安装包： rm -rf nacos-server-1.4.1.tar.gz 目录中最终样式： 目录内部： 2.4.端口配置与windows中类似 2.5.启动在..&#x2F;..&#x2F;..&#x2F;images&#x2F;nacos&#x2F;bin目录中，输入命令启动Nacos： sh startup.sh -m standalone 3.Nacos的依赖父工程： &lt;dependency> &lt;groupId>com.alibaba.cloud&lt;/groupId> &lt;artifactId>spring-cloud-alibaba-dependencies&lt;/artifactId> &lt;version>2.2.5.RELEASE&lt;/version> &lt;type>pom&lt;/type> &lt;scope>import&lt;/scope> &lt;/dependency> 客户端： &lt;!-- nacos客户端依赖包 --> &lt;dependency> &lt;groupId>com.alibaba.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId> &lt;/dependency>","categories":[{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.ehzyil.xyz/tags/Docker/"},{"name":"Nacos","slug":"Nacos","permalink":"https://blog.ehzyil.xyz/tags/Nacos/"}],"author":"ehzyil"},{"title":"[转载]Telegram（电报）：新手指南、使用教程及频道推荐","slug":"2023/Telegram：新手指南、使用教程及频道推荐","date":"2019-11-09T00:00:00.000Z","updated":"2022-11-28T00:00:00.000Z","comments":true,"path":"2019/11/09/2023/Telegram：新手指南、使用教程及频道推荐/","link":"","permalink":"https://blog.ehzyil.xyz/2019/11/09/2023/Telegram%EF%BC%9A%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97%E3%80%81%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E5%8F%8A%E9%A2%91%E9%81%93%E6%8E%A8%E8%8D%90/","excerpt":"Telegram 是迄今为止最棒的即时聊天软件，在这个自由新世界，不必自我审查（Freedom of speech）。","text":"Telegram 是迄今为止最棒的即时聊天软件，在这个自由新世界，不必自我审查（Freedom of speech）。 💡 全文有两万七千多字，善用右侧的目录栏和查找功能（Ctrl + F），助你快速定位想要看到的内容。你也可以移步到 Telegram 内阅读此文的 精简版。 🧱 TG 在中国大陆必须 翻墙 后才能使用。不过，学会科学上网，难道不是当代数字公民的必备技能吗？ 📁 tingtalk.me 在 2020-04-04 被墙了，如需在墙内传阅： 前往 GitHub 阅读 下载本文的 PDF 或 可编辑的 Markdown 源文档 ✈️ 电报介绍2013 年 5 月 20 日，斯诺登向《卫报》媒体透露棱镜计划（PRISM）： 我愿意牺牲掉这一切（工作、收入和女朋友）（把真相告诉世人），因为美国政府利用他们正在秘密建造的这一个庞大监视机器摧毁隐私、互联网自由和世界各地人们的基本自由的行为让他良心不安。by Edward Snowden 许多人第一次意识到 Ta 们的数字通信遭到了监视（The year Telegram was born was marked by the Snowden Revelations, when many people realized for the first time their digital communications were being watched.）。 2013 年 8 月 14 日，杜洛夫兄弟（Pavel Durov 和 Nikolai Durov）正式发布了开源（Open Source）的 Telegram（特指客户端）。这个充满理想主义的软件不接受外部投资（不需要向任何股东负责），也不会通过广告盈利，且挣钱永远不会是 Telegram 的终极目标（Making profits will never be an end-goal for Telegram），所以 Telegram 至今没有向第三方披露过一个字节的用户私人数据。Telegram 只会默默地践行一个理念：这个星球上的每个人都享有自由的权利（Everyone on the planet has a right to be free.）。This is the Telegram way： We believe that humans are inherently intelligent and benevolent beings that deserve to be trusted; trusted with freedom to share their thoughts, freedom to communicate privately, freedom to create tools. This philosophy defines everything we do. 我们相信人类天生就是聪明和仁慈的，值得信任的；坚信人类可以自由地分享想法，自由地私下交流，自由地创造工具。 这种哲学定义了我们所做的一切。by Pavel Durov 截止 2022 年 6 月 19 日，Telegram 已有 7 亿月活跃用户。 👍 近乎完美 **高度加密**：使用独有的网络传输协议 MTProto，无惧被黑客攻击。 **没有审查**：不用担心被封号，除非执法调查单位能证明用户是恐怖分子。 没有广告：一个纯碎极致的大众化即时通讯软件（Instant Messaging App）。 **不占内存**：聊天记录保存在云端（Cloud-Based），任何设备，无缝同步，随取随用。 超大群聊：封顶 20 万人，配合各种管理工具（例如限制发言间隔），让大型社群的交流也能井井有条。 **表情贴纸**：采用高清的 矢量 格式，并且支持自制表情包（Custom Sticker Sets）。 👎 瑕不掩瑜欢迎访问 Feature Suggestion Platform，向 Telegram 提交缺陷报告和功能建议（Bugs and Suggestions），一起帮扶 Telegram 做大做强。 🐼 中文搜索体验差毕竟是国外的软件，所以有一些水土不服也是可以理解的。以下方法可以助你更快地找到聊天记录和频道信息。 方法一：手动分词Telegram 的中文搜索是以「词组」为单位的，以标点符号或空格作为词的间隔。 辜鸿铭说过：真正的自由，并不意味着可以随心所欲，而是可以自由地做正确的事情。 在搜索框输入： 真正的自由 并不意味着可以随心所欲 而是可以自由地做正确的事情 都可搜到这条历史信息，因为标点符号充当了分词符。但是这些词未免太长了吧？！也记不住呀。不过别忘了 Telegram 分词规则，我们可以手动添加多余的标点符号（例如井号 ＃）或空格来分词： #辜鸿铭 说过：真正的 #自由，并不意味着可以随心所欲，而是可以自由地做正确的事情。 点击以下词组，或者在搜索框输入： #辜鸿铭 #自由 都可以找到这句话。建议频道主在发布内容的时候，用 # 打上相应的标签，方便订阅者找到想要的信息。 方法二：导出文件嫌手动分词太麻烦了？那就在电脑上导出 HTML 或 JSON 格式的聊天记录，想怎么搜就怎么搜。 不过，已经有人向 Telegram 提交了 改善中文搜索的提案，所以请为这个 Suggestion 投票和鼓气。拜托了。 关联阅读 为什么 Telegram 不能搜索中文讯息 - 翁君牧 Telegram 中文搜索方案探索 - Newlearner 🙃 硬币的另一面在加密通信「庇护」下的土壤，滋生了臭名昭著的网络性犯罪案件：N 号房事件。所幸的是，2020 年 11 月 26 日，主犯赵主彬一审判处监禁 40 年。 以及存在大量的 NSFW 内容、币圈广告和灰色产业链。科技能否向善，在于我们能否约束自己内心的 邪念。 📱 只能用手机号码注册 国外：手机号不用实名制，相对安全。 国内：通过 Google Voice 注册电报，并且可以绕开私聊限制。 而且，添加陌生人到通讯录（Add to contacts），记得每次都要取消勾选 Share my phone number with ***。Telegram 在这个方面上没有记住用户习惯，不应该呀，我去反馈反馈。 另外，如果你的手机号码，被某人保存在 Ta 的手机通讯录，Ta 也开放了通讯录给 Telegram。当你用这个手机号码注册 Telegram 后，Ta 就会第一时间知道你也加入 Telegram 了。所以对隐私有要求，请使用无需实名的虚拟号码或者单独小号来注册。 🗓️ 逃离微信我与 Telegram 的结缘，始于 2016 年春。那时我刚学会了科学上网，并下载了 Telegram 这个 Instant Messaging app。虽然我在电报上找不到朋友和我聊天，但我并没有卸载 Telegram，因为相比输入法和微信，Telegram 提供了丰富的 Emoji 供我选择。如此一来，我在 Android 手机上也能使用最新的 Emoji 点缀朋友圈。 近年来，天下苦微信久矣。 WeChat 掌门人可能是一个无马（码）不欢的 骑兵：每次登录微信电脑版都要扫码时，我的心中都会万马奔腾。 只能发送小于 100 MB 的文件；一次只能发送 9 张图片，而且每次发送都要勾选原图（Telegram 会记住这个用户习惯），下载原图文件名会改变。 占用巨大的存储空间，备份与同步的体验特别差，也不能换成欧盟号码导出用户数据。 注册微信的时候，用户会默认同意 腾讯微信软件许可及服务协议 ，其中在 7.1.2 提到一个霸王条款：「微信帐号的所有权归腾讯公司所有，用户完成申请注册手续后，仅获得微信帐号的使用权，且该使用权仅属于初始申请注册人。……」。 用户免费使用微信（无所有权），微信收集用户的私人数据，贩卖给广告商，这无可厚非。但当用户想要取回 Ta 所创造的内容（数字资产）时，例如导出微信个人数据（朋友圈数据和收藏功能数据等），只好借助欧盟的 GDPR（通用数据保护条例）行使数据可携权。于是我花了 5.26 USD（含税）买了一个比利时的手机号码，微信却猖言道：「由于当地法规限制，WeChat 暂不支援中国大陆用户将绑定的手机号码更换为欧盟手机号码。」 请问是哪条「当地法规」？这不是「法制」，而是「Fuck 制」：强奸一个个没有反抗能力的用户！2020-04-22 原谅我「口吐芬芳」，不懂中国特色。另外，随着言论审查力度的加大，任何「风吹草动」都要「斩草除根」： Cyberspace Administration of China（网信办）会因为我转发一则关于 中华人民共和国宪法修正案 的 Twitter 评论到 24 人的微信同学群，几小时后，请我到最近的派出所「喝茶」，那是我第一次坐警车。 微信上无法发送某些链接（例如纽约时报的文章），也不显示发送失败，让你以为发出去了，其实只对你可见。 微信会秋后算账，删除我之前写的一篇介绍 搜索技巧 的公众号文章，接着 博客 在中国大陆地区被墙，公众号 被永久封号。虽然公众号写了 4 年只有五百多个关注者，但每一个订阅者都来之不易。就这样，我跟读者们都失联了。 经过一系列发生我身上的 屏蔽事件 后，让我意识到中文互联网已经完全沦陷了，于是逃难到 Telegram。经过一段时间的使用后，我彻底成为了 Telegram 的 自来水，见人就夸。然而身边的亲朋好友却不为所动，不愿跟我一起 数字移民 到这个可以安心说话的地方，甘愿做「温室里的花朵」和「笼的传人」，享受表面上的岁月静好。我不会责怪 Ta 们，如果你看过《肖申克的救赎》，就能理解这种「放弃抗争」的心态。 刚入狱的时候，你痛恨周围的高墙，慢慢地，你习惯了生活在其中，最终你发现，自己不得不依靠它而生存。by The Shawshank Redemption 既然说不动认识我的人，但世界那么大，人口那么多，网络世界上一定有一群人，Ta 们和我一样，反感「温室园丁」不透明的做法，不愿做一只数字农场里的电子绵羊，相信「自由价更高」。 我要当一个 踹车轮 的人，为我心中的理想世界投票。因此我熬了几个月，把 Telegram 官网的 FAQ 和 Blog 全部看完了（从 2013 年创立电报至今），结合 Google 搜索引擎旁征博引，整理出这篇可能是中文互联网内容最翔实，排版最精美的《电报指南》，目的就是尽可能地为读者呈现 Telegram 的强大、私密以及友好的用户体验。 在 Durov’s Chat 用蹩脚的中式英语给教程做推广，受到 Pavel 的肯定。 2016 年国庆，我花了一周时间看完了「即刻 app」的所有主题（圈子），写了一篇三千多字文章：《即刻 App - 不再错过你感兴趣的资讯》(图文版 | 文字版）。即刻已经没有复活的可能了（即刻 App 居然在 2020 年 6 月 10 日回来了，但是缺失了话题追踪功能），Telegram 顺势成了新的资讯中心。 人生苦短，逃离微信（Escape from the WeChat）。stay-away-from-wechat 项目收集微信的反人性设计、无理审查行为、侵犯用户隐私、监控聊天记录、试图控制人民生活相关信息，期望用户认识到微信的弊病，倡导用户用脚投票、拒绝使用微信 👎️。欢迎提交 Issues 和 Pull requests 🤖️。 欢迎读者们转移到没有监控和审查的地方 ，一起在这片乐土上过上没羞没臊体面的数字生活。 🌟 注册使用接下来，就正式进入主题，教读者朋友们电报怎么使用。 👇 下载登录请进入 Telegram Apps 的官方下载页面，选择对应的平台，下载，安装，注册。 自由开放的 Telegram 在各平台都有数十种客户端，各有哪些优缺点，又该如何选择呢？请查阅 Telegram 客户端版本比较，但我不推荐使用第三方电报客户端，安全没有保证。 如果你有注册或登录问题 先从 常见登录问题 中寻找方法，无果，联系 Telegram： 途径一：请使用此 表格 与 Telegram 联系 途径二：在 Twitter 上联系 Telegram Login Help 不知何种原因遭到封禁，请写邮件给 &#114;&#x65;&#99;&#x6f;&#118;&#101;&#x72;&#64;&#116;&#101;&#x6c;&#101;&#x67;&#114;&#97;&#109;&#x2e;&#x6f;&#x72;&#103; 用英文写 Email 用 国际电话号码格式 书写被封的手机号码（中国的国家代码为 +86，美国的 +1） 真人值班，大概 24 小内就会被解封。但违反 Telegram 服务条款（Terms of Service），例如乱发广告，是不会被解封的。 参考资料： 记一次成功的 Telegram 账号解封 by @askahh Telegram 官方封禁的账号会受到什么处罚 by @TGgeek 如何在电脑上使用 Telegram for Desktop 打开客户端 点击右上角的 SETTINGS（设置）&gt; Connection type（连接类型）&gt; Use custom proxy（使用自定义代理）&gt; ADD PROXY（添加代理），以 Shadowsocks(R) 为例： SOCKS Hostname: 127.0.0.1 Port: 1080（不同的翻墙客户端，端口略有不同） 使用 Clash 翻墙的用户，可跳过这一步，选择 Use system proxy settings（使用系统代理设置）。 🐼 汉化界面既然已经出来混了（突破网络墙），首选使用英文版的 Telegram（突破语言墙），好像加起来也没几个单词。要是一点英文底子都没有： 点此安装官方简体中文语言包 选择 CHANGE（更改） 即可把界面语言替换为简体中文 截至 2020 年 11 月 01 日，Telegram for Android 翻译已完成 95% 了，但不妨碍日常使用。 🔒 账号设置 支持视频头像（Profile Videos）。 支持登录多个账号。手机端长按另一个账号，不切换账号，也能快速预览消息。Multiple accounts: preview chat list. You can press and hold on another connected account in Settings for a sneak peek of its chats list. 👤 设置用户名你可以在 Setting（设置）里面填写一个 Username（用户名）。设置后，别人能够在不知道你的电话号码的情况下，通过搜索用户名找到你。 用户名可以随时更改或删除（用户名为空）。 用户名不区分大小写（TingTalk &#x3D; tingtalk），但 Telegram 会记住大小写偏好。 侵权 如果一个骗子假装是你，联系 @notoscam。 如何举报假冒的频道或群组：点击右上角的 ... &gt; Report &gt; Fake Account 针对品牌方，用户名被占用怎么办？把你在 Facebook、Twitter 或 Instagram 的用户名（两个平台以上）发给 @username_bot。 如果你看到表情包（sticker sets）、频道（channels）或机器人（bots）侵犯了你的版权，请提交投诉到 &#100;&#x6d;&#x63;&#97;&#64;&#116;&#x65;&#x6c;&#101;&#103;&#x72;&#97;&#109;&#x2e;&#x6f;&#x72;&#x67; 联系人 添加和删除联系人（Contacts）都是单向操作，双方的通讯录都是独立的（你中有我，我中可能没有你），也没有通知。 （单向）添加联系人之后，加入共同小组后，通讯录好友会排在小组名单前面。如果你想加入一些不想让别人知道的群组，还是用小号吧。 点击左侧菜单栏 &gt; Contacts（联系人）&gt; Find People Nearby（寻找附近的人）。你也可以创建一个基于本地的群聊。 🔓 解除私聊限制自由的土壤吸引了比特币、社工库、NSFW 等灰色产业到电报野蛮生长，因为这些国产老鼠屎对电报的滥用，导致使用中国大陆的手机号码（+86）注册 Telegram 后，私聊 Ta 人时，可能会提示 Sorry, you can only send messages to mutual contacts at the momet.（对不起，你现在只能发送私信给双向联系人。），这表明此账号被判定为 Spam（垃圾信息）账号了。 如何解除私聊限制：在 Telegram 搜索 @SpamBot，点击 START，然后依次点击底部出现的菜单或回复以下话术（仅供参考）： But I can’t message non-contacts. No, I’ll never do any of this. I can’t chat with non-contacts. Accident. 大概半小时之后（有些人要十几天），即可解除禁言。 另外，若用户在 24 小时内访问超过 200 个群组或频道的链接（点击打开就算访问，不需要加入），就会被打入冷宫 24 小时。禁闭期间，无法通过链接访问新的群组或频道（点击链接一直转圈而无法访问）。 ⛑️ 隐私和安全依次点击 Setting（设置）&gt; Privacy and Security（隐私和安全） 🙈 隐藏手机号码Phone number（手机号码）&gt; Who can see my phone number（谁可以看到我的手机号码：Nobody 不允许任何人） 对隐私有要求，或者彻底解除 +86 开头的手机号码的私聊限制，可以把手机号码换绑到非中国区的手机号码，例如 Google Voice： 注册 Google 账号。 在 Google 或淘宝上搜索关键词 Google Voice 或 GV ，购买别人注册下来的号码。 转移到自己的 Google 账号上。 以及 🧨 再次强调：添加陌生人到通讯录（Add to contacts），记得每次都要取消勾选 Share my phone number with ***。 开启 SIM 卡的密码，纵使别人捡到你的手机（卡），没有 PIN 码就不能使用你的手机号码，也就不能收到登录验证码，以此来登录 Telegram。 🎭 匿名转发Forwarded messages: Nobody（引用转发来源：不允许任何人） 启用此设置后，转发你的消息将无法指向（链接）回你的帐户，只会在 From ***（来自***）字段中显示一个无法点击的昵称（非用户名）。而昵称不是唯一的，所以通过这种方式，将没有证据证明某条消息是你发送的（无法溯源）。 📍 隐藏通话 IP 地址Calls &gt; Peer-to-Peer：Nobody Telegram 为了提高语音通话的质量，默认采用端对端连接（Peer-to-Peer）。由于流量没有经过 Telegram 服务器中转，所以会暴露用户的 IP 地址。但是禁用端对端通话后，通话质量会略有下降。 另外，配合使用 Tor（The Onion Router、洋葱路由器）可以隐藏用户真实 IP 地址、避免网络监控及流量分析。 🔐 本机锁定码Passcode Lock 相当于给 Telegram 加上应用锁。这样一来，临时借用你设备的人也看不到你的小秘密。 设置完密码锁之后，可以在下方的自动锁（Auto-lock）设定时长，一旦超过时长未操作，那么 Telegram 将自动上锁。或者在聊天列表页面上主动点击锁头图标，Telegram 就会立即锁定应用，新消息通知将不包括文本或发件人姓名。再次进入应用时，要求输入锁定码。 在 Android 客户端上，还可以关闭「在任务切换页面显示内容」（Show App Content in Task Switcher），同时在 Telegram 内也无法截屏。 密码锁只在当前设备可用，不会同步到云端或其他设备，所以在不同的设备上可以设置不同的密码锁。如果忘记，只能重装 App，而且重新登录之后，需要重新设置新的密码锁，私密聊天（Secret Chat）也不会同步回来。 ✌️ 两步验证Two-step verification（两步验证：添加密码提示和 ❗️ 安全邮箱 ❗️） 以后登录时，输入验证码后，还要输入密码。 安全密码：请勿使用纯数字密码，可使用开源的 KeePass 生成高强度密码。 电子邮箱：忘记密码，可以通过 Email 找回密码。尽量选择国外邮箱服务，例如 Gmail 或者 Outlook 💥 删除账户Delete my account if away for 1 month/3 months/6 months/1 year （删除我的帐户若离线时间达 1 个月 &#x2F; 3 个月 &#x2F; 6 个月 &#x2F; 1 年） 自动删除：以上就是电报自带账户自毁机制（Account Self-Destruction） 主动删除：不想使用此账号，可 永久删除账户（Delete Account） 为什么要给账户设置自毁机制 Telegram 作为一个免费的非商业软件，没有任何收入来源，为了节约服务器的存储空间，Telegram 会自动删除长时间不上线的用户。再说了，Telegram 也不需要你的私人数据。 如果不慎丢失了 SIM（手机卡），此前未开启 SIM 卡的 PIN 码（强烈建议开启）和 Telegram 账户的两步验证，新的「主人」就能把你的 Telegram 账号占为己有。但是假如你设置了 1 个月不上线就销户，坏人在第 32 天捡到你的手机，不过此时你的 Telegram 账号已经不存在了。 🛡️ 其它隐私设置 使用没有个人特征的头像（记得在个人资料里删除历史头像）、昵称和用户名，确保硅基身份不会和碳基身份产生关联。 不使用 Touch ID 或 Face ID 解锁你的手机，只以密码锁屏（数字 + 字母），以免被「执法人员」控制身体后强行解锁。 💬 对话界面Telegram 有一个非常人性化的特性：记忆浏览进度，打开对话界面会自动跳转到未读消息 Unread Messages（The app restores your previous scroll position when you switch back to a chat）或者上次的未读位置。纵使重新安装 Telegram，没看完的消息，状态依旧是未读的。 ✏️ 消息发送前打上句号之前，检查一遍内容是否有误，虽然 Telegram 支持无限期的（撤回）修改。 ↩️ 引用消息手机 左滑（Swipe left）Reply 消息。 长按，在弹出的界面中选择 Reply。 电脑 左键双击消息的空白处，例如在时间附近。 左键单击对话框的右上角的 Reply。 右击消息，在弹出的菜单中选择 Reply。 点击引用的消息，就会向上滚动到原始消息（If you tap on the quote, the app scrolls up to the original message）。 假设从 Unread Messages 开始浏览动态（已发布 120 条 Post），遇到新消息引用了旧消息，例如庭说频道的第 100 条消息 https://t.me/tingtalk/100 引用第 56 条消息 https://t.me/tingtalk/56，点击引用的消息，即可定位到第 56 条消息。如何回到第 100 条消息，点击右下角的 🔽 就会回到第 100 条消息，而不是回到最新的消息（shows an arrow button to go back to the previous location. This makes navigating conversations in groups easy even if you’ve been away for a while）。这是一个非常动人的细节，深深地被 Telegram 折服。 电脑右击 &#x2F; 手机点按被引用消息（右下角有个 ↶），在弹出的菜单里选择 View * Reply，就能展开所有对此话题的讨论（回复）。 📝 文本格式化学会插入超文本链接，避免冗长的 URL 霸屏（简短的网址例外），是一种网络美德。 Markdown 语法官方客户端只支持以下Markdown 语法： 加粗（前后加入两个星号）：**bold** 删除线（前后加入两个波浪号）：~~strikethrough~~ 等宽字体（前后加入一个重音符）：`monospace` 斜体（前后加入两个下划线）：__italic__（原生 Markdown 语法是前后一个星号） 不支持使用 Markdown 语法 Create link，虽然可以通过快捷键 Ctrl + K 插入超链接，但略显麻烦。如果你是 Windows 10 用户和 Markdown 爱好者，我想到了一个优雅的写作方法。 @sudo_radio 提供信息：Telegram X for Android 也支持超链接语法。 公开版：最后一次更新是 2020 年 5 月 15 日 测试版：申请入口 | APKMirror 准备工作 下载 Unigram（下文 有安装教程） 下载 Typora 或其它 Markdown 编辑器 添加浏览器插件 油猴，并安装这个 脚本：快速复制 Markdown 格式（[标题](网址)）的超链接到剪贴板 写作流程 打开 Typora，开始写作 通过脚本快速插入超链接 写好后复制 Markdown 源代码，打开 Unigram 并粘贴 打上一些 #标签（Hashtags）给消息分类，方便读者检索 点击发送，Markdown 语法即可被渲染，包括超链接 这样写长文，一气呵成，Awesome！ 得益于 Telegram 账号可以登录很多个设备，用完 Unigram 和 Telegram X for Android 后，可切换回官方客户端。 关联阅读：文案风格指南 by 庭说 Desktop（桌面电脑端） 在编辑区输入文本。 左键选择想要格式化的文本。 右击 &gt; Formatting（格式选项）。 Formatting Shortcut for Windows Bold（加粗 ） Ctrl + B Italic（斜体） Ctrl + I Underline（下划线） Ctrl + U Strikethrough（删除线） Ctrl + Shift + X Monospace（等宽字体） Ctrl + Shift + M Create link（超链接） Ctrl + K Plain text（纯文本） Ctrl + Shift + N 重磅推荐 Ctrl + K！简单几步，让排版清清爽爽。 Android（安卓） 在编辑区输入文本。 长按选择想要格式化的文本。 原生 Android 系统会直接弹出格式化选项；魔改安卓系统（例如 MIUI）需要轻触界面右上角的三个点（在顶栏右侧，大概在通知栏电量 🔋 的下方。非常刁钻的一个位置），才能看到文本格式化选项。 iOS（iPhone &amp; iPad） 在编辑区输入文本。 长按选择想要格式化的文本，会弹出一些文字操作的选项。 轻触 BIU（或许藏在 ▶️ 后面），即可看到文本格式化选项。 如果觉得以上格式化文本的方法太麻烦，打乱了输入节奏，可在任意聊天框输入 @bold，接着使用 Markdown 编辑消息（最多输入 256 字符），最后选择 Custom markdown。 #️⃣ 主题标签任何以 # 开头的词组，以标点符号或空格结尾的词组（hashtags）都可以被点击搜索，也相当于用标签给消息分组。 康德说过：#自由 不是让你想做什么就做什么，自由是教你不想做什么，就可以不做什么。 消息发出后，#自由 就会变成一个可点击搜索的状态。 🌅 发送原图先选择想要发送的图片（不止于 9 张）： Android：点击弹出窗口右上角的三个点，Send without compression iOS、macOS 和 Windows：Send as a file 请注意： 原图不会压缩图片，但是会暴露文件名、隐藏的 GPS 信息和拍摄信息，慎重发送。 Telegram 会记住你的操作习惯，下次发送图片时不必再次勾选原图选项。 🎬 发送视频支持时间戳（Timestamp）：发送本地视频或 YouTube 视频时，在 Add a caption（添加标题）里标记你最喜欢的时刻（mark your favorite moments），例如： 建议直接跳到 05:06 开始欣赏，有惊喜。 05:06 会自动高亮显示，点击 05:06，视频就会从第 5 分 6 秒播放。其中 05:06 必填项，提示的话可以选填。 引用回复本地视频或 YouTube 视频也支持加入时间戳（Timestamps in replies and captions open videos and YouTube links to that exact moment.）。 你发现了吗？YouTube 和哔哩哔哩的评论区也是支持这种时间戳。 不支持时间戳的软件和网页，怎么办 YouTube：https://www.youtube.com/watch?v=SyM3jMFjess&amp;t=05m06s 右键点击进度条 在弹出的菜单中选择 复制当前时间的视频网址 哔哩哔哩：https://www.bilibili.com/video/av55857100?t=14m15s 不支持复制当前时间的视频网址，需要手动填写 参数说明 ?t= &#x2F; &amp;t= 时间 time h 时 hour m 分 minute s 秒 second 发送视频时，可选择压缩等级。Change the resolution of a video from the editor’s quality slider. 内置视频播放器（in-app media player）：直接在 app 内观看 YouTube 或 Vimeo 视频，不必跳转到浏览器或者相应的视频 app。操作逻辑与国外视频 App 保持一致：双击左侧快退，双击右侧快进。 制作 GIF 动图在发送视频时，点击视频打开编辑窗口，使其静音（tap the mute audio button），新的 GIF 就诞生了，还会自动保存在最近使用的 GIF 里（recent GIFs tab）。 📡 消息发送时手机上长按（电脑上右击）消息发送键： Send without sound（静音发送）：纵使对方在睡觉，你的 urgent idea 也不会搅人春梦，简直就是为健忘的人而设计。 Scheduled Message（定时发送） 发送日程消息时，对方是不知道你使用了定时发送。 在 Saved Messages（我的收藏）也可以发送定时消息作为提醒（Set a reminder）。 Send when * comes online（当对方上线时发送）：这样就可以排在对方聊天列表的前面（Put you right at the top of their chat list.）。此功能需要对方在隐私设置里开启展示最后上线时间（This option only appears for users who share their Last Seen status with you, and vice versa.） 接收者可屏蔽联系人 &#x2F; 群组 &#x2F; 频道的消息通知（Mute Notifications）： 1 个小时 4 个小时 18 个小时 3 天 永久静音 🔙 消息发送后点击消息，选择 Pin，即可在频道、群组或私聊界面中置顶多个消息（Multiple Pinned Messages）。在群组中置顶消息时，可强制通知全员（notify all members），即使成员的群组已经设置为静音。 ✔️ 消息状态消息的读取状态（回执）分为两种 One check（✔️）：发送成功。在微信，有些消息没发送出去，只对你可见，也不会有发送失败的感叹号 ❗。这是对用户赤裸裸的欺骗 。 Two checks（✔️✔️）：消息已阅。瞥见状态栏弹出来的消息，不会产生已读标记。因此，一直显示单勾，不代表对方没看到信息。 一个偷看消息的小技巧：在对话列表，长按头像可预览消息，但消息状态不会变为已读。（Pull up a preview of messages – without making messages as read. ） ✏️ 消息更正在 Telegram，说出去的话不会像泼出去的水收不回来，在 48 小时内（频道是无限期修改），你都可以重新编辑（Edit your messages after posting），包括文字、图片和视频（Edit sent media to re-crop, re-decorate or completely replace photos and videos.），所以： 文字出现 typos，不用删除，多久之前发的消息都能随时更正（Edit）。 图片忘记打马赛克，但因为有图片说明（配文），懒得撤回重输，可以当场抹除敏感信息，当场换图片（Replace Media）。 视频发错了，善后方式与图片同理。 图片换视频，视频换图片，Why not? 如何替换图片或视频？长按或右击消息，选择 Edit： 通用法：点击笔头图标 ✏️（或 Replace file&#x2F;photo），弹出资源窗口，选择正确的图片和视频即可替换。 桌面端：复制正确的图片和视频，回到 Telegram，粘贴即可替换。 并且支持直接标记别人发来的图片，修改完再发出去，无需保存在本地图库。Instantly edit and send back media you receive to add notations or decorations without saving it to your gallery. 在电脑端，把鼠标放在 edited 上，会显示最后修改消息的时间。 👇 长按消息 消息可以无限期撤回（Delete Messages）：删除信息时，勾选 Also delete for ***，聊天记录就可以双向删除，通话记录也支持这个特性（Call history can also be deleted for all sides at any time）。电报服务器更不会存储被删除的聊天记录和通话记录，因此数据将彻底永远消失。 选择部分消息（Select Parts of Messages）：长按 2 次消息，可选择部分文字，而不是复制全文（Copy Selected Text）。 长按网址或长串数字可以选择打开（Open）或者复制（Copy）。 转发消息时，长按联系人 &#x2F; 群组 &#x2F; 频道可多选。 🗣 朗读消息Announce Messages 目前由 iOS 用户独占。你可以让 Siri 在你的耳机里大声读出你收到的信息，即使是在洗碗的时候也可以保持聊天的最新状态。 开启路径：iOS Settings &gt; Notifications &gt; Announce Messages。 ⏳ 自动删除 自动删除只适用于定时器设置后发送的消息，以前的消息将保留在聊天记录中。 倒计时在消息发送时开始计时；而秘密聊天是阅后即焚，是从已读后开始计时。 🗣️ 语音消息 支持 2 倍速播放（2X playback）。 支持滑动进度条。（Slide forward and back on voice messages to skip ahead or repeat something you missed.） 记忆播放位置：超过 20 分钟的音频文件（2021 年 3 月 18 日取消此限制），Telegram 会帮你记住最后的播放位置，以便中断后再次收听（Telegram apps will remember your last position when resuming playback of audio files longer than 20 minutes.）。 此外，在 Telegram 上进行语音通话（打电话），需要在翻墙服务端&#x2F;客户端开启 UDP 转发。 📲 视频通话发起视频通话和音频通话后，如果屏幕上的 4 个 Emoji 一致，表示此连接已采用端到端加密，100% 安全。 Video Calls: All voice and video calls are protected with end-to-end encryption. To confirm your connection, compare the four emoji shown on screen. If they match with your partner’s, your call is 100% secure. 🙈 生动表情Emoji（绘文字）按关键字搜索表情（Search emoji by keyword）：在消息框输入关键词，就会弹出相关的 Emoji。 能触发 Emoji 的英文关键词合集 能触发 Emoji 的简体中文关键词合集 部分 Emoji 支持动态播放（Animated Emoji）在任意聊天窗口发送 1 个 非礼勿视猿 🙈（See-No-Evil Monkey），再动 Ta 试试，可爱吧！查看更多被 Telegram 赋予「生命」的动态 Emoji，请参阅 Telegram Animated Emoji List。 以下表情符号可以作为打赌小游戏（Emoji Game） 发送单个 触发效果 🎲 掷骰子 dice 🎯 扔飞镖 darts 🏀 投篮 basketball ⚽ 射门 football 🎳 保龄球 bowling 🎰 老虎机 jackpot &#x2F; slot machine 如何在句中（mid-message）快捷添加 Emoji？语法是 :（英文半角冒号） + 关键词。例如输入 I am :happy，就会弹出开心相关的 Emoji，这样就不用从 Emoji 面板挑选 Emoji 了。 Stickers（表情包） 截至 2021 年 1 月 13 日，Telegram 上已有 20,000+ 免费的高清表情包。 在聊天窗口输入 @sticker + Emoji，可以检索所有与 Emoji 相关表情包，例如 @sticker 👍。我非常喜欢这个表情包建议功能（Suggest stickers by emoji），经常能找到一些很有创意高清表情包，给聊天体验增色不少。 在哪里找表情包 官方： 打开 Telegram 的一个对话界面，输入框选择 Sticker（旁边是选择 Emoji 和 GIF） 往下拉，即可在顶部看到 Search sticker sets（只支持用英文关键词搜索） 网站： Stickers Cloud tlgrm：只支持用英文关键词搜索 群组：Stickers Cloud 频道：Trending Stickers 📤 如何导出电报上的表情包 选择一个 Sticker to GIF Converter，例如 @tgstogifbot 或 @Sticker2GIFBot（后一个 Bot 可下载整套表情包） 发送 Stickers，Bots 就会把 Telegram 上 tgs 格式的表情包转换为 gif 格式 🗜️ 在限制多多的微信 App 上，小于 1 MB 的 GIF 图片才会自动播放。如何压缩： 打开 图贴式（网站），选择 GIF 压缩 宽度设置为 240，压缩质量 70（默认） 选择或拖拽一个或多个 GIF 到压缩窗口，开始压缩 压缩完成后，（推荐使用 IDM）打包下载 此时某些表情包可能大于 1 MB，需要再压一次： 方法一：修改 图贴式 的压缩质量等级（压得太狠会失真） 方法二：使用 docsmall（网站）或者 图压（软件）二次压缩 为什么不用 Photoshop 压缩 GIF？因为会产生毛糙的白边。 两外，推荐一个可以批量修改图片尺寸的网站：iLoveIMG 📊 投票功能只支持在群组和频道中发起，因为 they feel lonely in one-on-one chats. 发起人 支持：匿名投票（Anonymous Voting）、多选（Multiple Answers）、答题模式（ Quiz Mode）。 不支持：修改发出的 Poll。 投票者 &#x2F; 答题者 不满意长按或右击投票（Poll）可以撤回投票（Retract vote）。 缺点 Telegram 内置投票可被任意用户转发至其他对话内（如群组）进行投票，这对于公开性的投票来说是增加统计数据量的好方式，但该特性对于有私密&#x2F;非公开需求的投票来说实为不利。（截止 2021 年 5 月 28 日 by TGgeek） 改善方法：通过 @vote 创建投票。 📖 通用技巧🌐 互联开放公开（Public）的频道或群组，是可以被搜索引擎抓取的（The contents of public channels can be seen on the Web without a Telegram account and are indexed by search engines.），并且不注册 Telegram 账号也看到公开频道或群组中的内容，方法就是在 Public link（公开链接）中加一个 s，例如在浏览器的地址栏输入 t.me&#x2F;s&#x2F;tingtalk，即可查阅庭说频道的所有内容。 一个用户最多可创建 10 个公开用户名（public usernames），包括公开的频道和群组。 🔍 全局搜索Search Filters: To quickly find a specific message or media item, search filters allow users to refine results by keyword, source, media type and time period – all at once. 这里指电报内的全局搜索。 隐藏技巧：如何按日期搜索？ 打开在 Telegram 移动端首页 点击搜索框 输入日期，即可按照日期筛选历史消息 2021：2021 年 01.2021 &#x2F; Jan 2021：2021 年 1 月 01.13.2021：2021 年 1 月 13 日 📅 创建日期在任意对话窗口（例如 Saved Messages）输入 https://t.me &#x2F; ID &#x2F; 1，例如 https://t.me/tingtalk/1 或者在浏览器的地址栏输入 https://t.me &#x2F; s &#x2F; ID &#x2F; 1，例如 https://t.me/s/tingtalk/1 就会跳转到该群组或频道（未删除的）第一条消息，在其上方，可以看到创建日期（Channel created） ☁️ 多端同步 Telegram 可以在多个设备上同时使用。以下是我的设备列表： 2 台 Windows 电脑（开机自启） 1 部 Android 手机 1 部 iPhone 手机 1 个 网页端 …… 并且具备以下优势： 登录过的设备，下次登录时，不必再次扫描二维码或者输入密码。 云草稿（Cloud drafts）：除了消息可在各个平台同步之外，连未完成编辑的消息（草稿）都可以跨设备同步。Now you can start typing on your phone, then continue on your computer – right where you left off. 但是长文本还是不要放在草稿箱了，就怕 Bug 爬上来。如果草稿丢失，在桌面端按 Ctrl + Z 试试。 与 WhatsApp 不同的是，手机下线 Telegram 后，其他设备的 Telegram 并不会退出。 允许传送最大 2000 MiB 的文件，简直就是绝佳的「文件传输助手」： 把 Saved Messages（收藏夹）当作是 GTD 中 Inbox。并且每条保存的消息都有一个 ▶️ 按钮，可以将你带到最初发布消息的位置。 建立多个私人频道（无数量限制），分类存放你的信息和资讯。你甚至可以在 Telegram 上传本地音乐或者录音到自己的频道，建立自己的云端音乐播放库和播客（Podcast）。Create playlists by sending multiple songs at the same time. 🖥️ 电脑版技巧Windows 的 Ctrl 等于 macOS 中 Command ⌘。 快速多选：在对话界面的空白位置，按住鼠标左键不放，然后推拽多选信息，接着即可转发或者删除。 链接直达：按住 Ctrl 再点击 URL，直接打开链接，不必弹窗确认（Open this link? CANCEL &#x2F; OPEN）。 缩放图片：按住 Ctrl 再旋转鼠标的滚轮，即可放大或缩小图片。 快捷回复：直接在桌面右下角的消息弹窗里回复消息。 快捷引用：左键双击消息的空白处，例如在时间附近，即可引用消息。 键盘快捷键聊天 Chats 动作 Action 快捷键 Shortcut 加速浏览聊天记录Speed up in-Chat Navigation Shift + Scroll 切换到下一个会话Move to the Chat Below Ctrl + TabCtrl + PageDownAlt + ↓ 切换到上一个会话Move to the Chat Above Ctrl + Shift + TabCtrl + PageUpAlt + ↑ 发送文件Send File Ctrl + O 退出 Exit返回 Go Back取消当前操作 Cancel Current Action Esc 消息 Messages 动作 Action 快捷键 Shortcut 引用消息Reply to a Message Ctrl + ↑Ctrl + ↓按住 Ctrl 不放，通过 ↑ &#x2F; ↓ 选择需要引用的消息 取消引用Cancel Reply Ctrl + ↓Esc 编辑最后发送的消息Edit Last Message Sent ↑ 编辑媒体（例如替换图片）Edit Media Ctrl + E 放大或缩小图片&#x2F;视频Zoom Image&#x2F;Video In&#x2F;Out Ctrl + + &#x2F; -（在数字小键盘）Ctrl + 鼠标滚轮 通过内联消息打开 Bot 配置文件Open Bot Profile via Inline Message Ctrl + 点击内联机器人的名字 搜索选定的会话的聊天记录Search Selected Chat Ctrl + F 分组 Folders 动作 Action 快捷键 Shortcut 切换到收藏夹（Save Messages） Ctrl + 0 直接切换到对应的分组Jump directly to the folder Ctrl + 1Ctrl + 2Ctrl + 3Ctrl + 4Ctrl + 5Ctrl + 6Ctrl + 7 切换到最后的分组Jump to the last folder Ctrl + 8 切换到归档对话（Archived Chats） Ctrl + 9 窗口相关 Window Related 动作 Action 快捷键 Shortcut 最小化到系统托盘Minimize to System Tray Ctrl + W 退出电报Quit Telegram Ctrl + Q 锁定电报Lock Telegram Ctrl + L 最小化到任务栏Minimize Telegram Ctrl + M 选取文字 Selected Text 动作 Action 快捷键 Shortcut 加粗Bold Ctrl + B 斜体Italic Ctrl + I 插入文本链接Create Link Ctrl + K 下划线Underline Ctrl + U 等宽字体Monospace Ctrl + Shift + M 纯文本（清除所有格式）Null &#x2F; Plain Text Ctrl + Shift + N 删除线Strikethrough Ctrl + Shift + X 鼠标快捷键 动作 Action 鼠标快捷键 Shortcut 引用Reply 左键双击消息Double click the message 多选消息Select Messages 在消息外拖拽多选Drag outside the messages 显示消息具体发送时间和最后更正时间Info about Messages 鼠标悬停在时间戳上Hover the timestamp 投票总数Amount of Votes in Poll 鼠标悬停在百分比上Hover percentage 转发消息Forward a message to a chat 拖拽消息到会话列表Drag the message to a chat in the list 静音发送Send Message Silently Send定时发送Schedule Message 右击发送键Right Click on Send Button 查看后续更新的 Keyboard&#x2F;Mouse shortcuts for Telegram Desktop，请访问 UseTheKeyboard 或 telegramdesktop&#x2F;tdesktop Wiki。 UnigramUnigram 是专为 Windows 10 开发的 Telegram 第三方开源客户端（基于 TDLib），并且被 官方认可。作为 UWP 应用，基本上 Mobile app 上有的功能，Unigram 都不落下。 相比官方的 Desktop 版： 支持 Instant View 支持端到端加密的私密聊天（ Secret Chats） 支持查看阅后即焚的照片和视频（Self-destruct） 在单独的窗口中打开聊天记录（Shift + 单击） 聊天输入框下可显示格式化文本菜单（Show formatting） 在输入框粘贴 Markdown 源码，发送后即可渲染，包括超链接（Ctrl + K） 频道主右击发送的动态，可查看统计信息（Statiatics）：此条信息的分享次数以及被分享到哪些公开频道 缺点： 不能最小化到系统托盘，必须常驻在任务栏 安装 Windows + S 调出搜索框，输入 区域 把 国际或地区 换到其它地方，例如 香港特别行政区 Windows + S 调出搜索框，输入 Store，打开 Microsoft Store，搜索 Unigram 并安装 把 国际或地区 改回 中国 使用以 Clash .NET 为例，如何设置网络代理，让 Unigram 连上国际互联网： 右击桌面任务栏托盘上的 Clash .NET，选择 UWP回环 &gt; 启动助手 在弹出的窗口确定两次（如果有） 勾选 Unigram，保存（Save Changes） 官方频道：Unigram News 内测频道：Unigram Mirror（无需通过 Microsoft Sotre 安装） 官方群组：Unigram Insiders 相关新闻：Unigram 现版本（v7.8.6586.0）会未加密保存媒体文件 - by TGgeek 关联阅读： Unigram 的安装及使用 - 404 Unigram 安装及使用教程 – Telegraph by TGgeek 📁 对话列表长按某个对话的左侧（头像）即可预览消息（Preview media）。 长按某个对话的右侧： 删除对话（Delete chat）：勾选 Also delete for ***，即可同时删除双方所有的聊天记录。 不用经过对方同意。如果你的朋友遭遇不测，你可以及时清除消息来保护自己和对方。 反之，需要保留证据时，请及时截图或在桌面端导出聊天记录。 归档对话（Archive chat）：把不常用的群组和频道放到归档文件夹中，精简对话列表，Everything in its place。 在移动端的对话列表里，从顶部往下拉，即可看到「已归档对话」，长按可标记全部归档对话为已读状态。 当未设置静音的存档对话收到通知时，它将从归档列表中返回到聊天列表中。 更改置顶对话的顺序 手机：长按某个对话的右侧，即可出现顺序操纵杆。 电脑：左键长按对话并拖动。 📂对话分组从 Settings &gt; Folders 进入 分组管理 设置： 最多创建 10 个分组。 每个分组都能置顶无数个对话（Unlimited Pins）。 默认分组 Unread：未读消息组，快速消灭未读红点。 Personal：个人私聊组。 Creat New Folder（新建分组）时有以下筛选条件可选： Contacts（联系人） Non Contacts（非联系人） Groups（群组） Channels（频道） Bots（机器人） …… 操作技巧 在对话列表界面，长按或者右击分组名可进行 Reorder（排序）、Edit（重命名）、Delete（删除）和 Mark ad read（标记为已读）等操作。 目前只支持在电脑客户端中设置分组图标（Folder Icons）。 🔴 关闭通知 Settings（设置）&gt; Notifications and Sounds（通知和声音）。 Badge Counter（未读消息数量显示）：取消 Include Muted Chats（包含已关闭通知的对话） 如此设置，只有未静音的对话（私聊 &#x2F; 群组 &#x2F; 频道）来消息了，才会收到「小红点」。 🧹 清除缓存此举只是暂时释放存储空间，因为媒体文件都会保留在 Telegram 云端，若需要可以再次下载，例如翻看历史消息的时候。 Settings（设置） Data and Storage（数据和存储） Storage Usage（存储使用情况） Clear Telegram Cache（清理缓存） 📲 导入数据每个人都可以通过 WhatsApp、 Line 和 KakaoTalk 等应用程序将聊天记录（包括视频和文档）迁移到 Telegram 上。以 WhatsApp 为例： iOS 打开 WhatsApp 的联系方式或群组信息页面（Contact Info or Group Info） 点击导出聊天（Export Chat） 然后在共享（Share）菜单中选择 Telegram Android 打开 WhatsApp 聊天 点击更多（More） &gt; 导出聊天（Export Chat） 然后在分享菜单中选择 Telegram 借助 Telegram 的云存档功能，再也不用担心聊天记录丢失的问题。 🗃️ 导出数据⚠️此功能需在 Telegram 电脑版 上运行。 The original meaning of the paper plane on the Telegram logo means “freedom”. For us, freedom of choice and data portability are paramount. People should be in complete control over their own data – and their own lives. Telegram 标志上的纸飞机的原意是「自由」。对我们来说，选择自由和数据便携性是最重要的。人们应该完全控制自己的数据——以及自己的生活。by Pavel Durov 聊天历史会被存储在 Telegram 云端，但是也可以导出部分（或全部）聊天记录到电脑上离线回味，而且排版还是原来的样子。 打开 Telegram Desktop 选择某个对话 点击对话界面右上角的设置（三个点 …） 导出聊天记录（Export chat history） 你也可以导出 Telegram 的所有数据。对，是所有，不仅仅是聊天记录，还有账号信息： 打开 Telegram Desktop 依次点击 Settings &gt; Advanced &gt; Export Telegram data 选择要导出的数据类型 🔞 解锁敏感内容如何在 iOS 原生客户端查看敏感内容，例如 NSFW： 登录 Telegram Web（网页版）或者下载并登录 Telegram Desktop（客户端）。 Settings（设置）&gt; Privacy and Security（隐私和安全）。 Sensitive content（敏感内容） 客户端：打开 Disable filtering（关闭过滤） 网页版：打开 Show Sensitive Content 操作完成后，重新启动 iOS 原生客户端，即可 Display sensitive media in public channels on all your Telegram devices（允许在您所有登录 Telegram 的设备上显示公共频道内的敏感内容）。 📣 频道推荐Channels 相当于公告板，是向大众传播信息的完美工具（The perfect tool for broadcasting messages to the masses），类似微信公众号，但比公众号好用得多。 通过 Post Widget，你可以将频道或公共群组的任何消息嵌入到任何地方。You can embed messages from public groups and channels anywhere. 📢 频道主 Hashtags：多用 # （标签）给消息分类，方便快速检索（点击高亮的关键词，或者在搜索框手动输入 # + 关键词），然后把标签放在置顶信息里，或频道介绍里。 频道分析（Channel Stats）📈：订阅人数超过 50 人（之前是 1,000 人）的频道会有详细的统计数据分析报告（Statistics）。 频道可以有无数个订阅者，但是创建者只能邀请前 200 个成员到你的频道。 重新编辑（Edit）消息，多久之前发的 Post 都可以。 支持删除消息通知，减少无关紧要的动态对订阅者的干扰。出现以下通知后，可立即长按删除： 更换频道置顶的消息通知 *** pinned *** 更换频道头像的消息通知 Channel photo updated 更改频道名字的消息通知 Channel name was changed to *** 如何让你的频道或群组被更多同好知道？ 打开 @zh_secretary_bot 发送频道 ID，例如 @tingtalk 编辑简介和标签后，即可提交收录到 @zh_secretary 人工审核通过后，就会在开源非盈利的 SE-索引公告板 被更多同好看到啦 相似的索引机器人还有 @PolarisseekBot。 2020 年 9 月 30 开始，电报频道原生支持评论功能（Channel Comments） 首先要 在频道的设置里绑定一个群聊（Group），频道中的每条新帖子（new post）都会自动转发到该群组并被置顶（Pin）。 频道发送消息后，有两个评论入口： 频道：点击 Leave a comment 即可进入留言板（无需加入讨论组）。 群组： 第一层评论：引用（Reply）回复对应的频道消息。 第二层评论：接龙引用第一层评论。 第 N 层评论：以此类推。 通过 @LikeComBot 给频道的消息下增加 Emoji 按钮，例如 👍、👎、😐。 🔔 订阅者 Subscriber Privacy：关注者无法得知频道创建者（creator）是谁，也无法得知谁关注了这个频道，但是频道主知道谁关注了频道。 若用户在 24 小时内访问超过 200 个群组或频道的链接（点击打开就算访问，不需要加入），就会被打入冷宫 24 小时。禁闭期间，无法通过链接访问新的群组或频道（点击链接一直转圈而无法访问）。 频道&#x2F;超级群组的关注上限是 500 个（具体数字未得到官方的求证），但是限制是一定存在的，因为限制提示语出现在官方翻译页面：抱歉，您已加入太多频道&#x2F;超级群组。请先退出一些频道&#x2F;超级群组后再加入。（Sorry, you have joined too many channels and supergroups. Please leave some before joining this one.） 去哪里找钟意的频道（Channel），群组（Group）和机器人（Bot）呢？ ☝️ 在 Telegram 内直接搜索关键词，但中文搜索识别较差。例如，「庭说」的频道是 https://t.me/tingtalk 搜索英文 tingtalk（t.me/ 后面的字符就是 ID），可以准确识别。 搜索中文 庭说，可能无法识别。 ✌️ 在 Google 上搜索，配合一些 Google 搜索技巧： 搜索结果较少：关键词 + site:t.me，例如 电子书 site:t.me 搜索结果较多：关键词 + telegram 及其别称，例如：电子书 telegram OR 电报 OR tg 这也证明了 Telegram 的内容是可以被 Google 等搜索引擎抓取的。反观国内的互联网江湖，各自割据，搞得网民苦不堪言。就拿微信来说，你不能在 Google 或者百度搜到公众号文章，这也是庭说另开一个独立博客的原因。 也意味着如果你没有做好隐私保护，请不要在公开频道或群组发言，小心不怀好意的 网络蜘蛛 爬到你身上。 👌 Telegram 搜索引擎（非官方），可能包含不少 NSFW 内容。 索引机器人 @zh_secretary_bot 👍（支持中文搜索） @PolarisseekBot 👍（支持中文搜索） @hao1234bot @hao6bot 网页版 Lyzem Search 名刀电报搜索 sssoou.com Telegram 公眾索引系統 tlgrm：只支持用英文关键词搜索 如何通过 RSS 订阅 Telegram 频道 有些用户觉得 Telegram 用手机号码注册不安全，但是又想第一时间获得 Telegram 公开频道的更新，那么可以 借助 RSSHub 生成电报公开频道的 RSS 订阅链接，例如： https:&#x2F;&#x2F;rsshub.app&#x2F;telegram&#x2F;channel&#x2F;tingtalk 只要把 tingtalk 替换成其他公共频道的 Permanent link（永久链接）后缀即可。 须知参差多态，乃是电报之福。术业有专攻，欢迎向我推荐其它领域的优质频道： 在 Telegram 搜索 @tingbot 简单说明推荐理由 优质频道将会更新在这篇《电报教程》里，让好内容得到更多的展现 以下是我收集的频道，不代表同意其观点，也许为了丰富文章内容。如果你发现某些频道开始「作恶」了或者失效了，请联系 @tingbot 从这个列表中删除。 2021 年，你需要多运动，多吃蔬果，偶尔听 播客，放下手机早点睡觉，少看鸡零狗碎的消息。 如何加入频道方法一：直接点击频道的名字，例如 庭说，浏览器会跳转到 Telegram 客户端并进入该频道方法二：复制频道的 ID，例如 tingtalk，粘贴在 Telegram 客户端首页的 🔍 搜索框，在搜索结果中找到该频道 ✈️ 电报官方频道 频道 详情 Telegram News 👍 电报官方新闻频道。 Durov’s Channel 👍 杜罗夫（Telegram 创始人和 CEO）的频道。 Telegram Tips 👍 电报小贴士（Tips）官方频道。 Telegram APKs for Android Official channel for Telegram Android APKs. You can also download them here. Telegram for macOS Updates This channel publishes release builds for Telegram macOS. Telegram Designers 向电报提你想要的功能 @design_bot BotNews The official source for news about the Telegram Bot API. Telegram Contests Here we announce Telegram coding contests in Android Java, iOS Swift, JS, C&#x2F;C++. Desktop Themes Channel 电脑客户端主题创建指引 | Custom Themes 的简单介绍 Android Themes Channel 安卓客户端主题创建指引 | 更多技术细节参阅 Custom Cloud Themes Telegram Auditions 加入 Telegram Support Force，帮扶 Telegram 做大做强，详情参阅这份 Initiative。 ISIS Watch 电报官方反恐频道：每日汇报有多少恐怖组织相关的频道被封了。 此外，Telegram 上也有 国家或地区的领导人官方频道。 用户创建 频道 详情 TGgeek 👍 TG 极客：分享 Telegram 使用技巧、重要资讯、常见问答、中文汉化、版本更新等信息。 电报小助手 用简体中文同步翻译官方 @TelegramTips 中的小技巧。 Trending Stickers Telegram 又新增了哪些表情包。 紙飛機 欢迎搭乘纸飞机，Porsche 和你聊聊 Telegram 的大小事。播客 RSS 订阅链接。 Anti Revoke Plugin Telegram 本地消息防撤回插件，安全性未知，只支持 Windows 32 位系统。GitHub 项目地址。 电报导航 SE-索引公告板 zh_secretary👍 Telegram 中文圈资源索引服务（包含 NSFW）。 北极星搜索登记板 PolarisseekIndex 电报指南 &amp; 精品排行榜 TgTrillion CN 导航 CN_DH简单好记的中文多功能公益导航频道。 Tg Tips Tg1230瞭望台旗下 TG 电报引航：电报操作、频道、广播、群组的信息库。 電報新群推送 Telegram Group Links linkpush本頻道是新群推送頻道一般只收錄剛剛建立的群組或者人數少於 150 的群組。 🦠 疫情 频道 详情 2019-nCoV 疫情实时播报 👍 COVID-19 中文消息 by NFNF。 Coronavirus Info 各国官方疫情通报频道列表（A list of official channels with information on COVID-19）。 Financial Times: Coronavirus news COVID-19 英文消息 by 金融时报。 📰 新闻在一个后真相时代，要分清事实和观点: 对于事实，要有多个独立信源交叉验证。 对于观点，要注意论述逻辑和因果关系。 频道 详情 看鉴中国 OutsightChina 👍 一个健康的社会，不该只有一种声音。看鉴中国，每天聚焦一则关于中国的新闻事件，带你对比来自中外不同媒体多元的、不一样的观点。 乌鸦观察 👍 不定期推送新闻和杂谈。 竹新社 7×24 不定时编译国内外媒体的即时新闻报道。 有据 China Fact Check 是一个专注于对中文国际资讯进行事实核查的计划，是基于志愿和网络协作原则的事实核查计划，努力连接大学、媒体和平台三方力量。 新闻实验室 推荐订阅方可成老师的 Newsletter。微信公众号文章备份。 南方周末 在这里，读懂中国。非官方。 iDaily 每日环球视野。 新周刊 一本杂志和一个时代的体温。 南都观察 RSS 地址：https://www.nanduguancha.cn/rss 新闻联播（文字版） 《新闻联播》是中国中央电视台每日在北京时间晚间 19:00 播出的一個重点时政新闻节目，于 1978 年 1 月 1 日启播。 中国数字时代消息推送 致力于聚合「中国的社会与政治新闻，和它在世界上的新兴的角色」有关的报道和评论。 多数派Masses 我们是一群反对资本主义、反对帝国主义、反对父权制、反对一切压迫和宰制的青年。Matters 的创作空间站 | Newsletter 60 秒读懂世界 来自 60 秒读懂世界公众号。 突发新闻 突发新闻推送服务（简体中文）。 NFW News for Work, Not for Work. 电报时报 提供全天候热点中国及国际新闻，涵盖突发新闻、时事、财经、娱乐、体育，评论、杂志和博客等。 蘋果日報 Apple Daily 为香港上市公司壹传媒旗下繁体中文报纸，由大股东黎智英所创立，被民主派支持者普遍认为是香港目前唯一未被「染红」的媒体。by 维基百科 台湾 中央社 香港 苹果日报 如题。 NGOCN NGOCN 是一家中国独立媒体，非营利性质，致力向公众提供进步、负责任且多元的纪实性内容，目前由认同其理念志愿者运营。 中华人民共和国外交部发言人表态 外交部负责处理中华人民共和国政府与世界其他国家政府及政府间国际组织的外交事务。 端傳媒 Initium Media 由程式自動獲取並推送端傳媒 RSS 所有文章，链接至官网。 端传媒 RSS 链接至 Telegraph 和官网。RSS 地址：https://rsshub.app/initium/latest/zh-hans 端传媒 每日推送端传媒（付费）文章.pdf。手头宽裕，还是 付费购买端会员 或购买 新闻通讯 Newsletter。 🌐 国外媒体（简体中文） 频道 详情 纽约时报中文网 👍 The New York Times (NYT) 创刊于 1851 年，世界上最著名的报纸之一。美国严肃报刊的代表，获得过 122 项普利策奖，是获奖最多的媒体。 BBC 中文网 BBC News 是世界最大的公共广播公司，位于英国，资金主要来自英国国民缴纳的电视牌照费，是一家独立运作的公共媒体（非商业媒体，也不由英国政府控制）。 联合早报 zaobao.sg 早报 + 晚报 + 新明新闻。 路透中文网 Reuters 世界三大通讯社之一，成立于 1851 年，总部位于英国伦敦。 德国之声 Deutsche Welle (DW) 按德国公法设立的国际化公共媒体，从联邦政府获得拨款，总部位于波恩和柏林。 澳大利亚广播公司 Australian Broadcasting Corporation (ABC) 是澳大利亚的国家公共广播机构，它由政府出资，向澳大利亚和海外提供电台、电视、互联网服务。总部设在悉尼。 法国国际广播电台 Radio France Internationale (RFI) 是法国专责世界大部分地区之国际广播的电台广播机构，现隶属法国国营国际广播公司法国世界媒体旗下。by 维基百科 美国之音中文网 Voice of America (VOA) 成立于 1942 年 2 月，是美国政府对外设立和资助的国有非军事国际广播宣传喉舌，由美国国际媒体署管理，旗下拥有广播电台与电视台，总部座落在首都华盛顿。by 维基百科 华尔街日报 RSS 地址：https://feedx.net/rss/wsj.xml 俄罗斯卫星通讯社新闻 Sputnik 是俄罗斯政府控制的新闻机构今日俄罗斯媒体集团于 2014 年 10 月开通的新闻通讯社、新闻网站、广播电台与媒体新闻中心。by 维基百科 韩国新闻 朝鲜日报 + 中央日报中文版 日本新闻 共同网 + 朝日新闻中文网 + 日本经济新闻中文版 双语新闻 纽约时报双语新闻 + 中国日报网英语点津 Twitter Subscription 搬运以下 Twitter 账号：BBC News 中文、DW 中文- 德国之声、国际特赦组织中文、纽约时报中文网。 新闻播报 PDF 每天为大家送来 NYT 和 BBC 的新闻 PDF。 What’s News 推送各种英文外刊和杂志的 PDF。 以上部分介绍来自西方媒体查一查。查询可信度和倾向性，请安装 浏览器插件，或者访问 微信小程序。 国家 &amp; 发言人（已认证） 频道 详情 Gov.sg 新加坡。 Donald Trump Jr 特朗普。 💸 财经新闻 频道 详情 财经快讯 全球财经资讯 24 小时不间断直播。 FT 中文网 Financial Times（金融时报）创刊于 1888 年，编辑总部位于伦敦，2015 年被日本经济新闻收购。 💾 科技 频道 详情 Solidot 👍 奇客的资讯，重要的东西。 Readhub 👍 readhub.cn 非官方 RSS 推送频道。 Newlearnerの自留地 👍 不定期推送 IT 相关资讯。 Appinn Feed 👍 分享免费、小巧、实用、有趣、绿色的软件。 少数派 👍 少数派致力于更好地运用数字产品或科学方法，帮助用户提升工作效率和生活品质。 科技爱好者周刊 👍 记录每周值得分享的科技内容，周五发布；非官方频道。科技爱好者周刊合集。 TestFlight 科技花 发布科技新闻、App 测试版链接、软件使用相关话题。 Hacker News Top stories from news.ycombinator.com (with 100+ score). V2EX - 最新&#x2F;最热主题 V2EX 是创意工作者们的社区，可以分享生活和事业。 科技圈的日常 科技圈内的大事小事。 Telegram 中文 NEWS 聪聪 的频道：提供印象笔记、Telegram、科学上网等新闻。Telegram 知识汇总。 每日消费电子观察 不公正，不客观，不理性。 cnBeta cnBeta.COM 中文业界资讯站是一个提供 IT 相关新闻资讯、技术文章和评论的观点的中文网站。 IT 之家 RSS 地址：https://www.ithome.com/rss/ APPDO 数字生活指南 优质数字生活指南，传递数码生活和设计理念。 VPS 信号旗播报 关注 VPS 和通信自由。 硬核小卒 分享优质的科技&#x2F;商业资讯。 知乎日报 越来越难用的问答网站。 Daily Tech News 每日科技新闻。 每日 AWESOME 观察 每日更新分享最炫酷的开源项目。 LetITFly News 主题包括但不限于 Android、Windows、Web、消费电子相关，吹水为主。 Science Science News channel, videos and articles - international project, 35+ countries. OnePlus Everything OnePlus. 老毛子 Padavan 固件发布 一个路由器固件。 油油分享频道 分享开源、优秀的软件，有趣、实用的网站资源。 Widget 优质工具和软件，以及有用有趣的科技资讯。 科技互联网 即刻精选 jike_read即刻精选，以及相关讨论。这里是即友们的 TG 自留地。 Apple AppleGuide AppleBuyersGuide小胖 的苹果产品购买指南，更系统请查看 AppleGuide.cn，不断完善中。 果核 Apple Nuts AppleNuts一个果粉（Hackl0us）的闲言碎语， 用来推送苹果（Apple） 相关的技术、新闻资讯、技巧、产品&#x2F;软件心得体会等。 AppPie AppPieApple 相关的数字生活指南。 iOS 限免与优质应用推荐 iosblackteckapp免费使用正版应用，以及分享 iOS 各种高效实用应用与实用黑技巧。 iOS Releases iOSUpdatesiOS, TvOS and watchOS signing status updates. This channel will notify you when apple starts or stops signing a firmware version. Android 问道 mdqwsf该频道 apk 为个人汉化而来。 软件 简悦 - SimpRead simpread让你瞬间进入沉浸式阅读的 Chrome 扩展，还原阅读的本质，提升你的阅读体验。希望做一些让这个世界变得更美好的小事。by Kenshin网站 | 订阅中心 📚 博主 频道 详情 庭说 👍 第一时间获取博客的更新通知以及碎片化思考。 庭说 - 唠叨频道 @tingtalk_all 发布一些主频道 @tingtalk 之外的增量更新以及碎片化思考。 小破不入渠 👍 科技评论人 Jesse Chan，博客是 大破进击。 一天世界 👍 一天世界，昆乱不挡。不鸟万如一主理。IPN 出品。 caoz 的梦呓 👍 认识曹政之后，感觉互联网终于入门了。by Fenng ZUOLUOTV 👍 科技 &#x2F; 旅行 &#x2F; 摄影 &#x2F; 生活方式 &#x2F; 博客 不求甚解 👍 Newlearnerの自留地 编辑；设计师 oooooohmygosh 的代言人。 小道消息 大道无形，小道消息；公众号备份站点。 卖桃者说 博客是 MacTalk：池建强的随想录关注技术和人文。 数字移民 无法肉身移民的情况下，在数字生活上追求一定的自由；博客。 Real Spencer Woo 开发者 &#x2F; 设计师 &#x2F; 少数派 &#x2F; 学生 &#x2F; 博客。 Sukka’s Notebook Belongs to Hexo dev team &#x2F; 博客。 扫地僧笔记 每天所见所闻所想，是个树洞。 一方天地 心留一方天地，世界依旧美好。 湾区日报 关注创业与技术，不定期推送 5 篇优质英文文章。 海龙说 牢记梦想，自然生长。by 郝海龙的博客 荔枝木 这个世界很复杂，我尝试着去理解它。 KAIX.IN 思考碎片，博客 更新。 TSBBLOG 影子的博客：独立观察及记录。 AK 讲废话 科普视频系列：无线技术、显示技术、翻墙技术…… P3TERX ZONE P3TERX 读作 Peter X。 值物志 分享各种值得尝试的事物：值得读的书、值得用的软件、值得看的电视剧…… 小虎の自留地 讨论家装心得或者有趣实用的家具电器。 Leonn 的博客 低价主机（VPS）资源。 Yachen’s Channel 刘亚晨是 Surge 的开发者| Yachen’s Blog BennyThink’s Blog 随便分享点什么，可能是某部剧，可能是某首歌，可能是一点点感动的瞬间，也可能是我最爱的老婆。 MolunSays 希冀笔尖之下，世界兴旺繁华 | 博客 日常人间观察 关心科技 &#x2F; 人文 &#x2F; 艺术 &#x2F; 城市公共空间 &#x2F; 女性和性别议题 &#x2F; 劳工权益 &#x2F; 个体叙事 &#x2F; 电影 &#x2F; 音乐 &#x2F; 书 &#x2F; 星星…… In The Flux 关于文化、艺术与技术的信息流。 为也行 「书籍 | 电影 | 资源 | 技巧 | 摸鱼图」大多原创，少部分转发。 Jerry Zhang 的频道 在渥太华的长春人。博客：Overflow，向信息过载的世界大喊。播客：《科技聚变》（TechFusion），我们谈论有关互联网的一切。 老人和糟 没有频道简介，科技相关。 Karen 医生の日常 一个小医生的通讯站。不想出名，只传播一些信息和科普。谨慎关注，会发一些血淋淋的图片。 人海拾贝FlipRadio 翻转电台的 Channel，一些零零散散的要分享的东西。 Find Blog 发现优秀的博客与创作者。 TomBen’s Web Excursions PhD Student、Productivity Enhancer、Writing Enthusiast 博客 | 少数派 熊言熊语 「熊言熊语」是一档关注学习分享和知识科普的 播客 栏目，我们希望用声音记录改变与成长。思考问题的熊和他的朋友们一起聊学习工作、聊科研科普。博客 | Newsletter Hell Cell 功能教学 通过 YouTube 视频讲解一些实用软件那些有用有趣的功能。 The Sociologist 我们只谈论记忆，因为不再有记忆。 每日摄影观察 一个不严肃的摄影频道。 中國家地理雜誌中文版 Hi 探險家，和國家地理一起探索世界吧！ EdNovas 的小站 @ednovas2网站：ednovas.xyz导航：navigate.ednovas.xyz gledos 的微型博客 gledos_microblogging请记住我们，因我们也在这世上爱过和笑过。 Route 66 Blog landofmaplex网站：留学、移民、程序员、死磕北美、加拿大、美国、跑路、移民生活。 中文独立博客列表 by timqian 🔔 RSS 频道 详情 RSSHub 布告栏 万物皆可 RSS。 All About RSS 关于 RSS 技术的应用、周边、介绍、方法、教程、指南、讨论、观点。 RSS 频道收集 收集推送 RSS 的频道，把 TG 变成 RSS 阅读器。 🎙 播客采用 RSS 订阅的播客，永远都不会过时。 频道 详情 「利器x播客」计划 官网 播客先声 分享关于播客的一切。by Zac 中文播客精选 分享精选优质中文播客，目前推荐单期节目为主。by 白昼电台 的主播 Stella Your Daily Dose of Podcast 每天推荐一集让人心潮澎湃、若有所思、打开新世界大门的播客节目。by 穿堂风推荐的播客会同步更新在 Medium我在豆瓣上分享了 400 集播客节目，有什么用？ 交差点 Technology alone is not enough. 不客观 Not Objective 一档搭建在 Telegram 的简易播客，纯主观感受。by 郝海龙 白昼电台 The Day 黑夜已深，白昼将近，我们就当脱去暗昧的行为，带上光明的兵器。 维生素 E 经济学与哲学知识分享。 Go 夜聊 一档由杨文和欧长坤主持的针对 Go 语言的播客节目 阿乐杂货铺 这里每日推送小人物播客及播客周边；职业发展、自我成长、读书电影、海外工作与生活碎片。 🎵 音乐 频道 详情 知音 👍 发一些关于音乐的东西。 Imusic 音乐，就是理想的挽歌，年代久远，依然飘扬。 杂鱼Music Channel 我相信，爱音乐的人都有着一颗柔软的心。 音乐世界 温柔被我唱成了歌，伴你人山人海不停留。 心声：音乐分享频道 分享一些能引起共鸣的音乐。 每日一歌 愿你也能在这里找到属于你自己的共鸣。 Classical Music 一起来听古典音乐吧。 蛙音通讯 Feels wonderful again man. 无损音乐频道 分享无损音乐、高品质音乐、原碟整轨分轨音频。 浦镇青年 李志。 崔健无损 中国大陆摇滚先驱者。 音乐分享频道 一般为无损音乐。 音乐屋 发现音乐新世界：live、黑胶、磁带 CXPLAY MUSIC 单曲音乐试听，专辑存放在 这。 下载音乐，还可以查阅下文中提到的音乐机器人。 🏫 读书人类的悲喜并不互通，但读书是走向共同理解的捷径。 频道 详情 Word Power Made Easy 利用词根（原始印欧语、拉丁语、古希腊语）学习英语单词。 英语精读学习 夜空中最亮的星，就是你自己！我们一起精读英语，一起进步，遇见更好的自己吧！资料不定时更新哟！ ENGLISH PODCASTS INFINITY PODCASTS CHANNEL WITHOUT ANY LIMITS. 中文社科讲座资讯 一个讲座信息聚合和 PPT 共享平台。 ReadFine 电子书屋 致力于电子书分享的读书频道。EPUB 电子书一站式阅读体验（包括豆瓣评分、书籍简介、封面截图），一键下载，享受读趣。 The Economist Sharing Channel Sharing the Economist and E-books every week. 什么书值得读 仅推送某亚原版资源，可同时下载 .azw3 .epub .mobi 的电子书。 好书分享频道 学习，是一辈子的大事。 小声读书 一个探索数字阅读可能性和未来的开放项目，致力于打破信息茧房，挖掘价值信息。 值得一看的文章 阅读更少，收获更多。 云上报刊亭 英文报刊杂志、电子书、报纸和外文杂志精选。 Λ-Reading 分享书和阅读、认知科学、科技哲学、新科技以及其它给生活带来一丝美好的事物 | Newsletter 臭（xiù）文字 诗歌频道；我是一个嗅觉特别发达的人，你说，然而，没有一种艺术可供我的鼻子用武，只有生命可以。 已有丹青約［書畫］ 高清油画档案（超过两万张）。 阿银书屋 偶尔更新，没事来看看。 红楼梦 每日一章 💞DreamOfRedMansions。 插播一个免费的广告：学英语，推荐购买郝海龙老师的《英语自学手册》（￥119）。 🚀 翻墙软件 频道 详情 Clash .NET 公告 👍&#x2F; A Clash GUI Proxy For Windows Based On .NET 5 Fndroid 的日常 👍 Clash for Windows Clash for Android Channel A Graphical user interface of Clash for Android Surfboard News 安卓专享的翻墙客户端，但不支持 SSR。用户手册 SagerNet Apks 支持 SOCKS、HTTP(S)、Shadowsocks、ShadowsocksR、VMess、VLESS、Trojan……等协议SagerNet 官网 AnXray Another Xray for Android GitHub Shadowrocket News iOS 上小火箭 Quantumult News Quantumult 的非官方频道。 Quantumult X News 此频道用于发布 Quantumult 与 Quantumult X 的相关资讯。 迷雾通（Geph） 与众不同的开源翻墙软件，提供完全免费的中速浏览，够浏览新闻、查邮件、看标清视频等。超快速度的付费 Plus 账号仅需 €5&#x2F;月。截至 2021 年 5 月 29 日，暂不支持 iOS 设备。 协议 &amp; 脚本 &amp; 规则 频道 详情 V2Fly Shadowsocks 是一个纯粹的代理工具，而 V2Ray 定位为一个平台，任何开发者都可以利用 V2Ray 提供的模块开发出新的代理软件。by 新 V2Ray 白话文指南 ACL4SSR https://github.com/ACL4SSR/ACL4SSR 官方频道。 QuanX &amp; Surge &amp; Loon 脚本收集 各种脚本。 QuantumultX 教程&amp;API&amp;解析器 如题。 Cool Scripts QuanX, Loon, Surge, JsBox, Pythonista, Scriptable, Shortcuts 等脚本分享。 DivineEngine 神机规则 评测 频道 详情 毒药机场评测 由于大陆地区网络环境十分复杂，测速不代表推荐。另外，有些机场会泄露个人信息，选购时多加搜索或者进入机场用户群打探打探。 品云☁️测速 细品各种云☁️。PinYun is a non-profit organization dedicated to making the internet a better place for everyone. 科学上网与机场观察 科学上网与机场相关观察、点评、随想和新闻资讯。 关联阅读 番茄食用指南（科学上网教程） | 庭说 番茄种植指南（梯子搭建教程） | 庭说 🗄️ 搬运 频道 详情 煎蛋无聊图 自动抓取煎蛋首页推荐无聊图及其评论。 内涵段子：皮一下 如题。 美图与沙雕 如题。 糗事百科 如题。 心惊报 又一个沙雕图频道，每日随缘更新。 你知道的太多了 不定期发布和转载各类不一定靠谱的内幕、流言蜚语、小知识等。 蛋挞报 分享阅读体验。 微信搬运工 有些微信的内容分享了之后就和谐了，本频道可以做个备份，以及丰富电报上的中文内容（不可否认还是有很多非政治的优质内容在微信公众号里）。 微博精选 来自微博的文章、资源和观点。 豆瓣精选 豆瓣书影音，以及相关讨论。 鹅组精选 豆瓣鹅组 非官方搬运。 即刻精选 精选即刻 app 热门话题更新。我的即刻 ID 是 Dr_Ting。 你不知道的内幕消息 同时抓取来自即刻 app 的 #大公司的负面新闻。 Matters 閲讀精選 matters.news 一個自主、永續、有價的創作與公共討論空間。 🆓 资源 频道 详情 Google Play 限免信息 不定时推介 Play Store 上的限免游戏和 App。 Price Tag 推荐 App 限免降价，推送好物好券。 纯粹的 App Store 应用推荐 iOS 实用免费、精选限免、优质冰点应用推荐。 反斗限免 这里有反斗软件和反斗限免的文章更新。更新频繁高。 如有乐享 更新 如有乐享博客 的内容：云服务器、优惠活动、羊毛信息以及各种 Bug。 iShare News 一个没有简介的资源分享频道。 Zapro Notice 软件分享。 App 喵 破解软件资源共享。 Google Drive 资源 各种 Google Drive 资源，包括电影、电子书、无损音乐等，10 万+ 关注。 Google Voice 靓号 一个 GV 卖家。 Windows 10 激活码分享 🤫 Office Tool Plus Office Tool Plus 是一个用于部署、激活Office、Visio、Project 的小工具。借助本工具，你可以快速地完成各项Office 部署工作。 你有一个打折需要了解 分享 Steam 的周榜、折扣、资讯、喜加一等。 52 破解信息 吾爱破解。 擅长搜索的高木同学 gaomutongxue 黑科技软件资源分享 kkaifenxiang 分享免费实用高效率网络资源、黑科技软件、实用黑技巧。 Discover good software ksc666 分享 Magisk、Riru、LSPosed、虚拟框架、Xposed 模块、Magisk 模块、Android、Windows……等软件。 破解安卓 VPN 软件 vpn_cracked 发布原创破解的 VPN 和各种软件，以及分享各类资源，多位安卓逆向大佬坐镇。 万能福利吧 @wnflb分享有趣的信息，包含网站、活动、网购、下载综合症、好孩子看不见等福利。 🎞️ 视频电影 &#x2F; 剧集 四库全书 video4lib 👍一个不断收集互联网有价值内容的企划。 电影频道 TGDY188精选国内外高分电影。 华联社电影频道 Cctv365 霸王龙发布频道 T_rex2333专注于韩美剧，选取优质影片源。 苍炎影院 cangyanmovie分享最新最热门的优质电影。 双语短视频合集 english_bilingual学习英语，了解世界。 动漫 海贼王更新提醒 tingtalk_op@TingTalk 子频道，试运营。由初中开始追 One Piece 的 Dr_Ting 创建， Rick and Morty@TingTalk 子频道，试运营。曾经把《瑞克和莫蒂》作为练口语的 素材，听了上百遍，但效果甚微，Wubba Lubba Dub-Dub。 下载站 Odyssey+ odysseyplus公益服食用指南。 PT 资讯频道 @PrivateTrackerNewsPrivate Tracker 资讯以及开放注册信息推送；PT 可以简单理解为私有化的 BT。 Sync 资源更新 @shenkey只发 key。 电视机顶盒 &amp; 手机影视 App @tvbox001 可看港台电视直播、美剧等。 😺 其它软件 Aria2 Channel @Aria2_ChannelAria2 完美配置、Pro Docker、Pro Core、一键安装管理脚本增强版 (GNU&#x2F;Linux)。 未分类 NBA tingtalk_nba@TingTalk 子频道，试运营。从高中开始只练跳投，因此严重偏科，不会突破，不会抢篮板，不会防守，但崴脚少了，命中率高了。 频道 详情 iYouPort IYP 不是过眼云烟的新闻网站，我们提供实战能力，这里是值得您反复回看的档案室。 安全上网注意事项 转载一些关于安全上网的文章，这些文章都比较浅显。 博海拾贝 博海拾贝 的网站：bh.sb 回形针PaperClip &amp; 灵光灯泡 回形针内容推送。 合租 Netflix、YouTube、Spotify、Office 365、HBO、Apple、Surge…… History Digging Past. Photos from Past who shaped today. 每日无数猫 让我们打造一个全是猫的世界！ฅ^•ﻌ•^ฅ NS 新闻转报 任天堂（Nintendo）相关的新闻。 基督讲道 基督讲道资源频道。 就要造反 此频道立足生活，以非常古怪的文字风格进行生存经验书写，绘制景观与消费社会中极具现实性的个案，以此为个体提供可操的、创造性的抵制策略与造反计谋。为一切造反者辩护，为所有无用与丰饶辩护。 残障之声 在态度和环境障碍相互作用存在的情况下，提供合理便利是全社会需要一同去解决的问题，残障人士应当理直气壮地要求这种权利和便利，去定义一个无障碍的社会。 每日一句 meiriyiju每天一句心灵鸡汤，配上必应每日壁纸。 Leanote leanote今天的定位（今天的是什么日子）：单向历、mono日签等 🤖 Bot 推荐Bots（机器人）就像运行在 Telegram 内部的小程序。借助 Telegram 开放的 APIs，可以实现很多让你意想不到的功能。 BotNews：The official source for news about the Telegram Bot API. 💠 内联机器人在任意对话界面的消息编辑框，输入 Inline Bots 的名字，即可将 Ta 们唤醒（Just type @inlinebots keywords in any chat.）。 Bot Info @bing 图片搜索 by Bing（支持中英文）。 @bold 👍 使用 Markdown 编辑消息（有字数限制）。 @coub Coub 是一个视频共享网站（时长十秒的循环视频）。 @creationdatebot 获取注册 Telegram 的日期。 @fanyi_bot 为全世界语言提供中文翻译。 @foursquare 帮你找到附近的餐馆或附近的地方，并将其地址发送给朋友。 @gamee 在群组中输入 @gamee，选择一个游戏，立刻和你的朋友 在 Telegram 上玩小游戏。 @gif 👍 GIF 动图搜索，支持中文。例如 @gif 你好。 @imdb 查看影视作品在 互联网电影资料库（IMDb）的评分。 @GoogleDEBot 在任意聊天框使用 Google 搜索引擎。 @like 👍 添加 emoji-based like buttons，例如 👍 &#x2F; 👎。在搜索框输入 @like，预设一些喜欢的投票符号（最多 6 个），然后就可以在聊天框输入 @like 调用这些预设。 @music 帮你找到动听的古典音乐。 @pic 图片搜索 by Yandex（支持中英文）。 @QuizBot 答题机器人：创建一份只有单选题的考卷。点此 开始测试你对 Telegram 的了解程度。访问 quiz.directory 查看更多问卷。 @sticker 👍 检索所有与 Emoji 相关表情包。例如 @sticker 😎 。 @telegraph 👍 登录和编辑 Telegraph 文章，并 统计 telegra.ph 的浏览量。 @vid 帮你查找 YouTube 视频（支持中文搜索）。 @vote 投票机器人。 @wiki 维基百科。搜索中文条目 @wiki zh 猫；搜索英文条目 @wiki en cat @youtube 帮你查找 YouTube 视频（不支持中文搜索）。 🧱 非内联机器人以下 Bots 不能在聊天窗口调取使用。 🧡 RSS 机器人 （如果）你不懂得 RSS，上网的效率和乐趣都要大打折扣。by 阮一峰 相比于传统的 RSS 客户端，Telegram 上的 RSS 订阅器的优点是： 自动记录上次浏览的位置 某些 RSS Bots 支持 ⚡ INSTANT VIEW 在 All About RSS 里推荐了很多 RSS Bots： @FeedManBot @TheFeedReaderBot：不仅可以订阅 RSS 源，还可以在 Telegram 上浏览 Twitter。 @Feed2Telegram_bot：免费用户只有 5 条 Feeds；发送 Twitter（推特）链接，即可追踪。 @el_monitorro_bot @newlearner_rss_bot @NodeRSS_bot …… 以上大部分 Bots 都能免费使用，但是保不齐哪天服务器撑不住，就停止运营了，所以记得定期导出 OPML 文件作为备份。 如果有 VPS，自己搭一个专用的 RSS Bot 会是不错的选择。 🎵 音乐机器人在 Telegram 上实现点歌自由，或者像我一样建立频道，存放喜欢的歌单：@tingtalk_fm 找一个音乐机器人，例如 @haoyybot 搜索歌名，选择歌曲，下载后转发到频道 修改（Edit）歌曲信息，加入标签（方便搜索）和链接（例如 YouTube 上的 MV） 如遇到版权限制，无法下载，换用其它歌曲 Bots： @ChinoNyanBot @vkm_bot @vkmusic_bot @u2bu_mp3_bot 听歌识曲 @SongIDbot 下载 YouTube 音频 @YTAudioBot 下载音乐、歌词、视频等媒体 @getmediabot 内置音频播放器 长按住「下一首」 和「上一首」按钮可以快进和倒带。Press and hold on the Next and Previous buttons to fast-forward and rewind. 💽 DC 查询Telegram 的服务器分布在世界各地的数据中心（Data Center，简称 DC）。如何查询自己所在的数据中心（好像没啥用）： 第一步，在隐私和安全设置中，（临时）开启所有人都能查看你的头像（Profile Photos）。 第二步，选择一个查询 Bot，例如 @WooMaiBot 第三步，发送 /dc，即可获得你所在的数据中心。 🎁 其它机器人 Bot Info @bingdict_bot 基于 Bing 开发的中英文翻译机器人。 @BotsArchiveBot 收集了上千个 Bots（仅限英文版）；官网。 @CorsaBot 👍 Make Instant View from any article. 快速把文章把文章备份到 Telegraph。 @cnLottery_bot Telegram 群组抽奖工具。 @DogFatherPublicbot App Store 价格监控。 @githubbot 推送 GitHub 仓库的动态。 @GmailBot 👍 在 Telegram 上收发 📧 Gmail。 @he_weather_bot 和风天气小棉袄。另外还有 WIEN 产品的 广州、深圳、东莞 的天气速报频道。 @IFTTT With this bot you can use IFTTT to link your Telegram groups or channels to more than 360 other services like Twitter and Instagram, or connected devices like Hue lights and Nest. @jobs_bot This bot lists career opportunities at Telegram and accepts candidates’ applications. Available at https://telegram.org/job @LivegramBot 👍 不加好友也能私聊，可用于收集反馈及绕开 +86 手机号码的限制。因为经过一层转发，消息一旦发送，便无法删除，但有个短暂的修改期。 @MakeQrBot 发送文字，生成对应的二维码。 @sssoou_bot Telegram 搜索，支持中文。 @Stickers 👍 创建属于自己的表情包。 @tweet_for_me_bot 在 Telegram 上发布 Twitter 动态。 @tgstogifbot 把 Telegram 上 tgs 格式的表情包转换为 gif 格式。 @utubebot 同时下载 YouTube 的视频和音频，不过会推送一些广告。 @verifybot 加了官方认证后，名字后面有个 ✅（verify a big andactive official channel, bot or public group）。 @zzzdmbot 真正值得买推送机器人，可以根据关键词订阅推送什么值得买精选优惠信息。 更多 Bots 推荐，请参阅 Raw 博客 以及 合集网。 👥 群组管理 群组人数最高 20 万。转换为广播组（Broadcast Group）之后，人数将不受限制，但只有管理员可以说话，但是成员仍然可以加入语音聊天。注意，切换之后，将不能回退到普通群组。 静音群组只有在 @ 提到你、 Reply 回复你和 Pin 管理员发布群通知时才会收到通知。 新加入的成员可以看到全部的群聊历史记录（New members can see the entire message history when they join）。 加入群组之前，可看到（单向）好友是否在列。如果不想让好友知道你加入了某个群组，注册一个小号吧。 支持在任何设备上发起语音聊天（Voice Chats） 在 Telegram Desktop 和原生 macOS app 中，支持设置发言快捷键，例如大小写锁定键。（On Telegram Desktop and in the native macOS app, you can choose a push-to-talk key for Voice Chats, to control your mic even when Telegram is not focused. At long last, we’ve found a use for your Caps Lock key.） 需要在翻墙服务&#x2F;软件上开启 UDP 转发。 截至 2021 年 1 月 13 日，语音聊天支持 5000 人同时参与。Voice Chats for up to 5,000 participants. 主持人可以控制发言者的音量（Adjust the Volume），这个设置是全局的，对所有听众有效。 庭说读者群组是 @tingtalk_group。 👮 管理之道 可删除成员的单条消息或全部消息。 慢速模式（Slow Mode）：在 10 秒 &#x2F; 30 秒 &#x2F; 1 分钟 &#x2F; 5 分钟 &#x2F; 15 分钟 &#x2F; 1 个小时内，成员只能发送一条消息，这样可以使小组中的对话更加有序，也可以提高每条信息的价值。 群组权限：管理员可以限制所有成员或单个成员只能发送特定类型的内容（Partial bans），如此一来，你便可建立一个没有表情贴纸或者 gif 的高质量交流社区。或者甚至完全限制成员发送信息，让管理员彼此交谈，而群成员则默默地旁观。 可对群主（creator）及管理员（administrators）设置自定义头衔（Custom title）。 管理日志（Recent Admin Actions）：当多个管理员在一起工作时，很容易搞不清楚哪个管理员或管理机器人，在什么时候做了什么。所以 Telegram 的管理员页面增加了「最近动作」功能。用于存储过去 48 小时内在组中执行的所有服务操作的日志，仅对管理员可见。 2020 年 9 月 30 开始，管理员可以匿名发布消息了（Anonymous Group Admins）。 成员超过 500 人后，自动开通群组分析功能。Owners of large groups with over 500 members can view beautiful, detailed graphs about its activity and growth. 🔐 管理机器人 在群组设置里搜索 Bots 名字，即可添加，然后赋予尽可能少的权限。 @policr_mini_bot 开源验证机器人，详情参见 官网 赋予 Delete messages 和 Ban users 权限 @DeleteEventsBot 或 @AntiServiceMessage_Bot 删除冗余的事件通知，例如谁加入了群组。强烈建议群组管理员装备上这个 Bot，减少信息噪音 赋予 Delete messages 权限 @areply_bot 自动解除关联频道（Linked hannel）转发到群组的自动置顶消息，并恢复之前的置顶消息 赋予 Pin messages 和 Delete messages 权限 其它管理机器人： @keyworddel_bot：自动删除群组推广、广告、博采等消息。 SCP-079-INFO：免费并开源的群组管理机器人，需 申请 通过后才能使用。查看项目介绍 🤐 私密聊天Cloud Chats（默认聊天模式） 客户端 -服务器 &#x2F; 服务器 - 客户端 信息存储在 Telegram Cloud 中进行加密，这使云消息既安全又可以立即从任何设备访问，即使丢失了设备。所以你不需要将所有的信息历史记录存储在手机上，当你需要的时候，你可以随时在 Telegram 下载（缓存）旧的信息和媒体，这为你节省了大量的磁盘空间和内存。 Secret Chats（不支持在 Windows 和 Web 上发起） 客户端 - 客户端 聊天记录不能云备份，因为私人数据没有经过 Telegram 的同步服务器，所以没有任何人可以破解，包含 Telegram 团队本身。 关联阅读：为什么电报的端到端加密不是默认的？ 只能通过原始设备访问历史消息。退出并再次登录，将失去所有的秘密聊天记录。 可设置阅后即焚（self-destruct）计时器，自动销毁消息或媒体文件（只适用于计时器设置后发送的消息，对早期的信息没有影响）。 不能转发消息。 不能编辑已发送的消息。 Android 设备不能截屏；iOS 设备可以截屏 ，但对方截屏时你会收到通知。不过，只建议与你信任的人分享敏感信息。 毕竟，对方可以用另外一台设备给屏幕拍照。 删除发送方的消息，接收方那边也会强制删除。 😎 TelegraphTelegraph 一个极简的匿名内容发布工具（Minimalist publishing tool）。如果内容侵权了，例如使用有版权的图片，可能文章会被投诉下架。 ☝️ 用法一此法不需要注册账号与下载软件。 在手机或电脑的浏览器的地址栏输入 telegra.ph 写文章 发布（PUBLISH） ⚠️ 内容发布之后，只要清除浏览器缓存，便无法再编辑文章，也不能追溯到文章作者。 ✌️ 用法二通过 Telegraph 机器人 @telegraph 管理文章： 能看到你发了多少篇文章，多少人看了（但是读者依旧看不到作者是谁）。 在任意聊天窗口输入 @telegraph，即可弹出你发布的所有文章。 📝 发布流程 在 Telegram 打开 Telegraph 的小管家 @telegraph 根据提示配置 Account 写文章：Author 可以写频道或者群组的 Public link（例如 https://t.me/tingtalk），读者点击作者即可跳转。 发布（PUBLISH） ✍️ 修改流程在任何一个设备（across any number of devices）都可以再次编辑文章的标题、作者和正文（除了文章链接）： Log in as *** on this device 返回 @telegraph My posts，点击文章的标题 电脑：自动跳转到浏览器打开，然后在右上角找到 EDIT 手机：点击界面的右上角 3 个点，选择 Open in… 用浏览器打开，滑倒文章底部即可看到 EDIT 🔗 如何让文章链接更美观URL &#x3D; https://telegra.ph&#x2F;首次输入的标题-首次发表时的月份 -首次发表时的日期 如果你用中文撰写标题，例如《选择 Telegraph 的 10 个理由》，那么文章链接会变得又臭又长，且不能从链接或者文章主题： https:&#x2F;&#x2F;telegra.ph&#x2F;%E9%80%89%E6%8B%A9-Telegraph-%E7%9A%84-10-%E4%B8%AA%E7%90%86%E7%94%B1-12-04 要想得到一个 friendlier-looking link，首次编辑请使用英文标题： 英文单词全部小写：10 reasons to choose the telegraph 把 the、a、an 等去掉：10 reasons to choose telegraph 用连字符 - 代替标点和空格：10-reasons-to-choose-telegraph 是否可以精简或者采用另外一种翻译：why-telegraph 再用 Edit 功能修改标题为中文：选择电报的 10 个理由。 📝 如何让文章排版更美观 使用 Markdown 写文章（例如 Typora） 复制 Markdown 预览（不是源代码） 粘贴到 telegra.ph，即可快速排出精美的文章（不支持某些格式，例如多级项目列表） 🔗 关联阅读 Telegram FAQ：电报常见问题（英文网页）。 The Evolution of Telegram：电报的演化（简史）。 Articles about Telegram：这些文章涵盖了 Telegram 及其赞助商、全球企业家帕维尔•杜罗夫（Pavel Durov）。 Telegram Messenger - Twitter：Keep evolving and stay in touch，官方推特会介绍各种 #TelegramTips。 Telegram - reddit Telegram logos：Please feel free to use these Telegram logos. Just make sure people understand you’re not representing Telegram officially. A long way from Moscow：’Russia’s Mark Zuckerberg’ took on the Kremlin - and lost his country by Christopher Miller Telegram X 介绍视频：超级炫酷，一定要看。 给电报找 Bug：欢迎安全研究人员将 Ta 们在 Telegram 应用程序或协议中发现 🧐 的任何问题提交到 security@telegram.org。 根据问题的严重程度，奖金从 500 美元到 10 万美元或更多。 Telegram 背后的故事 Telegram 传奇：一个关于俄罗斯富豪、黑客、极权和阴谋的创业故事 - 霍炬 关于 Telegram 的一些事 - 守望的麦子：我更加相信一定会有越来越多的理想主义者，他们相信 平等 和自由，坚守信念和价值观，每天充实地生活着。 我为什么选择 Telegram 来运营粉丝社群 | 规则、经验和思考 - 罗磊 Telegram 教程全指南 by TG极客 Telegram 群组、频道、机器人 - 汇总分享 - 聪聪 Blog Telegram 频道：Newlearnerの自留地 导航页 Stay home. Wash your hands. Be safe. And stay tuned for our next updates! It is already brewing in our dungeons! 呆在家，常洗手，敬请关注，更强大的 Telegram 已经在我们的地牢里酝酿中了！ 📞 与我联系欢迎读者在 Telegram 搜索 @tingbot 与我取得联系：指出此文疏漏，推荐优质频道和机器人，一起跨越数字鸿沟，共享信息自由。","categories":[],"tags":[{"name":"网上冲浪指南","slug":"网上冲浪指南","permalink":"https://blog.ehzyil.xyz/tags/%E7%BD%91%E4%B8%8A%E5%86%B2%E6%B5%AA%E6%8C%87%E5%8D%97/"},{"name":"Telegram","slug":"Telegram","permalink":"https://blog.ehzyil.xyz/tags/Telegram/"}],"author":"ehzyil"}],"categories":[{"name":"记录","slug":"记录","permalink":"https://blog.ehzyil.xyz/categories/%E8%AE%B0%E5%BD%95/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.ehzyil.xyz/categories/Linux/"},{"name":"技术","slug":"技术","permalink":"https://blog.ehzyil.xyz/categories/%E6%8A%80%E6%9C%AF/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.ehzyil.xyz/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"工具","slug":"工具","permalink":"https://blog.ehzyil.xyz/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Docker","slug":"Docker","permalink":"https://blog.ehzyil.xyz/categories/Docker/"}],"tags":[{"name":"git","slug":"git","permalink":"https://blog.ehzyil.xyz/tags/git/"},{"name":"踩坑","slug":"踩坑","permalink":"https://blog.ehzyil.xyz/tags/%E8%B8%A9%E5%9D%91/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.ehzyil.xyz/tags/Linux/"},{"name":"数据库","slug":"数据库","permalink":"https://blog.ehzyil.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Oracle","slug":"Oracle","permalink":"https://blog.ehzyil.xyz/tags/Oracle/"},{"name":"Docker","slug":"Docker","permalink":"https://blog.ehzyil.xyz/tags/Docker/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://blog.ehzyil.xyz/tags/PostgreSQL/"},{"name":"Prompt","slug":"Prompt","permalink":"https://blog.ehzyil.xyz/tags/Prompt/"},{"name":"Git","slug":"Git","permalink":"https://blog.ehzyil.xyz/tags/Git/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ehzyil.xyz/tags/MySQL/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://blog.ehzyil.xyz/tags/Mybatis/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://blog.ehzyil.xyz/tags/SpringBoot/"},{"name":"Spring","slug":"Spring","permalink":"https://blog.ehzyil.xyz/tags/Spring/"},{"name":"TypeScript","slug":"TypeScript","permalink":"https://blog.ehzyil.xyz/tags/TypeScript/"},{"name":"Vue","slug":"Vue","permalink":"https://blog.ehzyil.xyz/tags/Vue/"},{"name":"前端","slug":"前端","permalink":"https://blog.ehzyil.xyz/tags/%E5%89%8D%E7%AB%AF/"},{"name":"内网穿透","slug":"内网穿透","permalink":"https://blog.ehzyil.xyz/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"name":"Java","slug":"Java","permalink":"https://blog.ehzyil.xyz/tags/Java/"},{"name":"docker","slug":"docker","permalink":"https://blog.ehzyil.xyz/tags/docker/"},{"name":"linux","slug":"linux","permalink":"https://blog.ehzyil.xyz/tags/linux/"},{"name":"异常","slug":"异常","permalink":"https://blog.ehzyil.xyz/tags/%E5%BC%82%E5%B8%B8/"},{"name":"Java Script","slug":"Java-Script","permalink":"https://blog.ehzyil.xyz/tags/Java-Script/"},{"name":"AES","slug":"AES","permalink":"https://blog.ehzyil.xyz/tags/AES/"},{"name":"Azure","slug":"Azure","permalink":"https://blog.ehzyil.xyz/tags/Azure/"},{"name":"并发","slug":"并发","permalink":"https://blog.ehzyil.xyz/tags/%E5%B9%B6%E5%8F%91/"},{"name":"CentOS","slug":"CentOS","permalink":"https://blog.ehzyil.xyz/tags/CentOS/"},{"name":"网盘","slug":"网盘","permalink":"https://blog.ehzyil.xyz/tags/%E7%BD%91%E7%9B%98/"},{"name":"服务器","slug":"服务器","permalink":"https://blog.ehzyil.xyz/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"rss","slug":"rss","permalink":"https://blog.ehzyil.xyz/tags/rss/"},{"name":"render","slug":"render","permalink":"https://blog.ehzyil.xyz/tags/render/"},{"name":"JVM","slug":"JVM","permalink":"https://blog.ehzyil.xyz/tags/JVM/"},{"name":"Leetcode","slug":"Leetcode","permalink":"https://blog.ehzyil.xyz/tags/Leetcode/"},{"name":"软件","slug":"软件","permalink":"https://blog.ehzyil.xyz/tags/%E8%BD%AF%E4%BB%B6/"},{"name":"AOP","slug":"AOP","permalink":"https://blog.ehzyil.xyz/tags/AOP/"},{"name":"日志","slug":"日志","permalink":"https://blog.ehzyil.xyz/tags/%E6%97%A5%E5%BF%97/"},{"name":"图片上传","slug":"图片上传","permalink":"https://blog.ehzyil.xyz/tags/%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0/"},{"name":"Quartz","slug":"Quartz","permalink":"https://blog.ehzyil.xyz/tags/Quartz/"},{"name":"IntelliJ IDEA","slug":"IntelliJ-IDEA","permalink":"https://blog.ehzyil.xyz/tags/IntelliJ-IDEA/"},{"name":"JSP","slug":"JSP","permalink":"https://blog.ehzyil.xyz/tags/JSP/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.ehzyil.xyz/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"Yauaa","slug":"Yauaa","permalink":"https://blog.ehzyil.xyz/tags/Yauaa/"},{"name":"node.js","slug":"node-js","permalink":"https://blog.ehzyil.xyz/tags/node-js/"},{"name":"Spring Security","slug":"Spring-Security","permalink":"https://blog.ehzyil.xyz/tags/Spring-Security/"},{"name":"Nacos","slug":"Nacos","permalink":"https://blog.ehzyil.xyz/tags/Nacos/"},{"name":"网上冲浪指南","slug":"网上冲浪指南","permalink":"https://blog.ehzyil.xyz/tags/%E7%BD%91%E4%B8%8A%E5%86%B2%E6%B5%AA%E6%8C%87%E5%8D%97/"},{"name":"Telegram","slug":"Telegram","permalink":"https://blog.ehzyil.xyz/tags/Telegram/"}]}